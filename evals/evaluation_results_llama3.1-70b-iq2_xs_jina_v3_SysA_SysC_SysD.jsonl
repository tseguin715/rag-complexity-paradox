{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "According to [Doc 6993], Wall Street was released in 1987.", "latency_ms": 11284.336805343628, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer. It does not introduce any contradictions or extraneous incorrect information and is fully supported by the provided gold fact, so it is both correct and faithful.", "main_total_tokens": 2811, "main_prompt_tokens": 2793, "main_completion_tokens": 18, "judge_total_tokens": 475, "judge_prompt_tokens": 395, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "According to [Doc 69929], \"Brain Fluid Explosion Girl\" (2015) is a movie where classmates are forced to take part in a survival game using their cellphones.", "latency_ms": 8099.537372589111, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Brain Fluid Explosion Girl' and accurately states that it involves classmates forced into a survival game using their cell phones, matching the gold answer. It does not introduce contradictions or unsupported claims and explicitly attributes the information to the referenced document, so it is both correct and faithful.", "main_total_tokens": 2540, "main_prompt_tokens": 2502, "main_completion_tokens": 38, "judge_total_tokens": 518, "judge_prompt_tokens": 425, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "According to [Doc 112849], the TV show \"Dungeons & Dragons\" features six friends who are transported into the Dungeons & Dragons realm and must try to find a way home with the help of their guide 'Dungeon Master'.", "latency_ms": 9144.16527748108, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons' and matches the gold answer. It accurately states that six friends are transported into the Dungeons & Dragons realm and must find a way home with the help of their guide, the Dungeon Master, which is faithful to the referenced document and the known premise of the show.", "main_total_tokens": 2580, "main_prompt_tokens": 2531, "main_completion_tokens": 49, "judge_total_tokens": 537, "judge_prompt_tokens": 438, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe (1982) [Doc 2590]", "latency_ms": 6172.39236831665, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Nacht der Wölfe'. Adding the year and document reference does not alter correctness or contradict the given information, so the response is both correct and faithful.", "main_total_tokens": 2586, "main_prompt_tokens": 2569, "main_completion_tokens": 17, "judge_total_tokens": 491, "judge_prompt_tokens": 413, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900", "latency_ms": 5149.82533454895, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (1900) and is fully supported by the provided information.", "main_total_tokens": 2812, "main_prompt_tokens": 2809, "main_completion_tokens": 3, "judge_total_tokens": 440, "judge_prompt_tokens": 388, "judge_completion_tokens": 52}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June", "latency_ms": 5284.46888923645, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie matching the description and notes that the character’s name is also a calendar month. The gold answer is 'June' and the generated answer is also 'June', which is correct and fully aligned with the gold.", "main_total_tokens": 2899, "main_prompt_tokens": 2897, "main_completion_tokens": 2, "judge_total_tokens": 512, "judge_prompt_tokens": 433, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "According to the provided context, the highest rated Russian-language movie released in 1980 is \"Moscow Does Not Believe in Tears\" with a rating of 7.6 [Doc 61].", "latency_ms": 9142.298936843872, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger', but the generated answer states 'Moscow Does Not Believe in Tears', so it is factually incorrect. It also explicitly relies on 'the provided context' and a document reference that conflict with the gold label, so relative to the gold it is unfaithful as well.", "main_total_tokens": 3075, "main_prompt_tokens": 3034, "main_completion_tokens": 41, "judge_total_tokens": 540, "judge_prompt_tokens": 435, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "According to the provided context, the highest Russian-language movie rating in 1999 is 8.1 for the TV series \"Kamenskaya\" ([Doc 115256]).", "latency_ms": 8687.347888946533, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says the highest Russian-language movie rating in 1999 is 7.6, while the generated answer claims it is 8.1 for the TV series 'Kamenskaya'. This directly contradicts the gold, so it is factually incorrect in this evaluation context. It also introduces a specific title and rating not supported by the gold, so it is not faithful to the given answer.", "main_total_tokens": 2971, "main_prompt_tokens": 2932, "main_completion_tokens": 39, "judge_total_tokens": 534, "judge_prompt_tokens": 421, "judge_completion_tokens": 113}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "According to the provided context, there are two movies that meet the criteria:\n\n1. \"A Chinese Ghost Story\" (Rating: 7.3)\n2. \"Last Eunuch In China\" (Rating: 7.8)\n\nBoth of these movies were released in 1987, have a rating over 7, and are in Chinese.", "latency_ms": 11763.471364974976, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for how many movies match the criteria; the gold answer is 4, but the generated answer states there are two movies, which is numerically incorrect. It also adds specific movie titles and ratings that are not supported or referenced in the problem statement, so the response is not faithful to the given information.", "main_total_tokens": 2837, "main_prompt_tokens": 2764, "main_completion_tokens": 73, "judge_total_tokens": 558, "judge_prompt_tokens": 463, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "Based on the provided context, there is one TV show that meets the criteria:\n\n* Tian Mimi (2007) does not meet the year requirement.\n* No other Chinese TV shows with a rating over 7 were found.\n\nHowever, it's possible that some information might be missing or not provided in the context.", "latency_ms": 11357.894659042358, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer concludes there is one qualifying TV show while also listing only a non-qualifying show, ultimately not matching the gold result. This makes the answer incorrect. However, it appears to be faithful to the provided (though possibly incomplete) context, explicitly stating reasoning based on what was found in the context and acknowledging potential missing information, without inventing extra facts beyond that.", "main_total_tokens": 3093, "main_prompt_tokens": 3028, "main_completion_tokens": 65, "judge_total_tokens": 570, "judge_prompt_tokens": 455, "judge_completion_tokens": 115}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "According to the provided context, there is no information about a French language TV show released exclusively in 1980. However, \"Pé de Vento\" (Doc 112590) is a Portuguese TV series from 1980, and \"Luzie, der Schrecken der Straße\" (Doc 112657) is a German/Czech Children TV series from 1980.\n\nHowever, there are French language TV shows released in other years.", "latency_ms": 14089.164733886719, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates the French TV show is 'Fantômas', but the generated answer claims there is no information about a French language TV show released in 1980 and does not mention Fantômas at all. This is factually incorrect relative to the gold, so correctness is 0. The answer is consistent with itself and the (described) provided context, so it is faithful, leading to a faithfulness score of 1.", "main_total_tokens": 3036, "main_prompt_tokens": 2941, "main_completion_tokens": 95, "judge_total_tokens": 598, "judge_prompt_tokens": 477, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "According to [Doc 11205], the movie \"Kill Cruise\" (1990) is about two British beauties going to Barbados with a yacht captain who does not know what he's in for.", "latency_ms": 8917.833805084229, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise' and restates the description from the gold answer without adding unsupported details. It is both factually correct and faithful to the provided information.", "main_total_tokens": 2792, "main_prompt_tokens": 2749, "main_completion_tokens": 43, "judge_total_tokens": 494, "judge_prompt_tokens": 424, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 5838.906049728394, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which correctly identifies the movie title as 'Won't You Be My Neighbor?'. It is both correct and faithful.", "main_total_tokens": 2961, "main_prompt_tokens": 2953, "main_completion_tokens": 8, "judge_total_tokens": 471, "judge_prompt_tokens": 408, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "1988", "latency_ms": 5435.33182144165, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer (1988) is incorrect; the correct year is 2018. It directly contradicts the provided gold answer, so it is neither correct nor faithful to the reference.", "main_total_tokens": 2981, "main_prompt_tokens": 2978, "main_completion_tokens": 3, "judge_total_tokens": 449, "judge_prompt_tokens": 380, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy (1980) [Doc 112591]", "latency_ms": 6098.596572875977, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies the series as 'Astro Boy', matching the gold answer. However, it adds '(1980) [Doc 112591]', which is extra, somewhat speculative metadata not present in the question or gold answer, slightly reducing faithfulness.", "main_total_tokens": 2744, "main_prompt_tokens": 2730, "main_completion_tokens": 14, "judge_total_tokens": 485, "judge_prompt_tokens": 399, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "According to [Doc 67513], the woman and her young son were held captive for 7 years.", "latency_ms": 6712.54825592041, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says they were held captive for 7 years, and the generated answer correctly repeats that they were held captive for 7 years. It matches the gold fact and does not introduce contradictions or unsupported claims.", "main_total_tokens": 2544, "main_prompt_tokens": 2521, "main_completion_tokens": 23, "judge_total_tokens": 490, "judge_prompt_tokens": 416, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m", "latency_ms": 5364.121437072754, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The Brink's-Mat robbery involved approximately £26 million worth of gold bullion (plus cash and diamonds) at the time. The generated answer of £26m matches the given gold amount and is faithful to the prompt.", "main_total_tokens": 2857, "main_prompt_tokens": 2853, "main_completion_tokens": 4, "judge_total_tokens": 473, "judge_prompt_tokens": 398, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "According to the provided context, the most popular English-speaking TV show released in 1997 is \"Recess\" with a popularity score of 10.3614 [Doc 114720].", "latency_ms": 8890.120506286621, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer popularity score is 80.5772, while the generated answer gives 10.3614, which is numerically incorrect. It also introduces a specific show (\"Recess\") not supported by the gold, so the reasoning is not faithful to the reference answer.", "main_total_tokens": 3059, "main_prompt_tokens": 3019, "main_completion_tokens": 40, "judge_total_tokens": 519, "judge_prompt_tokens": 431, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "There is no information about Russian language TV shows released in 1997 in the provided context. However, there is a mention of a Russian comedy-drama movie titled \"A House ‎for Rent with All the Inconveniences\" released in 2016, not in 1997.", "latency_ms": 10975.614309310913, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that there was 1 Russian language TV show released in 1997, but the generated answer claims there is no information and does not provide the correct number. This makes the answer factually incorrect with respect to the gold. However, the response is faithful to the (stated) provided context, explicitly tying its claim to the lack of information in that context and not hallucinating a number, so it is faithful to the described context.", "main_total_tokens": 3163, "main_prompt_tokens": 3103, "main_completion_tokens": 60, "judge_total_tokens": 565, "judge_prompt_tokens": 442, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes (2014) [Doc 121928]", "latency_ms": 6727.648496627808, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the TV show in which Soo-wan undergoes eye transplant surgery that restores her sight. The gold answer is 'Angel Eyes'. The generated answer 'Angel Eyes (2014) [Doc 121928]' correctly identifies the same TV show, only adding the year and a document reference. This is fully correct and faithful to the gold answer.", "main_total_tokens": 3280, "main_prompt_tokens": 3267, "main_completion_tokens": 13, "judge_total_tokens": 511, "judge_prompt_tokens": 407, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "es", "latency_ms": 5359.385967254639, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'es' (Spanish) does not match the correct language 'Korean', so it is factually incorrect. It is also unfaithful to the provided gold answer, which clearly states 'Korean'.", "main_total_tokens": 2980, "main_prompt_tokens": 2978, "main_completion_tokens": 2, "judge_total_tokens": 454, "judge_prompt_tokens": 377, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "Based on the provided context, there are no English-speaking Horror movies released in 2009 with a popularity score greater than 7. However, I can provide you with a list of English-speaking Horror movies released in 2009 with their corresponding popularity scores:\n\n* The Dunwich Horror (2009) - 0.5234\n* Salvage (2009) - 1.3643\n* Slaughter (2009) - 1.2321\n* Cold Storage (2009) - 1.0688\n* Neighbor (2009) - 1.0794\n\nNote that the popularity scores are based on the provided context and may not reflect the actual popularity of these movies.\n\nIf you'd like, I can try to answer any other questions you have based on the provided context.", "latency_ms": 21121.33288383484, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there are 2 such movies, but the generated answer claims there are none. This directly contradicts the correct result, so the correctness is 0. The answer, however, appears internally consistent with the (unspecified) provided context it references and does not hallucinate beyond that context, so faithfulness is rated as 1.", "main_total_tokens": 3171, "main_prompt_tokens": 3004, "main_completion_tokens": 167, "judge_total_tokens": 659, "judge_prompt_tokens": 557, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Not found [Doc 45703], [Doc 8389], [Doc 23434], [Doc 250], [Doc 46637], [Doc 471], [Doc 9490], [Doc 148], [Doc 23611], [Doc 838], [Doc 278], [Doc 62218], [Doc 1335], [Doc 3142], [Doc 1079], [Doc 903], [Doc 23090], [Doc 56991], and [Doc 1844]", "latency_ms": 15868.627786636353, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The gold answer states clearly that there are no movies from 1979 in the database. The generated answer also effectively conveys that the system did not find a relevant result (listing many docs but still saying 'Not found'), so it is correct with respect to the question. However, it is not faithful to the gold response’s wording or content structure: the gold explicitly mentions the limitation of the database for year 1979, while the generated answer instead lists a series of document IDs and just says 'Not found' without clarifying the 1979 constraint. Thus correctness is 1 but faithfulness to the gold is 0.", "main_total_tokens": 3032, "main_prompt_tokens": 2920, "main_completion_tokens": 112, "judge_total_tokens": 664, "judge_prompt_tokens": 506, "judge_completion_tokens": 158}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "Not found [Doc 10621], [Doc 4834], [Doc 113788], [Doc 9405], [Doc 68724], [Doc 3944], [Doc 50182], [Doc 28792], [Doc 103759], [Doc 103759], [Doc 103759], [Doc 68688], [Doc 108860], [Doc 116574], [Doc 45780], [Doc 21498], and [Doc 11408]", "latency_ms": 16368.327856063843, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct response is that 'Back to the Future Part IV' does not exist; therefore, no release year can be given. The generated answer instead says 'Not found' followed by a list of document IDs, which does not clearly convey that the movie does not exist and is not an appropriate or correct answer to the question, so correctness is 0. In terms of faithfulness to the gold, the gold explicitly states nonexistence, while the generated answer only implies absence in some index and adds extraneous document references; it partially aligns in that it does not fabricate a year, but it does not clearly communicate the same information, so faithfulness is partial.", "main_total_tokens": 3863, "main_prompt_tokens": 3757, "main_completion_tokens": 106, "judge_total_tokens": 672, "judge_prompt_tokens": 505, "judge_completion_tokens": 167}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "There is no information provided about movies from 1980 about the Chernobyl disaster. However, there are several movies listed that were made after the Chernobyl disaster in 1986.\n\nThe least popular movie among those listed is \"Chornobyl: Chronicle of Difficult Weeks\" with a popularity score of 0.1058.", "latency_ms": 11779.271364212036, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that no movies from 1980 related to the Chernobyl disaster were found, which makes sense historically since the disaster occurred in 1986. The generated answer not only contradicts this by providing a specific film and popularity score, but also references a movie made after 1986 and still claims to answer a 1980-specific question. This is factually incorrect for the query and not supported by the provided gold information, so both correctness and faithfulness are 0.", "main_total_tokens": 3018, "main_prompt_tokens": 2947, "main_completion_tokens": 71, "judge_total_tokens": 600, "judge_prompt_tokens": 474, "judge_completion_tokens": 126}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are three documents with \"Astro Boy\" in the title:\n\n* [Doc 112591] Title: Astro Boy (TV, 1980)\n* [Doc 44947] Title: Astro Boy (Movie, 2009)\n* [Doc 116265] Title: Astro Boy (TV, 2003)", "latency_ms": 11474.51114654541, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.8318724637288826, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.8318724637288826, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 4 titles with 'Astro Boy' while the generated answer lists only 3, so the numerical answer is incorrect. However, the generated response is consistent with the documents it cites and does not contradict its own evidence, so it is faithful to the given information.", "main_total_tokens": 2917, "main_prompt_tokens": 2848, "main_completion_tokens": 69, "judge_total_tokens": 545, "judge_prompt_tokens": 454, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "There is no information about movies released in 2025 in the provided context. The context only provides information about movies up to 2024, and there are several movies that allude to the Holocaust among them.", "latency_ms": 9100.978136062622, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is the specific number 2. The generated answer instead claims there is no information about 2025 movies in the context and does not provide the required number, so it is factually incorrect relative to the gold. It also contradicts the premise that an exact count (2) is known from the context, so it is unfaithful to the given supervision.", "main_total_tokens": 2886, "main_prompt_tokens": 2842, "main_completion_tokens": 44, "judge_total_tokens": 534, "judge_prompt_tokens": 427, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "According to the provided context, there are two TV shows related to Super Mario Bros that were released in the 1990s:\n\n1. The Adventures of Super Mario Bros. 3 (1990) [Doc 113661]\n2. Super Mario World (1991) [Doc 113814]\n\nNote that these are the only two TV shows mentioned in the context that meet the specified criteria.", "latency_ms": 12791.184425354004, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there was 1 TV show, but the generated answer claims there were two, listing specific series from 1990 and 1991. This directly contradicts the gold, so the numerical answer is incorrect. It also asserts these are the only two shows meeting the criteria, which conflicts with the gold label and thus is not faithful to the expected answer.", "main_total_tokens": 2934, "main_prompt_tokens": 2852, "main_completion_tokens": 82, "judge_total_tokens": 569, "judge_prompt_tokens": 468, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "Based on the provided context, there is no information about any TV shows specifically about Super Mario Bros. being released between 2000 and 2010. However, it does mention a few TV shows related to Super Mario Bros., but they were released outside of this time frame:\n\n* The Adventures of Super Mario Bros. 3 (1990)\n* Super Mario World (1991)\n* The Super Mario Bros. Super Show! (1989)\n\nThere is no information about any TV shows specifically about Super Mario Bros. being released between 2000 and 2010.", "latency_ms": 15668.980121612549, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold answer is 0. The generated answer correctly concludes that there were no Super Mario Bros. TV shows released between 2000 and 2010, matching the gold. It references only the provided context shows and notes they are outside the range, without adding unsupported shows, so it is both correct and faithful.", "main_total_tokens": 2956, "main_prompt_tokens": 2839, "main_completion_tokens": 117, "judge_total_tokens": 599, "judge_prompt_tokens": 505, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "According to the provided context, the most popular TV show or movie about ninjas is \"UNDER NINJA\" with a popularity rating of 3.1982 [Doc 133621].", "latency_ms": 9496.68574333191, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is faithful to the provided context, correctly citing 'UNDER NINJA' and the popularity rating from Doc 133621. However, it is factually incorrect relative to the gold answer, which identifies 'Naruto' as the most popular ninja show; 'UNDER NINJA' is not the correct response here, so correctness is 0.", "main_total_tokens": 3186, "main_prompt_tokens": 3146, "main_completion_tokens": 40, "judge_total_tokens": 520, "judge_prompt_tokens": 421, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "There is no information about a Jurassic Park/Power Rangers crossover movie in the provided context.", "latency_ms": 6881.378173828125, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer clarifies that the movie does not exist and thus was never released. The generated answer instead says there is no information in the provided context, which does not correctly answer the question about release and is not faithful to the gold explanation that the movie simply was not released.", "main_total_tokens": 2973, "main_prompt_tokens": 2954, "main_completion_tokens": 19, "judge_total_tokens": 490, "judge_prompt_tokens": 403, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Not found [Doc 104474], [Doc 52809], [Doc 68094], [Doc 94912], [Doc 29287], [Doc 31359], [Doc 101349], [Doc 38983], [Doc 64947], [Doc 72290], [Doc 27405], [Doc 36116], [Doc 78864], [Doc 48119], [Doc 94912], [Doc 65585], [Doc 63545], and [Doc 11245].", "latency_ms": 16158.459901809692, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "Gold answer explains that no such movie was released. The generated answer only says 'Not found' and lists documents, which does not clearly convey that the correct answer is that the movie does not exist, so it is incorrect. It is somewhat faithful in that it reflects a failure to find such a movie but does not explicitly align with the gold statement.", "main_total_tokens": 3344, "main_prompt_tokens": 3232, "main_completion_tokens": 112, "judge_total_tokens": 596, "judge_prompt_tokens": 493, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "According to the provided documents, here are the Star Wars movies in chronological order by release year:\n\n1. Return of the Jedi (1983)\n2. The Clone Wars (2008)\n3. When Star Wars Ruled the World (2004) - Note: This is a documentary and not part of the main storyline.\n4. Empire of Dreams: The Story of the Star Wars Trilogy (2004)\n\nHowever, considering only the movies that are part of the main storyline, the 4th oldest would be:\n\n1. Return of the Jedi (1983)\n2. The Clone Wars (2008) - Note: This is an animated movie and a TV series.\n3. Star Wars: Episode III Revenge of the Sith (Not listed in the provided documents but known from outside knowledge to be released in 2005)\n4. Star Wars: Episode II Attack of the Clones (Not listed in the provided documents but known from outside knowledge to be released in 2002)\n\nSo, based on the information available and considering only the movies that are part of the main storyline, it is not possible to determine the 4th oldest Star Wars movie with certainty.", "latency_ms": 28440.725088119507, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'Star Wars: Droids - The Great Heep' as the 4th oldest Star Wars movie, while the generated answer does not mention this title at all and instead gives an incorrect and confused discussion of various films and documentaries, ultimately claiming it is not possible to determine the 4th oldest. This is factually incorrect relative to the gold. Additionally, the generated answer introduces outside knowledge (release years and films not in the provided documents) and reasons beyond the given gold answer, so it is not faithful to the gold reference.", "main_total_tokens": 3455, "main_prompt_tokens": 3217, "main_completion_tokens": 238, "judge_total_tokens": 769, "judge_prompt_tokens": 626, "judge_completion_tokens": 143}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "1988", "latency_ms": 5400.170087814331, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1988) does not match the gold answer (1986), so it is factually incorrect. However, the evaluation here is straightforward and fully based on the provided gold and generated outputs, so faithfulness is intact.", "main_total_tokens": 2981, "main_prompt_tokens": 2978, "main_completion_tokens": 3, "judge_total_tokens": 459, "judge_prompt_tokens": 380, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "According to [Doc 15236], the rating of the movie \"Forrest Gump\" is 8.466.", "latency_ms": 7326.65753364563, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer gives the exact rating 8.466, which matches the gold answer. It correctly attributes this to a document and does not introduce any unsupported claims, so it is both correct and faithful to the provided information.", "main_total_tokens": 2857, "main_prompt_tokens": 2831, "main_completion_tokens": 26, "judge_total_tokens": 495, "judge_prompt_tokens": 419, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med", "latency_ms": 4861.724376678467, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Mighty Med', which is the correct TV show name. It is fully correct and faithful.", "main_total_tokens": 2390, "main_prompt_tokens": 2385, "main_completion_tokens": 5, "judge_total_tokens": 453, "judge_prompt_tokens": 394, "judge_completion_tokens": 59}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina", "latency_ms": 5080.806732177734, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Argentina'), so it is both correct and faithful to the reference.", "main_total_tokens": 2766, "main_prompt_tokens": 2764, "main_completion_tokens": 2, "judge_total_tokens": 445, "judge_prompt_tokens": 392, "judge_completion_tokens": 53}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "According to [Doc 135231], Apprentice Li Nai falls into a trap designed by Jiang Jie, also known as the \"Chef Assassin\".", "latency_ms": 8009.626865386963, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is simply 'Jiang Jie'. The generated answer adds unsupported details: the apprentice is named 'Li Nai', and Jiang Jie is called the 'Chef Assassin'. These specifics are not in the gold answer and are likely hallucinated, so the response is not fully correct or faithful to the provided information.", "main_total_tokens": 2956, "main_prompt_tokens": 2925, "main_completion_tokens": 31, "judge_total_tokens": 506, "judge_prompt_tokens": 412, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "Based on the provided context, there are three Korean-language movies that mention assassins and were released after 2023:\n\n1. [Doc 48444] Title: The Yellow Sea (Year: 2010) - This movie is not after 2023.\n2. [Doc 67739] Title: Assassination (Year: 2015) - This movie is not after 2023.\n3. No other Korean-language movies that mention assassins and were released after 2023 are found in the provided context.\n\nHowever, there are two Korean-language movies that mention assassins but do not meet the \"after 2023\" criteria:\n\n* [Doc 59669] Title: Four Assassins (Year: 2013)\n* [Doc 48744] Title: Secret Reunion (Year: 2010)\n\nAnd one more movie that mentions assassin and was released after 2023 but is not in Korean language:\n* [Doc 104585] Title: The Killer (Year: 2023)", "latency_ms": 25153.194189071655, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 4, but the generated answer effectively concludes there are three relevant movies and does not state the correct count, so correctness is 0. It also introduces specific document IDs, titles, and years (e.g., The Yellow Sea 2010, Assassination 2015, Four Assassins 2013, Secret Reunion 2010, The Killer 2023) that are not verifiable from the given prompt and conflict with the gold label, so faithfulness is 0.", "main_total_tokens": 2985, "main_prompt_tokens": 2774, "main_completion_tokens": 211, "judge_total_tokens": 729, "judge_prompt_tokens": 594, "judge_completion_tokens": 135}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "According to the provided context, there is no information about a person named \"Steven Seagall\". However, there is a TV series titled \"Steven Seagal: Lawman\" (Doc 119040) which stars actor and martial artist Steven Seagal.\n\nAs for Bruce Lee, there are multiple movies mentioned in the context:\n\n1. [Doc 79200] Title: Bruce Lee\n2. [Doc 85241] Title: Bruce Lee and the Outlaw\n3. [Doc 70236] Title: Bruce Lee - The Fighter\n4. [Doc 56669] Title: I Am Bruce Lee\n5. [Doc 119040] does not feature Bruce Lee, but rather Steven Seagal.\n6. [Doc 107081] is about Stan Lee, not Bruce Lee.\n7. [Doc 46659] Title: The Legend of Bruce Lee\n8. [Doc 118291] Title: The Legend of Bruce Lee (TV series)\n9. [Doc 49705] Title: Bruce Lee, My Brother\n10. [Doc 16203] Title: Bruce Lee: The Immortal Dragon\n11. [Doc 4204] Title: Bruce Lee: The Legend\n12. [Doc 16319", "latency_ms": 30140.971899032593, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question asks specifically how many movies feature both Steven Seagal and Bruce Lee together. The gold answer correctly states that none can be found. The generated answer instead discusses the absence of a person named \"Steven Seagall\" in the context and then lists many Bruce Lee-related titles, but never answers the core question of how many joint movies there are. This makes it incorrect. In terms of faithfulness, the answer appears to be based on some provided document IDs and titles, but it introduces an inference about there being no information on Steven Seagall and then digresses into enumerating Bruce Lee films without tying this back to the joint film count. It also implicitly suggests relevance of those titles to the question, which is misleading. Thus, it is only partially faithful to the intended task and context.", "main_total_tokens": 3418, "main_prompt_tokens": 3162, "main_completion_tokens": 256, "judge_total_tokens": 843, "judge_prompt_tokens": 649, "judge_completion_tokens": 194}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3 [Doc 42750]", "latency_ms": 6174.284219741821, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (6.3) and does not add any contradictory or extraneous information, so it is both correct and faithful.", "main_total_tokens": 3014, "main_prompt_tokens": 3004, "main_completion_tokens": 10, "judge_total_tokens": 456, "judge_prompt_tokens": 393, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The context doesn't explicitly state the style of racing in the movie \"Fast Track: No Limits\". However, based on the plot description of other movies with similar themes, it can be inferred that the style of racing might be street racing or drag racing.", "latency_ms": 10250.352621078491, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.3, "justification": "The gold answer specifies 'Drift' as the racing style. The generated answer speculates about street or drag racing and claims the context does not state the style, which directly contradicts the gold. Therefore, it is factually incorrect. It is somewhat faithful in that it discusses types of racing and acknowledges uncertainty, but it invents an inference about street or drag racing that is unsupported, reducing faithfulness.", "main_total_tokens": 3225, "main_prompt_tokens": 3173, "main_completion_tokens": 52, "judge_total_tokens": 543, "judge_prompt_tokens": 428, "judge_completion_tokens": 115}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer", "latency_ms": 5626.243114471436, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and is correct and faithful to the provided information.", "main_total_tokens": 2897, "main_prompt_tokens": 2890, "main_completion_tokens": 7, "judge_total_tokens": 461, "judge_prompt_tokens": 412, "judge_completion_tokens": 49}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "According to [Doc 20603], the popularity score of Armageddon (1998) is 7.838.", "latency_ms": 7525.007247924805, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold value (7.838) and correctly attributes it to the cited document (Doc 20603). There is no hallucinated information; it is fully consistent with the provided evidence.", "main_total_tokens": 3017, "main_prompt_tokens": 2991, "main_completion_tokens": 26, "judge_total_tokens": 484, "judge_prompt_tokens": 410, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "According to [Doc 22488], the title of the 1999 psychological thriller movie that alludes to an extra sense is \"Victim\".", "latency_ms": 7766.3414478302, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct title of the 1999 psychological thriller that alludes to an extra sense is 'The Sixth Sense', not 'Victim', so the answer is factually incorrect. It also contradicts the given gold answer and appears to rely on an incorrect document reference, so it is not faithful to the provided information.", "main_total_tokens": 2786, "main_prompt_tokens": 2755, "main_completion_tokens": 31, "judge_total_tokens": 516, "judge_prompt_tokens": 421, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002 [Doc 27318]", "latency_ms": 6099.337339401245, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (2002) and does not introduce any unsupported information. It is both correct and faithful.", "main_total_tokens": 3100, "main_prompt_tokens": 3091, "main_completion_tokens": 9, "judge_total_tokens": 447, "judge_prompt_tokens": 389, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "[Doc 67500] The Transporter Refueled", "latency_ms": 6262.746334075928, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly giving the title 'The Transporter Refueled' and does not introduce any extraneous or contradictory information.", "main_total_tokens": 2909, "main_prompt_tokens": 2896, "main_completion_tokens": 13, "judge_total_tokens": 466, "judge_prompt_tokens": 403, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "[Doc 67500] states that the movie \"The Transporter Refueled\" was released in 2015.", "latency_ms": 7647.875785827637, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, which matches the gold answer. It also accurately attributes this information to the referenced document, so it is both correct and faithful.", "main_total_tokens": 3217, "main_prompt_tokens": 3191, "main_completion_tokens": 26, "judge_total_tokens": 486, "judge_prompt_tokens": 409, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "According to [Doc 24016], \"Waking the Dead\" is not about Abraham Lincoln coming back from the dead, but rather a congressional candidate who sees his love, presumed dead.\n\nHowever, I found that in [Doc 54943] \"Abraham Lincoln: Vampire Hunter\", President Lincoln's mother is killed by a supernatural creature, which fuels his passion to crush vampires and their slave-owning helpers. But this is not a romantic comedy.\n\nThere is no mention of Abraham Lincoln coming back from the dead in a romantic comedy in the provided context.", "latency_ms": 16728.132963180542, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly concludes that there is no romantic comedy in the given context where Abraham Lincoln comes back from the dead, matching the gold response. It accurately summarizes the cited documents and clearly states that neither involves Lincoln returning from the dead or being in a romantic comedy. The reasoning remains grounded in and consistent with the provided documents, without introducing unsupported claims.", "main_total_tokens": 2726, "main_prompt_tokens": 2612, "main_completion_tokens": 114, "judge_total_tokens": 603, "judge_prompt_tokens": 502, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Havana Darkness", "latency_ms": 5111.712455749512, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Havana Darkness' and correctly identifies the movie title alluding to a Hemingway manuscript. It is fully correct and faithful.", "main_total_tokens": 2698, "main_prompt_tokens": 2694, "main_completion_tokens": 4, "judge_total_tokens": 454, "judge_prompt_tokens": 390, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 5566.8933391571045, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Doll & Em'. It is both correct and faithful to the reference.", "main_total_tokens": 3040, "main_prompt_tokens": 3035, "main_completion_tokens": 5, "judge_total_tokens": 454, "judge_prompt_tokens": 392, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "According to [Doc 39389], Hallam's talent for spying on people reveals his darkest fears-and his most peculiar desires, as he is driven to expose the true cause of his mother's death. This is from the 2007 movie \"Hallam Foe\".", "latency_ms": 10570.776224136353, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2007 movie as \"Hallam Foe,\" matching the gold answer exactly. It does not add any contradictory or unsupported information about the movie beyond a paraphrased description, so it remains faithful to the referenced document and the question.", "main_total_tokens": 3150, "main_prompt_tokens": 3094, "main_completion_tokens": 56, "judge_total_tokens": 538, "judge_prompt_tokens": 453, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "According to [Doc 28027], the character's last name is \"Harris\", which sounds more like a friendly surname, but also has a bit of an edge to it. However, in this context, it seems that Toby Harris claims to be Daniella's legitimate father, so his intentions are unclear.\n\nIn another document, [Doc 8179], the character's last name is not mentioned, but the title \"Friend\" suggests that the person might be a friendly character.\n\nIt's worth noting that these are just interpretations based on the provided context, and more information would be needed to make a definitive judgment.", "latency_ms": 17392.980813980103, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a binary judgment (friend or foe) about how the last name sounds. The gold answer is 'A foe', but the generated response does not clearly choose that; it leans toward 'friendly' and then equivocates, so it is incorrect. It also introduces extraneous references to documents and specific surnames (e.g., 'Harris') that are not present in the gold and are speculative, rather than directly answering the question with the expected label. Thus it is neither correct nor faithful to the target answer.", "main_total_tokens": 3212, "main_prompt_tokens": 3086, "main_completion_tokens": 126, "judge_total_tokens": 649, "judge_prompt_tokens": 510, "judge_completion_tokens": 139}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "According to [Doc 124583], Sam, the main character in the 2017 TV show \"Atypical\", is an 18-year-old on the autism spectrum.", "latency_ms": 8119.881391525269, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old, which matches the gold answer. It does not introduce contradictions or unsupported claims beyond citing a generic document reference, so it is faithful to the provided information.", "main_total_tokens": 2719, "main_prompt_tokens": 2683, "main_completion_tokens": 36, "judge_total_tokens": 497, "judge_prompt_tokens": 423, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "According to [Doc 124583], Sam is on the autism spectrum.", "latency_ms": 6094.928503036499, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the TV show 'Atypical', Sam is on the autism spectrum, matching the gold answer exactly. It stays faithful to the question and does not introduce contradictions or irrelevant details.", "main_total_tokens": 2622, "main_prompt_tokens": 2606, "main_completion_tokens": 16, "judge_total_tokens": 476, "judge_prompt_tokens": 402, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix", "latency_ms": 5576.479911804199, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Chamonix'), so it is both correct and fully faithful to the given reference.", "main_total_tokens": 3068, "main_prompt_tokens": 3064, "main_completion_tokens": 4, "judge_total_tokens": 457, "judge_prompt_tokens": 400, "judge_completion_tokens": 57}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "According to [Doc 100481], the answer is \"rains\". The sentence should read: \"Hazuki clips her nails every time it rains.\"", "latency_ms": 8654.096364974976, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the missing word as 'rains' and reconstructs the sentence accurately, so correctness is 1. It is mostly faithful to the task, but it introduces an unsupported citation to 'Doc 100481', which is not in the user prompt, so faithfulness is slightly reduced.", "main_total_tokens": 3412, "main_prompt_tokens": 3380, "main_completion_tokens": 32, "judge_total_tokens": 517, "judge_prompt_tokens": 422, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland", "latency_ms": 5663.541793823242, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Greenland'), so it is both correct and fully faithful to the provided reference.", "main_total_tokens": 3220, "main_prompt_tokens": 3217, "main_completion_tokens": 3, "judge_total_tokens": 451, "judge_prompt_tokens": 395, "judge_completion_tokens": 56}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "According to [Doc 96838], the movie \"One Afternoon\" (2021) matches this description.", "latency_ms": 6754.49538230896, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.7, "justification": "The generated answer identifies the same movie as the gold label, 'One Afternoon', so it is correct. However, it introduces an unsupported citation to a specific document ('Doc 96838') that is not given in the prompt, which makes the explanation partially unfaithful to the provided context.", "main_total_tokens": 2502, "main_prompt_tokens": 2478, "main_completion_tokens": 24, "judge_total_tokens": 518, "judge_prompt_tokens": 426, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "According to [Doc 93260], the answer is \"The Life Ahead\".", "latency_ms": 9971.402168273926, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, 'The Life Ahead', and does not introduce any unsupported information. It is both correct and faithful.", "main_total_tokens": 2938, "main_prompt_tokens": 2921, "main_completion_tokens": 17, "judge_total_tokens": 474, "judge_prompt_tokens": 414, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "According to [Doc 73859], the 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team.", "latency_ms": 8266.193151473999, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer is factually correct: the 2016 documentary 'McLaren' is indeed about Bruce McLaren. However, it cites a specific document ID ([Doc 73859]) that is not present in the provided context, so the reference to that source is not grounded in the given information, reducing faithfulness to the context.", "main_total_tokens": 2820, "main_prompt_tokens": 2784, "main_completion_tokens": 36, "judge_total_tokens": 519, "judge_prompt_tokens": 420, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "Not found in [Doc 42583].", "latency_ms": 5923.49910736084, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the name of his racing team, and the gold answer is 'The McLaren Motor Racing team'. The generated answer says the information is not found, which is factually incorrect given the gold, so correctness is 0. However, it appears the model is honestly reporting that it did not find the answer in the referenced document, so with respect to the provided context it is faithful, giving a faithfulness score of 1.", "main_total_tokens": 2950, "main_prompt_tokens": 2940, "main_completion_tokens": 10, "judge_total_tokens": 513, "judge_prompt_tokens": 392, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The text does not provide information about the country of origin for \"he\".", "latency_ms": 6101.3243198394775, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that he is from New Zealand, but the generated answer claims the text does not provide this information. This is incorrect relative to the gold, so correctness is 0. However, the generated answer is internally consistent with its stated reasoning and does not fabricate details beyond its claim, so faithfulness to its own reasoning is high.", "main_total_tokens": 2560, "main_prompt_tokens": 2544, "main_completion_tokens": 16, "judge_total_tokens": 491, "judge_prompt_tokens": 391, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "According to the provided context, there is no information that suggests John McClane blows up the Great Wall of China in any movie, including those released in 2013.\n\nHowever, it's possible you're confusing this with a different movie or scene. The Great Wall is featured in the 2016 movie \"The Great Wall\" (Doc 72056), but there's no mention of John McClane blowing it up.\n\nJohn McClane is a character from the Die Hard series, and one of the movies released in 2013 is indeed \"A Good Day to Die Hard\" (Doc 58791). But according to its plot description, there's no scene involving the Great Wall of China being blown up.", "latency_ms": 19143.7726020813, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly indicates that there is no 2013 movie in which John McClane blows up the Great Wall of China, matching the gold answer in substance, so correctness is 1. However, the instructions say to answer based only on the provided gold/context, which is simply that there is no such movie in the database. The model adds extra speculative information about possible confusion with other movies and references specific documents and plots not mentioned in the gold answer. This goes beyond the given information, so faithfulness is 0.", "main_total_tokens": 3257, "main_prompt_tokens": 3112, "main_completion_tokens": 145, "judge_total_tokens": 679, "judge_prompt_tokens": 541, "judge_completion_tokens": 138}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "According to [Doc 28589], the answer is \"Tomie: Forbidden Fruit\".", "latency_ms": 7056.882381439209, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2002 Japanese horror film as \"Tomie: Forbidden Fruit\" and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 3088, "main_prompt_tokens": 3069, "main_completion_tokens": 19, "judge_total_tokens": 502, "judge_prompt_tokens": 435, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 years", "latency_ms": 6025.511264801025, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer numerically and semantically, correctly stating that they have been married for 65 years.", "main_total_tokens": 3461, "main_prompt_tokens": 3458, "main_completion_tokens": 3, "judge_total_tokens": 453, "judge_prompt_tokens": 397, "judge_completion_tokens": 56}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King teams up with a female warrior named Tala, who is the sister of The Nubian King. Together they search for a legendary relic known as The Book of Souls, which will allow them to put an end to an evil warlord. [Doc 82582]", "latency_ms": 11465.072631835938, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks what the Scorpion King and Tala search for in 'Scorpion King: Book of Souls'. The gold answer is 'The Book of Souls'. The generated answer states they search for a legendary relic known as The Book of Souls, which is correct and directly matches the gold. It is faithful to the cited document and does not introduce contradictions, only adds compatible plot context.", "main_total_tokens": 3516, "main_prompt_tokens": 3456, "main_completion_tokens": 60, "judge_total_tokens": 559, "judge_prompt_tokens": 451, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no movie titled \"Zombies on the Moon\".", "latency_ms": 6617.706298828125, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the movie is not in the database, while the generated answer asserts that no such movie exists at all. This is stronger and potentially incorrect, as such a movie could exist outside the database. It also contradicts the gold answer, so it is not faithful.", "main_total_tokens": 3190, "main_prompt_tokens": 3177, "main_completion_tokens": 13, "judge_total_tokens": 491, "judge_prompt_tokens": 403, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Not found.", "latency_ms": 5374.48525428772, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database. The generated answer, 'Not found.', conveys the same information. It is correct relative to the gold and faithful to the implied knowledge that the movie is not present.", "main_total_tokens": 2932, "main_prompt_tokens": 2928, "main_completion_tokens": 4, "judge_total_tokens": 469, "judge_prompt_tokens": 392, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Not found [Doc 70593], [Doc 48709], [Doc 132506], [Doc 11800], [Doc 30896], [Doc 99959], [Doc 133164], [Doc 27475], [Doc 58699], [Doc 65150], [Doc 66972], [Doc 29451], [Doc 296], [Doc 27475], [Doc 697], [Doc 8853], [Doc 13807], [Doc 81169], [Doc 89836], [Doc 102842], and [Doc 108750]", "latency_ms": 17333.33110809326, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "Both answers correctly convey that the movie/release year is not available. The gold answer is a natural-language apology; the generated answer states 'Not found' and lists internal doc IDs. That is still effectively the same information (item not in database / not found), so correctness is 1. Faithfulness is slightly reduced because the extra document ID list is irrelevant noise compared to the concise gold response, but it does not contradict or change the core meaning.", "main_total_tokens": 2762, "main_prompt_tokens": 2634, "main_completion_tokens": 128, "judge_total_tokens": 642, "judge_prompt_tokens": 518, "judge_completion_tokens": 124}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Not found.", "latency_ms": 5782.839775085449, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers indicate that the movie is not found / not in the database, which is an appropriate and correct response given the lack of information. The generated answer is faithful to the gold answer’s intent and content, despite being more concise.", "main_total_tokens": 3154, "main_prompt_tokens": 3150, "main_completion_tokens": 4, "judge_total_tokens": 476, "judge_prompt_tokens": 394, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Not found", "latency_ms": 5383.145332336426, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, and the generated answer 'Not found' conveys the same information. It is correct relative to the gold and faithful to the implied lack of data.", "main_total_tokens": 2977, "main_prompt_tokens": 2974, "main_completion_tokens": 3, "judge_total_tokens": 466, "judge_prompt_tokens": 393, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Not found [Doc 59782], [Doc 66739], [Doc 66884], [Doc 37366], [Doc 125026], [Doc 79072], [Doc 38039], [Doc 14694], [Doc 132406], [Doc 70493], [Doc 32014], [Doc 46964], [Doc 56666], [Doc 103583], [Doc 9702], [Doc 44556], [Doc 95232], and [Doc 80750]", "latency_ms": 15393.611669540405, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "Both answers correctly convey that the movie is not found, so correctness is 1. However, the generated answer adds specific document IDs and implies a retrieval process and evidence that are not present in the gold response, going beyond the gold explanation. This extra, ungrounded detail makes it less faithful to the gold, so faithfulness is 0.", "main_total_tokens": 2750, "main_prompt_tokens": 2638, "main_completion_tokens": 112, "judge_total_tokens": 601, "judge_prompt_tokens": 500, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no movie titled \"The Man Who Ate the Sun\". However, there is a movie titled \"A Son of the Sun\" which was released in 2008 [Doc 42376].", "latency_ms": 9717.878341674805, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer correctly states that the movie is not in the database and does not fabricate information. The generated answer incorrectly asserts that there is no movie with that title and then introduces an unrelated movie and year, which is not supported by the gold. It both fails to answer correctly and is not faithful to the given reference response.", "main_total_tokens": 3494, "main_prompt_tokens": 3452, "main_completion_tokens": 42, "judge_total_tokens": 528, "judge_prompt_tokens": 432, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Not found in provided context.", "latency_ms": 6190.986633300781, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided context/database, which is correct and faithful to the given limitation.", "main_total_tokens": 3338, "main_prompt_tokens": 3331, "main_completion_tokens": 7, "judge_total_tokens": 464, "judge_prompt_tokens": 403, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Not found in provided context.", "latency_ms": 5737.990379333496, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not provided in the database. The generated answer says the information is not found in the provided context. Both convey the same limitation based on available data. This is correct and faithful to the gold.", "main_total_tokens": 2981, "main_prompt_tokens": 2974, "main_completion_tokens": 7, "judge_total_tokens": 480, "judge_prompt_tokens": 401, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context does not provide information about who directed the movie \"Journey into Gaza\".", "latency_ms": 6782.834053039551, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not provide information about the director of 'Journey into Gaza', which matches the gold answer that the database lacks this information. It is both factually correct given the context and fully faithful to it.", "main_total_tokens": 2950, "main_prompt_tokens": 2932, "main_completion_tokens": 18, "judge_total_tokens": 488, "judge_prompt_tokens": 409, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Not found.", "latency_ms": 5431.8554401397705, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not available in the database. The generated answer 'Not found.' conveys the same limitation and does not introduce any incorrect director name. It is correct relative to the database and fully faithful to the gold intent.", "main_total_tokens": 2915, "main_prompt_tokens": 2911, "main_completion_tokens": 4, "judge_total_tokens": 481, "judge_prompt_tokens": 400, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "According to [Doc 98375], this happens in the movie \"Rise of the Mummy\" (2021).", "latency_ms": 7331.852436065674, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.7, "justification": "The generated answer correctly identifies the movie as \"Rise of the Mummy,\" so correctness is high. However, it fabricates a citation to an unspecified 'Doc 98375,' which is not grounded in the provided information, reducing faithfulness.", "main_total_tokens": 2816, "main_prompt_tokens": 2790, "main_completion_tokens": 26, "judge_total_tokens": 497, "judge_prompt_tokens": 415, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "According to [Doc 35237], this happens in the movie \"Stranger in My Bed\" (2005).", "latency_ms": 7198.860168457031, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Stranger in My Bed', which matches the gold answer, and it attributes the information to the referenced document without adding unsupported details beyond the year, which is accurate.", "main_total_tokens": 2802, "main_prompt_tokens": 2777, "main_completion_tokens": 25, "judge_total_tokens": 493, "judge_prompt_tokens": 420, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "According to [Doc 123938], this happens in \"Game of Silence\".", "latency_ms": 5809.626579284668, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a movie, but the gold answer and the generated response both refer to 'Game of Silence', which is a TV series, not a movie. Thus the answer is incorrect. The generated response also fabricates a citation to 'Doc 123938', which is not grounded in the provided context, reducing faithfulness.", "main_total_tokens": 2277, "main_prompt_tokens": 2260, "main_completion_tokens": 17, "judge_total_tokens": 511, "judge_prompt_tokens": 413, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 5097.179889678955, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'The Craigslist Killer.'", "main_total_tokens": 2644, "main_prompt_tokens": 2640, "main_completion_tokens": 4, "judge_total_tokens": 445, "judge_prompt_tokens": 395, "judge_completion_tokens": 50}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "According to [Doc 66094], this happens in the movie \"Wood Job!\".", "latency_ms": 7160.446882247925, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Wood Job!', which matches the gold answer exactly. The brief reference style ('According to [Doc 66094]') does not contradict the question or introduce unsupported claims; it is simply a citation placeholder. Therefore, the response is both factually correct and faithful to the task.", "main_total_tokens": 3147, "main_prompt_tokens": 3129, "main_completion_tokens": 18, "judge_total_tokens": 513, "judge_prompt_tokens": 417, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase (2017) [Doc 78373]", "latency_ms": 6724.261522293091, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Sesame Street: The Magical Wand Chase', matching the gold answer exactly apart from the added year and document reference, which do not contradict the gold. Thus it is both correct and faithful to the provided gold answer.", "main_total_tokens": 2812, "main_prompt_tokens": 2792, "main_completion_tokens": 20, "judge_total_tokens": 510, "judge_prompt_tokens": 427, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "According to [Doc 74597], this happens in the movie \"Beyond\" (2016).", "latency_ms": 6258.86607170105, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot; the gold answer is 'Beyond'. The generated answer states that this happens in the movie 'Beyond' (2016), which is correct. The mention of a document ID and year does not contradict the gold and is consistent, so the answer is both correct and faithful to the provided information.", "main_total_tokens": 2353, "main_prompt_tokens": 2332, "main_completion_tokens": 21, "judge_total_tokens": 527, "judge_prompt_tokens": 428, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "According to [Doc 22478], this happens in the movie \"6ixtynin9\".", "latency_ms": 6377.077341079712, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"6ixtynin9,\" which matches the gold answer exactly. It does not introduce any extraneous or contradictory information and remains faithful to the query and provided context.", "main_total_tokens": 2434, "main_prompt_tokens": 2412, "main_completion_tokens": 22, "judge_total_tokens": 497, "judge_prompt_tokens": 423, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "According to [Doc 32410], this happens in the movie \"Club Dread\".", "latency_ms": 6789.999961853027, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'Club Dread', matching the gold answer exactly, so correctness is 1. The mention of '[Doc 32410]' as a source is fabricated in this context since no such document was provided in the prompt, slightly reducing faithfulness.", "main_total_tokens": 2986, "main_prompt_tokens": 2968, "main_completion_tokens": 18, "judge_total_tokens": 532, "judge_prompt_tokens": 442, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet (2010) [Doc 48873]", "latency_ms": 6440.3040409088135, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as 'Toilet (2010)'. It is correct and fully aligned with the provided reference.", "main_total_tokens": 3123, "main_prompt_tokens": 3110, "main_completion_tokens": 13, "judge_total_tokens": 488, "judge_prompt_tokens": 425, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "According to [Doc 1024], this happens in the movie \"Electric Angel\".", "latency_ms": 6550.024032592773, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold label exactly, identifying the movie as 'Electric Angel'. It does not introduce contradictions or hallucinated details beyond referencing a document, so it is both correct and faithful to the provided information.", "main_total_tokens": 2762, "main_prompt_tokens": 2744, "main_completion_tokens": 18, "judge_total_tokens": 507, "judge_prompt_tokens": 433, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "According to the provided context, the lowest rated movie from 2023 is \"Rare Objects\" with a rating of 4.3 [Doc 105190].", "latency_ms": 6151.85546875, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates that the lowest rated movie from 2023 is 'Mega Ape', but the generated answer claims it is 'Rare Objects' with a rating of 4.3. This directly contradicts the gold, so the answer is factually incorrect relative to the task. It also introduces a specific rating and citation not supported by the gold reference, so it is not faithful to the provided ground truth.", "main_total_tokens": 2871, "main_prompt_tokens": 2837, "main_completion_tokens": 34, "judge_total_tokens": 523, "judge_prompt_tokens": 414, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "According to the provided context, the highest rated TV show from 2010 is \"Hawaii Five-0\" with a rating of 7.755 and popularity of 48.2796.", "latency_ms": 8230.647325515747, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says the highest rated TV show from 2010 is 'Tea House', while the generated answer claims it is 'Hawaii Five-0' with a specific rating and popularity. This directly contradicts the gold, so the generated answer is factually incorrect relative to the gold and not faithful to it.", "main_total_tokens": 3249, "main_prompt_tokens": 3208, "main_completion_tokens": 41, "judge_total_tokens": 517, "judge_prompt_tokens": 422, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "According to the provided context, the highest rated Japanese language movie from 1999 is \"Kikujiro\" with a rating of 7.5 [Doc 22213].", "latency_ms": 9329.062938690186, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says the highest rated Japanese language movie from 1999 is 'The Uncut Story of Mewtwo's Origin', while the generated answer claims it is 'Kikujiro'. This directly contradicts the gold, so the answer is incorrect. It also is not faithful to the provided gold/context, which explicitly names a different movie, so faithfulness is also zero.", "main_total_tokens": 3180, "main_prompt_tokens": 3142, "main_completion_tokens": 38, "judge_total_tokens": 537, "judge_prompt_tokens": 428, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "According to the provided context, the lowest-rated movie from 1980 is \"Don't Answer the Phone!\" with a rating of 4.6. The language of this movie is en (English).", "latency_ms": 9058.08687210083, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980. Both the gold answer and the generated answer state that the language is English. The generated answer is correct and aligned with the gold answer, and it stays within the given context, so it is both correct and faithful.", "main_total_tokens": 3052, "main_prompt_tokens": 3010, "main_completion_tokens": 42, "judge_total_tokens": 513, "judge_prompt_tokens": 423, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "According to [Doc 115187], Freaks and Geeks had a popularity rating of 5.8043.\n\nHowever, according to [Doc 115196], Popular had a popularity rating of 4.5113, but according to [Doc 115269], Who Wants to Be a Millionaire? had a popularity rating of 2.3756, which is lower than the one mentioned in [Doc 115187].\n\nBut, according to [Doc 115222], The Man Show had a popularity rating of 3.286.\n\nHowever, according to [Doc 115196] and [Doc 115269], Popular and Who Wants to Be a Millionaire? are not the TV shows with the highest popularity in 1999.\n\nAccording to [Doc 115187], Freaks and Geeks is not the only one with high popularity. According to [Doc 115196], Zorra Total had a popularity rating of 4.6605, which is higher than Popular's rating.\n\nHowever, according to [Doc 115269] and [Doc 115222], Who Wants to Be a Millionaire? and The Man Show are not the TV shows with the highest popularity in 1999.\n\nAccording to [Doc 115196], Z", "latency_ms": 30410.930156707764, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks which TV show in 1999 had the highest popularity, and the gold answer is \"Law & Order: Special Victims Unit.\" The generated answer instead discusses popularity ratings for other shows (Freaks and Geeks, Popular, Who Wants to Be a Millionaire?, The Man Show, Zorra Total) and never identifies Law & Order: SVU, so it is incorrect. However, within the context provided, the assistant consistently cites and reasons about the given documents without inventing new facts beyond them, so its answer is faithful to the (albeit incomplete/truncated) cited sources.", "main_total_tokens": 3646, "main_prompt_tokens": 3390, "main_completion_tokens": 256, "judge_total_tokens": 800, "judge_prompt_tokens": 646, "judge_completion_tokens": 154}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2 [Doc 79355]", "latency_ms": 5964.172124862671, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 and does not introduce any unsupported information besides a harmless document reference, so it is both correct and faithful.", "main_total_tokens": 2946, "main_prompt_tokens": 2936, "main_completion_tokens": 10, "judge_total_tokens": 457, "judge_prompt_tokens": 392, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "According to [Doc 22481], the movie \"The Story of Us\" was released in 1999.\n\nHowever, there is also another movie with the same title released in 2019, as mentioned in [Doc 88811].", "latency_ms": 9766.08657836914, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer expects the 2019 film, but the generated answer gives 1999 as the release year, which is incorrect for this question. However, the answer is faithful to the provided documents by citing both the 1999 and 2019 films and attributing them to specific docs, so faithfulness is high even though it does not match the gold label.", "main_total_tokens": 2970, "main_prompt_tokens": 2920, "main_completion_tokens": 50, "judge_total_tokens": 539, "judge_prompt_tokens": 433, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The number of votes for \"My Dog Skip\" is not explicitly mentioned in the provided context. However, it has a rating of 6.7 and a popularity score of 1.7749.", "latency_ms": 8684.938192367554, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the film received 279 votes. The generated answer claims the number of votes is not mentioned and instead talks about rating and popularity, which directly contradicts the gold. Thus it is factually incorrect relative to the gold and not faithful to the provided information.", "main_total_tokens": 2732, "main_prompt_tokens": 2690, "main_completion_tokens": 42, "judge_total_tokens": 504, "judge_prompt_tokens": 422, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The number of votes for the movie \"Oh My God!\" is not explicitly stated in the provided context. However, it does mention that the rating for the 2018 movie \"Oh My God!\" is 3.8 and its popularity is 0.8047.", "latency_ms": 10428.182601928711, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly states that \"Oh My God!\" received 12 votes, but the generated answer claims the number of votes is not stated and instead discusses rating and popularity. This is factually incorrect relative to the gold, so correctness is 0. The response is consistent with its own stated context and does not invent specific vote numbers, so it is faithful to the (mistaken) view of the context, giving a faithfulness score of 1.", "main_total_tokens": 3038, "main_prompt_tokens": 2982, "main_completion_tokens": 56, "judge_total_tokens": 555, "judge_prompt_tokens": 436, "judge_completion_tokens": 119}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "[Doc 46069] states that the movie \"The Last Days of Lehman Brothers\" was released in 2009.", "latency_ms": 8115.949630737305, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie was released in 2009, matching the gold answer, and it is directly supported by the cited document, so it is both correct and faithful.", "main_total_tokens": 3338, "main_prompt_tokens": 3311, "main_completion_tokens": 27, "judge_total_tokens": 482, "judge_prompt_tokens": 413, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "[Doc 48035] states that the language of the movie \"The King's Speech\" is English, with a rating of 7.734 and popularity of 4.8806.", "latency_ms": 9001.330852508545, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'en', i.e., English. The generated answer explicitly states that the language of the movie 'The King's Speech' is English, which matches the gold. It also correctly attributes this information to the referenced document. There are no contradictions or hallucinations relevant to the question, so the response is both correct and faithful to the cited source.", "main_total_tokens": 3134, "main_prompt_tokens": 3094, "main_completion_tokens": 40, "judge_total_tokens": 524, "judge_prompt_tokens": 421, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The movie \"Wall Street\" was released in 1987 [Doc 6993].", "latency_ms": 5819.422245025635, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'Wall Street' was released in 1987, which matches the gold answer. It does not introduce contradictions or extraneous claims; the bracketed citation does not affect faithfulness.", "main_total_tokens": 2231, "main_prompt_tokens": 2212, "main_completion_tokens": 19, "judge_total_tokens": 470, "judge_prompt_tokens": 396, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie where classmates have to play a survival game using their cell phones is \"Brain Fluid Explosion Girl\" [Doc 69929].", "latency_ms": 6536.437749862671, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Brain Fluid Explosion Girl' and remains faithful to the provided information without adding unsupported details.", "main_total_tokens": 1900, "main_prompt_tokens": 1872, "main_completion_tokens": 28, "judge_total_tokens": 478, "judge_prompt_tokens": 415, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "According to [Doc 112849], six friends need to rely on a Dungeon Master to get home in the TV show \"Dungeons & Dragons\".", "latency_ms": 6794.174671173096, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' which matches the gold answer. It is faithful to the provided information and does not introduce unsupported details.", "main_total_tokens": 1889, "main_prompt_tokens": 1858, "main_completion_tokens": 31, "judge_total_tokens": 486, "judge_prompt_tokens": 420, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones is Nacht der Wölfe (Night of the Wolves), as described in [Doc 2590].", "latency_ms": 8164.695262908936, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe', which matches the gold answer. The brief mention of an external document is consistent with the question and does not introduce contradictory or unsupported information, so it is faithful.", "main_total_tokens": 1950, "main_prompt_tokens": 1905, "main_completion_tokens": 45, "judge_total_tokens": 521, "judge_prompt_tokens": 442, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The year in which the events of Picnic at Hanging Rock take place is 1900, specifically on Valentine's Day. [Doc 126191]", "latency_ms": 7295.773506164551, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of Picnic at Hanging Rock take place in 1900, matching the gold answer exactly. The additional detail about Valentine’s Day is accurate and consistent with the source, so the response is both correct and faithful to the referenced document.", "main_total_tokens": 2197, "main_prompt_tokens": 2165, "main_completion_tokens": 32, "judge_total_tokens": 502, "judge_prompt_tokens": 414, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The movie is \"June\" (2004), which is about a young violinist named June who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her.\n\nSource: [Doc 32483]", "latency_ms": 10533.3833694458, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"June\" and accurately restates the description from the provided source. It matches the gold answer and remains faithful to the given document without adding unsupported details.", "main_total_tokens": 2233, "main_prompt_tokens": 2167, "main_completion_tokens": 66, "judge_total_tokens": 566, "judge_prompt_tokens": 496, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1572.2191333770752, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a particular movie title, while the generated answer says 'I don't know,' which does not provide the correct movie and is therefore factually incorrect. However, the generated answer does not invent any incorrect facts; it accurately reflects a lack of knowledge, so it is fully faithful to the model's own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 495, "judge_prompt_tokens": 399, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 412.55855560302734, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific numeric rating (7.6), but the model replied with 'I don't know,' which does not answer the question and is therefore incorrect. However, the response is faithful because the model did not hallucinate or fabricate an answer—it honestly expressed uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 476, "judge_prompt_tokens": 389, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1276.960849761963, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.366123019050001, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is incorrect. The model directly answered the question without inventing additional details, so it is faithful to the (incorrect) prediction it provided.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 470, "judge_prompt_tokens": 394, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 204.29611206054688, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer '0' does not match the gold answer '2', so it is factually incorrect. The model appears to directly answer the question without hallucinating extra information or changing the query, so it is faithful to the user question, but the numerical result is wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 482, "judge_prompt_tokens": 395, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1630.4333209991455, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answer is factually incorrect because the correct answer is 'Fantômas'. However, it is faithful in that it honestly states uncertainty ('I don't know') without fabricating an alternative answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 460, "judge_prompt_tokens": 389, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" (1990).", "latency_ms": 5651.745319366455, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise' (1990), which matches the gold answer exactly and stays fully faithful to the given information without adding incorrect details.", "main_total_tokens": 2107, "main_prompt_tokens": 2082, "main_completion_tokens": 25, "judge_total_tokens": 473, "judge_prompt_tokens": 407, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 8251.128911972046, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, providing the correct movie title 'Won't You Be My Neighbor?'. It is accurate and consistent with the given information, with no additions or contradictions.", "main_total_tokens": 2411, "main_prompt_tokens": 2371, "main_completion_tokens": 40, "judge_total_tokens": 510, "judge_prompt_tokens": 440, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The year it was released is not found in the provided context.", "latency_ms": 5734.992980957031, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 2018, but the generated answer claims the year is not in the context instead of answering 2018, so it is factually incorrect. However, the answer is faithful to the provided (though unspecified) context, since it does not invent a year and explicitly states that the information is not found.", "main_total_tokens": 2304, "main_prompt_tokens": 2290, "main_completion_tokens": 14, "judge_total_tokens": 487, "judge_prompt_tokens": 390, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is Astro Boy [Doc 112591].", "latency_ms": 6613.054275512695, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy', which matches the gold answer exactly. It is also faithful to the information, only adding a plausible citation-like note without contradicting the source.", "main_total_tokens": 2061, "main_prompt_tokens": 2034, "main_completion_tokens": 27, "judge_total_tokens": 486, "judge_prompt_tokens": 413, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "According to [Doc 67513], the woman and her son were held captive for 7 years in the movie \"Room\" released in 2015.", "latency_ms": 6870.677709579468, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, matching the gold answer, so correctness is 1. However, it cites a specific document identifier ([Doc 67513]) that is not present in the provided context, which is a hallucinated source, so faithfulness is 0.", "main_total_tokens": 1813, "main_prompt_tokens": 1780, "main_completion_tokens": 33, "judge_total_tokens": 524, "judge_prompt_tokens": 426, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "According to [Doc 133469], Brink's-Mat security depot was holding gold bullion worth £26m before it was broken into on 26 November 1983.", "latency_ms": 7653.942346572876, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold reference exactly: it states that Brink's-Mat security depot was holding gold bullion worth £26m before the 26 November 1983 break-in. It does not introduce any contradictions or extraneous claims and remains faithful to the provided information.", "main_total_tokens": 2088, "main_prompt_tokens": 2050, "main_completion_tokens": 38, "judge_total_tokens": 516, "judge_prompt_tokens": 430, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6582.950830459595, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 80.5772, but the model said it did not know, so the answer is incorrect. However, the model did not fabricate information and honestly stated uncertainty, so it is fully faithful to its own stated knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 476, "judge_prompt_tokens": 396, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 396.8048095703125, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states 1, while the generated answer is 0, so the generated answer is factually incorrect relative to the gold. However, the model does not hallucinate any additional explanation or unsupported details; it simply gives an incorrect number, so it is still fully faithful to its (wrong) output.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 479, "judge_prompt_tokens": 385, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in \"Angel Eyes\" (TV show). [Doc 121928]", "latency_ms": 7548.938989639282, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Angel Eyes,' which matches the gold answer exactly and stays faithful to the provided information.", "main_total_tokens": 2484, "main_prompt_tokens": 2447, "main_completion_tokens": 37, "judge_total_tokens": 488, "judge_prompt_tokens": 430, "judge_completion_tokens": 58}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of this movie is French (fr).", "latency_ms": 5663.12837600708, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because it states the language is French, while the gold label says Korean. However, it is fully faithful to the user-provided generated text and gold label in this evaluation context.", "main_total_tokens": 2136, "main_prompt_tokens": 2125, "main_completion_tokens": 11, "judge_total_tokens": 460, "judge_prompt_tokens": 386, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 829.716682434082, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is factually incorrect. However, given only the numeric output, there is no indication of misuse or misinterpretation of the question beyond getting the number wrong; thus it is fully faithful to the user query in form, but incorrect in content.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 394, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The best-rated movie released in 1979 is not found in the provided context. However, I can suggest some movies from other years that have high ratings:\n\n* \"1939: Hollywood's Greatest Year\" (2009) - Rating: 7.5\n* \"Refused\" (1980) - Rating: 7.5\n* \"Pop\" (1999) - Rating: 7.25", "latency_ms": 11336.766719818115, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that there are no movies from 1979 in the database, so the correct reply is to say that no such movie is available. The generated answer instead speculates about suggestions from other years and lists specific movies and ratings not grounded in the gold/context. This contradicts the gold response and introduces unsupported details, making it both incorrect and unfaithful.", "main_total_tokens": 2105, "main_prompt_tokens": 2018, "main_completion_tokens": 87, "judge_total_tokens": 587, "judge_prompt_tokens": 481, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "There is no mention of a movie titled \"Back to the Future Part IV\" in the provided context. The highest numbered title mentioned is \"Back to the Future Part III\", which was released in 1990, as stated in [Doc 10621].", "latency_ms": 10210.999965667725, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.7, "faithfulness_score": 0.6, "justification": "The gold answer clearly states that 'Back to the Future Part IV' does not exist in the database, aligning with reality. The generated answer instead focuses on the provided context and notes that only Part III (1990) is mentioned, implicitly avoiding giving a year for Part IV. This partially aligns with the gold in that it does not fabricate a release year, but it fails to directly answer the user’s question or mirror the gold’s explicit statement of non-existence. Thus, correctness is partial, since the user’s question is effectively unanswered. Faithfulness is also partial: the answer is grounded in the context (Doc 10621) but interprets the task as context-limited rather than addressing the true underlying fact that Part IV does not exist in the database as per the gold.", "main_total_tokens": 2949, "main_prompt_tokens": 2896, "main_completion_tokens": 53, "judge_total_tokens": 645, "judge_prompt_tokens": 452, "judge_completion_tokens": 193}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly notes that it cannot find any movies or shows matching the criteria, which aligns with the gold answer stating that there are no 1980 movies about the Chernobyl disaster. It is faithful to the information implied by the gold answer and does not introduce contradictory or unsupported claims, only an additional suggestion to verify the time period, which is reasonable.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 538, "judge_prompt_tokens": 434, "judge_completion_tokens": 104}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 562.974214553833, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (4), so it is incorrect. However, there is no evidence of the model contradicting or ignoring given information beyond providing the wrong count, so faithfulness to the prompt is maintained.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 388, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 231.71353340148926, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is factually incorrect. However, the answer is directly responsive to the user query and does not introduce extraneous or fabricated reasoning beyond giving the (incorrect) number, so it is fully faithful to the prompt.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 476, "judge_prompt_tokens": 386, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 233.72173309326172, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is numerically incorrect compared to the gold answer (0 vs 1), so correctness is 0. The model clearly provides a definite numeric answer without contradicting its own reasoning or adding extraneous fabricated details, so its internal faithfulness to its own (albeit unstated) reasoning is acceptable, giving a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 389, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 333.00280570983887, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (0) and correctly reflects that there were no Super Mario Bros TV shows released between 2000 and 2010. The reasoning is consistent with the question and does not introduce unsupported information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 468, "judge_prompt_tokens": 391, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 138.06986808776855, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto'. The generated answer says 'I don't know', which is incorrect. However, it does not introduce any incorrect facts; it simply expresses lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 466, "judge_prompt_tokens": 386, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "There is no mention of a Jurassic Park/Power Ranger crossover movie in the provided context.", "latency_ms": 5212.383508682251, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that such a movie was not released. The generated answer instead says only that there is no mention of a crossover movie in the provided context, which does not explicitly address whether the movie was released. This makes the generated answer incorrect with respect to the question, though it is faithful to the (implied) context it references.", "main_total_tokens": 2310, "main_prompt_tokens": 2291, "main_completion_tokens": 19, "judge_total_tokens": 503, "judge_prompt_tokens": 403, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Barbie does not fight the Predator in any of these movies.", "latency_ms": 5828.463792800903, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.5, "faithfulness_score": 0, "justification": "The gold answer clarifies that such a movie exists conceptually but was never released. The generated answer states that Barbie does not fight the Predator in any movie, implying nonexistence rather than unreleased status. This partially aligns in practice (no released movie features this), but contradicts the specific gold framing, so correctness is partial. It is not faithful to the gold answer’s nuance about an unreleased movie, so faithfulness is low.", "main_total_tokens": 2373, "main_prompt_tokens": 2359, "main_completion_tokens": 14, "judge_total_tokens": 516, "judge_prompt_tokens": 394, "judge_completion_tokens": 122}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The 4th oldest Star Wars movie is Return of the Jedi (1983) [Doc 2858].", "latency_ms": 6684.783697128296, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The generated answer is incorrect: according to the gold label, the 4th oldest Star Wars movie is 'Star Wars: Droids - The Great Heep', not 'Return of the Jedi (1983)'. The answer contradicts the gold fact, so correctness is 0. It is mostly faithful to its own cited document (it presents Return of the Jedi with a date and citation), with no obvious hallucinated extra claims beyond the incorrect ranking, so faithfulness is relatively high but not perfect.", "main_total_tokens": 2329, "main_prompt_tokens": 2305, "main_completion_tokens": 24, "judge_total_tokens": 547, "judge_prompt_tokens": 412, "judge_completion_tokens": 135}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The year it was released is not found in the provided context.", "latency_ms": 5677.918195724487, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1986, but the generated response claims the year is not found instead of providing 1986, so it is factually incorrect. However, relative to the (unstated) context, it does not invent a different year and honestly reports lack of information, so it remains faithful to the context as given.", "main_total_tokens": 2304, "main_prompt_tokens": 2290, "main_completion_tokens": 14, "judge_total_tokens": 488, "judge_prompt_tokens": 390, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 390.3930187225342, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the rating of the movie Forrest Gump, and the gold answer provides a specific numeric rating (8.466). The generated answer says 'I don't know,' which does not match the required numeric value, so it is incorrect. However, the model does not invent any incorrect facts or contradict context; it simply expresses a lack of knowledge, so it is fully faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 511, "judge_prompt_tokens": 398, "judge_completion_tokens": 113}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is Mighty Med [Doc 121125].", "latency_ms": 5459.418058395386, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med', which exactly matches the gold answer, and does not introduce any contradictory or extraneous information beyond an irrelevant citation marker.", "main_total_tokens": 1844, "main_prompt_tokens": 1815, "main_completion_tokens": 29, "judge_total_tokens": 488, "judge_prompt_tokens": 420, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "According to [Doc 53582], seven prisoners escaped from a prison in Buenos Aires, Argentina.", "latency_ms": 6053.476810455322, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the country; the gold answer is Argentina. The generated answer states that seven prisoners escaped from a prison in Buenos Aires, Argentina, which correctly identifies the country as Argentina and does not contradict the gold. It is fully correct and faithful.", "main_total_tokens": 2059, "main_prompt_tokens": 2038, "main_completion_tokens": 21, "judge_total_tokens": 493, "judge_prompt_tokens": 411, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The apprentice falls into Jiang Jie's trap.\n\nSource:\n[Doc 135231] Title: Sweet Trap\nYear: 2024", "latency_ms": 7080.082893371582, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: the apprentice falls into Jiang Jie's trap. This is correct according to the provided source and is fully faithful to the given information.", "main_total_tokens": 2262, "main_prompt_tokens": 2233, "main_completion_tokens": 29, "judge_total_tokens": 477, "judge_prompt_tokens": 411, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 769.7932720184326, "retrieval_recall": 0.0, "retrieval_mrr": 0.07692307692307693, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (4), so it is factually incorrect. However, given the prompt and information available in the conversation, the model appears to be directly using its internal reasoning (even though it is wrong) rather than hallucinating about any external or unseen data, so it is faithful to its own (albeit incorrect) reasoning process.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 493, "judge_prompt_tokens": 385, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "37", "latency_ms": 117.10882186889648, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says there are no movies with both Bruce Lee and Steven Seagal, while the generated answer claims there are 37. This is factually incorrect and contradicts the provided gold answer, so it is neither correct nor faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 474, "judge_prompt_tokens": 395, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is not found in the provided context. However, there are multiple mentions of \"Mater\" in other documents, but none of them have the exact title \"Tokyo Mater\". \n\nHowever, [Doc 42750] and another document with the same content mention a movie titled \"Tokyo Mater\" with a rating of 6.3.\n\nSo, based on this information, it can be inferred that the rating of Tokyo Mater (2008) is likely to be 6.3, but this cannot be confirmed without more specific information about the title.", "latency_ms": 15316.403865814209, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0.7, "faithfulness_score": 0.7, "justification": "The correct answer is 6.3, and the generated response eventually states that Tokyo Mater (2008) has a rating of 6.3, so it reaches the right number. However, it adds unnecessary uncertainty, saying it cannot be confirmed without more information, despite already citing a specific document that gives the correct rating. This hedging and contradiction reduces both correctness (since the answer is not stated clearly and confidently) and faithfulness (since the reasoning is somewhat inconsistent with the evidence it cites).", "main_total_tokens": 2398, "main_prompt_tokens": 2275, "main_completion_tokens": 123, "judge_total_tokens": 638, "judge_prompt_tokens": 502, "judge_completion_tokens": 136}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in \"Fast Track: No Limits\" is street racing.", "latency_ms": 6347.252130508423, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies 'Drift' as the style of racing, but the generated answer says 'street racing', which is incorrect. However, the response is fully derived from the prompt content (question, gold, and generated answer) without hallucinating any external information, so it is faithful to the given content.", "main_total_tokens": 2456, "main_prompt_tokens": 2439, "main_completion_tokens": 17, "judge_total_tokens": 488, "judge_prompt_tokens": 394, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little is \"Confronting a Serial Killer\". [Doc 131298]", "latency_ms": 8583.43768119812, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Confronting a Serial Killer,' and does not introduce any unsupported information beyond what is implied.", "main_total_tokens": 2278, "main_prompt_tokens": 2233, "main_completion_tokens": 45, "judge_total_tokens": 516, "judge_prompt_tokens": 450, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 6609.624862670898, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold value (7.838) and correctly identifies it as the popularity score of Armageddon (1998). It is fully supported by the provided information.", "main_total_tokens": 2337, "main_prompt_tokens": 2314, "main_completion_tokens": 23, "judge_total_tokens": 476, "judge_prompt_tokens": 407, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"Victim\" [Doc 22488].", "latency_ms": 7026.267051696777, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'The Sixth Sense.' The generated answer incorrectly states the title is 'Victim,' which is factually wrong. Additionally, the model fabricates a document reference [Doc 22488] as support, indicating it is not faithful to the provided gold answer or to reliable sources.", "main_total_tokens": 2225, "main_prompt_tokens": 2196, "main_completion_tokens": 29, "judge_total_tokens": 510, "judge_prompt_tokens": 419, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in 2002 [Doc 27318].", "latency_ms": 6049.62420463562, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer, and it remains faithful to the given information.", "main_total_tokens": 2388, "main_prompt_tokens": 2372, "main_completion_tokens": 16, "judge_total_tokens": 456, "judge_prompt_tokens": 395, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\". [Doc 67500]", "latency_ms": 7371.186971664429, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2015 reboot title as 'The Transporter Refueled'. It stays fully consistent with the provided information and does not introduce contradictions.", "main_total_tokens": 2392, "main_prompt_tokens": 2362, "main_completion_tokens": 30, "judge_total_tokens": 491, "judge_prompt_tokens": 420, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The Transporter Refueled was released in the year 2015 [Doc 67500].", "latency_ms": 6646.8470096588135, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer, and it does not introduce any contradictions or unsupported claims beyond citing a document reference.", "main_total_tokens": 2501, "main_prompt_tokens": 2480, "main_completion_tokens": 21, "judge_total_tokens": 477, "judge_prompt_tokens": 404, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "The romantic comedy where Abraham Lincoln comes back from the dead is not found in the provided context.", "latency_ms": 5895.984172821045, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that no such romantic comedy is identified/found. This matches the information available and does not introduce unsupported claims, so the response is both correct and faithful to the context.", "main_total_tokens": 1935, "main_prompt_tokens": 1915, "main_completion_tokens": 20, "judge_total_tokens": 481, "judge_prompt_tokens": 409, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by Ernest Hemingway is \"Havana Darkness\".", "latency_ms": 6377.554416656494, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'Havana Darkness' as the movie title. It is fully faithful to the provided information with no extraneous or conflicting details.", "main_total_tokens": 2075, "main_prompt_tokens": 2051, "main_completion_tokens": 24, "judge_total_tokens": 477, "judge_prompt_tokens": 409, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show \"Doll & Em\". [Doc 122259]", "latency_ms": 7186.907052993774, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em' and matches the gold reference exactly. There is no contradiction or hallucination; it directly uses the information provided in the prompt.", "main_total_tokens": 2274, "main_prompt_tokens": 2244, "main_completion_tokens": 30, "judge_total_tokens": 488, "judge_prompt_tokens": 417, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in the movie \"Hallam Foe\" (2007). [Doc 39389]", "latency_ms": 8049.570560455322, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Hallam Foe\" and matches the gold answer exactly, including the year 2007, without adding unsupported information.", "main_total_tokens": 2263, "main_prompt_tokens": 2223, "main_completion_tokens": 40, "judge_total_tokens": 502, "judge_prompt_tokens": 438, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The answer to your question cannot be found in the provided context.", "latency_ms": 5786.428213119507, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'A foe'. The generated answer says the answer cannot be found in the context, which is incorrect relative to the gold; therefore correctness is 0. The model does not hallucinate a wrong substantive answer but rather expresses uncertainty about the context, so it remains faithful to its own stated inference limits, giving faithfulness a 1.", "main_total_tokens": 2372, "main_prompt_tokens": 2358, "main_completion_tokens": 14, "judge_total_tokens": 501, "judge_prompt_tokens": 399, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "The age of Sam in the 2017 TV show Atypical is 18 years old [Doc 124583].", "latency_ms": 6377.140760421753, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: Sam is 18 years old. It does not introduce any contradictory or unsupported information beyond the cited document reference, so it is both correct and faithful.", "main_total_tokens": 2053, "main_prompt_tokens": 2028, "main_completion_tokens": 25, "judge_total_tokens": 482, "judge_prompt_tokens": 412, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam is on the autism spectrum. [Doc 124583]", "latency_ms": 5393.934965133667, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which matches the gold answer exactly. It is faithful to the provided information and does not introduce any unsupported claims.", "main_total_tokens": 2017, "main_prompt_tokens": 2003, "main_completion_tokens": 14, "judge_total_tokens": 466, "judge_prompt_tokens": 400, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "According to [Doc 59659], Sandra was born in Chamonix.", "latency_ms": 6279.169797897339, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, which matches the gold answer. It also explicitly attributes this information to a document (Doc 59659), indicating it is grounded in the provided source, so it is both correct and faithful.", "main_total_tokens": 2449, "main_prompt_tokens": 2432, "main_completion_tokens": 17, "judge_total_tokens": 496, "judge_prompt_tokens": 413, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "it rains", "latency_ms": 5085.4527950286865, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'it rains' matches the gold answer 'rains' semantically and grammatically, correctly completing the sentence. It is fully faithful to the provided gold answer.", "main_total_tokens": 2557, "main_prompt_tokens": 2554, "main_completion_tokens": 3, "judge_total_tokens": 460, "judge_prompt_tokens": 393, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "According to [Doc 114810], Tabaluga is the last of the dragons and the crown-prince of Greenland.", "latency_ms": 6808.5877895355225, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which matches the gold answer. It does not introduce any contradictions or unsupported claims and aligns with the provided information.", "main_total_tokens": 2275, "main_prompt_tokens": 2249, "main_completion_tokens": 26, "judge_total_tokens": 489, "judge_prompt_tokens": 419, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers\" is [Doc 96838] Title: One Afternoon", "latency_ms": 8211.698293685913, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'One Afternoon' and restates the description without adding unsupported details or contradictions. It is both correct relative to the gold answer and faithful to the given information.", "main_total_tokens": 1870, "main_prompt_tokens": 1823, "main_completion_tokens": 47, "judge_total_tokens": 519, "judge_prompt_tokens": 448, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie where a holocaust survivor takes care of a 12 year old street kid who recently robbed her is \"The Life Ahead\" (2020) with a rating of 6.746 [Doc 93260].", "latency_ms": 6383.837699890137, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.7, "justification": "The answer correctly identifies the movie as \"The Life Ahead,\" matching the gold answer, so correctness is 1. However, it fabricates a specific rating value (6.746) and cites a non-existent document reference [Doc 93260], which is not supported by the provided context, reducing faithfulness.", "main_total_tokens": 2159, "main_prompt_tokens": 2109, "main_completion_tokens": 50, "judge_total_tokens": 543, "judge_prompt_tokens": 447, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary “McLaren” is about Bruce McLaren, a New Zealander who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 8616.386413574219, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and matches the gold answer. The added detail about him being a New Zealander who founded the McLaren Motor Racing team is accurate and consistent with known facts, so the response is both correct and faithful.", "main_total_tokens": 2137, "main_prompt_tokens": 2103, "main_completion_tokens": 34, "judge_total_tokens": 508, "judge_prompt_tokens": 418, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The name of his racing team is not found in the provided context.", "latency_ms": 5770.523309707642, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the team as 'The McLaren Motor Racing team', but the generated answer claims the name is not found. This is factually incorrect given the gold reference, so correctness is 0. However, the generated answer stays within the provided context by not hallucinating a name, so it is faithful to the (assumed) context, yielding a faithfulness score of 1.", "main_total_tokens": 2261, "main_prompt_tokens": 2246, "main_completion_tokens": 15, "judge_total_tokens": 508, "judge_prompt_tokens": 397, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The country is not specified in the provided context.", "latency_ms": 4959.453105926514, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states the country is New Zealand, but the generated answer claims the country is not specified. This directly contradicts the gold, so it is factually incorrect relative to the labeled answer. However, it is faithful to the given generated response and does not introduce new unsupported information.", "main_total_tokens": 1964, "main_prompt_tokens": 1953, "main_completion_tokens": 11, "judge_total_tokens": 475, "judge_prompt_tokens": 386, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 5562.217473983765, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the answer is not available / not found in the context, and neither fabricates a movie title. The generated answer is fully consistent with the information given and matches the intent of the gold response, so it is both correct and faithful.", "main_total_tokens": 2441, "main_prompt_tokens": 2430, "main_completion_tokens": 11, "judge_total_tokens": 495, "judge_prompt_tokens": 408, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with this description: \"When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental\" is Tomie: Forbidden Fruit.\n\nSource:\n[Doc 28589]", "latency_ms": 10162.21284866333, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Tomie: Forbidden Fruit') and aligns with the provided source document. There are no additions or contradictions, so it is both correct and faithful.", "main_total_tokens": 2287, "main_prompt_tokens": 2226, "main_completion_tokens": 61, "judge_total_tokens": 546, "judge_prompt_tokens": 477, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years. [Doc 98566]", "latency_ms": 6768.943548202515, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: both state that Nino and Caterina have been married for 65 years. The model explicitly grounds this in the referenced document, so it is both correct and faithful to the provided evidence.", "main_total_tokens": 2681, "main_prompt_tokens": 2661, "main_completion_tokens": 20, "judge_total_tokens": 491, "judge_prompt_tokens": 413, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King and Tala search for The Book of Souls, which will allow them to put an end to an evil warlord. [Doc 82582]", "latency_ms": 7943.473100662231, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies that they are searching for the Book of Souls, matching the gold answer. The added detail about putting an end to an evil warlord is accurate to the film and does not contradict the referenced document, so the response remains faithful to the source.", "main_total_tokens": 2550, "main_prompt_tokens": 2515, "main_completion_tokens": 35, "judge_total_tokens": 512, "judge_prompt_tokens": 427, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 5405.341386795044, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie that is not in the database/context. The gold answer states this explicitly, and the generated answer correctly indicates that the answer is not found in the provided context. Both convey that the information is unavailable, so the generated answer is both correct and faithful to the context.", "main_total_tokens": 2291, "main_prompt_tokens": 2280, "main_completion_tokens": 11, "judge_total_tokens": 495, "judge_prompt_tokens": 401, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Galactic Baker\" is not found in the provided context.", "latency_ms": 5919.337272644043, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database. The generated answer similarly states that the release year is not found in the provided context, which is effectively equivalent: it does not fabricate a year and aligns with the given information. Therefore it is both correct relative to the task and faithful to the context.", "main_total_tokens": 2166, "main_prompt_tokens": 2148, "main_completion_tokens": 18, "judge_total_tokens": 500, "judge_prompt_tokens": 406, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 4907.288312911987, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the information is not available. The generated answer is consistent with the given context and does not hallucinate a release year.", "main_total_tokens": 1926, "main_prompt_tokens": 1915, "main_completion_tokens": 11, "judge_total_tokens": 465, "judge_prompt_tokens": 401, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 5564.645767211914, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about a movie not in the database/context. The gold answer and the generated answer both state that the answer is not available, which is correct given the information. The generated answer is fully consistent with the context and does not hallucinate a date.", "main_total_tokens": 2446, "main_prompt_tokens": 2435, "main_completion_tokens": 11, "judge_total_tokens": 484, "judge_prompt_tokens": 401, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 5361.800909042358, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer and the generated answer are semantically equivalent: both state that the requested information is unavailable. There is no hallucinated year or fabricated detail. The response correctly and faithfully reflects the lack of data in the context.", "main_total_tokens": 2279, "main_prompt_tokens": 2268, "main_completion_tokens": 11, "judge_total_tokens": 477, "judge_prompt_tokens": 401, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"The Last Pixel\" was released is not found in the provided context.", "latency_ms": 5579.181909561157, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database; the generated answer states that the release year is not found in the provided context. Both convey that the information is unavailable. This is correct and faithful to the gold.", "main_total_tokens": 1980, "main_prompt_tokens": 1962, "main_completion_tokens": 18, "judge_total_tokens": 482, "judge_prompt_tokens": 406, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 5912.299871444702, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the answer/release year is not available in the given information. This is correct and faithful to the (lack of) context, since no release year is provided and the model does not fabricate one.", "main_total_tokens": 2725, "main_prompt_tokens": 2714, "main_completion_tokens": 11, "judge_total_tokens": 481, "judge_prompt_tokens": 402, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Days of Lehman Brothers\" is not found in the provided context.", "latency_ms": 6931.48136138916, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided data/context. This matches the limitation implied by the database and does not hallucinate a director name, so the response is both correct and faithful to the given information.", "main_total_tokens": 2631, "main_prompt_tokens": 2608, "main_completion_tokens": 23, "judge_total_tokens": 500, "judge_prompt_tokens": 419, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"A Love to Die For\" is not found in the provided context.", "latency_ms": 6389.997482299805, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the director information is not available in the database. The generated answer states that the director is not found in the provided context, which is effectively the same limitation. It does not hallucinate a director and stays within the given information, so it is both correct relative to the gold and faithful to the context limitation.", "main_total_tokens": 2399, "main_prompt_tokens": 2378, "main_completion_tokens": 21, "judge_total_tokens": 512, "judge_prompt_tokens": 415, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not found in the provided context.", "latency_ms": 6219.332456588745, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director of 'Journey into Gaza' is not available in the provided information. This is correct given the premise that the database/context does not include the director, and the generated answer does not introduce any unsupported details.", "main_total_tokens": 2262, "main_prompt_tokens": 2242, "main_completion_tokens": 20, "judge_total_tokens": 493, "judge_prompt_tokens": 411, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Night at Tremore Beach\" is not found in the provided context.", "latency_ms": 6173.964738845825, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the given data/context. This matches the limitation of the database/context, so the answer is correct and faithful.", "main_total_tokens": 2014, "main_prompt_tokens": 1991, "main_completion_tokens": 23, "judge_total_tokens": 487, "judge_prompt_tokens": 419, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie is \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 5705.9900760650635, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies the movie as 'Rise of the Mummy,' matching the gold answer, so correctness is 1. However, it adds a citation-like note '[Doc 98375]' that is not grounded in the user context, slightly reducing faithfulness from perfect, hence 0.8.", "main_total_tokens": 2014, "main_prompt_tokens": 1996, "main_completion_tokens": 18, "judge_total_tokens": 497, "judge_prompt_tokens": 407, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death and flees to escape her abusive spouse is \"Stranger in My Bed\" (2005). [Doc 35237]", "latency_ms": 8043.464422225952, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Stranger in My Bed,\" which matches the gold answer. It does not introduce contradictory information and remains faithful to the question and the provided gold reference.", "main_total_tokens": 2167, "main_prompt_tokens": 2130, "main_completion_tokens": 37, "judge_total_tokens": 502, "judge_prompt_tokens": 432, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie that matches the given description is not found in the provided context. However, a similar plot is described in [Doc 123938] Title: Game of Silence, but it's a TV series, not a movie.\n\nIf you'd like to explore more options or provide additional details, I can try to help further.", "latency_ms": 9884.336948394775, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the movie in which the described plot occurs, and the gold answer is 'Game of Silence'. The generated answer says it is not found and then states that 'Game of Silence' is a TV series, not a movie, and does not provide the correct title as the answer. This is factually incorrect with respect to the task (which expects 'Game of Silence'), so correctness is 0. However, the answer is faithful to the given context, explicitly referencing the provided document and its classification as a TV series, so faithfulness is 1.", "main_total_tokens": 1779, "main_prompt_tokens": 1712, "main_completion_tokens": 67, "judge_total_tokens": 607, "judge_prompt_tokens": 461, "judge_completion_tokens": 146}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie that matches the given context is \"The Craigslist Killer\" (2011), as it is based on a true story about a killer who used Craigslist to find victims.", "latency_ms": 7430.866956710815, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Craigslist Killer\" and accurately reflects the gold answer. It is faithful to the given prompt and does not introduce contradictions or extraneous claims beyond elaborating that it is based on a true story of a killer who used Craigslist, which aligns with known information about the film.", "main_total_tokens": 2096, "main_prompt_tokens": 2060, "main_completion_tokens": 36, "judge_total_tokens": 521, "judge_prompt_tokens": 427, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie where this happens is \"Wood Job!\" (2014).", "latency_ms": 5880.9494972229, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\" and even includes the accurate release year, 2014. It matches the gold answer exactly in title, with no contradictions or hallucinations, so both correctness and faithfulness are full.", "main_total_tokens": 2399, "main_prompt_tokens": 2384, "main_completion_tokens": 15, "judge_total_tokens": 495, "judge_prompt_tokens": 414, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie where a playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure is \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 8942.036390304565, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase.' It stays faithful to the given information without introducing unrelated content.", "main_total_tokens": 2173, "main_prompt_tokens": 2125, "main_completion_tokens": 48, "judge_total_tokens": 523, "judge_prompt_tokens": 456, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which the following happens: \"When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives\" is described in [Doc 74597] with title \"Beyond\".", "latency_ms": 8883.337497711182, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates that the correct movie title is 'Beyond'. The generated answer correctly identifies the movie as 'Beyond' and restates the plot description without adding unsupported details, so it is both correct and faithful to the given information.", "main_total_tokens": 1819, "main_prompt_tokens": 1764, "main_completion_tokens": 55, "judge_total_tokens": 540, "judge_prompt_tokens": 462, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie that matches the given context is [Doc 22478] Title: 6ixtynin9. According to its description, it's about \"A woman, fired from a financial corporation during the Asia crisis, returns home with no money.\"", "latency_ms": 8868.194341659546, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies the movie as '6ixtynin9' and accurately repeats the relevant description, so correctness is full. Minor faithfulness issue: it includes an internal reference marker '[Doc 22478]' that suggests reliance on a specific retrieved document not visible to the user, slightly reducing faithfulness to the instruction context.", "main_total_tokens": 1913, "main_prompt_tokens": 1860, "main_completion_tokens": 53, "judge_total_tokens": 554, "judge_prompt_tokens": 453, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie is Club Dread (2004).", "latency_ms": 5127.877473831177, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' matching the gold answer. Adding '(2004)' is accurate but introduces extra information not present in the gold; however, it does not contradict it. Correctness is full, and faithfulness is very high with only minor, non-contradictory embellishment.", "main_total_tokens": 2189, "main_prompt_tokens": 2179, "main_completion_tokens": 10, "judge_total_tokens": 534, "judge_prompt_tokens": 434, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie that matches the given context is not found in the provided documents.", "latency_ms": 6034.81125831604, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is explicitly given as 'Toilet', but the generated answer claims the movie is not found, which is incorrect. However, the model is faithful to its own (incomplete) retrieval, so faithfulness is high even though the content is wrong.", "main_total_tokens": 2434, "main_prompt_tokens": 2418, "main_completion_tokens": 16, "judge_total_tokens": 513, "judge_prompt_tokens": 429, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie that matches the given description is \"Electric Angel\" (1981). The description states that it's a typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century. This information can be found in [Doc 1024].", "latency_ms": 11116.692543029785, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The question asks which movie matches the description, and the gold answer is 'Electric Angel.' The generated answer correctly identifies 'Electric Angel' as the movie, so the correctness is 1. However, the answer claims that this information can be found in [Doc 1024], which is an invented citation and not grounded in any provided context, so the faithfulness is 0.", "main_total_tokens": 2180, "main_prompt_tokens": 2106, "main_completion_tokens": 74, "judge_total_tokens": 595, "judge_prompt_tokens": 487, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is \"Down Low\" with a rating of 5.207 [Doc 106995].", "latency_ms": 4980.648517608643, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer identifies 'Mega Ape' as the lowest rated movie from 2023, while the generated answer incorrectly states it is 'Down Low' with a rating of 5.207. This directly contradicts the gold, so the correctness is 0. The generated answer, however, clearly attributes its information to a document ([Doc 106995]) and does not introduce unsupported details beyond that citation, so it is faithful to its stated source, giving a faithfulness score of 1.", "main_total_tokens": 2055, "main_prompt_tokens": 2027, "main_completion_tokens": 28, "judge_total_tokens": 539, "judge_prompt_tokens": 408, "judge_completion_tokens": 131}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 1518.4354782104492, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Tea House', but the generated answer says 'I don't know', which is incorrect. However, the model does not fabricate an answer and admits uncertainty, so it is fully faithful to its own (stated) knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 466, "judge_prompt_tokens": 386, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 618.6168193817139, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which is incorrect because there is a specific correct answer provided in the gold. However, the response does not introduce any fabricated or conflicting information; it simply expresses a lack of knowledge, so it is fully faithful to the (lack of) reasoning and information used.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 395, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is \"A Story of No Importance\" with a rating of 5.0 [Doc 849]. The language of this movie is French (\"fr\").", "latency_ms": 7587.686538696289, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980. The gold answer is English, but the generated answer states that the language is French, which is factually incorrect given the gold. However, the generated answer consistently uses the information it presents (movie title, rating, language) without contradicting itself or inventing details beyond that, so it is faithful to its own provided context while still being incorrect relative to the gold.", "main_total_tokens": 2455, "main_prompt_tokens": 2415, "main_completion_tokens": 40, "judge_total_tokens": 543, "judge_prompt_tokens": 422, "judge_completion_tokens": 121}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity is Freaks and Geeks, according to [Doc 115187].", "latency_ms": 6938.331842422485, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold label identifies 'Law & Order: Special Victims Unit' as the most popular 1999 TV show, while the model answered 'Freaks and Geeks.' It is also unfaithful because it fabricates support from a document ([Doc 115187]) that we cannot verify, instead of aligning with the provided gold answer.", "main_total_tokens": 2372, "main_prompt_tokens": 2345, "main_completion_tokens": 27, "judge_total_tokens": 523, "judge_prompt_tokens": 416, "judge_completion_tokens": 107}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2. [Doc 79355]", "latency_ms": 6187.080860137939, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and stays within the information given, citing the document. Both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2265, "main_prompt_tokens": 2245, "main_completion_tokens": 20, "judge_total_tokens": 465, "judge_prompt_tokens": 401, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in 1999 [Doc 22481].", "latency_ms": 6255.577564239502, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states the movie was released in 1999, but the gold label says 2019, so it is factually incorrect relative to the provided gold. However, it appears to be faithful to the cited document [Doc 22481], so faithfulness is high even though correctness against the gold is zero.", "main_total_tokens": 2261, "main_prompt_tokens": 2240, "main_completion_tokens": 21, "judge_total_tokens": 500, "judge_prompt_tokens": 404, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "2", "latency_ms": 322.47376441955566, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 279, while the generated answer is 2, so the generated answer is factually incorrect. The question is simple and the model did not follow the provided gold information, so it is not faithful to the source.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 462, "judge_prompt_tokens": 383, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "11", "latency_ms": 115.59247970581055, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer gives 11 votes, but the gold/reference answer is 12 votes, so the content is incorrect. However, the model clearly attempts to answer the specific question without adding extraneous or fabricated context, so it is faithful to the task prompt.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 466, "judge_prompt_tokens": 383, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009. [Doc 46069]", "latency_ms": 6017.561912536621, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie was released in 2009, matching the gold answer. It is fully supported by the provided information and does not invent any unsupported details.", "main_total_tokens": 2603, "main_prompt_tokens": 2578, "main_completion_tokens": 25, "judge_total_tokens": 478, "judge_prompt_tokens": 411, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English (en). [Doc 48035]", "latency_ms": 6592.222690582275, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the original language is English (en). The generated answer states that the original language of \"The King's Speech\" is English and even includes the language code (en), matching the gold exactly. It is both correct and faithful to the gold.", "main_total_tokens": 2296, "main_prompt_tokens": 2272, "main_completion_tokens": 24, "judge_total_tokens": 488, "judge_prompt_tokens": 405, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "1987", "latency_ms": 5460.9668254852295, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (1987), which is the correct release year of the film 'Wall Street.'", "main_total_tokens": 2784, "main_prompt_tokens": 2779, "main_completion_tokens": 5, "judge_total_tokens": 437, "judge_prompt_tokens": 381, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Brain Fluid Explosion Girl", "latency_ms": 5815.945148468018, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie. It is both correct and faithful.", "main_total_tokens": 2445, "main_prompt_tokens": 2438, "main_completion_tokens": 7, "judge_total_tokens": 444, "judge_prompt_tokens": 392, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 5927.359819412231, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Dungeons & Dragons' and correctly identifies the TV show. It is both correct and faithful.", "main_total_tokens": 2420, "main_prompt_tokens": 2413, "main_completion_tokens": 7, "judge_total_tokens": 453, "judge_prompt_tokens": 394, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 5896.043062210083, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Nacht der Wölfe'), correctly identifying the movie about rival German and Turkish teen gangs in Munich. It is both correct and fully faithful to the reference.", "main_total_tokens": 2490, "main_prompt_tokens": 2481, "main_completion_tokens": 9, "judge_total_tokens": 475, "judge_prompt_tokens": 403, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900", "latency_ms": 5910.3844165802, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (1900), so it is both correct and faithful.", "main_total_tokens": 2762, "main_prompt_tokens": 2757, "main_completion_tokens": 5, "judge_total_tokens": 439, "judge_prompt_tokens": 388, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June", "latency_ms": 2004.9021244049072, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the name of a movie character whose name is also a calendar month; both the gold and generated answers are 'June', which matches and is correct and faithful.", "main_total_tokens": 134, "main_prompt_tokens": 132, "main_completion_tokens": 2, "judge_total_tokens": 500, "judge_prompt_tokens": 433, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 486.9036674499512, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is known and provided in the gold response, but the generated answer claims not to know. This is incorrect because there is a specific, correct movie title. However, the model does not hallucinate or fabricate an incorrect movie; it honestly states lack of knowledge, so it is fully faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 495, "judge_prompt_tokens": 399, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 200.5321979522705, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numerical rating (7.6). The generated answer says 'I don't know,' which does not provide the correct value, so correctness is 0. However, the answer does not fabricate information and honestly admits lack of knowledge, so it is faithful to the model's uncertainty, giving faithfulness a 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 488, "judge_prompt_tokens": 389, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 23122.145891189575, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.366123019050001, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is factually incorrect for this query. However, the assistant faithfully provided a single numeric response to the user’s question without adding unrelated information, so it is faithful to the (albeit incorrect) underlying computation or retrieval it performed.", "main_total_tokens": 1672, "main_prompt_tokens": 1458, "main_completion_tokens": 214, "judge_total_tokens": 488, "judge_prompt_tokens": 394, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 516.0152912139893, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there are 2 such TV shows, but the generated answer says there are none and that it cannot find any, which is factually incorrect. However, the model does not fabricate specific data; it openly admits inability to find matching shows rather than inventing numbers or titles, so it is faithful to its own (though limited) retrieval and reasoning.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 509, "judge_prompt_tokens": 404, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 224.92003440856934, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because the correct answer is 'Fantômas', not 'I don't know'. However, the model does not hallucinate an incorrect specific answer; it admits not knowing, which is faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 389, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise", "latency_ms": 5112.032175064087, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Kill Cruise' exactly matches the gold answer and correctly identifies the movie described by the question.", "main_total_tokens": 2645, "main_prompt_tokens": 2640, "main_completion_tokens": 5, "judge_total_tokens": 440, "judge_prompt_tokens": 387, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 6551.563739776611, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct title of the movie about Fred Rogers and his TV series. It is both correct and fully faithful to the provided gold reference.", "main_total_tokens": 3012, "main_prompt_tokens": 3002, "main_completion_tokens": 10, "judge_total_tokens": 477, "judge_prompt_tokens": 408, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2022", "latency_ms": 7050.872087478638, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 2022, but the correct year is 2018, so the answer is factually wrong. The model is, however, directly answering the question without adding extra unsupported content, so it is faithful to its own (incorrect) prediction.", "main_total_tokens": 3337, "main_prompt_tokens": 3329, "main_completion_tokens": 8, "judge_total_tokens": 464, "judge_prompt_tokens": 380, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy", "latency_ms": 5778.994560241699, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Astro Boy' exactly matches the gold answer and correctly identifies the animated TV series about a young boy robot in 2030 AD. It is fully correct and faithful to the reference.", "main_total_tokens": 2649, "main_prompt_tokens": 2643, "main_completion_tokens": 6, "judge_total_tokens": 461, "judge_prompt_tokens": 389, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 years", "latency_ms": 5453.847646713257, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (7 years), correctly stating the length of captivity for the woman and her son in the 2015 film 'Room.'", "main_total_tokens": 2373, "main_prompt_tokens": 2368, "main_completion_tokens": 5, "judge_total_tokens": 461, "judge_prompt_tokens": 397, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m", "latency_ms": 4633.538246154785, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The Brink's-Mat security depot was holding about £26 million in gold before the 26 November 1983 robbery. The generated answer matches the provided gold answer exactly, so it is both correct and faithful.", "main_total_tokens": 2067, "main_prompt_tokens": 2063, "main_completion_tokens": 4, "judge_total_tokens": 472, "judge_prompt_tokens": 398, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 832.1771621704102, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score, which is provided in the gold answer (80.5772). The generated answer says, 'I don't know,' which does not provide the requested value and is therefore incorrect. However, it does not fabricate any information and honestly indicates a lack of knowledge, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 494, "judge_prompt_tokens": 396, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 201.7672061920166, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there was 1 Russian language TV show released in 1997, but the generated answer claims it cannot find any, implying zero. This is factually incorrect (score 0). However, the answer does not invent specific false details; it merely states a lack of information, so it is internally consistent with the model’s apparent knowledge and thus faithful to its own reasoning (score 1).", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 508, "judge_prompt_tokens": 394, "judge_completion_tokens": 114}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes", "latency_ms": 5400.365591049194, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Angel Eyes', which is the correct TV show where Soo-wan regains her sight after an eye transplant. The response is both correct and fully faithful to the provided gold answer.", "main_total_tokens": 3075, "main_prompt_tokens": 3070, "main_completion_tokens": 5, "judge_total_tokens": 473, "judge_prompt_tokens": 397, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "ko", "latency_ms": 7477.5285720825195, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is the language name 'Korean', and the generated answer is 'ko', which is the correct ISO 639-1 code for Korean. They refer to the same language, so the response is both correct and faithful to the gold.", "main_total_tokens": 2959, "main_prompt_tokens": 2942, "main_completion_tokens": 17, "judge_total_tokens": 459, "judge_prompt_tokens": 377, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 967.059850692749, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there are 2 such movies, while the generated answer claims there are none, making it factually incorrect. However, the generated answer does not invent specific incorrect data; it instead expresses an inability to find matches, which is consistent with a limitation rather than fabrication, so it is faithful to its own (albeit incomplete) reasoning.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 505, "judge_prompt_tokens": 403, "judge_completion_tokens": 102}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 1535.6693267822266, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states explicitly that there are no movies from 1979 in the database. The generated answer says only 'I don't know,' which is not equivalent: it omits the key constraint about the database contents. Relative to the gold, this changes the meaning, so it is not fully faithful. It is also not fully correct with respect to the specified behavior because the correct system behavior is to report the lack of 1979 movies in the database, not generic ignorance.", "main_total_tokens": 580, "main_prompt_tokens": 578, "main_completion_tokens": 2, "judge_total_tokens": 526, "judge_prompt_tokens": 399, "judge_completion_tokens": 127}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "There is no mention of a movie titled \"Back to the Future Part IV\" in the provided context.", "latency_ms": 9609.109878540039, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there is no information or mention of a movie titled 'Back to the Future Part IV.' The generated response is accurate with respect to the question and is faithful to the (implied) context of what is known, matching the gold answer in substance.", "main_total_tokens": 3529, "main_prompt_tokens": 3505, "main_completion_tokens": 24, "judge_total_tokens": 511, "judge_prompt_tokens": 421, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both gold and generated answers correctly state that no such movies exist for that year, given that the Chernobyl disaster occurred in 1986, making the question unanswerable as posed. The generated answer is faithful to the gold in meaning and does not add incorrect specifics; it only suggests the user verify the event timing, which is reasonable and consistent.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 536, "judge_prompt_tokens": 434, "judge_completion_tokens": 102}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 521.0230350494385, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 titles, but the generated answer claims there are none, which is factually incorrect. However, the model does not hallucinate any specific incorrect titles or numbers; it simply states it cannot find any, so it remains faithful to its (limited) retrieval and does not invent information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 397, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "50", "latency_ms": 23982.762813568115, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (50) does not match the gold answer (2), so it is incorrect. The number is a direct response to the question, so it is faithful to the user’s request but factually wrong relative to the gold label.", "main_total_tokens": 3516, "main_prompt_tokens": 3324, "main_completion_tokens": 192, "judge_total_tokens": 466, "judge_prompt_tokens": 386, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 738.4731769561768, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show in the 1990s about Super Mario Bros. The generated answer says there were none, which is factually incorrect and contradicts the gold label, so correctness is 0. However, within the context of the model’s own (incorrect) retrieval, it does not invent specific false shows or movies; it simply states inability to find any, so its internal reasoning is faithful to its (flawed) knowledge, yielding faithfulness 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 529, "judge_prompt_tokens": 398, "judge_completion_tokens": 131}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 169.27242279052734, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer specifies that the number of TV shows is 0. The generated answer states that it cannot find any movies or shows matching the criteria, which effectively corresponds to 0 such shows. This is correct relative to the gold. The answer is also faithful, as it does not introduce unsupported claims or contradictions; it simply expresses an inability to find any, aligning with the gold label of 0.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 512, "judge_prompt_tokens": 400, "judge_completion_tokens": 112}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 377.61402130126953, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because Naruto is widely recognized as the most popular ninja-related show, while the model responded with ignorance. However, it is faithful in that it does not invent a wrong fact; it accurately states that it does not know.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 466, "judge_prompt_tokens": 386, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 1727.9486656188965, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states definitively that the movie was not released, while the generated answer says 'I don't know,' which is factually different and therefore incorrect. However, the generated answer does not invent any false details and accurately reflects uncertainty, so it remains faithful to the user query in that it does not hallucinate an incorrect release date.", "main_total_tokens": 539, "main_prompt_tokens": 537, "main_completion_tokens": 2, "judge_total_tokens": 488, "judge_prompt_tokens": 389, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Barbie & Kendra Save the Tiger King", "latency_ms": 8059.436798095703, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer incorrectly names an existing parody movie instead of acknowledging that no such Barbie vs. Predator movie exists and that it was not released. This contradicts the gold answer and is factually wrong, so it fails both correctness and faithfulness.", "main_total_tokens": 2997, "main_prompt_tokens": 2975, "main_completion_tokens": 22, "judge_total_tokens": 470, "judge_prompt_tokens": 390, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "4", "latency_ms": 6159.8029136657715, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer of '4' is incorrect; the question asks for a movie title, and the gold answer is a specific film, 'Star Wars: Droids - The Great Heep'. The model response is not only factually wrong but also not faithful to the question format or the gold answer, providing a number instead of a movie title.", "main_total_tokens": 2853, "main_prompt_tokens": 2849, "main_completion_tokens": 4, "judge_total_tokens": 492, "judge_prompt_tokens": 391, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "2002", "latency_ms": 8635.029792785645, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer '2002' is incorrect; the correct year is 1986. The answer is faithful to the (incorrect) model output being evaluated but not to the gold reference.", "main_total_tokens": 3229, "main_prompt_tokens": 3204, "main_completion_tokens": 25, "judge_total_tokens": 449, "judge_prompt_tokens": 380, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 483.417272567749, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the rating of the movie Forrest Gump, and the gold answer is 8.466. The generated answer says 'I don't know,' which does not provide the correct rating and is therefore incorrect. However, it does not hallucinate or provide false information; it simply admits lack of knowledge, so it is fully faithful to the model's stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 504, "judge_prompt_tokens": 398, "judge_completion_tokens": 106}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med", "latency_ms": 4495.011568069458, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Mighty Med', which is the correct name of the TV show described. It is both correct and fully faithful to the reference.", "main_total_tokens": 2413, "main_prompt_tokens": 2406, "main_completion_tokens": 7, "judge_total_tokens": 460, "judge_prompt_tokens": 394, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina", "latency_ms": 5661.075592041016, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: both state that the prison was in Argentina. The response is correct and faithful to the provided reference.", "main_total_tokens": 2670, "main_prompt_tokens": 2666, "main_completion_tokens": 4, "judge_total_tokens": 452, "judge_prompt_tokens": 392, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "Jiang Jie", "latency_ms": 6135.9052658081055, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Jiang Jie'), so it is correct and fully faithful.", "main_total_tokens": 2821, "main_prompt_tokens": 2814, "main_completion_tokens": 7, "judge_total_tokens": 438, "judge_prompt_tokens": 386, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "50", "latency_ms": 25973.867177963257, "retrieval_recall": 0.0, "retrieval_mrr": 0.07692307692307693, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer (50) does not match the gold answer (4), so it is factually incorrect. There is no indication it is based on the gold reference; it appears to be an arbitrary number, so it is also unfaithful.", "main_total_tokens": 3681, "main_prompt_tokens": 3480, "main_completion_tokens": 201, "judge_total_tokens": 466, "judge_prompt_tokens": 385, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 4566.148519515991, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly conveys that there are no movies matching the criteria (i.e., none with both Bruce Lee and Steven Seagal), which aligns with the gold answer. It is faithful to the question’s intent and does not introduce unsupported details.", "main_total_tokens": 2242, "main_prompt_tokens": 2240, "main_completion_tokens": 2, "judge_total_tokens": 486, "judge_prompt_tokens": 406, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3", "latency_ms": 4945.778846740723, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer (6.3) matches the gold rating exactly, so it is both correct and faithful.", "main_total_tokens": 2292, "main_prompt_tokens": 2288, "main_completion_tokens": 4, "judge_total_tokens": 440, "judge_prompt_tokens": 387, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "Drift", "latency_ms": 7845.231294631958, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Drift'), so it is both correct and faithful.", "main_total_tokens": 3208, "main_prompt_tokens": 3189, "main_completion_tokens": 19, "judge_total_tokens": 430, "judge_prompt_tokens": 379, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer", "latency_ms": 6341.859817504883, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie title as 'Confronting a Serial Killer.'", "main_total_tokens": 2844, "main_prompt_tokens": 2835, "main_completion_tokens": 9, "judge_total_tokens": 466, "judge_prompt_tokens": 412, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838", "latency_ms": 4923.279762268066, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) with correct value and formatting, so it is both correct and faithful.", "main_total_tokens": 2331, "main_prompt_tokens": 2327, "main_completion_tokens": 4, "judge_total_tokens": 448, "judge_prompt_tokens": 389, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense", "latency_ms": 5991.621971130371, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie title 'The Sixth Sense' as the 1999 psychological thriller alluding to an extra sense. It is both correct and faithful.", "main_total_tokens": 2771, "main_prompt_tokens": 2765, "main_completion_tokens": 6, "judge_total_tokens": 465, "judge_prompt_tokens": 394, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002", "latency_ms": 6144.833564758301, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (2002), so it is both correct and faithful.", "main_total_tokens": 2963, "main_prompt_tokens": 2958, "main_completion_tokens": 5, "judge_total_tokens": 434, "judge_prompt_tokens": 383, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled", "latency_ms": 6493.3226108551025, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the title of the 2015 reboot of 'The Transporter' as 'The Transporter Refueled'. There is no added or missing information.", "main_total_tokens": 2965, "main_prompt_tokens": 2956, "main_completion_tokens": 9, "judge_total_tokens": 470, "judge_prompt_tokens": 397, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015", "latency_ms": 6249.707937240601, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2015), so it is both correct and faithful to the reference.", "main_total_tokens": 3078, "main_prompt_tokens": 3073, "main_completion_tokens": 5, "judge_total_tokens": 440, "judge_prompt_tokens": 387, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "I don't know.", "latency_ms": 2622.0030784606934, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no such movie description can be found, which is effectively equivalent to saying the answer is unknown. The generated answer says 'I don't know,' which does not introduce any incorrect information and aligns in meaning with the gold. Therefore it is both correct and faithful.", "main_total_tokens": 560, "main_prompt_tokens": 558, "main_completion_tokens": 2, "judge_total_tokens": 481, "judge_prompt_tokens": 394, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Havana Darkness", "latency_ms": 5775.315523147583, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Havana Darkness' and correctly identifies the movie title that alludes to a manuscript written by Ernest Hemingway. It is both correct and fully faithful to the reference.", "main_total_tokens": 2640, "main_prompt_tokens": 2634, "main_completion_tokens": 6, "judge_total_tokens": 462, "judge_prompt_tokens": 390, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 6122.070074081421, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Doll & Em') and correctly identifies the TV show referenced in the question, so it is both correct and faithful.", "main_total_tokens": 2852, "main_prompt_tokens": 2845, "main_completion_tokens": 7, "judge_total_tokens": 456, "judge_prompt_tokens": 392, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam Foe", "latency_ms": 6148.658752441406, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Hallam Foe', correctly identifying the 2007 movie. It is fully correct and faithful.", "main_total_tokens": 2854, "main_prompt_tokens": 2847, "main_completion_tokens": 7, "judge_total_tokens": 463, "judge_prompt_tokens": 403, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Foe", "latency_ms": 7926.044464111328, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'A foe' and the generated answer is 'Foe', which is semantically equivalent. The response is correct and faithful to the gold.", "main_total_tokens": 2684, "main_prompt_tokens": 2657, "main_completion_tokens": 27, "judge_total_tokens": 447, "judge_prompt_tokens": 388, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18", "latency_ms": 5776.366949081421, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (18), and it is a straightforward factual question with no reasoning discrepancies.", "main_total_tokens": 2579, "main_prompt_tokens": 2575, "main_completion_tokens": 4, "judge_total_tokens": 443, "judge_prompt_tokens": 389, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Autism spectrum", "latency_ms": 5841.951370239258, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying that Sam is on the autism spectrum in the 2017 TV show Atypical.", "main_total_tokens": 2596, "main_prompt_tokens": 2590, "main_completion_tokens": 6, "judge_total_tokens": 449, "judge_prompt_tokens": 389, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix", "latency_ms": 6289.514541625977, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Chamonix' with no additions or omissions, so it is both correct and faithful.", "main_total_tokens": 3043, "main_prompt_tokens": 3037, "main_completion_tokens": 6, "judge_total_tokens": 458, "judge_prompt_tokens": 400, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "it rains", "latency_ms": 6435.2734088897705, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'it rains' matches the gold answer 'rains' in meaning and is correct and faithful to the provided information.", "main_total_tokens": 3189, "main_prompt_tokens": 3184, "main_completion_tokens": 5, "judge_total_tokens": 446, "judge_prompt_tokens": 393, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland", "latency_ms": 6001.798629760742, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and is correct: Tabaluga is the crown prince of Greenland.", "main_total_tokens": 2847, "main_prompt_tokens": 2842, "main_completion_tokens": 5, "judge_total_tokens": 448, "judge_prompt_tokens": 395, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon", "latency_ms": 5578.257083892822, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('One Afternoon'). The description aligns with the film, so the response is both correct and faithful.", "main_total_tokens": 2429, "main_prompt_tokens": 2423, "main_completion_tokens": 6, "judge_total_tokens": 465, "judge_prompt_tokens": 406, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The Life Ahead", "latency_ms": 3390.5131816864014, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'The Life Ahead' as the movie described. It is fully correct and fully faithful to the reference.", "main_total_tokens": 2701, "main_prompt_tokens": 2695, "main_completion_tokens": 6, "judge_total_tokens": 464, "judge_prompt_tokens": 401, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "Bruce McLaren", "latency_ms": 5991.585969924927, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying Bruce McLaren as the subject of the 2016 documentary 'McLaren'. It is both correct and fully faithful to the gold reference.", "main_total_tokens": 2659, "main_prompt_tokens": 2654, "main_completion_tokens": 5, "judge_total_tokens": 457, "judge_prompt_tokens": 387, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren Motor Racing team", "latency_ms": 6496.746778488159, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer in content, differing only by the definite article, which does not change the meaning. It is both correct and faithful to the reference.", "main_total_tokens": 2536, "main_prompt_tokens": 2523, "main_completion_tokens": 13, "judge_total_tokens": 453, "judge_prompt_tokens": 388, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "Sweden", "latency_ms": 6396.674156188965, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'Sweden' is incorrect given the gold answer 'New Zealand.' It is also unfaithful to the gold reference, directly contradicting it.", "main_total_tokens": 2496, "main_prompt_tokens": 2484, "main_completion_tokens": 12, "judge_total_tokens": 441, "judge_prompt_tokens": 377, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "None", "latency_ms": 6186.878681182861, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'None' is not a correct response to the question, as the accurate reply is that there is no such movie; this matches the gold answer. Thus correctness is 0. It is also unfaithful to the gold answer, which explicitly states the database lacks such a movie, so faithfulness is 0.", "main_total_tokens": 3040, "main_prompt_tokens": 3036, "main_completion_tokens": 4, "judge_total_tokens": 497, "judge_prompt_tokens": 399, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit", "latency_ms": 6165.632247924805, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the 2002 Japanese horror film based on the description. It is fully correct and faithful.", "main_total_tokens": 2848, "main_prompt_tokens": 2840, "main_completion_tokens": 8, "judge_total_tokens": 483, "judge_prompt_tokens": 422, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65", "latency_ms": 6488.020420074463, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (65 years), so it is both factually correct and fully faithful to the reference.", "main_total_tokens": 3277, "main_prompt_tokens": 3273, "main_completion_tokens": 4, "judge_total_tokens": 453, "judge_prompt_tokens": 396, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Book of Souls", "latency_ms": 6449.012041091919, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies that they are searching for The Book of Souls. It is both correct and faithful.", "main_total_tokens": 3162, "main_prompt_tokens": 3155, "main_completion_tokens": 7, "judge_total_tokens": 457, "judge_prompt_tokens": 398, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2645.627975463867, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, effectively conveying a lack of information. The generated answer, 'I don't know,' is consistent with this and does not introduce incorrect details. It is both correct and faithful to the intended response.", "main_total_tokens": 569, "main_prompt_tokens": 567, "main_completion_tokens": 2, "judge_total_tokens": 477, "judge_prompt_tokens": 395, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2637.8324031829834, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly convey lack of knowledge about the movie, and the generated response is faithful to the information in the prompt.", "main_total_tokens": 605, "main_prompt_tokens": 603, "main_completion_tokens": 2, "judge_total_tokens": 452, "judge_prompt_tokens": 393, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2500.7259845733643, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly indicate lack of knowledge about the movie's release year. The generated answer is factually equivalent and faithful to the gold response in content and intent.", "main_total_tokens": 550, "main_prompt_tokens": 548, "main_completion_tokens": 2, "judge_total_tokens": 462, "judge_prompt_tokens": 395, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2486.7544174194336, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer convey that the system does not have the information / does not know. The question itself has no known factual answer since the movie is not in the database, so saying 'I don't know' is effectively correct given the context. The generated response faithfully reflects the underlying knowledge state implied by the gold answer.", "main_total_tokens": 578, "main_prompt_tokens": 576, "main_completion_tokens": 2, "judge_total_tokens": 494, "judge_prompt_tokens": 395, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "2002", "latency_ms": 5881.196022033691, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer claims a specific release year (2002) for a movie that, according to the gold answer, is not in the database and likely does not exist. This makes the answer factually incorrect relative to the provided ground truth. It is also unfaithful because the model should have indicated lack of information, matching the gold response, rather than fabricating a date.", "main_total_tokens": 2859, "main_prompt_tokens": 2854, "main_completion_tokens": 5, "judge_total_tokens": 501, "judge_prompt_tokens": 394, "judge_completion_tokens": 107}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2905.0843715667725, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer express lack of knowledge about the movie. The generated answer is correct given the task and faithful to the gold response, just phrased more concisely.", "main_total_tokens": 545, "main_prompt_tokens": 543, "main_completion_tokens": 2, "judge_total_tokens": 463, "judge_prompt_tokens": 393, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2479.761838912964, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer and the generated answer are semantically equivalent: both express lack of knowledge about the movie. The generated response does not introduce any incorrect facts and is fully faithful to the gold intent.", "main_total_tokens": 569, "main_prompt_tokens": 567, "main_completion_tokens": 2, "judge_total_tokens": 466, "judge_prompt_tokens": 396, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of \"The Last Days of Lehman Brothers\" is not mentioned in the provided context.", "latency_ms": 7956.380605697632, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the database does not provide the director. The generated answer correctly states that the director is not mentioned in the provided context, matching the limitation. It does not hallucinate a director name, so it is both correct with respect to the task and faithful to the given information.", "main_total_tokens": 3204, "main_prompt_tokens": 3181, "main_completion_tokens": 23, "judge_total_tokens": 506, "judge_prompt_tokens": 417, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 2638.458728790283, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the system cannot answer because the database lacks the director information. The generated answer says 'I don't know,' which is effectively the same epistemic stance—no fabricated director is given. Thus it is correct relative to the gold and fully faithful to the provided information.", "main_total_tokens": 587, "main_prompt_tokens": 585, "main_completion_tokens": 2, "judge_total_tokens": 486, "judge_prompt_tokens": 399, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of \"Journey into Gaza\" is not mentioned in the provided context.", "latency_ms": 7220.927476882935, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both gold and generated answers state that the director is not provided/mentioned in the available information, so the model correctly abstains and stays faithful to the context.", "main_total_tokens": 2843, "main_prompt_tokens": 2823, "main_completion_tokens": 20, "judge_total_tokens": 472, "judge_prompt_tokens": 409, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "There is no information about the director of \"The Last Night at Tremore Beach\" in the provided context.", "latency_ms": 7472.041845321655, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer state that the director information is not available in the provided data. This matches the constraints of the database/context and does not hallucinate a director. Thus the answer is correct with respect to the task and fully faithful to the given context.", "main_total_tokens": 2617, "main_prompt_tokens": 2592, "main_completion_tokens": 25, "judge_total_tokens": 505, "judge_prompt_tokens": 419, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 6090.865135192871, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Rise of the Mummy') and correctly identifies the movie based on the description, with no added or incorrect information.", "main_total_tokens": 2631, "main_prompt_tokens": 2622, "main_completion_tokens": 9, "judge_total_tokens": 459, "judge_prompt_tokens": 396, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 6081.301212310791, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Stranger in My Bed') and is faithful to the given information.", "main_total_tokens": 2742, "main_prompt_tokens": 2734, "main_completion_tokens": 8, "judge_total_tokens": 454, "judge_prompt_tokens": 400, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 5418.858289718628, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer and the generated answer are identical ('Game of Silence'), and they correctly identify the work described, so the response is both correct and faithful.", "main_total_tokens": 2303, "main_prompt_tokens": 2297, "main_completion_tokens": 6, "judge_total_tokens": 462, "judge_prompt_tokens": 400, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 6063.843250274658, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct movie title for the described plot. It is both correct and fully faithful to the reference.", "main_total_tokens": 2634, "main_prompt_tokens": 2628, "main_completion_tokens": 6, "judge_total_tokens": 457, "judge_prompt_tokens": 395, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 6281.07476234436, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Wood Job!' and correctly identifies the movie based on the plot description, with no additional incorrect information.", "main_total_tokens": 3018, "main_prompt_tokens": 3012, "main_completion_tokens": 6, "judge_total_tokens": 463, "judge_prompt_tokens": 403, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 7591.479778289795, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase'. It is fully correct and faithful to the provided gold reference.", "main_total_tokens": 2794, "main_prompt_tokens": 2772, "main_completion_tokens": 22, "judge_total_tokens": 484, "judge_prompt_tokens": 417, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond", "latency_ms": 5356.740713119507, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Beyond'). The description corresponds to the film 'Beyond,' so the answer is correct and fully faithful to the provided gold label.", "main_total_tokens": 2349, "main_prompt_tokens": 2345, "main_completion_tokens": 4, "judge_total_tokens": 474, "judge_prompt_tokens": 409, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9", "latency_ms": 5835.676908493042, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('6ixtynin9'), correctly identifying the movie and faithfully reflecting the reference.", "main_total_tokens": 2447, "main_prompt_tokens": 2438, "main_completion_tokens": 9, "judge_total_tokens": 466, "judge_prompt_tokens": 409, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread", "latency_ms": 6005.021810531616, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Club Dread'), correctly identifying the movie from the description and remaining faithful to the provided information.", "main_total_tokens": 2795, "main_prompt_tokens": 2790, "main_completion_tokens": 5, "judge_total_tokens": 486, "judge_prompt_tokens": 427, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet", "latency_ms": 6217.018365859985, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title matching the described plot; the gold answer is 'Toilet' and the generated answer is also 'Toilet'. This is correct and fully consistent with the provided gold answer and description.", "main_total_tokens": 3031, "main_prompt_tokens": 3026, "main_completion_tokens": 5, "judge_total_tokens": 490, "judge_prompt_tokens": 415, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel", "latency_ms": 5821.530103683472, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Electric Angel'). The reasoning aligns fully with the provided reference, so it is both correct and faithful.", "main_total_tokens": 2687, "main_prompt_tokens": 2682, "main_completion_tokens": 5, "judge_total_tokens": 478, "judge_prompt_tokens": 418, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "5.207", "latency_ms": 4345.240354537964, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'Mega Ape' while the generated answer is '5.207', which is not a movie title and does not match the correct response. It is therefore factually incorrect and not faithful to the gold.", "main_total_tokens": 2589, "main_prompt_tokens": 2583, "main_completion_tokens": 6, "judge_total_tokens": 461, "judge_prompt_tokens": 385, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 312.502384185791, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific fact (the highest rated TV show from 2010, with gold answer 'Tea House'). The generated answer says 'I don't know,' which does not provide the correct answer. Therefore correctness is 0. However, the model is honest about its lack of knowledge and does not hallucinate or contradict given information, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 488, "judge_prompt_tokens": 386, "judge_completion_tokens": 102}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 245.23067474365234, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which is incorrect because the gold answer provides a specific movie title. However, the model does not hallucinate an alternative answer and honestly reports uncertainty, so it is fully faithful to its own (lack of) knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 395, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "French", "latency_ms": 5317.862987518311, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the gold answer says the language is English, while the generated answer says French. However, the model’s answer is internally consistent and directly addresses the question, so it is faithful to the user query even though it is factually wrong.", "main_total_tokens": 3000, "main_prompt_tokens": 2996, "main_completion_tokens": 4, "judge_total_tokens": 468, "judge_prompt_tokens": 384, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity is Freaks and Geeks, with a popularity rating of 5.8043.", "latency_ms": 8838.050127029419, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold label states that the TV show is 'Law & Order: Special Victims Unit', while the model responded with 'Freaks and Geeks'. It is also unfaithful to the gold answer, explicitly contradicting it.", "main_total_tokens": 2936, "main_prompt_tokens": 2904, "main_completion_tokens": 32, "judge_total_tokens": 504, "judge_prompt_tokens": 419, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2", "latency_ms": 4888.197422027588, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer (7.2) exactly matches the gold rating (7.2), so it is both correct and faithful.", "main_total_tokens": 2262, "main_prompt_tokens": 2258, "main_completion_tokens": 4, "judge_total_tokens": 443, "judge_prompt_tokens": 386, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "1999", "latency_ms": 6031.079053878784, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1999) does not match the gold answer (2019), so it is factually incorrect with respect to the specified gold label. However, the assistant is consistently answering the user question and not introducing unrelated information, so it is faithful to its own (albeit incorrect) response.", "main_total_tokens": 2831, "main_prompt_tokens": 2826, "main_completion_tokens": 5, "judge_total_tokens": 479, "judge_prompt_tokens": 387, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "1", "latency_ms": 4789.798736572266, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1) does not match the gold answer (279), so it is incorrect. However, it appears to be a direct attempt to answer the question without adding unsupported information, so it is faithful to the prompt.", "main_total_tokens": 2203, "main_prompt_tokens": 2201, "main_completion_tokens": 2, "judge_total_tokens": 455, "judge_prompt_tokens": 383, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "6", "latency_ms": 15897.082090377808, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (6) does not match the gold answer (12), so it is factually incorrect. However, it directly answers the question with a numeric value in the same format as the gold, so it is faithful to the user query and not hallucinating a different kind of response.", "main_total_tokens": 2410, "main_prompt_tokens": 2287, "main_completion_tokens": 123, "judge_total_tokens": 473, "judge_prompt_tokens": 383, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "2009", "latency_ms": 6752.722978591919, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2009), so it is both correct and fully supported by the provided reference.", "main_total_tokens": 3171, "main_prompt_tokens": 3166, "main_completion_tokens": 5, "judge_total_tokens": 445, "judge_prompt_tokens": 390, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_llama3.1-70b-iq2_xs", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en", "latency_ms": 5987.1320724487305, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The movie \"The King's Speech\" is originally in English. The generated answer matches the gold answer and is both correct and faithful to the question.", "main_total_tokens": 2874, "main_prompt_tokens": 2870, "main_completion_tokens": 4, "judge_total_tokens": 444, "judge_prompt_tokens": 384, "judge_completion_tokens": 60}
