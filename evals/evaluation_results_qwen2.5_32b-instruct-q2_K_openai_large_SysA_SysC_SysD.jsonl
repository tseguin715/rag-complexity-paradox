{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "Wall Street was released in the year 1987 [Doc 6993].", "latency_ms": 28536.041021347046, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, which matches the gold answer. It does not introduce any contradictions or unsupported information beyond a generic document reference.", "main_total_tokens": 3203, "main_prompt_tokens": 3182, "main_completion_tokens": 21, "judge_total_tokens": 461, "judge_prompt_tokens": 394, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "In the movie \"Brain Fluid Explosion Girl\" [Doc 69929], Hana Ichii and her classmates are forced to take part in a survival game using their cellphones.", "latency_ms": 3406.9907665252686, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Brain Fluid Explosion Girl' and accurately restates the premise about classmates forced into a survival game using their cell phones. It is fully consistent with the gold answer and does not introduce contradictions or unsupported claims.", "main_total_tokens": 3142, "main_prompt_tokens": 3102, "main_completion_tokens": 40, "judge_total_tokens": 503, "judge_prompt_tokens": 423, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show where six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons,\" which aired in 1983 [Doc 112849].", "latency_ms": 3553.882360458374, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.4306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.719741384391281, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Dungeons & Dragons,\" matching the gold answer exactly. The added detail about it airing in 1983 is accurate and consistent with known facts, and there is no hallucinated or contradictory information. Therefore, it is both correct and faithful.", "main_total_tokens": 3325, "main_prompt_tokens": 3283, "main_completion_tokens": 42, "judge_total_tokens": 514, "judge_prompt_tokens": 425, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie that matches your description is \"Nacht der Wölfe\" [Doc 2590]. This film tells the story of two rival gangs in Munich, with one gang composed of German teens and the other of Turkish teens.", "latency_ms": 4026.130437850952, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, naming 'Nacht der Wölfe' as the movie about two rival gangs in Munich (German and Turkish teens), and stays consistent with the described context.", "main_total_tokens": 3114, "main_prompt_tokens": 3064, "main_completion_tokens": 50, "judge_total_tokens": 517, "judge_prompt_tokens": 445, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of Picnic at Hanging Rock take place in the year 1900 [Doc 126191].", "latency_ms": 3554.7425746917725, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of Picnic at Hanging Rock take place in the year 1900, which matches the gold answer. It is also faithful, as it directly answers the question without adding unsupported information.", "main_total_tokens": 2943, "main_prompt_tokens": 2914, "main_completion_tokens": 29, "judge_total_tokens": 481, "judge_prompt_tokens": 406, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie described is \"June.\" It tells the story of June, a young violinist who works in an oddball Venice coffee shop during the day, rehearses for a professional music career at night, and spends all her time daydreaming about meeting her destined love. The plot revolves around June's life and her romantic interests [Doc 32483].", "latency_ms": 4369.048118591309, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.95, "justification": "The generated answer correctly identifies the movie as \"June,\" matching the gold answer. The description of the plot aligns with the question and appears plausible, but the explicit citation of a specific document ID ([Doc 32483]) is fabricated and not grounded in the provided context, slightly reducing faithfulness.", "main_total_tokens": 3453, "main_prompt_tokens": 3372, "main_completion_tokens": 81, "judge_total_tokens": 599, "judge_prompt_tokens": 507, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "The highest rated Russian-language movie released in 1980 based on the provided context is \"Moscow Does Not Believe in Tears\" with a rating of 7.6 [Doc 61].", "latency_ms": 3487.502098083496, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates that the highest rated Russian-language movie released in 1980 is 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger'. The generated answer instead claims it is 'Moscow Does Not Believe in Tears' with a rating of 7.6, which directly contradicts the gold. Therefore, it is factually incorrect with respect to the task. Additionally, the generated answer references a document [Doc 61] and claims to be based on provided context, but in this evaluation setting we cannot verify that context and it clearly conflicts with the gold label, so it is not faithful to the ground truth for this task.", "main_total_tokens": 3139, "main_prompt_tokens": 3096, "main_completion_tokens": 43, "judge_total_tokens": 596, "judge_prompt_tokens": 434, "judge_completion_tokens": 162}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest-rated Russian-language movie from 1999 based on the provided documents is \"The Rifleman of the Voroshilov Regiment\" with a rating of 7.117 [Doc 22123].", "latency_ms": 3462.2907638549805, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the highest Russian-language movie rating in 1999 is 7.6, while the generated answer claims a rating of 7.117, so it is numerically incorrect. It also references a specific document and movie title that contradict the gold label, so it is not faithful to the provided ground truth for this evaluation.", "main_total_tokens": 3017, "main_prompt_tokens": 2967, "main_completion_tokens": 50, "judge_total_tokens": 527, "judge_prompt_tokens": 427, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "Based on the provided documents, there are three movies that meet all the criteria: released in 1987, have a rating over 7, and are in Chinese (cn):\n\n1. **A Chinese Ghost Story** [Doc 7064] - Rating: 7.3\n2. **Last Eunuch In China** [Doc 7754] - Rating: 7.8\n3. **The Last Emperor** [Doc 6989] - Rating: 7.599\n\nTherefore, the answer is 3 movies.", "latency_ms": 5136.342525482178, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer is 4, but the generated answer states 3, so the correctness is wrong. The reasoning appears grounded in specific documents listing three qualifying movies, but it misses at least one additional qualifying film, indicating incomplete use or recall of the source data. Thus, it is partially faithful to the provided documents (no clear hallucinations about those three), but not fully faithful to the full set of relevant documents.", "main_total_tokens": 3176, "main_prompt_tokens": 3053, "main_completion_tokens": 123, "judge_total_tokens": 620, "judge_prompt_tokens": 503, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "Based on the provided documents, there is only one TV show that meets all the criteria: it was released in 1987, has a rating over 7, and is in Chinese. This is \"Dream of the Red Chamber\" with a rating of 9.4 [Doc 113285].", "latency_ms": 4427.474021911621, "retrieval_recall": 0.0, "retrieval_mrr": 0.07142857142857142, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there are 2 such TV shows, but the generated answer claims there is only one, so the numerical result is incorrect. However, the assistant correctly and consistently describes what is in the provided documents (identifying one matching show and its details), so it is faithful to the given context even though it disagrees with the external gold label.", "main_total_tokens": 3062, "main_prompt_tokens": 2994, "main_completion_tokens": 68, "judge_total_tokens": 557, "judge_prompt_tokens": 454, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "Based on the provided documents, there isn't a specific mention of any French language TV show that was released exclusively in 1980. Document [Doc 112642] mentions \"Fantômas,\" which is described as a TV series from 1980 with French language, but it's not explicitly stated to be the only one. Other documents list items from different years or do not specify release details that confirm exclusivity for a French language TV show in 1980.", "latency_ms": 4603.189945220947, "retrieval_recall": 1.0, "retrieval_mrr": 0.2, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.38685280723454163, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.38685280723454163, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Fantômas' as the only French language TV show released in 1980. The generated answer claims that there is no specific mention of a unique French TV show in 1980 and therefore does not provide 'Fantômas' as the answer, making it factually incorrect relative to the gold. However, it accurately reflects the content it describes from the documents (e.g., that Fantômas is a 1980 French TV series but not stated as the only one), so it is faithful to its stated source analysis.", "main_total_tokens": 3073, "main_prompt_tokens": 2967, "main_completion_tokens": 106, "judge_total_tokens": 621, "judge_prompt_tokens": 478, "judge_completion_tokens": 143}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie that features two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" [Doc 11205].", "latency_ms": 3466.5355682373047, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' which matches the gold answer. It also closely follows the referenced document, so it is both correct and faithful to the provided information.", "main_total_tokens": 3219, "main_prompt_tokens": 3188, "main_completion_tokens": 31, "judge_total_tokens": 479, "judge_prompt_tokens": 410, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie that discusses Fred Rogers speaking to children about some of life's most important issues through his television program is \"Won't You Be My Neighbor?\" [Doc 84838]", "latency_ms": 4267.412185668945, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the title of the movie as 'Won't You Be My Neighbor?', which matches the gold answer exactly. It is fully faithful to the provided information, adding only a brief, accurate restatement of the question context without introducing errors or unsupported claims.", "main_total_tokens": 3581, "main_prompt_tokens": 3538, "main_completion_tokens": 43, "judge_total_tokens": 524, "judge_prompt_tokens": 439, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The document does not specify a single release year for an unnamed movie, but rather provides details on various movies with different release years. For example:\n\n- \"Year of the Horse\" was released in 1997 [Doc 19452].\n- \"YEAR\" was released in 2023 [Doc 107925].\n- \"What It Was\" was released in 2014 [Doc 65131].\n\nIf you are asking about a specific movie, please clarify which one.", "latency_ms": 4931.142568588257, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is a specific year (2018), so the correct response should be that year. The generated answer instead claims the document does not specify a single release year and lists several unrelated films with different years, failing to provide 2018. This is incorrect relative to the gold. However, within its own reasoning, it accurately reflects that it cannot identify a single year and cites example films and years consistently, so it is faithful to the (incorrect) model reasoning rather than hallucinating a definite year for the asked item.", "main_total_tokens": 3417, "main_prompt_tokens": 3304, "main_completion_tokens": 113, "judge_total_tokens": 610, "judge_prompt_tokens": 473, "judge_completion_tokens": 137}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that fits your description is \"Astro Boy\" from [Doc 112591]. It's set in 2030 AD and revolves around a young boy robot who, after being abandoned by his creator, is rescued and adopted by Dr. Elefun, who guides him to use his special powers to aid and protect mankind and robot-kind.", "latency_ms": 4392.173767089844, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy,' matching the gold answer exactly. The additional description about the setting (2030 AD) and plot details about the boy robot and Dr. Elefun are consistent with the known premise of the series and do not contradict the question or gold answer, so the response is both correct and faithful.", "main_total_tokens": 3604, "main_prompt_tokens": 3526, "main_completion_tokens": 78, "judge_total_tokens": 559, "judge_prompt_tokens": 457, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 3153.5284519195557, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: both state they were held captive for 7 years. It is correct and faithful to the reference.", "main_total_tokens": 2780, "main_prompt_tokens": 2744, "main_completion_tokens": 36, "judge_total_tokens": 484, "judge_prompt_tokens": 424, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26m when it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 3471.4345932006836, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before the break-in on 26 November 1983, matching the provided gold answer exactly and remaining consistent with the cited document.", "main_total_tokens": 3121, "main_prompt_tokens": 3077, "main_completion_tokens": 44, "judge_total_tokens": 506, "judge_prompt_tokens": 428, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "To determine the popularity score of the most popular English-speaking TV show released in 1997, we need to compare the popularity scores of all the English-language shows from that year. Here are the relevant entries:\n\n- [Doc 114544] Title: 7th Heaven | Popularity: 8.3175\n- [Doc 114769] Title: USA High | Popularity: 3.535\n- [Doc 114728] Title: Just Shoot Me! | Popularity: 7.771\n\nAmong these, the highest popularity score is for \"7th Heaven\" with a score of **8.3175**.\n\nTherefore, the popularity score of the most popular English-speaking TV show released in 1997 is **8.3175**.", "latency_ms": 5996.817588806152, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.7, "justification": "The gold answer is 80.5772, while the generated answer is 8.3175, so the numerical result is clearly incorrect (score 0 for correctness). The reasoning process is partially grounded in the provided documents (it lists specific shows and their popularity scores) but is incomplete and misses the actual most popular show and its much higher score; it also implicitly assumes the provided list is exhaustive without justification. Thus it is only partially faithful to the true evidence, warranting a faithfulness score below 1 but above 0.", "main_total_tokens": 3055, "main_prompt_tokens": 2870, "main_completion_tokens": 185, "judge_total_tokens": 691, "judge_prompt_tokens": 550, "judge_completion_tokens": 141}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "Based on the provided documents, there is no information about any Russian language TV shows that were released specifically in 1997. The closest document provides details for a movie titled \"The Red and the Black\" which was released in 1997 but it is listed as a French language production [Doc 19935].", "latency_ms": 3845.7870483398438, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states the count is 1, so the correct response should be 1. The generated response says there is no information about any Russian language TV shows released in 1997, implying the answer cannot be determined, which contradicts the gold. Thus, the answer is factually incorrect relative to the gold (score 0). However, it accurately reflects what it claims about the provided documents (that they do not show any such TV show and that the closest is a French movie), so it is faithful to the described context (score 1).", "main_total_tokens": 2909, "main_prompt_tokens": 2838, "main_completion_tokens": 71, "judge_total_tokens": 590, "judge_prompt_tokens": 446, "judge_completion_tokens": 144}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 3674.3147373199463, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Angel Eyes'. It is fully supported by the provided information and contains no additional unsupported claims.", "main_total_tokens": 3429, "main_prompt_tokens": 3389, "main_completion_tokens": 40, "judge_total_tokens": 494, "judge_prompt_tokens": 429, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French [Doc 110681].", "latency_ms": 3174.631118774414, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer claims the movie language is French, while the gold label is Korean, so it is factually incorrect. It also introduces an unsupported movie title (\"Foreign Language\") and a document reference with no basis in the given QA pair, so it is not faithful to the provided information.", "main_total_tokens": 3283, "main_prompt_tokens": 3261, "main_completion_tokens": 22, "judge_total_tokens": 483, "judge_prompt_tokens": 393, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "Based on the provided documents, there are no English-speaking (language: en) horror movies from 2009 with a popularity score greater than 7. The highest popularity scores for English-speaking horror movies from 2009 are:\n\n- \"Possessed\" with a popularity of 1.9113 [Doc 45493]\n- \"Perkins' 14\" with a popularity of 1.1125 [Doc 46432]\n\nBoth are below the threshold of 7.", "latency_ms": 4617.5172328948975, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 2 such movies, while the generated answer claims there are none, so it is factually incorrect relative to the gold. However, the generated answer is internally consistent with the documents it cites and does not invent unsupported details, so it is faithful to its given context.", "main_total_tokens": 3045, "main_prompt_tokens": 2931, "main_completion_tokens": 114, "judge_total_tokens": 576, "judge_prompt_tokens": 490, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The provided context does not contain any information about movies released in 1979, so I cannot determine the best-rated movie from that year based on the given documents.", "latency_ms": 3324.50795173645, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Given the gold answer that the database has no movies from 1979, the generated answer correctly states that the context does not contain any information about movies released in 1979 and thus it cannot determine the best-rated movie for that year. This is both correct with respect to the task and faithful to the provided context.", "main_total_tokens": 3083, "main_prompt_tokens": 3047, "main_completion_tokens": 36, "judge_total_tokens": 523, "judge_prompt_tokens": 428, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "None of the documents provided contain information about a movie titled \"Back to the Future Part IV.\" The documents mention movies like \"Back to the Future,\" \"Back to the Future Part II,\" and \"Back to the Future Part III,\" but there is no reference to a fourth part in any of these documents.", "latency_ms": 4281.565427780151, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer fails to explicitly state that Back to the Future Part IV does not exist or that no release year exists; it only mentions that the provided documents lack information. Therefore, it does not correctly answer the user’s question about the release year. It is also unfaithful to the gold answer, which clearly states that the movie does not exist in the database, while the generated answer frames it merely as an absence of information in the documents, implying uncertainty rather than nonexistence.", "main_total_tokens": 3683, "main_prompt_tokens": 3620, "main_completion_tokens": 63, "judge_total_tokens": 591, "judge_prompt_tokens": 462, "judge_completion_tokens": 129}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "None of the documents provided contain information on a movie from 1980 that deals with the nuclear Chernobyl disaster. The earliest mentioned is \"Chornobyl: Chronicle of Difficult Weeks\" from 1990 [Doc 11722]. Therefore, based on the given context, there's no popularity score to report for a movie from 1980 about the Chernobyl disaster.", "latency_ms": 4579.2646408081055, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer incorrectly states that the earliest mentioned Chernobyl-related movie is from 1990, which conflicts with the gold answer indicating that no such movies from 1980 exist at all. It also introduces specific document-based details (e.g., Doc 11722 and a 1990 film) that are not supported by the gold answer, making it unfaithful to the reference and factually incorrect relative to the task's ground truth.", "main_total_tokens": 3479, "main_prompt_tokens": 3390, "main_completion_tokens": 89, "judge_total_tokens": 603, "judge_prompt_tokens": 482, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are three entries with \"Astro Boy\" in the title:\n\n1. Astro Boy (Movie, 2009) [Doc 44947]\n2. Astro Boy (TV, 2003) [Doc 116265]\n3. Astro Boy vs The Junkyard Pirates (Movie, 2010) [Doc 49121]\n\nTherefore, the answer is 3 TV shows or movies have 'Astro Boy' in the title.", "latency_ms": 4694.351434707642, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 1.1510196182278052, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.773714921545009, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states there are 3 titles with 'Astro Boy' in the title, but the gold standard says the correct number is 4, so the answer is factually incorrect. However, the assistant’s reasoning is internally consistent with the count it provides and does not contradict its own listed items, so it is faithful to its own chain of thought, just incomplete relative to the gold.", "main_total_tokens": 3219, "main_prompt_tokens": 3111, "main_completion_tokens": 108, "judge_total_tokens": 588, "judge_prompt_tokens": 477, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "Based on the provided documents, there are no movies explicitly described as alluding to the Holocaust for the year 2025. However, two documentaries listed under 2025 do not directly relate to the Holocaust:\n\n1. \"Enigma\" focuses on the identities of April Ashley and Amanda Lear.\n2. \"Gaza: How to Survive a Warzone\" is about life in Gaza during conflict with Israel.\n\nNone of these are described as alluding to the Holocaust. [Doc 112426], [Doc 112486]", "latency_ms": 5072.689533233643, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 2 such movies, but the generated answer concludes there are no movies in 2025 that allude to the Holocaust, so it is factually incorrect with respect to the gold. However, the answer is faithful to the documents as summarized in the explanation: it clearly states what the docs say about the two listed films and that neither is described as alluding to the Holocaust, without inventing unsupported details.", "main_total_tokens": 3532, "main_prompt_tokens": 3412, "main_completion_tokens": 120, "judge_total_tokens": 610, "judge_prompt_tokens": 491, "judge_completion_tokens": 119}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "Based on the provided documents, there are three TV shows related to Super Mario Bros that were released in the 1990s:\n\n1. **The Adventures of Super Mario Bros. 3** - Released in 1990 [Doc 113661].\n2. **Super Mario World** - Released in 1991 [Doc 113814].\n\nThese are the only TV shows mentioned that were released during the 1990s decade.", "latency_ms": 4805.485010147095, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says there was 1 TV show; the generated answer concludes there are three (though only two are listed), so the count is incorrect. It also explicitly contradicts the gold, so faithfulness to the gold standard is low.", "main_total_tokens": 3449, "main_prompt_tokens": 3344, "main_completion_tokens": 105, "judge_total_tokens": 555, "judge_prompt_tokens": 475, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "Based on the provided documents, there is only one TV show related to Super Mario Bros that was released between 2000 and 2010. This is \"Super Mario World\" which aired in 1991 [Doc 113814]. However, since it was released before the specified time frame (2000-2010), there are no TV shows about Super Mario Bros that were released between 2000 and 2010 mentioned in the provided documents.", "latency_ms": 4512.70866394043, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the number of TV shows released between 2000 and 2010 about Super Mario Bros, and the gold answer is 0. The generated answer first claims there is one TV show in that period, then contradicts itself by concluding there are none in the specified timeframe, and also cites a show from 1991 as being between 2000 and 2010. This is logically inconsistent and incorrect, so correctness is 0. It also refers to \"provided documents\" and Doc 113814, which are not present in the conversation, so its reasoning is not grounded in the given context, making faithfulness 0.", "main_total_tokens": 2986, "main_prompt_tokens": 2875, "main_completion_tokens": 111, "judge_total_tokens": 643, "judge_prompt_tokens": 481, "judge_completion_tokens": 162}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "Based on the provided documents, the most popular TV show or movie about ninjas is \"Supah Ninjas,\" which has a popularity rating of 2.8397 [Doc 119858]. This series aired regularly and was renewed for a second season, indicating its popularity among viewers.", "latency_ms": 4092.9737091064453, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto', a globally popular ninja-themed anime. The generated answer instead selects 'Supah Ninjas' based only on a local 'popularity rating' from the provided documents, which is not a correct answer to the general question of the most popular ninja TV show or movie. Thus, it is factually incorrect relative to the gold standard. However, the answer is faithful to the cited document: it correctly reports that 'Supah Ninjas' has a popularity rating of 2.8397 in Doc 119858 and that it was renewed for a second season, indicating popularity in that dataset. Therefore, correctness is 0 while faithfulness to the source is 1.", "main_total_tokens": 3340, "main_prompt_tokens": 3275, "main_completion_tokens": 65, "judge_total_tokens": 613, "judge_prompt_tokens": 440, "judge_completion_tokens": 173}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "There is no information provided about a Jurassic Park/Power Rangers crossover movie. The documents list separate entries for movies in each franchise but do not mention any crossovers between the two. [Doc 14060] [Doc 76939] [Doc 86272]", "latency_ms": 3909.980058670044, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a release date of a Jurassic Park/Power Rangers crossover movie. The gold answer clarifies that no such movie was released. The generated answer correctly states that there is no information about such a crossover movie and that the documents only list separate entries, implying the crossover does not exist. This aligns with the gold answer. The response is consistent with the provided documents and does not introduce unsupported claims, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3178, "main_prompt_tokens": 3113, "main_completion_tokens": 65, "judge_total_tokens": 562, "judge_prompt_tokens": 439, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "The provided documents do not mention any movie where Barbie fights a Predator. Therefore, based on the given context, there is no movie in which Barbie fights the Predator.", "latency_ms": 3669.2168712615967, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about a non-existent movie; the gold answer clarifies that such a movie was not released. The generated answer correctly states that there is no movie in which Barbie fights the Predator, which is equivalent in meaning. It is consistent with the given information and does not invent details, so it is both correct and faithful.", "main_total_tokens": 3528, "main_prompt_tokens": 3494, "main_completion_tokens": 34, "judge_total_tokens": 512, "judge_prompt_tokens": 415, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The provided documents do not contain information about the chronological order of Star Wars movies or their release dates. Therefore, based on the given context, it's not possible to determine what the 4th oldest Star Wars movie is.", "latency_ms": 3796.8549728393555, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the 4th oldest Star Wars movie and the gold answer is a specific title, indicating the correct answer is known and does not depend on the provided documents. The generated answer claims it cannot be determined from the context, which is incorrect relative to the gold answer, so correctness is 0. However, the generated answer accurately reflects the limitation of the provided documents and does not fabricate an answer, so it is faithful to the stated constraints, giving faithfulness a score of 1.", "main_total_tokens": 3507, "main_prompt_tokens": 3461, "main_completion_tokens": 46, "judge_total_tokens": 565, "judge_prompt_tokens": 433, "judge_completion_tokens": 132}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The document does not specify a single release year for an unnamed movie, but rather provides details on various movies with different release years. For example:\n\n- \"Year of the Horse\" was released in 1997 [Doc 19452].\n- \"YEAR\" was released in 2023 [Doc 107925].\n- \"What It Was\" was released in 2014 [Doc 65131].\n\nIf you are asking about a specific movie, please clarify which one.", "latency_ms": 4829.988718032837, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The gold answer clearly indicates the intended answer is the single year 1986, so the correct response should have been '1986'. The generated answer instead claims there is no single release year and lists unrelated examples, so it is factually incorrect for this QA pair. It is mostly faithful to the (hypothetical) document it references by discussing multiple movies and their years, but since the evaluation is against the gold answer, the content is not aligned with the required specific movie/year, so faithfulness is not perfect.", "main_total_tokens": 3417, "main_prompt_tokens": 3304, "main_completion_tokens": 113, "judge_total_tokens": 606, "judge_prompt_tokens": 473, "judge_completion_tokens": 133}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The rating of the movie Forrest Gump is 8.466 [Doc 15236].", "latency_ms": 3252.441883087158, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating value 8.466 and does not introduce contradictions or unsupported claims. It is both correct and faithful.", "main_total_tokens": 3234, "main_prompt_tokens": 3209, "main_completion_tokens": 25, "judge_total_tokens": 473, "judge_prompt_tokens": 413, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 3184.129238128662, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the show as 'Mighty Med' and matches the gold answer exactly, with no extra unsupported claims beyond the citation styling.", "main_total_tokens": 2914, "main_prompt_tokens": 2877, "main_completion_tokens": 37, "judge_total_tokens": 484, "judge_prompt_tokens": 423, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In \"Túnel de los huesos\" (2011), seven prisoners escaped from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 3823.2860565185547, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the country where the prison was located. The gold answer is Argentina. The generated answer states that the prisoners escaped from the Villa Devoto prison in Buenos Aires, Argentina, which correctly includes the country Argentina and is consistent with the gold. It does not contradict the gold and only adds more specific but accurate detail, so it is both correct and faithful.", "main_total_tokens": 3670, "main_prompt_tokens": 3630, "main_completion_tokens": 40, "judge_total_tokens": 530, "judge_prompt_tokens": 425, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice, Li Nai, falls into the trap designed by Jiang Jie [Doc 135231].", "latency_ms": 3545.0901985168457, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies that the apprentice falls into Jiang Jie's trap, matching the gold answer. However, it adds the specific name 'Li Nai' for the apprentice, which is not supported or referenced in the gold answer and may not be grounded in the provided context, slightly reducing faithfulness.", "main_total_tokens": 3203, "main_prompt_tokens": 3165, "main_completion_tokens": 38, "judge_total_tokens": 506, "judge_prompt_tokens": 413, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "Based on the provided documents, there are two Korean-language movies that mention assassins after 2023:\n\n1. \"The Assassin\" (Year: 2023) [Doc 106219]\n2. \"Kill Boksoon\" (Year: 2023) [Doc 105198]\n\nBoth movies are from the year 2023 and have Korean as their language.", "latency_ms": 4520.979881286621, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that there are 4 Korean-language movies mentioning assassins after 2023, while the generated answer claims there are 2. This directly contradicts the gold, so the correctness is 0. However, the generated answer is internally consistent with the documents it cites and does not hallucinate beyond them, so it is faithful to the provided context, yielding a faithfulness score of 1.", "main_total_tokens": 3192, "main_prompt_tokens": 3098, "main_completion_tokens": 94, "judge_total_tokens": 573, "judge_prompt_tokens": 459, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "The provided documents do not contain any information about movies that feature both Steven Seagal and Bruce Lee together. Therefore, based on the given context, there are no movies listed that include both actors.", "latency_ms": 3739.3476963043213, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information about any movies featuring both Steven Seagal and Bruce Lee, which aligns with the gold answer. It remains within the provided context and does not introduce unsupported claims, so it is both correct and faithful.", "main_total_tokens": 3322, "main_prompt_tokens": 3282, "main_completion_tokens": 40, "judge_total_tokens": 513, "judge_prompt_tokens": 432, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 3322.7648735046387, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (6.3) and attributes it correctly. It is both correct and faithful to the given information.", "main_total_tokens": 3102, "main_prompt_tokens": 3076, "main_completion_tokens": 26, "judge_total_tokens": 463, "judge_prompt_tokens": 403, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The provided context does not specify a particular style of racing for \"The Last Race.\" However, the plot description indicates that it focuses on stock car racing at a small town track. Therefore, we can infer that the style of racing in this documentary is grassroots or local stock car racing [Doc 86410].", "latency_ms": 4210.685253143311, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly identifies the racing style as drift, but the generated answer instead infers stock car racing, which is incorrect. The reasoning is consistent with the provided context (so it is faithful to that context), but it contradicts the gold label, making the final answer factually incorrect for the question being evaluated.", "main_total_tokens": 3614, "main_prompt_tokens": 3548, "main_completion_tokens": 66, "judge_total_tokens": 530, "judge_prompt_tokens": 440, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and her relationship with the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 3684.4449043273926, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'Confronting a Serial Killer,' matching the gold answer exactly. It stays faithful to the information given and does not introduce any contradictory or unsupported details; the brief elaboration about her relationship with Samuel Little is consistent with the described subject of the film.", "main_total_tokens": 3165, "main_prompt_tokens": 3111, "main_completion_tokens": 54, "judge_total_tokens": 546, "judge_prompt_tokens": 455, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 3372.4124431610107, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and clearly attributes it, so it is both correct and faithful to the reference.", "main_total_tokens": 3369, "main_prompt_tokens": 3339, "main_completion_tokens": 30, "judge_total_tokens": 468, "judge_prompt_tokens": 407, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The document does not provide a specific title for a 1999 psychological thriller movie that directly alludes to an \"extra sense\" as described in your question. However, based on the context provided, \"The Sixth Sense\" is a 1999 movie which involves a character with an extra sense (the ability to see dead people), fitting the criteria of involving an extra sense and being a psychological thriller. [Doc 22098]", "latency_ms": 4309.704542160034, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.6, "justification": "The correct answer is simply \"The Sixth Sense.\" The generated answer incorrectly claims that the document does not provide a specific title, then conditionally suggests \"The Sixth Sense\" instead of directly answering. This makes the response effectively incorrect for the QA task. Faithfulness is partial: it does mention the right movie and attributes it to the document, but adds an unnecessary and inaccurate statement about the document not providing the title, so it is not fully faithful.", "main_total_tokens": 3202, "main_prompt_tokens": 3107, "main_completion_tokens": 95, "judge_total_tokens": 601, "judge_prompt_tokens": 478, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 3165.5848026275635, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer. It does not introduce any contradictions or extraneous unsupported claims, so it is both correct and faithful.", "main_total_tokens": 3312, "main_prompt_tokens": 3289, "main_completion_tokens": 23, "judge_total_tokens": 469, "judge_prompt_tokens": 397, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 3566.6887760162354, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the title as 'The Transporter Refueled'. It is fully supported by the referenced document and contains no extra unsupported claims.", "main_total_tokens": 3469, "main_prompt_tokens": 3434, "main_completion_tokens": 35, "judge_total_tokens": 488, "judge_prompt_tokens": 420, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in the year 2015 [Doc 67500].", "latency_ms": 3239.0358448028564, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. The citation format does not contradict the information, so the response is both correct and faithful.", "main_total_tokens": 3347, "main_prompt_tokens": 3319, "main_completion_tokens": 28, "judge_total_tokens": 481, "judge_prompt_tokens": 406, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "None of the romantic comedies listed involve Abraham Lincoln coming back from the dead. The documents provided mention Abraham Lincoln in connection with movies like \"Abraham Lincoln: Vampire Hunter\" and \"Abraham Lincoln vs. Zombies,\" but these are categorized under genres including horror, action, and thriller rather than romantic comedy. [Doc 54943] [Doc 57213]", "latency_ms": 4111.3879680633545, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a romantic comedy where Abraham Lincoln comes back from the dead; the gold answer says no such movie description can be found. The generated answer states that none of the romantic comedies listed involve Abraham Lincoln returning from the dead and that the only mentioned Lincoln films are in other genres. This matches the gold answer in substance and is consistent with the referenced documents, so it is both correct and faithful to the provided information.", "main_total_tokens": 3090, "main_prompt_tokens": 3009, "main_completion_tokens": 81, "judge_total_tokens": 581, "judge_prompt_tokens": 464, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by an unknown writer, which Max Perkins believes to be a literary genius, is \"Genius\" [Doc 73141]. However, it's important to note that the document does not explicitly state that the manuscript is by Ernest Hemingway. It only mentions that Ernest Hemingway is one of the authors signed by Max Perkins.", "latency_ms": 4265.83194732666, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.75, "justification": "The question asks for a movie title that alludes to a manuscript written by Ernest Hemingway; the gold answer is 'Havana Darkness.' The generated answer instead gives 'Genius,' which is incorrect, so correctness is 0. The answer does engage with the provided document context and notes that the manuscript is not explicitly said to be by Hemingway, which shows some grounding in the doc, but it still fabricates the specific answer and misidentifies the film, so faithfulness is partial rather than complete.", "main_total_tokens": 3204, "main_prompt_tokens": 3120, "main_completion_tokens": 84, "judge_total_tokens": 597, "judge_prompt_tokens": 463, "judge_completion_tokens": 134}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the British comedy series \"Doll & Em\" [Doc 122259].", "latency_ms": 3197.1993446350098, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em,' which matches the gold answer exactly. It is fully supported by the referenced document and does not introduce any contradictions or unsupported claims, so both correctness and faithfulness are complete.", "main_total_tokens": 2979, "main_prompt_tokens": 2944, "main_completion_tokens": 35, "judge_total_tokens": 498, "judge_prompt_tokens": 418, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The 2007 movie where Hallam, who has a talent for spying on people and attempts to find the cause of his mother's death, is \"Hallam Foe\" [Doc 39389].", "latency_ms": 3594.386100769043, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2007 movie as \"Hallam Foe,\" which matches the gold answer exactly. It is faithful to the question and does not introduce unsupported information beyond a reference marker.", "main_total_tokens": 3230, "main_prompt_tokens": 3182, "main_completion_tokens": 48, "judge_total_tokens": 512, "judge_prompt_tokens": 440, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The description provided does not specify whose last name is in question, but based on the context given, there isn't any character with a last name that initially sounds like they are either a friend or a foe. The descriptions focus more on plot details rather than naming conventions that might lead to such an interpretation [Doc 22549].", "latency_ms": 4014.9495601654053, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The gold answer is an explicit label: 'A foe'. The generated answer denies that there is any such last name, so it is factually incorrect relative to the gold; correctness is 0. The response does reference the (imagined) context and document and does not hallucinate specific new entities, so it is largely grounded in its own stated context, yielding a relatively high faithfulness score.", "main_total_tokens": 3312, "main_prompt_tokens": 3242, "main_completion_tokens": 70, "judge_total_tokens": 559, "judge_prompt_tokens": 451, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "The age of Sam in the 2017 TV show \"Atypical\" is 18 [Doc 124583].", "latency_ms": 3228.5590171813965, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 in the 2017 TV show Atypical, which matches the gold answer exactly. It does not introduce any contradictory or extra speculative information and remains grounded in the provided fact.", "main_total_tokens": 3023, "main_prompt_tokens": 2991, "main_completion_tokens": 32, "judge_total_tokens": 489, "judge_prompt_tokens": 412, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 2905.616044998169, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, matching the gold answer exactly. It does not add incorrect information and remains faithful to the source.", "main_total_tokens": 2753, "main_prompt_tokens": 2724, "main_completion_tokens": 29, "judge_total_tokens": 473, "judge_prompt_tokens": 410, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Sandra was born in Chamonix according to the plot description for [Doc 59659].", "latency_ms": 3475.0959873199463, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that Sandra was born in Chamonix, which matches the gold answer exactly. It does not introduce contradictory or extraneous claims beyond referencing a document, so it is both correct and faithful.", "main_total_tokens": 3689, "main_prompt_tokens": 3665, "main_completion_tokens": 24, "judge_total_tokens": 489, "judge_prompt_tokens": 416, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "in this japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 3363.1086349487305, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly fills in the blank with 'rains' and matches the provided gold answer exactly. It is fully faithful to the given information with no additions or alterations.", "main_total_tokens": 3347, "main_prompt_tokens": 3321, "main_completion_tokens": 26, "judge_total_tokens": 478, "judge_prompt_tokens": 412, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the described German language animated series, Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 3639.1797065734863, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which matches the gold answer exactly. It is fully supported by the provided information and does not add incorrect or speculative details, so both correctness and faithfulness are maximal.", "main_total_tokens": 3467, "main_prompt_tokens": 3432, "main_completion_tokens": 35, "judge_total_tokens": 505, "judge_prompt_tokens": 424, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is titled **One Afternoon** [Doc 96838].", "latency_ms": 3636.760711669922, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title \"One Afternoon\" and does not introduce any contradictions or unsupported claims, aside from an internal document reference that does not affect the correctness of the movie identification.", "main_total_tokens": 2861, "main_prompt_tokens": 2810, "main_completion_tokens": 51, "judge_total_tokens": 519, "judge_prompt_tokens": 449, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" [Doc 93260]. This movie has a rating of 6.746 and is described as being set in seaside Italy where the protagonist, a Holocaust survivor with a daycare business, takes in the young street kid.", "latency_ms": 31503.968954086304, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The answer correctly identifies the movie as \"The Life Ahead\" and accurately reflects the description provided in the referenced document, including the Holocaust survivor caring for the 12-year-old street kid who robbed her and the film’s rating context. It remains faithful to the given information without introducing contradictions.", "main_total_tokens": 3254, "main_prompt_tokens": 3172, "main_completion_tokens": 82, "judge_total_tokens": 561, "judge_prompt_tokens": 473, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 3599.9412536621094, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the subject as New Zealander Bruce McLaren, founder of the McLaren motor racing team, matching the gold answer. However, the added citation-like reference '[Doc 73859]' is hallucinated in this context, slightly reducing faithfulness.", "main_total_tokens": 3391, "main_prompt_tokens": 3353, "main_completion_tokens": 38, "judge_total_tokens": 505, "judge_prompt_tokens": 417, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The documents provided do not specify the name of Paul Newman's racing team. However, it is mentioned that he won four national championships as a driver and eight championships as an owner over a thirty-five year span [Doc 68207].", "latency_ms": 3923.2678413391113, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer clearly states that the racing team is the McLaren Motor Racing team. The generated answer incorrectly claims the documents do not specify the name of Paul Newman’s racing team and instead discusses his championships, failing to provide the correct team name. This is factually incorrect relative to the gold. It is also unfaithful to the task because the model should extract or state the team name, not deny its existence in the documents when the gold indicates it is present.", "main_total_tokens": 3607, "main_prompt_tokens": 3556, "main_completion_tokens": 51, "judge_total_tokens": 550, "judge_prompt_tokens": 430, "judge_completion_tokens": 120}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The question \"What country is he from?\" does not specify which character's nationality is being asked about. Since multiple characters are mentioned across the documents provided (such as the suicidal general in Croatia, the priest in Poland, and others), a precise answer cannot be given without additional context to identify the specific individual referred to.", "latency_ms": 3994.5099353790283, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The gold answer indicates the person is from New Zealand. The generated answer claims the question cannot be answered due to lack of context, which is incorrect given the gold label. Thus, correctness is 0. However, given only the user question, the assistant’s reasoning about ambiguity is internally consistent and grounded in the provided context, so faithfulness to its own reasoning and the visible prompt is relatively high, though not perfect due to ignoring the gold reference.", "main_total_tokens": 3363, "main_prompt_tokens": 3298, "main_completion_tokens": 65, "judge_total_tokens": 563, "judge_prompt_tokens": 440, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "The provided documents do not contain information about a 2013 movie in which John McClane blows up the Great Wall of China. The plot details for \"A Good Day to Die Hard\" from [Doc 58791] and other related documents describe McClane's mission in Moscow but do not mention anything about blowing up the Great Wall of China. Therefore, based on the given context, there is no evidence to support that John McClane blows up the Great Wall of China in any 2013 movie.", "latency_ms": 5059.503078460693, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is a simple statement that there is no such movie in the database, which is effectively the correct handling of a trick/false premise question. The generated answer invents references to documents ([Doc 58791]) and specific plot details that are not present in the prompt, so it is not faithful to the provided context. In addition, instead of simply acknowledging the absence of such a movie, it asserts there is \"no evidence\" in the (imaginary) documents, which does not match the gold response style and relies on hallucinated sources. Therefore, it is both unfaithful and not fully correct relative to the gold standard.", "main_total_tokens": 3621, "main_prompt_tokens": 3511, "main_completion_tokens": 110, "judge_total_tokens": 664, "judge_prompt_tokens": 503, "judge_completion_tokens": 161}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with the described plot is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 3394.6444988250732, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Tomie: Forbidden Fruit,\" which matches the gold answer exactly. It is faithful to the provided information and does not introduce unsupported details, only restating the title with a citation placeholder.", "main_total_tokens": 3411, "main_prompt_tokens": 3379, "main_completion_tokens": 32, "judge_total_tokens": 520, "judge_prompt_tokens": 443, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years in the 2021 movie \"Lei mi parla ancora\" [Doc 98566].", "latency_ms": 3940.6070709228516, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, which matches the gold answer. It is consistent and faithful to the provided information without adding unsupported details.", "main_total_tokens": 3817, "main_prompt_tokens": 3778, "main_completion_tokens": 39, "judge_total_tokens": 494, "judge_prompt_tokens": 424, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In \"Scorpion King: Book of Souls\" (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls. This artifact is sought after because it will allow them to put an end to an evil warlord [Doc 82582].", "latency_ms": 4201.830625534058, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in 'Scorpion King: Book of Souls' (2018), the Scorpion King and Tala search for the Book of Souls, matching the gold answer. The added detail about it being a legendary relic and its purpose does not contradict the gold answer and is consistent with the film’s plot, so the response is both correct and faithful.", "main_total_tokens": 3590, "main_prompt_tokens": 3526, "main_completion_tokens": 64, "judge_total_tokens": 556, "judge_prompt_tokens": 451, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no document provided that mentions a movie titled \"Zombies on the Moon\". Therefore, based on the given context, the release year for \"Zombies on the Moon\" cannot be determined.", "latency_ms": 3435.3761672973633, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly states that the release year cannot be determined, so correctness is high. However, the reference gold answer says the movie is not in the database, while the generated answer invents a rationale about there being no provided document, which is a different and unsupported premise relative to the gold. Thus it is not fully faithful to the intended response style/context.", "main_total_tokens": 2994, "main_prompt_tokens": 2953, "main_completion_tokens": 41, "judge_total_tokens": 535, "judge_prompt_tokens": 431, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "None of the documents provided contain information about a movie titled \"The Galactic baker\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 3430.6998252868652, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The gold answer acknowledges not knowing the movie at all, while the generated answer limits its uncertainty to the provided documents, implying there might be knowledge elsewhere. This is slightly less faithful to the intended behavior of the gold, which is to state it is not in the database. However, the generated answer still correctly concludes that the release year cannot be provided, so it is fully correct with respect to the question; the small deduction in faithfulness is for the nuanced difference in scope of knowledge claims.", "main_total_tokens": 3302, "main_prompt_tokens": 3267, "main_completion_tokens": 35, "judge_total_tokens": 554, "judge_prompt_tokens": 423, "judge_completion_tokens": 131}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document provided does not contain information about a movie titled \"Return of the Null Pointer\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 3547.4019050598145, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie that is not in the database. The gold answer says the movie is not in the database. The generated answer states that the provided document does not contain information about the movie and thus it cannot provide the release year. This is effectively equivalent in meaning: both deny knowledge of the movie and do not invent a release year. It is correct (no fabricated date) and faithful to the lack of information in the context.", "main_total_tokens": 3280, "main_prompt_tokens": 3243, "main_completion_tokens": 37, "judge_total_tokens": 551, "judge_prompt_tokens": 427, "judge_completion_tokens": 124}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document provided does not contain information about a movie titled \"Data Science: The Movie\". Therefore, I cannot provide the release year for this specific title.", "latency_ms": 3386.859655380249, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about the release year of a movie; the gold answer correctly states the movie is not in the database. The generated answer similarly explains that the document does not contain information about the movie and thus it cannot provide the year. This is correct (no fabricated date) and faithful to the lack of information, matching the intent of the gold response.", "main_total_tokens": 3235, "main_prompt_tokens": 3203, "main_completion_tokens": 32, "judge_total_tokens": 524, "judge_prompt_tokens": 422, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information on a movie titled \"Python's Revenge 2\". Therefore, I cannot specify the release year for this particular title based on the given context.", "latency_ms": 3390.9454345703125, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer states that the movie is not in the database, while the generated answer adds a specific claim about 'provided documents' and frames the lack of knowledge as being limited to the given context. Since we cannot verify document context here, that explanation is not fully faithful to the gold. Additionally, the question expects simply acknowledging not knowing the movie; tying it specifically to document context slightly changes the meaning and is not strictly correct relative to the gold standard behavior.", "main_total_tokens": 3226, "main_prompt_tokens": 3189, "main_completion_tokens": 37, "judge_total_tokens": 552, "judge_prompt_tokens": 427, "judge_completion_tokens": 125}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "None of the documents provided contains information about a movie titled \"The Last Pixel.\" Therefore, it is not possible to determine in which year this specific movie was released based on the given context.", "latency_ms": 3315.1564598083496, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year cannot be determined from the provided documents, which matches the gold response that the movie is not in the database. It does not invent a year or contradict the given context, so it is both correct and faithful.", "main_total_tokens": 2933, "main_prompt_tokens": 2894, "main_completion_tokens": 39, "judge_total_tokens": 509, "judge_prompt_tokens": 427, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no document provided for a movie titled \"The man who ate the sun\". Therefore, based on the information given, I cannot provide a release year for this specific title.", "latency_ms": 3673.1443405151367, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year cannot be provided or that the movie is not in the database / information given. The generated answer does not hallucinate a year and is consistent with the available information, matching the intent of the gold answer. Therefore it is both correct and faithful.", "main_total_tokens": 3627, "main_prompt_tokens": 3590, "main_completion_tokens": 37, "judge_total_tokens": 519, "judge_prompt_tokens": 428, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The document does not provide information about who directed the movie \"The Last Days of Lehman Brothers\".", "latency_ms": 3488.0518913269043, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not provided, and therefore the question cannot be answered from the document. This is correct given the context and fully faithful to the database limitation.", "main_total_tokens": 3714, "main_prompt_tokens": 3693, "main_completion_tokens": 21, "judge_total_tokens": 488, "judge_prompt_tokens": 417, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "None of the documents provided contain information about a movie titled \"A Love to Die For\" or its director. Therefore, based on the given context, the director of the movie \"A Love to Die For\" is not found.", "latency_ms": 3533.533811569214, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not contain information about the movie or its director and therefore the director is not found in the given context. This aligns with the gold answer, which says the system cannot answer because the director information is not in the database. There is no hallucinated director name or unsupported claim, so it is both correct and faithful to the provided context.", "main_total_tokens": 3147, "main_prompt_tokens": 3100, "main_completion_tokens": 47, "judge_total_tokens": 548, "judge_prompt_tokens": 441, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The document does not specify who directed the movie \"Journey into Gaza\". [Doc 111360]", "latency_ms": 3357.914686203003, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the document does not specify the director of \"Journey into Gaza,\" which is equivalent in meaning to the gold answer that the director information is not provided. It does not introduce any unsupported details and remains faithful to the described database content.", "main_total_tokens": 3388, "main_prompt_tokens": 3362, "main_completion_tokens": 26, "judge_total_tokens": 496, "judge_prompt_tokens": 413, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The document provided does not contain information about who directed the TV show \"The Last Night at Tremore Beach\". [Doc 135098]", "latency_ms": 3243.2198524475098, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the document does not contain information about who directed the work in question, matching the gold answer that the director information is not provided. It does mistakenly call it a TV show instead of a movie, but this does not contradict the database content and does not affect the core claim, so the response is still effectively correct and faithful to the given data.", "main_total_tokens": 2984, "main_prompt_tokens": 2951, "main_completion_tokens": 33, "judge_total_tokens": 531, "judge_prompt_tokens": 425, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The scenario where a group of archaeology students awaken an ancient mummy occurs in the movie \"Rise of the Mummy\" [Doc 98375]. In this plot, the students are trapped in a time loop and must defeat the mummy to escape, as it seems the mummy needs to collect souls to be able to walk the earth.", "latency_ms": 4017.214775085449, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'Rise of the Mummy,' matching the gold answer, so correctness is 1. The added plot details about a time loop and soul-collecting mummy go beyond the provided gold and source snippet; while they may be true, they are not grounded in the given information, so faithfulness is slightly reduced.", "main_total_tokens": 3080, "main_prompt_tokens": 3005, "main_completion_tokens": 75, "judge_total_tokens": 561, "judge_prompt_tokens": 457, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The scenario where Sarah fakes her own death to escape her abusive spouse is described in the plot of the movie \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 3350.0547409057617, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Stranger in My Bed,' which matches the gold answer. It remains faithful to the provided information, only restating and slightly rephrasing the scenario without adding unsupported details.", "main_total_tokens": 3000, "main_prompt_tokens": 2961, "main_completion_tokens": 39, "judge_total_tokens": 506, "judge_prompt_tokens": 431, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The scenario described—where a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years—is from the plot of \"Game of Silence\" [Doc 123938].", "latency_ms": 3164.966106414795, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie given a specific plot scenario, and the gold answer is 'Game of Silence.' The generated answer correctly identifies 'Game of Silence' and matches the gold, with no extra contradictory information. It is fully correct and faithful to the provided gold answer.", "main_total_tokens": 2722, "main_prompt_tokens": 2678, "main_completion_tokens": 44, "judge_total_tokens": 521, "judge_prompt_tokens": 435, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie that is based on a true story about a killer who uses Craigslist is \"The Craigslist Killer\" [Doc 53039]. This film describes how Philip Markoff, a charismatic and popular medical student at Boston University, leads a double life as a brutal sexual deviant who abuses prostitutes he finds via Craigslist.", "latency_ms": 3880.2671432495117, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'The Craigslist Killer,' and the generated answer correctly identifies the same movie and provides accurate, relevant details consistent with that title. It does not introduce contradictions or hallucinated information about a different film, so it is both correct and faithful to the query and gold response.", "main_total_tokens": 3109, "main_prompt_tokens": 3042, "main_completion_tokens": 67, "judge_total_tokens": 543, "judge_prompt_tokens": 455, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The described scenario occurs in the movie \"Wood Job!\" [Doc 66094]. In this plot, after Yuki Hirano fails his university entrance exam, he decides to pursue the role of a forestry trainee and is eventually sent to Kamusari Village for work experience.", "latency_ms": 4066.91575050354, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\" and accurately reflects the plot details given in the gold answer. It is faithful to the query and does not introduce contradictions; the extra detail about Kamusari Village is consistent with the film's plot.", "main_total_tokens": 3570, "main_prompt_tokens": 3510, "main_completion_tokens": 60, "judge_total_tokens": 539, "judge_prompt_tokens": 455, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The described scenario occurs in the movie titled \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 3531.8098068237305, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase.' There are no contradictions or hallucinations relative to the provided information.", "main_total_tokens": 3502, "main_prompt_tokens": 3473, "main_completion_tokens": 29, "judge_total_tokens": 502, "judge_prompt_tokens": 434, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The scenario where a 7-year-old boy must find the hero within himself to save his trapped and burning parents due to a rock fall is from the movie titled \"Beyond\" [Doc 74597].", "latency_ms": 3384.5415115356445, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot; the gold answer is \"Beyond.\" The generated answer identifies the movie as \"Beyond\" and restates the scenario, so it matches the gold exactly. There are no contradictions or hallucinations beyond citing a generic document reference, which does not alter the core answer. Thus it is both correct and faithful to the gold.", "main_total_tokens": 3089, "main_prompt_tokens": 3044, "main_completion_tokens": 45, "judge_total_tokens": 553, "judge_prompt_tokens": 449, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The scenario where a woman, fired from a financial corporation during the Asia crisis, returns home with no money is described in the plot of the movie \"6ixtynin9\" [Doc 22478].", "latency_ms": 3594.555139541626, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"6ixtynin9,\" which matches the gold answer. It is fully supported by the referenced document, so both correctness and faithfulness are maximal.", "main_total_tokens": 3224, "main_prompt_tokens": 3177, "main_completion_tokens": 47, "judge_total_tokens": 516, "judge_prompt_tokens": 445, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie where a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort, a hedonistic island paradise for swingers, and it's up to the club's staff to stop the violence or at least hide it is \"Club Dread\" [Doc 32410].", "latency_ms": 3837.4197483062744, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer exactly. It is fully faithful to the given information and does not introduce contradictions or irrelevant content.", "main_total_tokens": 3224, "main_prompt_tokens": 3162, "main_completion_tokens": 62, "judge_total_tokens": 550, "judge_prompt_tokens": 482, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is titled \"Toilet\" [Doc 48873].", "latency_ms": 4341.416120529175, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" which matches the gold answer exactly. It directly restates the information without adding unsupported details, remaining fully faithful to the provided content.", "main_total_tokens": 3876, "main_prompt_tokens": 3819, "main_completion_tokens": 57, "judge_total_tokens": 535, "judge_prompt_tokens": 466, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The description provided matches the plot details for the movie \"Electric Angel\" [Doc 1024]. This experimental film utilizes various audiovisual techniques to create a sense of polymorphic eroticism, inspired by European and Mediterranean cinematography from the 20th century.", "latency_ms": 3950.4737854003906, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The answer correctly identifies the movie as 'Electric Angel,' matching the gold label, so correctness is 1. However, it cites a specific document ID ([Doc 1024]) and implies access to a source that is not present in the given context, which is an unsupported fabrication, so faithfulness is 0.", "main_total_tokens": 3423, "main_prompt_tokens": 3367, "main_completion_tokens": 56, "judge_total_tokens": 562, "judge_prompt_tokens": 467, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 based on the provided documents is \"Doomsday Stories\" with a rating of 2.167 [Doc 108366].", "latency_ms": 2474.3776321411133, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates that the lowest rated movie from 2023 is 'Mega Ape', but the generated answer claims it is 'Doomsday Stories' with a rating of 2.167. This is factually incorrect relative to the gold answer, so correctness is 0. However, the generated answer is internally consistent and explicitly cites the (hypothetical) document it relies on, so it is faithful to its stated source, giving a faithfulness score of 1.", "main_total_tokens": 3082, "main_prompt_tokens": 3038, "main_completion_tokens": 44, "judge_total_tokens": 545, "judge_prompt_tokens": 416, "judge_completion_tokens": 129}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "The highest rated TV show from 2010 based on the provided documents is \"Downton Abbey\" with a rating of 8.1 [Doc 119146].", "latency_ms": 2872.8785514831543, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the highest rated TV show from 2010, and the gold answer is 'Tea House'. The generated answer instead states that it is 'Downton Abbey' with a rating of 8.1, which directly contradicts the gold answer. There is no support in the provided context for this claim, so the response is both incorrect relative to the gold and unfaithful to the supposed source documents.", "main_total_tokens": 3694, "main_prompt_tokens": 3652, "main_completion_tokens": 42, "judge_total_tokens": 527, "judge_prompt_tokens": 416, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest-rated Japanese language movie from 1999 based on the provided documents is \"Ghost Dog: The Way of the Samurai\" with a rating of 7.333 [Doc 22364]. However, it's worth noting that \"When the Last Sword Is Drawn,\" which has a rating of 7.509 [Doc 30175], is also from 1999 but was released in 2003 according to the document. If we strictly consider the year mentioned as 1999, \"Ghost Dog: The Way of the Samurai\" would be the highest-rated Japanese language movie.", "latency_ms": 5206.964731216431, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer states that the highest rated Japanese language movie from 1999 is \"The Uncut Story of Mewtwo's Origin.\" The generated answer instead selects \"Ghost Dog: The Way of the Samurai\" and reasons about ratings and dates from documents, which contradicts the gold. Therefore it is incorrect with respect to the task. In terms of faithfulness, the answer appears to be grounded in the (not-shown) documents it cites, consistently referring to ratings and years from those documents, so it is at least partially faithful to its own sources, even though it does not match the gold. Hence a low, but non-zero, faithfulness score.", "main_total_tokens": 3174, "main_prompt_tokens": 3035, "main_completion_tokens": 139, "judge_total_tokens": 677, "judge_prompt_tokens": 510, "judge_completion_tokens": 167}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 based on the provided documents is \"Bloodeaters\" with a rating of 4.0, and its language is English (en) [Doc 820].", "latency_ms": 3585.4642391204834, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The answer correctly identifies the language as English, matching the gold answer. It also remains faithful to the provided generated content and the referenced document, which explicitly states the movie is in English. No hallucinated details contradict the source.", "main_total_tokens": 3132, "main_prompt_tokens": 3086, "main_completion_tokens": 46, "judge_total_tokens": 500, "judge_prompt_tokens": 424, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show with the highest popularity in 1999 based on the provided context is \"The Sopranos\" with a popularity rating of 59.4284 [Doc 115146].", "latency_ms": 3751.8930435180664, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold label identifies 'Law & Order: Special Victims Unit' as the correct TV show, while the model answered 'The Sopranos'. It is also unfaithful to the gold answer, asserting a different show as having the highest popularity in 1999 based on some context rather than matching the expected output.", "main_total_tokens": 3477, "main_prompt_tokens": 3427, "main_completion_tokens": 50, "judge_total_tokens": 531, "judge_prompt_tokens": 429, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 3154.7558307647705, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and clearly attributes it to the document. It is both correct and faithful to the provided information.", "main_total_tokens": 3274, "main_prompt_tokens": 3252, "main_completion_tokens": 22, "judge_total_tokens": 463, "judge_prompt_tokens": 400, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "There are multiple movies titled \"The Story of Us\" mentioned in the context provided:\n\n1. One was released in 1999 [Doc 22481].\n2. Another one was released in 2025 [Doc 112077].\n3. And another one was released in 2019 [Doc 88811].\n\nSo, the years of release for \"The Story of Us\" are 1999, 2019, and 2025.", "latency_ms": 4857.145309448242, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a single release year and the gold label is 2019. The generated answer lists three different years (1999, 2019, 2025) and presents them all as answers, so it is not correct with respect to the gold. It also references multiple documents and years that are not present in the provided QA pair, making claims about 1999 and 2025 that are unsupported by the given context, so it is not faithful.", "main_total_tokens": 3280, "main_prompt_tokens": 3165, "main_completion_tokens": 115, "judge_total_tokens": 602, "judge_prompt_tokens": 476, "judge_completion_tokens": 126}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The document does not provide the exact number of votes for \"My Dog Skip,\" but it provides the popularity score, which is 1.7749 [Doc 24114]. The rating of 6.7 out of 10 also indicates some level of reception or voting but does not specify the number of votes.", "latency_ms": 4123.121023178101, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates that \"My Dog Skip\" received 279 votes. The generated answer incorrectly claims the document does not provide the exact number of votes, so it is factually incorrect with respect to the gold. However, it remains faithful to the cited document content by accurately describing what it (believes) the document contains, without inventing a number, so it is internally faithful.", "main_total_tokens": 3401, "main_prompt_tokens": 3330, "main_completion_tokens": 71, "judge_total_tokens": 554, "judge_prompt_tokens": 445, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "None of the documents provided contain information about votes received for any of the movies titled \"Oh My God!\". The question cannot be answered based on the given context.", "latency_ms": 3287.337303161621, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that \"Oh My God!\" received 12 votes. The generated answer claims that the question cannot be answered from the context and does not provide 12, so it is incorrect relative to the gold. However, if the provided context indeed lacked vote information, the generated response is faithful to that context by not hallucinating an answer.", "main_total_tokens": 3056, "main_prompt_tokens": 3023, "main_completion_tokens": 33, "judge_total_tokens": 515, "judge_prompt_tokens": 413, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 3170.541763305664, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie \"The Last Days of Lehman Brothers\" was released in 2009, which matches the gold answer. It is also faithful to the provided document reference, with no hallucinated information.", "main_total_tokens": 3133, "main_prompt_tokens": 3104, "main_completion_tokens": 29, "judge_total_tokens": 487, "judge_prompt_tokens": 410, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 3451.4589309692383, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the original language of \"The King's Speech\" is English, which matches the gold answer 'en'. It does not add incorrect information; the bracketed citation does not contradict the claim, so the response is both correct and faithful.", "main_total_tokens": 3617, "main_prompt_tokens": 3593, "main_completion_tokens": 24, "judge_total_tokens": 485, "judge_prompt_tokens": 402, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The movie \"Wall Street\" was released in the year 1987 [Doc 6993].", "latency_ms": 2770.0982093811035, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'Wall Street' was released in 1987, matching the gold answer exactly. It also appears to be grounded in the provided document reference, so it is both correct and faithful.", "main_total_tokens": 2436, "main_prompt_tokens": 2411, "main_completion_tokens": 25, "judge_total_tokens": 474, "judge_prompt_tokens": 398, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie in which classmates have to play a survival game using their cell phones is \"Brain Fluid Explosion Girl\" [Doc 69929]. In this movie, Hana Ichii and her classmates are forced to take part in a survival game that utilizes their cellphones after waking up locked in a cage.", "latency_ms": 3348.2091426849365, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Brain Fluid Explosion Girl' and accurately describes the premise involving classmates forced into a survival game using their cell phones. It aligns with the gold answer and does not introduce unsupported or contradictory information, demonstrating both correctness and faithfulness.", "main_total_tokens": 2399, "main_prompt_tokens": 2334, "main_completion_tokens": 65, "judge_total_tokens": 532, "judge_prompt_tokens": 448, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show where six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" from 1983 [Doc 112849]. In this animated series, the group is transported into the Dungeons & Dragons realm and must try to find their way back with the help of their guide, 'Dungeon Master'.", "latency_ms": 3711.3542556762695, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.4306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.719741384391281, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as the 1983 animated series \"Dungeons & Dragons\" and accurately describes its premise in line with the gold answer. It stays faithful to the question and does not introduce contradictions or irrelevant information.", "main_total_tokens": 2468, "main_prompt_tokens": 2393, "main_completion_tokens": 75, "judge_total_tokens": 537, "judge_prompt_tokens": 458, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie described as being about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is \"Nacht der Wölfe\" [Doc 2590]. It's important to note that this description also refers to it as a \"German version of 'West Side Story'.\"", "latency_ms": 3378.124713897705, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe,' which matches the gold answer. The added note about it being a German version of 'West Side Story' is consistent with the described plot and does not contradict the source, so the response is both correct and faithful.", "main_total_tokens": 2346, "main_prompt_tokens": 2277, "main_completion_tokens": 69, "judge_total_tokens": 548, "judge_prompt_tokens": 463, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of Picnic at Hanging Rock take place in the year 1900 [Doc 126191].", "latency_ms": 2514.770984649658, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of Picnic at Hanging Rock take place in 1900, matching the gold answer exactly. It is also faithful to the provided evidence snippet referencing the same year.", "main_total_tokens": 2182, "main_prompt_tokens": 2153, "main_completion_tokens": 29, "judge_total_tokens": 477, "judge_prompt_tokens": 406, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The movie described is \"June,\" released in 2004 [Doc 32483]. This film centers around June, a young violinist who works at an oddball Venice coffee shop by day while rehearsing for her professional music career and daydreaming about the beautiful man she believes is destined to be with her. The plot matches exactly with the description provided in the question.", "latency_ms": 3928.457021713257, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the 2004 movie fitting the described plot, and the gold answer is \"June.\" The generated answer correctly identifies the film as \"June\" and accurately restates the plot elements from the question without introducing contradictions or unsupported details. It remains faithful to the information implied by the question and aligned with the gold answer.", "main_total_tokens": 2652, "main_prompt_tokens": 2570, "main_completion_tokens": 82, "judge_total_tokens": 606, "judge_prompt_tokens": 508, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1408.5967540740967, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answer is factually incorrect because the gold label provides a specific movie title, while the model says it does not know. However, it is faithful in that it does not hallucinate an incorrect movie title and honestly states ignorance.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 477, "judge_prompt_tokens": 399, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 530.9767723083496, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric value (the highest Russian-language movie rating in 1999, which is 7.6), but the generated answer says 'I don't know' and does not provide the correct value. This is incorrect with respect to the task. However, the answer does not fabricate any information; it simply admits not knowing, so it is fully faithful to the model's stated knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 502, "judge_prompt_tokens": 389, "judge_completion_tokens": 113}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1532.5639247894287, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 0.9828920819566879, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3364045224324153, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is incorrect. The model directly answered the question without adding extraneous information, so it is faithful to the prompt despite being wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 468, "judge_prompt_tokens": 394, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 359.09295082092285, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. However, the evaluation is faithful, as it directly compares the provided generated answer to the given gold answer without introducing external assumptions.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 471, "judge_prompt_tokens": 395, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1624.7434616088867, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which is incorrect because the correct answer is 'Fantômas.' However, the response does not introduce any incorrect facts; it simply admits lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 471, "judge_prompt_tokens": 389, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie that matches your description is \"Kill Cruise\" [Doc 11205]. It describes two British beauties going to Barbados with a yacht captain, though it does not specify what the captain does not know. The plot suggests there might be some unexpected or undisclosed circumstances awaiting them.", "latency_ms": 3298.426628112793, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise', matching the gold answer. However, it adds extra speculative commentary about the plot (\"though it does not specify what the captain does not know\" and \"unexpected or undisclosed circumstances\") that is not supported by the given question or gold answer, slightly reducing faithfulness.", "main_total_tokens": 2374, "main_prompt_tokens": 2312, "main_completion_tokens": 62, "judge_total_tokens": 541, "judge_prompt_tokens": 442, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 3011.216402053833, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'Won't You Be My Neighbor?' which matches the gold answer exactly and stays faithful to the given information.", "main_total_tokens": 2522, "main_prompt_tokens": 2479, "main_completion_tokens": 43, "judge_total_tokens": 502, "judge_prompt_tokens": 440, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The question \"What year was it released?\" is not specific to a particular movie title from the provided documents. However, if you are referring to one of the documents specifically, please clarify which document or movie title you are asking about.\n\nFor example:\n- \"Year of the Horse\" was released in 1997 [Doc 19452].\n- \"YEAR\" was released in 2023 [Doc 107925].\n\nIf you need information for another specific document, please let me know.", "latency_ms": 4323.884010314941, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is a specific year (2018). The generated answer does not provide 2018 and instead claims the question is not specific, adding unrelated example films and years. This is incorrect relative to the gold and introduces unsupported content, so it is neither correct nor faithful.", "main_total_tokens": 2709, "main_prompt_tokens": 2598, "main_completion_tokens": 111, "judge_total_tokens": 563, "judge_prompt_tokens": 476, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is \"Astro Boy\" [Doc 112591]. This show describes a scenario where, after being abandoned by his creator, a young boy robot is rescued and adopted by Dr. Elefun who guides him to use his special powers to aid and protect mankind and robot-kind in the year 2030 AD.", "latency_ms": 3904.9501419067383, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy,' which matches the gold answer exactly. The additional description about the plot and setting in 2030 AD is consistent with typical descriptions of the show and does not contradict the question or provided gold answer. The answer remains grounded in the prompt and does not introduce unsupported claims, so it is both correct and faithful.", "main_total_tokens": 2620, "main_prompt_tokens": 2531, "main_completion_tokens": 89, "judge_total_tokens": 573, "judge_prompt_tokens": 467, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 2890.385627746582, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, matching the gold answer. It does not introduce any contradictions or unsupported claims, so it is both correct and faithful.", "main_total_tokens": 2086, "main_prompt_tokens": 2050, "main_completion_tokens": 36, "judge_total_tokens": 497, "judge_prompt_tokens": 424, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26m before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 2963.8137817382812, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before the 26 November 1983 break-in, matching the provided gold value of £26m. It does not introduce contradictions or unsupported details beyond rephrasing the question and answer, so it is both correct and faithful to the source.", "main_total_tokens": 2382, "main_prompt_tokens": 2338, "main_completion_tokens": 44, "judge_total_tokens": 529, "judge_prompt_tokens": 428, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6397.172927856445, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score (80.5772). The generated answer says 'I don't know,' which is incorrect relative to the gold answer, so correctness is 0. However, the model does not hallucinate a value and explicitly admits not knowing, which is faithful to its uncertainty, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 496, "judge_prompt_tokens": 396, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 296.5879440307617, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says 1 TV show, while the generated answer says 0. This is factually incorrect, so correctness is 0. The model directly answers the question and does not introduce unrelated or fabricated details beyond the numeric answer, so it is faithful to its own (incorrect) reasoning, giving faithfulness a 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 385, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "The answer to your question is found in [Doc 121928]. Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" (2014).", "latency_ms": 3439.016342163086, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Angel Eyes,\" matching the gold answer exactly. It does not contradict the provided information and stays faithful to the question and gold answer, only adding the year, which is accurate and non-misleading.", "main_total_tokens": 2622, "main_prompt_tokens": 2568, "main_completion_tokens": 54, "judge_total_tokens": 522, "judge_prompt_tokens": 441, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French [Doc 110681].", "latency_ms": 2630.066156387329, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer says the language is French, while the gold label says Korean. This is factually incorrect. It also introduces an unsupported movie title (\"Foreign Language\") and document reference, so it is not faithful to the given gold answer.", "main_total_tokens": 2407, "main_prompt_tokens": 2385, "main_completion_tokens": 22, "judge_total_tokens": 473, "judge_prompt_tokens": 393, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 624.718427658081, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.5, "precision_at_5": 0.2, "ndcg_at_5": 0.6131471927654584, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.6131471927654584, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2 while the generated answer is 1, so the generated answer is factually incorrect. However, the model directly attempted to answer the user’s question without introducing unrelated or fabricated content, so it is faithful to the task setup even though the final value is wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 486, "judge_prompt_tokens": 394, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Based on the provided context, there is no movie from 1979 with a rating and title that stands out as the best-rated one. All documents list movies from other years, with none specifically from 1979. Therefore, based strictly on this context, we cannot determine the best-rated movie released in 1979.", "latency_ms": 3483.84428024292, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.5, "faithfulness_score": 1, "justification": "The gold answer explicitly states that there are no movies from 1979 in the database. The generated answer instead says there is no movie from 1979 in the provided context and that, based on that, we cannot determine the best-rated 1979 movie. This is faithful to the given context (it does not invent any 1979 movie or rating), but it is not fully correct relative to the gold: it should directly state that there are no movies from 1979 in the database, rather than framing it only as a limitation of the context. Hence, partially correct and fully faithful.", "main_total_tokens": 2471, "main_prompt_tokens": 2399, "main_completion_tokens": 72, "judge_total_tokens": 609, "judge_prompt_tokens": 460, "judge_completion_tokens": 149}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The provided context does not mention a fourth part to the Back to the Future series. The documents list details for parts one through three but do not include any information about a part four. Therefore, based strictly on the given context, there is no answer to the question regarding the release year of \"Back to the Future Part IV\".", "latency_ms": 3763.4284496307373, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.7, "justification": "The question is unanswerable because Back to the Future Part IV does not exist, so the generated answer is effectively correct in stating there is no answer. However, the gold answer more clearly states the movie does not exist, while the generated answer frames it as a limitation of the provided context/docs. This is slightly less faithful to the gold rationale, which is about nonexistence, not just absence from the context.", "main_total_tokens": 2804, "main_prompt_tokens": 2737, "main_completion_tokens": 67, "judge_total_tokens": 583, "judge_prompt_tokens": 466, "judge_completion_tokens": 117}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 15.327930450439453, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there are no movies from 1980 about the Chernobyl disaster, since the disaster occurred in 1986. The generated response is faithful to the information in the gold answer, differing only in wording and adding a suggestion to reconsider the time period, without introducing incorrect facts.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 530, "judge_prompt_tokens": 434, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 280.9643745422363, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer '0' is incorrect because the gold answer is '4'. The model is consistent with itself (no internal contradiction), so faithfulness is high, but the factual correctness relative to the gold is zero.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 463, "judge_prompt_tokens": 388, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 314.8808479309082, "retrieval_recall": 0.0, "retrieval_mrr": 0.125, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.19342640361727081, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1) does not match the gold answer (2), so it is factually incorrect for this evaluation context. The answer is directly responsive to the user question and does not add any unsupported content, so it is faithful to the prompt.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 468, "judge_prompt_tokens": 386, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 251.42550468444824, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because there was one Super Mario Bros. TV show released in the 1990s (The Super Mario Bros. Super Show! aired in 1989, but the relevant 1990s show is Super Mario World, released in 1991). The gold label correctly states 1, while the generated answer states 0. However, the evaluation here correctly reflects the mismatch between the gold and generated answers, so it is faithful to the provided information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 517, "judge_prompt_tokens": 389, "judge_completion_tokens": 128}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 235.75592041015625, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0 and the generated answer is also 0. Given the question is factual and the generated answer exactly matches the gold, it is correct. The explanation is fully faithful to the provided gold answer and question.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 468, "judge_prompt_tokens": 391, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 327.75259017944336, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto'. The generated answer says 'I don't know,' which is incorrect. However, it does not contradict the gold answer or hallucinate an alternative; it simply expresses ignorance, so it is fully faithful to the absence of knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 386, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The provided documents do not mention any crossover movie between Jurassic Park and Power Rangers. Therefore, based on the given context, there is no information about when such a crossover movie was released [None of the Documents].", "latency_ms": 3060.9073638916016, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold says explicitly that the movie was not released. The generated answer only says there is no information about when it was released in the provided documents, which leaves open the possibility that it was released but just not mentioned. Thus it is not fully correct relative to the gold. However, the answer is faithful to the stated reasoning about the documents: it accurately reflects the absence of evidence in the context and does not fabricate a date, so faithfulness is high.", "main_total_tokens": 2444, "main_prompt_tokens": 2401, "main_completion_tokens": 43, "judge_total_tokens": 550, "judge_prompt_tokens": 427, "judge_completion_tokens": 123}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "None of the documents provided contain information about Barbie fighting a Predator. Therefore, based on the given context, there is no movie where Barbie fights the Predator.", "latency_ms": 3002.0129680633545, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.5, "faithfulness_score": 1, "justification": "The gold answer states clearly that the movie was not released, implying such a movie exists but unreleased. The generated answer instead concludes that there is no movie where Barbie fights the Predator, which is a different claim. It is partially aligned (both deny the existence of a released, known movie) but not fully correct relative to the gold. However, the generated answer is fully faithful to the provided context because it explicitly bases its conclusion on the absence of supporting documents and does not hallucinate details.", "main_total_tokens": 2787, "main_prompt_tokens": 2755, "main_completion_tokens": 32, "judge_total_tokens": 545, "judge_prompt_tokens": 413, "judge_completion_tokens": 132}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "Based on the provided documents, none of them directly list Star Wars movies. However, if we were to refer to the most commonly known order of the original and prequel trilogies up until 1985 (the latest year mentioned in the context for a movie that could potentially be considered), the fourth oldest Star Wars movie would be \"Star Wars: Episode IV - A New Hope,\" released in 1977. However, this information is not found within the provided documents [Doc X].", "latency_ms": 4326.431512832642, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct answer is 'Star Wars: Droids - The Great Heep' as given by the gold label, while the generated answer states 'Star Wars: Episode IV - A New Hope,' which is incorrect. The reasoning acknowledges that the information is not in the provided documents and tries to rely on outside knowledge, but it still arrives at the wrong movie. It is partially faithful in that it admits the documents do not contain the answer and does not fabricate document content, yet it goes beyond the documents to produce an unsupported guess, so faithfulness is only partial.", "main_total_tokens": 2859, "main_prompt_tokens": 2755, "main_completion_tokens": 104, "judge_total_tokens": 634, "judge_prompt_tokens": 488, "judge_completion_tokens": 146}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The question \"What year was it released?\" is not specific to a particular movie title from the provided documents. However, if you are referring to one of the documents specifically, please clarify which document or movie title you are asking about.\n\nFor example:\n- \"Year of the Horse\" was released in 1997 [Doc 19452].\n- \"YEAR\" was released in 2023 [Doc 107925].\n\nIf you need information for another specific document, please let me know.", "latency_ms": 4397.012710571289, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "Gold answer is a specific year (1986). The generated answer does not provide 1986 and instead gives unrelated examples, so it is incorrect. It also invents document references and movie titles without grounding in the given gold, so it is only partially faithful at best.", "main_total_tokens": 2709, "main_prompt_tokens": 2598, "main_completion_tokens": 111, "judge_total_tokens": 564, "judge_prompt_tokens": 476, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 328.08995246887207, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct rating is 8.466, while the generated answer says it does not know the rating. This is incorrect. However, the model does not fabricate an incorrect rating and openly admits not knowing, which is faithful to its (claimed) uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 398, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 2704.7390937805176, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the show as 'Mighty Med,' which matches the gold answer exactly. It stays faithful to the question and does not introduce contradictions or unsupported details; the bracketed citation does not conflict with the content.", "main_total_tokens": 2204, "main_prompt_tokens": 2167, "main_completion_tokens": 37, "judge_total_tokens": 500, "judge_prompt_tokens": 423, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "The seven prisoners escaped from a prison in Argentina [Doc 53582]. Specifically, they escaped from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 3218.609571456909, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Argentina as the country where the prison break occurred, which matches the gold answer. It also stays consistent with the provided information and does not introduce contradictions, so it is both correct and faithful.", "main_total_tokens": 2845, "main_prompt_tokens": 2802, "main_completion_tokens": 43, "judge_total_tokens": 501, "judge_prompt_tokens": 427, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice Li Nai falls into a trap designed by Jiang Jie [Doc 135231].", "latency_ms": 2782.0513248443604, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks whose trap the apprentice falls into. The gold answer is Jiang Jie. The generated answer states that the apprentice falls into a trap designed by Jiang Jie. Even though it adds the apprentice’s name (Li Nai) and a citation, it correctly identifies Jiang Jie as the person whose trap it is. This is both correct relative to the gold and faithful to the described content.", "main_total_tokens": 2385, "main_prompt_tokens": 2349, "main_completion_tokens": 36, "judge_total_tokens": 519, "judge_prompt_tokens": 411, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 656.1861038208008, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.7095272044910244, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.7095272044910244, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer '0' is numerically incorrect compared to the gold answer '4'. However, the model made no additional claims or explanations beyond the single number, so there is no unfaithful reasoning or hallucinated content beyond the incorrect value itself.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 467, "judge_prompt_tokens": 385, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "47", "latency_ms": 234.0855598449707, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '47' is factually incorrect given the gold statement that no such movies can be found. It is also unfaithful to the gold answer, which explicitly says there are none, while the generated answer invents a specific number.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 476, "judge_prompt_tokens": 395, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 2661.7507934570312, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the rating of Tokyo Mater (2008) is 6.3, which matches the gold answer. It does not introduce any conflicting or extraneous information, so it is both correct and faithful.", "main_total_tokens": 2353, "main_prompt_tokens": 2327, "main_completion_tokens": 26, "judge_total_tokens": 480, "judge_prompt_tokens": 403, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in \"The Last Race\" is grassroots auto racing, specifically stock car racing [Doc 86410]. The plot description mentions a small town stock car track and drivers struggling to hold onto an American racing tradition, which indicates that the focus is on grassroots or local level stock car racing.", "latency_ms": 3473.08349609375, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the racing style is drifting, but the generated answer claims it is grassroots stock car racing, which is incorrect. It also fabricates a specific movie title and plot details not supported by the provided Q/Gold pair, so it is not faithful to the given information.", "main_total_tokens": 2597, "main_prompt_tokens": 2532, "main_completion_tokens": 65, "judge_total_tokens": 527, "judge_prompt_tokens": 439, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 3065.3445720672607, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the title as \"Confronting a Serial Killer\" and remains faithful to the provided information.", "main_total_tokens": 2371, "main_prompt_tokens": 2321, "main_completion_tokens": 50, "judge_total_tokens": 512, "judge_prompt_tokens": 451, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 2780.827045440674, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and correctly attributes it to the movie Armageddon (1998). It does not introduce any unsupported information and aligns with the provided data, so it is both correct and faithful.", "main_total_tokens": 2540, "main_prompt_tokens": 2510, "main_completion_tokens": 30, "judge_total_tokens": 488, "judge_prompt_tokens": 407, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098]. This film belongs to the genres of Mystery, Thriller, and Drama, and it tells a story about child psychologist Malcolm Crowe meeting with a nine-year-old boy named Cole Sear who harbors a dark secret.", "latency_ms": 3472.266435623169, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'The Sixth Sense,' matching the gold answer exactly, and any additional plot/genre details are plausible and do not contradict the question or known facts.", "main_total_tokens": 2327, "main_prompt_tokens": 2248, "main_completion_tokens": 79, "judge_total_tokens": 533, "judge_prompt_tokens": 463, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 2758.228063583374, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that The Transporter was released in 2002, which matches the gold answer exactly, and it is consistent with the provided information.", "main_total_tokens": 2575, "main_prompt_tokens": 2552, "main_completion_tokens": 23, "judge_total_tokens": 459, "judge_prompt_tokens": 397, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 2927.9043674468994, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the title as 'The Transporter Refueled.' It does not introduce contradictions or extraneous claims beyond citing a document, so it is both correct and faithful.", "main_total_tokens": 2616, "main_prompt_tokens": 2581, "main_completion_tokens": 35, "judge_total_tokens": 495, "judge_prompt_tokens": 420, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The year 'The Transporter Refueled' was released is 2015 [Doc 67500].", "latency_ms": 2729.6712398529053, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, which matches the gold answer. It is fully consistent and grounded in the provided information.", "main_total_tokens": 2470, "main_prompt_tokens": 2442, "main_completion_tokens": 28, "judge_total_tokens": 475, "judge_prompt_tokens": 406, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "None of the documents provided contain information about Abraham Lincoln coming back from the dead in a romantic comedy. The closest entries are movies where Abraham Lincoln is involved with zombies or vampires, but these are categorized under horror and action genres rather than romantic comedies [Doc 57213], [Doc 54943].", "latency_ms": 3348.346948623657, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a romantic comedy where Abraham Lincoln comes back from the dead. The gold answer indicates that no such movie description can be found. The generated answer correctly states that none of the provided documents describe Abraham Lincoln returning from the dead in a romantic comedy, and clarifies that related entries are in horror/action genres instead. This matches the gold answer in substance and does not introduce unsupported claims, so it is both correct and faithful to the given information.", "main_total_tokens": 2312, "main_prompt_tokens": 2244, "main_completion_tokens": 68, "judge_total_tokens": 573, "judge_prompt_tokens": 451, "judge_completion_tokens": 122}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "None of the movies listed directly allude to a manuscript written by Ernest Hemingway, except for their general inspiration or adaptation from his works. However, there is no specific mention of a movie that alludes to an unpublished manuscript (like a particular manuscript) in the provided documents.\n\nThe closest reference to a manuscript is in the plot description for \"Hello Hemingway\" which mentions a young girl reading Ernest Hemingway's \"The Old Man and the Sea,\" but this does not constitute an allusion to an unpublished or specific manuscript. [Doc 11652]", "latency_ms": 4344.805955886841, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the title of a movie that alludes to a manuscript written by Ernest Hemingway; the correct answer is 'Havana Darkness'. The generated answer incorrectly claims that no such movie is mentioned and instead discusses 'Hello Hemingway', thus failing to provide the correct title. This makes the response factually incorrect relative to the gold answer, so correctness is 0. However, the explanation given is internally consistent with the (incorrect) interpretation of the provided documents and does not contradict its own reasoning, so in terms of faithfulness to its own cited evidence and logic, it scores 1.", "main_total_tokens": 2418, "main_prompt_tokens": 2299, "main_completion_tokens": 119, "judge_total_tokens": 647, "judge_prompt_tokens": 497, "judge_completion_tokens": 150}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "The TV show where Doll heads to Hollywood to be with her childhood best friend Em is \"Doll & Em\" [Doc 122259].", "latency_ms": 2757.510185241699, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em,' which matches the gold answer exactly. It remains faithful to the provided information and does not introduce unsupported claims.", "main_total_tokens": 2263, "main_prompt_tokens": 2229, "main_completion_tokens": 34, "judge_total_tokens": 485, "judge_prompt_tokens": 418, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "In the 2007 movie \"Hallam Foe,\" Hallam has a talent for spying on people and attempts to find the cause of his mother’s death, as described in the plot summary [Doc 39389]. However, he ends up searching the rooftops of the city for love instead.", "latency_ms": 3420.8083152770996, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as \"Hallam Foe,\" matching the gold answer. It is therefore fully correct. It does add an extra plot detail about searching the rooftops of the city for love, which, while plausible, is not supported by the provided question context and cites a generic document reference, so there's a slight faithfulness concern about sourcing that extra information.", "main_total_tokens": 2544, "main_prompt_tokens": 2477, "main_completion_tokens": 67, "judge_total_tokens": 569, "judge_prompt_tokens": 460, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "None of the provided documents contain information about a character whose last name sounds like they are either a friend or foe at first glance. Therefore, based strictly on the context given, there is no answer found.", "latency_ms": 3064.0511512756348, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'A foe', but the generated answer says there is no answer found. This is incorrect relative to the gold label, so correctness is 0. However, the model’s statement that the context does not contain the information is internally consistent and does not fabricate a specific answer, so it is faithful to the (unstated) context, giving faithfulness a 1.", "main_total_tokens": 2445, "main_prompt_tokens": 2403, "main_completion_tokens": 42, "judge_total_tokens": 536, "judge_prompt_tokens": 427, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "In the 2017 TV show Atypical, Sam is 18 years old [Doc 124583].", "latency_ms": 2610.7616424560547, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, matching the gold answer exactly and staying faithful to the provided information without adding unsupported details.", "main_total_tokens": 2234, "main_prompt_tokens": 2204, "main_completion_tokens": 30, "judge_total_tokens": 481, "judge_prompt_tokens": 410, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 2441.580057144165, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam in the 2017 TV show 'Atypical' is on the autism spectrum, matching the gold answer exactly, and it remains faithful to the provided information.", "main_total_tokens": 2079, "main_prompt_tokens": 2050, "main_completion_tokens": 29, "judge_total_tokens": 481, "judge_prompt_tokens": 410, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "According to the provided context [Doc 59659], Sandra was born in Chamonix as stated in the plot summary for the movie \"À corde tendue\".", "latency_ms": 3043.8244342803955, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, which matches the gold answer. It does not introduce any contradictions or unsupported details, and explicitly attributes the information to the provided context, so it is both correct and faithful.", "main_total_tokens": 2747, "main_prompt_tokens": 2709, "main_completion_tokens": 38, "judge_total_tokens": 509, "judge_prompt_tokens": 430, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In this Japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 2768.5065269470215, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly fills in the blank with 'rains', matching the gold answer exactly. The response is fully consistent with the provided information and does not introduce unsupported details.", "main_total_tokens": 2599, "main_prompt_tokens": 2573, "main_completion_tokens": 26, "judge_total_tokens": 478, "judge_prompt_tokens": 412, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 2995.567560195923, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which matches the gold answer exactly. It is fully supported by the provided information and does not introduce unsupported content.", "main_total_tokens": 2538, "main_prompt_tokens": 2503, "main_completion_tokens": 35, "judge_total_tokens": 494, "judge_prompt_tokens": 424, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers\" is titled **One Afternoon** [Doc 96838].", "latency_ms": 3076.1523246765137, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'One Afternoon' and does not add unsupported information. It is fully correct and faithful to the provided gold answer.", "main_total_tokens": 2154, "main_prompt_tokens": 2103, "main_completion_tokens": 51, "judge_total_tokens": 511, "judge_prompt_tokens": 449, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie where a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is titled \"The Life Ahead\" [Doc 93260]. This film has a rating of 6.746 and is described as taking place in seaside Italy, involving a Holocaust survivor with a daycare business who takes in the young street kid.", "latency_ms": 2758.5418224334717, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Life Ahead\" and accurately reflects the description of the film present in the referenced document, including the Holocaust survivor caring for the 12-year-old street kid who robbed her and the Italian seaside setting. There are no contradictions or hallucinations relative to the provided gold answer and document.", "main_total_tokens": 2385, "main_prompt_tokens": 2305, "main_completion_tokens": 80, "judge_total_tokens": 567, "judge_prompt_tokens": 471, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859]. It tells the story of how a man from humble beginnings could take on the elite of motor racing and win [Doc 73859].", "latency_ms": 3772.594213485718, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the subject of the 2016 documentary 'McLaren' as New Zealander Bruce McLaren, founder of the McLaren motor racing team, which matches the gold answer. The additional contextual details about his humble beginnings and racing success are accurate and consistent with known information about the film, with no contradictions or hallucinations relative to the question.", "main_total_tokens": 2525, "main_prompt_tokens": 2457, "main_completion_tokens": 68, "judge_total_tokens": 549, "judge_prompt_tokens": 444, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "None of the documents provided contain information about a specific racing team led by a character named Sonny Hayes in the context of the movie \"F1.\" Therefore, based strictly on the given context, there is no name for his racing team. [Doc 111503]", "latency_ms": 3479.3224334716797, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the name of his racing team, and the gold answer is 'The McLaren Motor Racing team.' The generated answer claims that no name is given in the documents and thus provides no team name. This is incorrect relative to the gold answer, so correctness is 0. However, the generated answer explicitly bases its statement on the lack of information in the provided documents and does not invent a name, so it remains faithful to the stated context and its own limitations, yielding a faithfulness score of 1.", "main_total_tokens": 2680, "main_prompt_tokens": 2621, "main_completion_tokens": 59, "judge_total_tokens": 572, "judge_prompt_tokens": 436, "judge_completion_tokens": 136}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "None of the provided documents directly answer the question \"What country is he from?\" without additional context about who \"he\" refers to. Each document describes a different title, plot, or character with no specific individual identified as \"he.\" Therefore, based strictly on the given context, there is not enough information to determine what country any particular person is from.", "latency_ms": 3764.4529342651367, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the correct country is New Zealand, but the generated answer claims there is not enough information to determine the country, which is incorrect relative to the gold. However, the generated answer is faithful to its own stated reasoning and does not introduce contradictions or fabricate details beyond the provided (though unseen) context, so faithfulness is high even though the final conclusion is wrong compared to the gold.", "main_total_tokens": 2617, "main_prompt_tokens": 2545, "main_completion_tokens": 72, "judge_total_tokens": 559, "judge_prompt_tokens": 447, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based on the provided context, there is no information that indicates John McClane blows up the Great Wall of China in any 2013 movie. The plot descriptions for movies involving John McClane in 2013, specifically \"A Good Day to Die Hard,\" do not mention anything about blowing up the Great Wall of China. [Doc 58791]", "latency_ms": 3858.3452701568604, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "Correctness: The question is essentially about a non-existent movie scenario. The gold answer says such a movie is not in the database, which semantically means there is no such movie; the generated answer correctly states there is no information about John McClane blowing up the Great Wall of China in any 2013 movie, and correctly notes that \"A Good Day to Die Hard\" does not feature this, so it is factually correct. Faithfulness: The task is to answer exactly as per the gold: \"I do not have a movie like this in my database.\" The generated answer instead adds context about plots and a specific movie and references a document, deviating in style and content from the concise gold response, so it is not fully faithful to the reference answer.", "main_total_tokens": 2693, "main_prompt_tokens": 2614, "main_completion_tokens": 79, "judge_total_tokens": 658, "judge_prompt_tokens": 471, "judge_completion_tokens": 187}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie that matches the given description is \"Tomie: Forbidden Fruit\" [Doc 28589]. This film's plot aligns with the provided details about a lonely high school girl befriending a strange, mysterious and beautiful girl, where their meeting was not purely accidental.", "latency_ms": 3522.136449813843, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Tomie: Forbidden Fruit,' which matches the gold answer exactly. The explanation is consistent with the description and does not introduce unsupported information, so it is both correct and faithful.", "main_total_tokens": 2595, "main_prompt_tokens": 2527, "main_completion_tokens": 68, "judge_total_tokens": 553, "judge_prompt_tokens": 478, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie \"Lei mi parla ancora,\" Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 3271.042585372925, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, which matches the gold answer. It is fully consistent with the provided information and does not introduce contradictions, so it is both correct and faithful.", "main_total_tokens": 2936, "main_prompt_tokens": 2897, "main_completion_tokens": 39, "judge_total_tokens": 502, "judge_prompt_tokens": 424, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for the legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 3330.925941467285, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Scorpion King and Tala are searching for The Book of Souls, which matches the gold answer. The extra description as a legendary relic is consistent with the movie’s premise and does not contradict any provided information, so the answer is both correct and faithful.", "main_total_tokens": 2794, "main_prompt_tokens": 2752, "main_completion_tokens": 42, "judge_total_tokens": 517, "judge_prompt_tokens": 429, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"Zombies on the Moon\". Therefore, based strictly on the context given, it is not possible to determine in what year such a title was released [None of the Documents].", "latency_ms": 2944.197177886963, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie. The gold answer says the movie is not in the database. The generated answer states that the provided documents do not contain information about that movie and thus the release year cannot be determined. This is logically correct and consistent with the gold, and it correctly reflects the absence of information in the given context.", "main_total_tokens": 2314, "main_prompt_tokens": 2266, "main_completion_tokens": 48, "judge_total_tokens": 539, "judge_prompt_tokens": 438, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain any information regarding the release year of \"The Galactic baker\". [Not Found]", "latency_ms": 2786.5400314331055, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available/unknown. The generated answer correctly notes that the documents do not contain the information, which is consistent with the gold response. It does not fabricate a year and stays faithful to the (lack of) source content.", "main_total_tokens": 2584, "main_prompt_tokens": 2561, "main_completion_tokens": 23, "judge_total_tokens": 499, "judge_prompt_tokens": 411, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about \"Return of the Null Pointer\". Therefore, the release year cannot be determined from the given context.", "latency_ms": 2750.079870223999, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both answers correctly state that the release year cannot be provided; the generated answer attributes the lack of information to the provided documents, which is faithful to the constraint-based setting and does not invent a year.", "main_total_tokens": 2547, "main_prompt_tokens": 2518, "main_completion_tokens": 29, "judge_total_tokens": 490, "judge_prompt_tokens": 419, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information regarding the release year of \"Data Science: The Movie\". Therefore, I cannot cite a specific document for this query as none address it directly. [Not Found]", "latency_ms": 2902.104616165161, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not contain information about the release year and does not fabricate a date, aligning with the gold answer that the movie is not in the database. It is faithful to the (implied) context by explicitly noting the absence of relevant information and not citing any non-existent source.", "main_total_tokens": 2519, "main_prompt_tokens": 2478, "main_completion_tokens": 41, "judge_total_tokens": 525, "judge_prompt_tokens": 431, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"Python's Revenge 2\" or its release year. Therefore, based on the given context, the answer to the question is that the document does not provide this information [Doc X].", "latency_ms": 3133.692502975464, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there is no information about the movie or its release year. The generated answer is consistent with the given context and does not hallucinate a release year, so it is both correct and faithful.", "main_total_tokens": 2409, "main_prompt_tokens": 2359, "main_completion_tokens": 50, "judge_total_tokens": 518, "judge_prompt_tokens": 440, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"The Last Pixel.\" Therefore, based on the given context, it is not possible to determine in what year \"The Last Pixel\" was released [None of the Documents].", "latency_ms": 2918.7357425689697, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not contain information about a movie titled 'The Last Pixel' and that the release year cannot be determined from the given context. This aligns with the gold answer, which also indicates a lack of information about the movie. The response does not hallucinate a release year and stays faithful to the provided context.", "main_total_tokens": 2201, "main_prompt_tokens": 2154, "main_completion_tokens": 47, "judge_total_tokens": 535, "judge_prompt_tokens": 435, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "None of the documents provided contain information about a movie titled \"The man who ate the sun\". Therefore, based on the given context, the release year cannot be determined.", "latency_ms": 3044.640302658081, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer admits the movie is not in the database, while the generated answer says the documents do not contain information and thus the year cannot be determined. Without knowing the actual movie details or the document set, we cannot verify that the claim about the documents is factually correct, so correctness with respect to the real world is unclear and should not be credited. However, the generated answer stays within the given context and does not hallucinate a release year, matching the gold answer’s epistemic stance, so it is fully faithful to the task and context.", "main_total_tokens": 2766, "main_prompt_tokens": 2731, "main_completion_tokens": 35, "judge_total_tokens": 568, "judge_prompt_tokens": 426, "judge_completion_tokens": 142}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not contain information about who directed the movie \"The Last Days of Lehman Brothers\". [Doc 46069] [Doc 84954] [Doc 59921] [Doc 82320] [Doc 51309] [Doc 65806] [Doc 51958] [Doc 95332] [Doc 72912] [Doc 60566] [Doc 53858] [Doc 36509]", "latency_ms": 4953.859090805054, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the director information is not available in the provided context/database. The generated response stays faithful to the given documents by explicitly noting the absence of the director information and does not hallucinate an answer.", "main_total_tokens": 2927, "main_prompt_tokens": 2797, "main_completion_tokens": 130, "judge_total_tokens": 568, "judge_prompt_tokens": 490, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about the director or plot details for a movie titled \"A Love to Die For\". Therefore, based on the given context, it is not possible to answer who directed this specific movie [Not Found].", "latency_ms": 3083.0934047698975, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the director information for 'A Love to Die For' is not available in the provided data. The generated response stays faithful to the constraint of using only the given context and does not hallucinate an answer, so it is both correct and faithful.", "main_total_tokens": 2401, "main_prompt_tokens": 2353, "main_completion_tokens": 48, "judge_total_tokens": 531, "judge_prompt_tokens": 442, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not contain information about who directed the movie \"Journey into Gaza\". [Doc 111360]", "latency_ms": 2773.9551067352295, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the provided context does not contain information about the director of 'Journey into Gaza,' which matches the gold answer indicating the database lacks this information. It does not introduce unsupported facts and is fully faithful to the given context.", "main_total_tokens": 2526, "main_prompt_tokens": 2497, "main_completion_tokens": 29, "judge_total_tokens": 496, "judge_prompt_tokens": 416, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The document provided does not contain information regarding the director of the movie \"The Last Night at Tremore Beach\". [Doc 135098] provides details on the plot and other metadata but omits the director's name.", "latency_ms": 2829.9686908721924, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not provided in the database. The generated answer correctly states that the document does not contain information about the director and is consistent with the gold. It does not fabricate a director and clearly attributes the lack of information to the provided document, so it is both correct and faithful.", "main_total_tokens": 2137, "main_prompt_tokens": 2088, "main_completion_tokens": 49, "judge_total_tokens": 535, "judge_prompt_tokens": 441, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" [Doc 98375]. In this plot, these students are trapped in a time loop and must defeat the mummy to escape, as it collects the souls of those who woke him to walk the earth.", "latency_ms": 3395.6847190856934, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.92, "justification": "The generated answer correctly identifies the movie as \"Rise of the Mummy,\" matching the gold answer, so correctness is 1. The extra plot details (time loop, soul collecting) are not supported by the prompt or gold and appear hallucinated, but they do not contradict the core identification. This yields a slightly reduced faithfulness score.", "main_total_tokens": 2353, "main_prompt_tokens": 2283, "main_completion_tokens": 70, "judge_total_tokens": 554, "judge_prompt_tokens": 453, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The described scenario where Sarah fakes her own death to escape her abusive spouse occurs in the movie \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 2811.476469039917, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Stranger in My Bed,' which matches the gold answer exactly. It remains faithful to the provided information and only adds a citation-style reference that does not contradict or extend beyond the given content.", "main_total_tokens": 2366, "main_prompt_tokens": 2330, "main_completion_tokens": 36, "judge_total_tokens": 506, "judge_prompt_tokens": 428, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie in which a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years is \"Game of Silence\" [Doc 123938].", "latency_ms": 2927.6742935180664, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, 'Game of Silence', and does not introduce extraneous or contradictory information, so it is both correct and faithful.", "main_total_tokens": 2054, "main_prompt_tokens": 2015, "main_completion_tokens": 39, "judge_total_tokens": 493, "judge_prompt_tokens": 430, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie that features a plot based on a true story about a killer from the site Craigslist is titled \"The Craigslist Killer.\" This film, released in 2011, tells the story of Philip Markoff, a medical student at Boston University who leads a double life as a brutal sexual deviant targeting prostitutes he contacts through Craigslist [Doc 53039].", "latency_ms": 3476.231575012207, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer' and even gives accurate contextual details, so it is correct. However, it cites a specific document ID [Doc 53039], which is fabricated in this context and not grounded in any provided source, so the answer is not fully faithful to the available information.", "main_total_tokens": 2250, "main_prompt_tokens": 2173, "main_completion_tokens": 77, "judge_total_tokens": 554, "judge_prompt_tokens": 463, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which Yuki Hirano fails his university entrance exam and then sets his eyes on the role of a forestry trainee is titled \"Wood Job!\" [Doc 66094].", "latency_ms": 3223.512649536133, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\" which matches the gold answer exactly, and it stays faithful to the information implied in the question without adding unsupported claims (the bracketed doc reference is acceptable as a citation style).", "main_total_tokens": 2678, "main_prompt_tokens": 2636, "main_completion_tokens": 42, "judge_total_tokens": 516, "judge_prompt_tokens": 437, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie where a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure is titled \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 3317.7223205566406, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase' and stays faithful to the provided information without adding unsupported details.", "main_total_tokens": 2690, "main_prompt_tokens": 2636, "main_completion_tokens": 54, "judge_total_tokens": 526, "judge_prompt_tokens": 459, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The described scenario where \"a 7 year-old boy must find the hero within him in order to save their lives\" when his parents are trapped inside a burning car due to a rock fall happens in the movie titled \"Beyond.\" [Doc 74597]", "latency_ms": 3148.0460166931152, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Beyond,\" matching the gold answer exactly. It stays entirely consistent with the provided description and does not introduce contradictions or unsupported claims, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2355, "main_prompt_tokens": 2299, "main_completion_tokens": 56, "judge_total_tokens": 535, "judge_prompt_tokens": 460, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie described where a woman is fired from a financial corporation during the Asia crisis and returns home with no money is \"6ixtynin9\" [Doc 22478].", "latency_ms": 2826.786518096924, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"6ixtynin9,\" which matches the gold label. The answer is concise and directly supported by the given gold answer, with no extra unsupported claims, so it is both correct and faithful.", "main_total_tokens": 2248, "main_prompt_tokens": 2207, "main_completion_tokens": 41, "judge_total_tokens": 519, "judge_prompt_tokens": 439, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The described scenario occurs in the movie \"Club Dread\" [Doc 32410]. In this film, when a serial killer disrupts the activities at Coconut Pete's Coconut Beach Resort – which is portrayed as an island haven for swingers – it becomes the responsibility of the club’s staff to either stop or conceal the violence.", "latency_ms": 3559.601306915283, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer. It remains faithful to the user's question and does not introduce contradictions, only adding a brief, accurate elaboration of the plot scenario.", "main_total_tokens": 2497, "main_prompt_tokens": 2429, "main_completion_tokens": 68, "judge_total_tokens": 565, "judge_prompt_tokens": 489, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is titled \"Toilet\" [Doc 48873].", "latency_ms": 3546.210527420044, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" matching the gold answer exactly. It stays faithful to the question and does not introduce contradictions or extraneous fabricated details beyond restating the plot and title.", "main_total_tokens": 2985, "main_prompt_tokens": 2928, "main_completion_tokens": 57, "judge_total_tokens": 539, "judge_prompt_tokens": 466, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie that matches the description provided is \"Electric Angel\" [Doc 1024]. This experimental film uses various audiovisual techniques to create a sense of polymorphic eroticism as developed by European and Mediterranean cinematography in the 20th century, combining animation and live action methods.", "latency_ms": 3798.4704971313477, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Electric Angel,\" which matches the gold answer. It stays faithful to the given information and does not introduce contradictory or unsupported claims; the elaboration about the film’s style is consistent with the description provided in the question.", "main_total_tokens": 2519, "main_prompt_tokens": 2458, "main_completion_tokens": 61, "judge_total_tokens": 555, "judge_prompt_tokens": 472, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 based on the provided documents is \"Doomsday Stories\" with a rating of 2.167 [Doc 108366].", "latency_ms": 2220.80397605896, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the lowest rated movie from 2023 is 'Mega Ape', while the generated answer claims it is 'Doomsday Stories'. This directly contradicts the gold label, so the generated answer is factually incorrect relative to the task. Additionally, it cites a specific document and rating that conflict with the gold answer, indicating it is not faithful to the reference/gold information that should guide the response.", "main_total_tokens": 2372, "main_prompt_tokens": 2328, "main_completion_tokens": 44, "judge_total_tokens": 533, "judge_prompt_tokens": 416, "judge_completion_tokens": 117}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 1455.315351486206, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Tea House', but the model responded with 'I don't know', which is not the correct answer, so correctness is 0. However, the model did not hallucinate an incorrect specific show and instead admitted uncertainty, which is faithful to its lack of knowledge, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 386, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 765.3486728668213, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct movie ('The Uncut Story of Mewtwo's Origin'), so correctness is 0. However, it does not invent false information and accurately reflects a lack of knowledge, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 395, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 based on the provided documents is \"At a Lame Pace\" with a rating of 4.5 [Doc 837]. The language of this movie is Spanish (es) [Doc 837].", "latency_ms": 3204.3519020080566, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the language is English, but the generated answer states it is Spanish, so the answer is factually incorrect relative to the gold. It also conflicts with the information implied by the reference (gold), so it is unfaithful to the provided ground truth.", "main_total_tokens": 2338, "main_prompt_tokens": 2282, "main_completion_tokens": 56, "judge_total_tokens": 518, "judge_prompt_tokens": 432, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "Based on the provided documents and strictly adhering to the information given, the TV show with the highest popularity in 1999 is \"The Sopranos\" [Doc 115146] with a popularity rating of 59.4284.", "latency_ms": 3443.671226501465, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the most popular 1999 TV show is 'Law & Order: Special Victims Unit', but the generated answer claims it is 'The Sopranos', so the content is incorrect relative to the gold. It also references a specific document and popularity score that are not supported by the given gold answer, indicating it is not faithful to the expected information for this evaluation.", "main_total_tokens": 2733, "main_prompt_tokens": 2673, "main_completion_tokens": 60, "judge_total_tokens": 543, "judge_prompt_tokens": 438, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 2834.2947959899902, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 and attributes it to a document, so it is both correct and faithful to the provided information.", "main_total_tokens": 2570, "main_prompt_tokens": 2548, "main_completion_tokens": 22, "judge_total_tokens": 463, "judge_prompt_tokens": 400, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in the year 1999 [Doc 22481].", "latency_ms": 2716.0375118255615, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states the movie was released in 1999, while the gold label says 2019, so it is factually incorrect relative to the gold. However, the answer is faithful to the cited document [Doc 22481], so faithfulness is high.", "main_total_tokens": 2396, "main_prompt_tokens": 2368, "main_completion_tokens": 28, "judge_total_tokens": 492, "judge_prompt_tokens": 406, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 320.3871250152588, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (3) does not match the gold answer (279), so it is incorrect. It is, however, a direct numerical response to the question without introducing unrelated information, so it is faithful to the user query.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 460, "judge_prompt_tokens": 383, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "12", "latency_ms": 265.3238773345947, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (12), so it is both correct and faithful to the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 436, "judge_prompt_tokens": 383, "judge_completion_tokens": 53}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in the year 2009 [Doc 46069].", "latency_ms": 2738.974094390869, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Last Days of Lehman Brothers' was released in 2009, matching the gold answer. It is also faithful to the cited document reference without adding unsupported information.", "main_total_tokens": 2492, "main_prompt_tokens": 2461, "main_completion_tokens": 31, "judge_total_tokens": 484, "judge_prompt_tokens": 412, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 2836.085557937622, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the original language is English (\"en\"), and the generated response states that the original language is English, matching the gold. The added citation does not contradict the gold and stays within the provided information. Therefore, the answer is both correct and faithful.", "main_total_tokens": 2790, "main_prompt_tokens": 2766, "main_completion_tokens": 24, "judge_total_tokens": 488, "judge_prompt_tokens": 402, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "1987 [Doc 6993]", "latency_ms": 4190.237522125244, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer, and does not introduce any contradictory or extraneous information beyond a document reference.", "main_total_tokens": 3071, "main_prompt_tokens": 3004, "main_completion_tokens": 67, "judge_total_tokens": 453, "judge_prompt_tokens": 387, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "I don't know.", "latency_ms": 1756.4470767974854, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model said it did not know the answer. The correct answer is 'Brain Fluid Explosion Girl', so the response is incorrect. However, the model did not fabricate any information and accurately reported its lack of knowledge, so it is fully faithful.", "main_total_tokens": 631, "main_prompt_tokens": 586, "main_completion_tokens": 45, "judge_total_tokens": 467, "judge_prompt_tokens": 392, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 5146.148443222046, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.4306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.719741384391281, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Dungeons & Dragons.'", "main_total_tokens": 3167, "main_prompt_tokens": 3054, "main_completion_tokens": 113, "judge_total_tokens": 446, "judge_prompt_tokens": 394, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 4561.110734939575, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'Nacht der Wölfe.'", "main_total_tokens": 2972, "main_prompt_tokens": 2885, "main_completion_tokens": 87, "judge_total_tokens": 456, "judge_prompt_tokens": 403, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900 [Doc 126191]", "latency_ms": 4772.2837924957275, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('1900') and does not add any contradictory information, so it is both correct and faithful.", "main_total_tokens": 2884, "main_prompt_tokens": 2781, "main_completion_tokens": 103, "judge_total_tokens": 453, "judge_prompt_tokens": 394, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June", "latency_ms": 696.5537071228027, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title, whose answer is 'June'. Both the gold and generated answers are 'June', which is correct and fully consistent with the given information.", "main_total_tokens": 132, "main_prompt_tokens": 130, "main_completion_tokens": 2, "judge_total_tokens": 499, "judge_prompt_tokens": 433, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 298.68602752685547, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is known (The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger), so saying 'I don't know' is factually incorrect. However, the model did not hallucinate an incorrect movie and simply admitted not knowing, which is faithful to its own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 488, "judge_prompt_tokens": 399, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 375.0178813934326, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because the correct value is 7.6, not 'I don't know'. However, it is faithful to the model's uncertainty and does not hallucinate a specific incorrect value.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 463, "judge_prompt_tokens": 389, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "8", "latency_ms": 2548.9797592163086, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 0.9828920819566879, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3364045224324153, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 4 but the generated answer is 8, so the generated answer is incorrect. The evaluation correctly compares the provided gold and generated values, so it is faithful to the given information.", "main_total_tokens": 1581, "main_prompt_tokens": 1561, "main_completion_tokens": 20, "judge_total_tokens": 466, "judge_prompt_tokens": 394, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 267.63391494750977, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 2, but the generated answer claims there are no matching movies or shows, which is incorrect. It also contradicts the provided gold data, so it is not faithful to the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 477, "judge_prompt_tokens": 404, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 281.2662124633789, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because the correct answer is 'Fantômas', not 'I don't know'. However, the model did not introduce any fabricated facts; it simply admitted lack of knowledge, so it is fully faithful to its own reasoning.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 471, "judge_prompt_tokens": 389, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise [Doc 11205]", "latency_ms": 5013.709306716919, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' which matches the gold answer exactly. It is faithful to the user's query and does not introduce unsupported information.", "main_total_tokens": 3021, "main_prompt_tokens": 2905, "main_completion_tokens": 116, "judge_total_tokens": 457, "judge_prompt_tokens": 393, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 4141.636371612549, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie title, so it is both correct and faithful.", "main_total_tokens": 3186, "main_prompt_tokens": 3124, "main_completion_tokens": 62, "judge_total_tokens": 462, "judge_prompt_tokens": 408, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018 [Doc 84838]", "latency_ms": 4924.698114395142, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer gives the correct year (2018), matching the gold answer exactly and staying faithful to the provided information.", "main_total_tokens": 3192, "main_prompt_tokens": 3098, "main_completion_tokens": 94, "judge_total_tokens": 441, "judge_prompt_tokens": 386, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy [Doc 112591]", "latency_ms": 6485.820293426514, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy', which matches the gold answer exactly. There are no additional or contradictory details, so it is fully faithful to the provided information.", "main_total_tokens": 3355, "main_prompt_tokens": 3171, "main_completion_tokens": 184, "judge_total_tokens": 465, "judge_prompt_tokens": 395, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 years [Doc 67513]", "latency_ms": 4186.259984970093, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('7 years') and does not introduce any extraneous or conflicting information, so it is both correct and faithful.", "main_total_tokens": 2737, "main_prompt_tokens": 2661, "main_completion_tokens": 76, "judge_total_tokens": 465, "judge_prompt_tokens": 403, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m [Doc 133469]", "latency_ms": 2512.7294063568115, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The Brink's-Mat security depot was holding about £26 million in gold before the robbery on 26 November 1983, which matches the generated answer and is consistent with well-known historical accounts. The answer is faithful to the provided document reference and does not introduce unsupported information.", "main_total_tokens": 2366, "main_prompt_tokens": 2351, "main_completion_tokens": 15, "judge_total_tokens": 491, "judge_prompt_tokens": 404, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 720.5295562744141, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score, which is given in the gold answer as 80.5772. The generated answer says 'I don't know,' which does not provide the required value and is therefore incorrect. However, it does not fabricate any information or contradict known facts; it simply admits ignorance, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 501, "judge_prompt_tokens": 396, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 265.6395435333252, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer indicates there was 1 Russian language TV show released in 1997, but the generated answer says it cannot find any, which is factually incorrect. However, the model does not hallucinate a wrong number; it expresses uncertainty/lack of knowledge rather than fabricating an answer, so it is partially faithful to its own (claimed) knowledge state.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 501, "judge_prompt_tokens": 394, "judge_completion_tokens": 107}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes [Doc 121928]", "latency_ms": 5072.570562362671, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: both state 'Angel Eyes' as the TV show. No extraneous or conflicting information is introduced, so it is fully correct and faithful.", "main_total_tokens": 3313, "main_prompt_tokens": 3216, "main_completion_tokens": 97, "judge_total_tokens": 471, "judge_prompt_tokens": 403, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of this movie is English [Doc 25900].", "latency_ms": 5250.351667404175, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because the gold label says the movie language is Korean, but the model answered English. However, it is faithful to the given generated text and gold label; the evaluation correctly reflects the discrepancy without introducing new information.", "main_total_tokens": 3233, "main_prompt_tokens": 3128, "main_completion_tokens": 105, "judge_total_tokens": 469, "judge_prompt_tokens": 389, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 721.3022708892822, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.5, "precision_at_5": 0.2, "ndcg_at_5": 0.6131471927654584, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.6131471927654584, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1) does not match the gold answer (2), so it is incorrect. The explanation is not shown here, but the numeric output clearly differs from the ground truth, so faithfulness to the task result is inadequate even if it followed the question logically; however, given only the numeric prediction, we judge it as faithfully reflecting the model’s own (incorrect) computation, so faithfulness is high while correctness is low.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 516, "judge_prompt_tokens": 394, "judge_completion_tokens": 122}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 2455.970287322998, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly indicate lack of knowledge about the best-rated 1979 movie. The generated answer is fully consistent with the information in the gold (no contradiction or hallucination), just phrased more generally. Thus it is correct and faithful to the gold answer.", "main_total_tokens": 703, "main_prompt_tokens": 626, "main_completion_tokens": 77, "judge_total_tokens": 486, "judge_prompt_tokens": 399, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "I don't know.", "latency_ms": 2461.26389503479, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer correctly states that 'Back to the Future Part IV' does not exist. The generated answer, 'I don't know,' is effectively correct as there is no such movie and it does not invent a release year, so it is factually consistent. It is also faithful to the information in the gold, as it does not contradict it or add hallucinated details.", "main_total_tokens": 711, "main_prompt_tokens": 646, "main_completion_tokens": 65, "judge_total_tokens": 510, "judge_prompt_tokens": 404, "judge_completion_tokens": 106}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there are no movies from 1980 about the Chernobyl disaster (which occurred in 1986), so the requested popularity score cannot be provided. The generated answer is faithful to the question and does not introduce unsupported claims; it simply suggests checking the event timing, which is reasonable and consistent with known facts.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 537, "judge_prompt_tokens": 434, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 281.264066696167, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect given the gold answer of 4. However, it does not introduce any fabricated details; it simply states inability to find any matches, so it is still faithful to its own (albeit incorrect) claim.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 477, "judge_prompt_tokens": 397, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 3383.298635482788, "retrieval_recall": 0.0, "retrieval_mrr": 0.125, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.19342640361727081, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer says there are no such movies or shows, which is factually incorrect given the gold label. However, given the model’s knowledge cutoff and limitations, it is honest about not finding matches and does not hallucinate, so it is faithful to its own accessible information and limitations.", "main_total_tokens": 3717, "main_prompt_tokens": 3715, "main_completion_tokens": 2, "judge_total_tokens": 492, "judge_prompt_tokens": 395, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 453.1416893005371, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show, but the generated answer claims there were none, which is factually incorrect, so correctness is 0. The answer is faithful to the question in that it attempts to answer how many such shows existed, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 398, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 234.37809944152832, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the correct number of TV shows released between 2000 and 2010 about Super Mario Bros is 0. The generated answer says it cannot find any movies or shows matching those criteria, which effectively implies there are none fitting the specified range and topic. This aligns with the gold answer, so it is correct. The reasoning stays within the information given by the question and does not introduce unsupported facts, so it is faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 521, "judge_prompt_tokens": 400, "judge_completion_tokens": 121}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 234.37833786010742, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto' as the most popular ninja TV show. The generated answer says 'I don't know,' which is incorrect relative to the gold, so correctness is 0. However, the model is honest about its uncertainty and does not hallucinate, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 386, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 2115.2102947235107, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the movie does not exist, so the correct response should say that it was never released. The generated answer says 'I don't know,' which is not correct with respect to the gold but is still faithful to the user's query in that it does not hallucinate a release date or movie details.", "main_total_tokens": 657, "main_prompt_tokens": 600, "main_completion_tokens": 57, "judge_total_tokens": 483, "judge_prompt_tokens": 389, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 1894.9098587036133, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the movie was not released, implying awareness that such a movie was planned or rumored but never came out. The generated answer says 'I don't know,' which does not convey the key fact and is therefore incorrect relative to the gold. It is also not faithful to the gold answer because it omits the crucial information that the movie was not released.", "main_total_tokens": 644, "main_prompt_tokens": 591, "main_completion_tokens": 53, "judge_total_tokens": 491, "judge_prompt_tokens": 386, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "I don't know.", "latency_ms": 1986.3135814666748, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct 4th oldest Star Wars movie given in the gold answer, so it is incorrect. However, it does not fabricate information and accurately represents the model's uncertainty, so it is fully faithful to the prompt and its own knowledge state.", "main_total_tokens": 715, "main_prompt_tokens": 662, "main_completion_tokens": 53, "judge_total_tokens": 486, "judge_prompt_tokens": 393, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "I don't know.", "latency_ms": 2650.132894515991, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model response is faithful to its uncertainty but incorrect, since the correct answer is 1986 and the model said it did not know.", "main_total_tokens": 773, "main_prompt_tokens": 697, "main_completion_tokens": 76, "judge_total_tokens": 440, "judge_prompt_tokens": 381, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 531.2576293945312, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific numerical rating (8.466), while the generated answer states uncertainty ('I don't know'). This does not match the correct value, so correctness is 0. However, the model does not fabricate an answer and honestly admits not knowing, so it is fully faithful to its own uncertainty, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 501, "judge_prompt_tokens": 398, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med [Doc 121125]", "latency_ms": 4264.594554901123, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Mighty Med', correctly identifying the TV show. The added bracketed note does not change the content; the core answer is fully correct and faithful to the gold.", "main_total_tokens": 2856, "main_prompt_tokens": 2778, "main_completion_tokens": 78, "judge_total_tokens": 473, "judge_prompt_tokens": 400, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina [Doc 53582]", "latency_ms": 5237.936735153198, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (Argentina) and does not introduce any unsupported information beyond a document reference, so it is both correct and faithful.", "main_total_tokens": 3573, "main_prompt_tokens": 3470, "main_completion_tokens": 103, "judge_total_tokens": 460, "judge_prompt_tokens": 398, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "Jiang Jie [Doc 135231]", "latency_ms": 4739.648103713989, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying Jiang Jie as the person whose trap the apprentice falls into. It is both correct and fully faithful to the provided information.", "main_total_tokens": 3036, "main_prompt_tokens": 2942, "main_completion_tokens": 94, "judge_total_tokens": 457, "judge_prompt_tokens": 392, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "2", "latency_ms": 3565.37127494812, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.7095272044910244, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.7095272044910244, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 4, but the generated answer is 2, so it is incorrect. The evaluation itself correctly identifies the mismatch between gold and generated answers, so it is faithful to the provided information.", "main_total_tokens": 3809, "main_prompt_tokens": 3804, "main_completion_tokens": 5, "judge_total_tokens": 458, "judge_prompt_tokens": 385, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "30", "latency_ms": 3059.5223903656006, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '30' is factually incorrect, as the gold response indicates there are no movies featuring both Bruce Lee and Steven Seagal together. It is also unfaithful to the gold answer, which clearly states an inability to find any such movies, not a specific number.", "main_total_tokens": 2472, "main_prompt_tokens": 2417, "main_completion_tokens": 55, "judge_total_tokens": 483, "judge_prompt_tokens": 395, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3 [Doc 42750]", "latency_ms": 2406.623125076294, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer (6.3) matches the gold rating exactly and does not introduce any unsupported information, so it is both correct and faithful.", "main_total_tokens": 2353, "main_prompt_tokens": 2340, "main_completion_tokens": 13, "judge_total_tokens": 453, "judge_prompt_tokens": 393, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "drift-style [Doc 42750]", "latency_ms": 4700.413465499878, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is exactly 'Drift', indicating the expected label is a single word. The generated answer 'drift-style [Doc 42750]' alters the format and includes an extraneous document tag, so it does not correctly or faithfully match the gold answer.", "main_total_tokens": 3172, "main_prompt_tokens": 3093, "main_completion_tokens": 79, "judge_total_tokens": 471, "judge_prompt_tokens": 386, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer [Doc 131298]", "latency_ms": 4416.158199310303, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Confronting a Serial Killer'. The bracketed document tag does not change the correctness or contradict the question, so the response is correct and faithful to the prompt.", "main_total_tokens": 3030, "main_prompt_tokens": 2952, "main_completion_tokens": 78, "judge_total_tokens": 491, "judge_prompt_tokens": 418, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838 [Doc 20603]", "latency_ms": 2836.2786769866943, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) with no extraneous or conflicting information, so it is both correct and faithful.", "main_total_tokens": 2538, "main_prompt_tokens": 2523, "main_completion_tokens": 15, "judge_total_tokens": 456, "judge_prompt_tokens": 395, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense [Doc 22098]", "latency_ms": 4528.903961181641, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'The Sixth Sense.' The bracketed document reference does not change the factual content and stays faithful to the question and answer.", "main_total_tokens": 2967, "main_prompt_tokens": 2874, "main_completion_tokens": 93, "judge_total_tokens": 471, "judge_prompt_tokens": 400, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002 [Doc 27318]", "latency_ms": 4069.7858333587646, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (2002) and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 3211, "main_prompt_tokens": 3160, "main_completion_tokens": 51, "judge_total_tokens": 442, "judge_prompt_tokens": 389, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled [Doc 67500]", "latency_ms": 3257.164478302002, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, providing the correct movie title without adding unsupported information. The citation tag does not alter the correctness or introduce unfaithful content.", "main_total_tokens": 3229, "main_prompt_tokens": 3211, "main_completion_tokens": 18, "judge_total_tokens": 468, "judge_prompt_tokens": 403, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015 [Doc 67500]", "latency_ms": 4669.115781784058, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold year 2015 exactly and does not introduce any unsupported information beyond the citation label, so it is both correct and faithful.", "main_total_tokens": 3148, "main_prompt_tokens": 3061, "main_completion_tokens": 87, "judge_total_tokens": 455, "judge_prompt_tokens": 393, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "I don't know.", "latency_ms": 2364.9275302886963, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no such movie description can be found, which is equivalent in practice to saying the answer is not known. The generated answer says 'I don't know,' which aligns with the gold. It is both correct and faithful to the gold response.", "main_total_tokens": 686, "main_prompt_tokens": 611, "main_completion_tokens": 75, "judge_total_tokens": 477, "judge_prompt_tokens": 394, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Garden of Eden [Doc 43581]", "latency_ms": 5085.51812171936, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'Garden of Eden' is not the correct movie title; the correct title is 'Havana Darkness'. The response is therefore both incorrect and unfaithful to the gold answer.", "main_total_tokens": 3039, "main_prompt_tokens": 2917, "main_completion_tokens": 122, "judge_total_tokens": 467, "judge_prompt_tokens": 397, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 4753.7782192230225, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Doll & Em', so it is both correct and faithful.", "main_total_tokens": 2992, "main_prompt_tokens": 2895, "main_completion_tokens": 97, "judge_total_tokens": 452, "judge_prompt_tokens": 392, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam Foe [Doc 39389]", "latency_ms": 4644.47808265686, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Hallam Foe') and does not introduce any contradictions or unsupported information. It is fully correct and faithful.", "main_total_tokens": 3201, "main_prompt_tokens": 3116, "main_completion_tokens": 85, "judge_total_tokens": 471, "judge_prompt_tokens": 409, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Yes, his last name sounds at first like he's a foe [Doc 39389].", "latency_ms": 6694.666862487793, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks whether his last name sounds at first like he’s a friend or a foe. The gold answer is 'A foe.' The generated answer states that his last name sounds like he’s a foe, which matches the gold answer. It is correct and faithful to the provided gold answer.", "main_total_tokens": 3617, "main_prompt_tokens": 3455, "main_completion_tokens": 162, "judge_total_tokens": 494, "judge_prompt_tokens": 404, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18 [Doc 124583]", "latency_ms": 4200.496196746826, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states Sam is 18, which matches the gold answer. The citation format does not contradict the content, so the response is both correct and faithful.", "main_total_tokens": 2894, "main_prompt_tokens": 2820, "main_completion_tokens": 74, "judge_total_tokens": 459, "judge_prompt_tokens": 395, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam is on the autism spectrum [Doc 124583].", "latency_ms": 4293.505907058716, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which exactly matches the gold answer. It is fully consistent with the provided information and does not introduce any unsupported claims.", "main_total_tokens": 2719, "main_prompt_tokens": 2633, "main_completion_tokens": 86, "judge_total_tokens": 466, "judge_prompt_tokens": 399, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix [Doc 59659]", "latency_ms": 4389.8046016693115, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Chamonix as Sandra's birthplace, matching the gold answer exactly, and does not introduce any extra information beyond a document reference.", "main_total_tokens": 3407, "main_prompt_tokens": 3350, "main_completion_tokens": 57, "judge_total_tokens": 468, "judge_prompt_tokens": 406, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481]", "latency_ms": 4167.14072227478, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and does not introduce any unsupported information.", "main_total_tokens": 3257, "main_prompt_tokens": 3200, "main_completion_tokens": 57, "judge_total_tokens": 449, "judge_prompt_tokens": 398, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland [Doc 114810]", "latency_ms": 4547.90472984314, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks where Tabaluga is crown-prince; the correct answer is Greenland. The generated answer matches this exactly and does not introduce any unsupported information, so it is both correct and faithful to the gold reference.", "main_total_tokens": 3184, "main_prompt_tokens": 3103, "main_completion_tokens": 81, "judge_total_tokens": 476, "judge_prompt_tokens": 401, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon [Doc 96838]", "latency_ms": 3325.455904006958, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer; the added document tag does not contradict the source. Both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2736, "main_prompt_tokens": 2707, "main_completion_tokens": 29, "judge_total_tokens": 471, "judge_prompt_tokens": 412, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The Life Ahead [Doc 93260]", "latency_ms": 3015.1150226593018, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('The Life Ahead') and correctly identifies the movie described in the question. It is fully faithful to the prompt and does not introduce any extraneous or incorrect information.", "main_total_tokens": 3004, "main_prompt_tokens": 2933, "main_completion_tokens": 71, "judge_total_tokens": 479, "judge_prompt_tokens": 407, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary \"McLaren\" is about Bruce McLaren.\n\nDescription: The story of New Zealander Bruce McLaren, who founded the McLaren Motor Racing team, showing the world that a man of humble beginnings could take on the elite of motor racing and win. [Doc 73859]", "latency_ms": 5967.897891998291, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the 2016 documentary 'McLaren' is about Bruce McLaren, matching the gold answer and aligning with the provided description. It is both factually correct and fully faithful to the source.", "main_total_tokens": 3213, "main_prompt_tokens": 3054, "main_completion_tokens": 159, "judge_total_tokens": 523, "judge_prompt_tokens": 447, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren Motor Racing team [Doc 73859]", "latency_ms": 5699.028730392456, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer in content (the name of the team is the McLaren Motor Racing team). The added citation tag does not change the factual content and is consistent with the prompt context. Therefore it is both correct and faithful.", "main_total_tokens": 3199, "main_prompt_tokens": 3067, "main_completion_tokens": 132, "judge_total_tokens": 474, "judge_prompt_tokens": 394, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "I don't know.", "latency_ms": 3087.8148078918457, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'New Zealand', but the generated answer is 'I don't know', which is incorrect. However, it is faithful in that it does not hallucinate an incorrect country and honestly expresses a lack of knowledge.", "main_total_tokens": 767, "main_prompt_tokens": 671, "main_completion_tokens": 96, "judge_total_tokens": 456, "judge_prompt_tokens": 380, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "I don't know.", "latency_ms": 2591.148853302002, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about a non-existent movie scenario; the gold answer correctly states that there is no such movie in the database. The generated answer says 'I don't know,' which is also correct, as there is indeed no known 2013 movie where John McClane blows up the Great Wall of China. Therefore, it is correct and faithful to the (lack of) information in the prompt.", "main_total_tokens": 705, "main_prompt_tokens": 627, "main_completion_tokens": 78, "judge_total_tokens": 513, "judge_prompt_tokens": 402, "judge_completion_tokens": 111}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit [Doc 28589]", "latency_ms": 4200.782775878906, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the film as 'Tomie: Forbidden Fruit'. It is fully correct and faithful to the provided information.", "main_total_tokens": 3235, "main_prompt_tokens": 3169, "main_completion_tokens": 66, "judge_total_tokens": 490, "judge_prompt_tokens": 428, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years. [Doc 98566]", "latency_ms": 5064.078807830811, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (65 years) and directly reflects the provided information without adding unsupported details.", "main_total_tokens": 3646, "main_prompt_tokens": 3554, "main_completion_tokens": 92, "judge_total_tokens": 467, "judge_prompt_tokens": 413, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King and Tala search for the legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 5900.956153869629, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that they search for The Book of Souls, matching the gold answer. The added description as a legendary relic does not contradict the gold and is faithful to the source context referenced.", "main_total_tokens": 3553, "main_prompt_tokens": 3413, "main_completion_tokens": 140, "judge_total_tokens": 488, "judge_prompt_tokens": 417, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1952.003002166748, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is not in the database, so not knowing the year is correct. The generated answer honestly states uncertainty and does not fabricate information, fully aligning with the gold response.", "main_total_tokens": 612, "main_prompt_tokens": 560, "main_completion_tokens": 52, "judge_total_tokens": 465, "judge_prompt_tokens": 395, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2210.967779159546, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer expresses that the movie is not in the database, which is equivalent to not knowing the release year. The generated answer, 'I don't know,' matches this meaning. It is correct relative to the gold and does not introduce any unsupported information, so it is fully faithful.", "main_total_tokens": 692, "main_prompt_tokens": 620, "main_completion_tokens": 72, "judge_total_tokens": 481, "judge_prompt_tokens": 393, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2183.3598613739014, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer admits not knowing the release year, which is consistent with the gold response that the movie is not in the database. It does not hallucinate a year or contradict the gold, so it is both correct relative to the task and faithful to the information given.", "main_total_tokens": 633, "main_prompt_tokens": 579, "main_completion_tokens": 54, "judge_total_tokens": 480, "judge_prompt_tokens": 395, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1659.2967510223389, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the information is not available; saying 'I don't know' is an appropriate and correct response and matches the lack of knowledge without hallucinating.", "main_total_tokens": 599, "main_prompt_tokens": 561, "main_completion_tokens": 38, "judge_total_tokens": 458, "judge_prompt_tokens": 395, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1763.0865573883057, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly indicate lack of knowledge about the movie. The generated answer is consistent with the gold answer and does not hallucinate any information, so it is both correct and faithful.", "main_total_tokens": 666, "main_prompt_tokens": 618, "main_completion_tokens": 48, "judge_total_tokens": 466, "judge_prompt_tokens": 395, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2054.62646484375, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is not in the database, i.e., the system does not know the year. The generated answer \"I don't know.\" expresses the same lack of knowledge, which is correct and faithful to the gold.", "main_total_tokens": 618, "main_prompt_tokens": 558, "main_completion_tokens": 60, "judge_total_tokens": 471, "judge_prompt_tokens": 393, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1852.8571128845215, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer convey a lack of knowledge about the movie’s release year. The gold says it is not in the database; the generated answer says “I don't know.” This is effectively the same information: the system cannot provide the year. There is no factual claim made, so correctness is full, and the generated answer is fully faithful to the gold response in meaning.", "main_total_tokens": 668, "main_prompt_tokens": 618, "main_completion_tokens": 50, "judge_total_tokens": 501, "judge_prompt_tokens": 396, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 2163.555145263672, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the database does not provide the director information. The generated answer says 'I don't know,' which is consistent with not having that information. It does not introduce any unsupported facts and matches the limitation described in the gold answer.", "main_total_tokens": 666, "main_prompt_tokens": 602, "main_completion_tokens": 64, "judge_total_tokens": 480, "judge_prompt_tokens": 401, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 2198.2059478759766, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states a lack of knowledge, matching the gold answer that the director information is not available. It does not introduce any unsupported details and remains faithful to the given information.", "main_total_tokens": 645, "main_prompt_tokens": 580, "main_completion_tokens": 65, "judge_total_tokens": 467, "judge_prompt_tokens": 399, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 2170.236349105835, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates that the database does not provide the director information and thus the question cannot be answered. The generated answer, 'I don't know,' is effectively equivalent in content: it does not fabricate a director and aligns with the absence of information. Therefore it is correct and faithful to the gold.", "main_total_tokens": 658, "main_prompt_tokens": 593, "main_completion_tokens": 65, "judge_total_tokens": 483, "judge_prompt_tokens": 397, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 1757.1613788604736, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the director, but the gold answer states that this information is not in the database. The generated answer \"I don't know\" correctly reflects that the system cannot provide the director, so it is correct and faithful to the database limitations.", "main_total_tokens": 648, "main_prompt_tokens": 603, "main_completion_tokens": 45, "judge_total_tokens": 482, "judge_prompt_tokens": 401, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 4314.708948135376, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Rise of the Mummy') and correctly identifies the movie based on the described plot, so it is both correct and faithful.", "main_total_tokens": 2982, "main_prompt_tokens": 2902, "main_completion_tokens": 80, "judge_total_tokens": 461, "judge_prompt_tokens": 396, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 4601.689100265503, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Stranger in My Bed') and correctly identifies the movie described. It is fully correct and faithful.", "main_total_tokens": 3048, "main_prompt_tokens": 2964, "main_completion_tokens": 84, "judge_total_tokens": 460, "judge_prompt_tokens": 400, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 4754.75287437439, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The description refers to the TV series 'Game of Silence', and both the gold answer and generated answer match exactly, so it is correct and faithful.", "main_total_tokens": 2757, "main_prompt_tokens": 2657, "main_completion_tokens": 100, "judge_total_tokens": 461, "judge_prompt_tokens": 400, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 5271.619558334351, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer, 'The Craigslist Killer,' exactly matches the gold answer and correctly identifies the movie described, so it is both correct and faithful.", "main_total_tokens": 2962, "main_prompt_tokens": 2840, "main_completion_tokens": 122, "judge_total_tokens": 455, "judge_prompt_tokens": 395, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 4043.5633659362793, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct movie title for the described plot. It is both correct and fully faithful to the reference.", "main_total_tokens": 3315, "main_prompt_tokens": 3269, "main_completion_tokens": 46, "judge_total_tokens": 465, "judge_prompt_tokens": 403, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The Magical Wand Chase", "latency_ms": 3672.579526901245, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer omits the key part of the title, 'Sesame Street:', so it does not exactly match the gold answer and is therefore incorrect. It is also not fully faithful to the gold reference title.", "main_total_tokens": 3371, "main_prompt_tokens": 3345, "main_completion_tokens": 26, "judge_total_tokens": 488, "judge_prompt_tokens": 414, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Title: Beyond [Doc 74597]", "latency_ms": 4613.685369491577, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated title exactly matches the gold answer, identifying the movie as 'Beyond'. The response is fully correct and faithful to the provided gold label.", "main_total_tokens": 3016, "main_prompt_tokens": 2922, "main_completion_tokens": 94, "judge_total_tokens": 477, "judge_prompt_tokens": 417, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9 [Doc 22478]", "latency_ms": 3988.715171813965, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('6ixtynin9') and does not introduce any extraneous or contradictory information. It is both correct and fully faithful to the given gold reference.", "main_total_tokens": 2891, "main_prompt_tokens": 2828, "main_completion_tokens": 63, "judge_total_tokens": 486, "judge_prompt_tokens": 415, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread [Doc 32410]", "latency_ms": 4699.376583099365, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie described; the gold answer is 'Club Dread'. The generated answer is also 'Club Dread', which is correct. The added reference tag '[Doc 32410]' does not change the answer and does not contradict the prompt, so the response is both correct and faithful to the given information.", "main_total_tokens": 3183, "main_prompt_tokens": 3087, "main_completion_tokens": 96, "judge_total_tokens": 530, "judge_prompt_tokens": 433, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Title: Toilet [Doc 48873]", "latency_ms": 4938.724040985107, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The described plot (a 30-something Gundam fan and engineer moving in with his reclusive brother Maury and younger sister Lisa to care for their Japanese grandmother after their mother's funeral) correctly corresponds to the movie 'Toilet.' The generated answer, 'Title: Toilet,' matches the gold answer exactly and is fully faithful to the question and gold reference.", "main_total_tokens": 3699, "main_prompt_tokens": 3613, "main_completion_tokens": 86, "judge_total_tokens": 526, "judge_prompt_tokens": 423, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel [Doc 1024]", "latency_ms": 4388.850212097168, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, identifying the movie as 'Electric Angel'. It is both correct and faithful to the provided gold label.", "main_total_tokens": 3212, "main_prompt_tokens": 3142, "main_completion_tokens": 70, "judge_total_tokens": 484, "judge_prompt_tokens": 424, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "Doomsday Stories [Doc 108366]", "latency_ms": 3383.2221031188965, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer does not match the gold answer ('Mega Ape') and instead gives 'Doomsday Stories [Doc 108366]'. It is therefore incorrect, and there is no evidence it was derived from or faithful to the provided gold reference.", "main_total_tokens": 3023, "main_prompt_tokens": 2924, "main_completion_tokens": 99, "judge_total_tokens": 473, "judge_prompt_tokens": 391, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 374.9990463256836, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the highest rated TV show from 2010 is 'Tea House'. The generated answer says 'I don't know,' which does not match the correct answer, so it is incorrect. However, the model is honest about its lack of knowledge and does not fabricate information, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 386, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 375.00548362731934, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer contradicts the gold answer by claiming not to know, so it is incorrect. However, it does not introduce any fabricated or misleading information beyond admitting ignorance, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 466, "judge_prompt_tokens": 395, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The language of the lowest-rated movie from 1980 is Spanish [Doc 837]. This conclusion is based on the provided documents where \"At a Lame Pace\" with a rating of 4.5 and \"Bloodeaters\" with a rating of 4.0 are among the lowest rated movies from 1980, but \"At a Lame Pace\" has a slightly higher rating making \"Bloodeaters\" in English the actual lowest-rated. However, since there is another movie \"From Riches to Rags\" also rated 4.0 and in Chinese [Doc 738], the lowest-rated movie with unique language is \"Bloodeaters\" in English. Yet, strictly following the context provided, the first identified lowest-rated movie from 1980 with a unique rating was \"At a Lame Pace\" which is in Spanish.", "latency_ms": 8021.558046340942, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is English, but the generated answer concludes Spanish, so it is incorrect. The reasoning is also internally inconsistent and misinterprets the documents, inventing a notion of “unique language” and ending with Spanish despite acknowledging English as the actual lowest-rated movie language. This contradicts the given gold label and does not faithfully follow the described document evidence.", "main_total_tokens": 3155, "main_prompt_tokens": 2875, "main_completion_tokens": 280, "judge_total_tokens": 654, "judge_prompt_tokens": 556, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was The Sopranos [Doc 115146].", "latency_ms": 5391.6497230529785, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold label indicates that 'Law & Order: Special Victims Unit' was the TV show in 1999 with the highest popularity, while the model answered 'The Sopranos'. Additionally, it cites a document [Doc 115146] that is not part of the provided context here, so the reasoning is not verifiably grounded in the given information.", "main_total_tokens": 3423, "main_prompt_tokens": 3305, "main_completion_tokens": 118, "judge_total_tokens": 523, "judge_prompt_tokens": 412, "judge_completion_tokens": 111}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2 [Doc 79355]", "latency_ms": 2583.7862491607666, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and does not introduce unsupported information beyond a document reference, so it is both correct and faithful.", "main_total_tokens": 2574, "main_prompt_tokens": 2561, "main_completion_tokens": 13, "judge_total_tokens": 455, "judge_prompt_tokens": 392, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "1999 [Doc 22481]", "latency_ms": 4272.157907485962, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer gives 1999, but the gold answer is 2019, so it is factually incorrect. However, the model consistently outputs the single year without contradictions or hallucinated extra details, so it is faithful to its own (incorrect) statement.", "main_total_tokens": 3037, "main_prompt_tokens": 2959, "main_completion_tokens": 78, "judge_total_tokens": 477, "judge_prompt_tokens": 393, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 2371.8318939208984, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer ('3') does not match the gold answer ('279'), so it is factually incorrect. There is no indication it followed any supporting context or source; thus it is also unfaithful to the required answer.", "main_total_tokens": 2434, "main_prompt_tokens": 2428, "main_completion_tokens": 6, "judge_total_tokens": 460, "judge_prompt_tokens": 383, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 2067.384719848633, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 12, but the generated response claims it cannot find any movies or shows matching the criteria instead of giving the correct number of votes. This is factually incorrect relative to the gold answer, so correctness is 0. However, the answer is internally consistent with its own stated limitations and does not fabricate a number, so it is faithful to the model's stated knowledge limits, giving a faithfulness score of 1.", "main_total_tokens": 2449, "main_prompt_tokens": 2447, "main_completion_tokens": 2, "judge_total_tokens": 511, "judge_prompt_tokens": 392, "judge_completion_tokens": 119}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "2009 [Doc 46069]", "latency_ms": 4591.64834022522, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'The Last Days of Lehman Brothers' was released in 2009, which matches the gold answer. It is fully supported by the provided information and does not introduce any extraneous or conflicting details.", "main_total_tokens": 3162, "main_prompt_tokens": 3070, "main_completion_tokens": 92, "judge_total_tokens": 476, "judge_prompt_tokens": 396, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b-instruct-q2_K", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is en [Doc 48035].", "latency_ms": 4950.31213760376, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The answer correctly states that the original language is English (\"en\"), matching the gold label. However, it needlessly includes a document reference bracket, which is not supported by the given context, slightly reducing faithfulness.", "main_total_tokens": 3478, "main_prompt_tokens": 3395, "main_completion_tokens": 83, "judge_total_tokens": 480, "judge_prompt_tokens": 402, "judge_completion_tokens": 78}
