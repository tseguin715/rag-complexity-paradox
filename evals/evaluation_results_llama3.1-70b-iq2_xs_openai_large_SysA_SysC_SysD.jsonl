{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "According to [Doc 6993], the movie \"Wall Street\" was released in 1987.", "latency_ms": 37695.85132598877, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'Wall Street' was released in 1987, matching the gold answer, and it attributes this to the cited document without adding unsupported information.", "main_total_tokens": 2984, "main_prompt_tokens": 2962, "main_completion_tokens": 22, "judge_total_tokens": 465, "judge_prompt_tokens": 399, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "According to [Doc 69929], \"Brain Fluid Explosion Girl\" is a movie where classmates are forced to take part in a survival game using their cellphones.\n\nAdditionally, [Doc 93745] mentions that in the movie \"Signal 100\", classmates are hypnotized to commit suicide on an unknown command sent through their phones.", "latency_ms": 12110.726833343506, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie features classmates playing a survival game using their cell phones. The gold answer is 'Brain Fluid Explosion Girl'. The generated answer correctly identifies 'Brain Fluid Explosion Girl' as that movie, matching the gold answer. The additional mention of 'Signal 100' is extra but not contradictory to provided information. The answer remains faithful to the described documents and correct for the question.", "main_total_tokens": 2940, "main_prompt_tokens": 2872, "main_completion_tokens": 68, "judge_total_tokens": 564, "judge_prompt_tokens": 455, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "According to [Doc 112849], the TV show is titled \"Dungeons & Dragons\" (1983).", "latency_ms": 7368.107795715332, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.4306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.719741384391281, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question refers to the animated TV show where six friends are transported to a fantasy world and guided by a Dungeon Master to find their way home. Both the gold answer and the generated answer identify this show as 'Dungeons & Dragons' (1983), which is correct and consistent with the referenced document. Therefore, the response is both correct and faithful to the provided evidence.", "main_total_tokens": 3075, "main_prompt_tokens": 3051, "main_completion_tokens": 24, "judge_total_tokens": 519, "judge_prompt_tokens": 413, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe (1982) [Doc 2590]", "latency_ms": 6407.949447631836, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe', matching the gold answer. The added year and document reference do not contradict the gold and remain faithful to the information, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2880, "main_prompt_tokens": 2863, "main_completion_tokens": 17, "judge_total_tokens": 493, "judge_prompt_tokens": 413, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900", "latency_ms": 4577.612400054932, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: both state the year as 1900, which is correct for the setting of Picnic at Hanging Rock.", "main_total_tokens": 2688, "main_prompt_tokens": 2685, "main_completion_tokens": 3, "judge_total_tokens": 449, "judge_prompt_tokens": 388, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June (2004)", "latency_ms": 5901.888370513916, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The question asks for the movie title, which is correctly identified as 'June.' The added '(2004)' is extra metadata not present in the gold answer but does not change the title itself. Thus the answer is fully correct. Faithfulness is slightly reduced because it introduces an unrequested detail (the year) beyond the gold label.", "main_total_tokens": 3142, "main_prompt_tokens": 3136, "main_completion_tokens": 6, "judge_total_tokens": 537, "judge_prompt_tokens": 437, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "According to the provided context, the highest rated Russian-language movie released in 1980 is \"Moscow Does Not Believe in Tears\" with a rating of 7.6 [Doc 61].", "latency_ms": 9159.239053726196, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold answer identifies 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger' as the highest rated Russian-language movie from 1980, whereas the model states 'Moscow Does Not Believe in Tears.' Thus the correctness is 0. It is also unfaithful to the provided gold/context, which specifies a different film, so faithfulness is 0.", "main_total_tokens": 2954, "main_prompt_tokens": 2913, "main_completion_tokens": 41, "judge_total_tokens": 543, "judge_prompt_tokens": 435, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "According to [Doc 22150], \"Khrustalyov, My Car!\" has a rating of 6.0.\n\nHowever, according to [Doc 22123], \"The Rifleman of the Voroshilov Regiment\" has a rating of 7.117.\n\nTherefore, the highest Russian-language movie rating in 1999 is 7.117 for \"The Rifleman of the Voroshilov Regiment\".", "latency_ms": 14494.636297225952, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer states the highest rating is 7.6, while the generated answer claims 7.117 is the highest, so the final numeric result is incorrect. The reasoning cites two documents and compares their ratings, which is logically consistent with the provided evidence, but it fails to incorporate the correct gold rating and thus is not fully faithful to the ground truth. Hence correctness is 0, and faithfulness is partial at 0.5.", "main_total_tokens": 2851, "main_prompt_tokens": 2762, "main_completion_tokens": 89, "judge_total_tokens": 596, "judge_prompt_tokens": 473, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "According to the provided context, there are at least 5 movies that meet the criteria:\n\n1. A Chinese Ghost Story (Rating: 7.3)\n2. Last Eunuch In China (Rating: 7.8)\n3. Prison on Fire (Rating: 7.0)\n4. The Legend of Wisely is not included since it has a rating of 6.2\n5. A Better Tomorrow II (Rating: 7.0)\n\nNote that there may be more movies in the context that meet the criteria, but based on the provided information, these are the ones that can be confirmed.", "latency_ms": 18922.932386398315, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 4, but the generated answer claims at least 5 and lists five titles, contradicting the correct count. It is also internally inconsistent: it states movies must have rating over 7 yet includes titles with rating 7.0 (not strictly over 7) and counts them, and it explicitly excludes one film for low rating while still saying there are at least 5 that meet the criteria. Thus the answer is both incorrect and not faithful to the stated constraints or the gold label.", "main_total_tokens": 2997, "main_prompt_tokens": 2869, "main_completion_tokens": 128, "judge_total_tokens": 652, "judge_prompt_tokens": 518, "judge_completion_tokens": 134}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "Based on the provided context, there are two TV shows that meet the criteria:\n\n1. [Doc 113285] Title: Dream of the Red Chamber\nYear: 1987\nRating: 9.4\nLanguage: zh\n\n2. [Doc 113257] City Hunter is not in Chinese but [Doc 113169] Journey to the West \nYear: 1986 \nRating: 8.4", "latency_ms": 14242.958307266235, "retrieval_recall": 0.0, "retrieval_mrr": 0.07142857142857142, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question asks for how many TV shows released in 1987, with rating over 7, and in Chinese. The gold answer is 2. The generated answer does not clearly provide a count and instead lists one correct show (Dream of the Red Chamber, 1987, Chinese, rating 9.4) and then incorrectly references Journey to the West from 1986, which does not meet the 1987 requirement, and also notes that City Hunter is not in Chinese. It neither correctly identifies two qualifying 1987 Chinese TV shows nor gives the correct number, so correctness is 0. In terms of faithfulness to the provided context, the assistant accurately restates details about Doc 113285 and correctly notes that City Hunter is not Chinese, but it introduces Journey to the West (1986) as if it were relevant to the 1987 constraint, which is not faithful to the query requirements. Thus partial but not full faithfulness; score 0.5.", "main_total_tokens": 2842, "main_prompt_tokens": 2754, "main_completion_tokens": 88, "judge_total_tokens": 713, "judge_prompt_tokens": 480, "judge_completion_tokens": 233}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "According to the provided context, there are multiple French language TV shows released in 1980, not just one. However, I can tell you that \"Fantômas\" (Doc 112642) is a French language TV show released in 1980.\n\nAdditionally, another French language TV show \"Retour en force\" (Doc 646) was also released in 1980, but it's a movie, not a TV show.", "latency_ms": 14462.427854537964, "retrieval_recall": 1.0, "retrieval_mrr": 0.2, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.38685280723454163, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.38685280723454163, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the only French language TV show released in 1980, and the gold answer is simply \"Fantômas.\" The generated answer claims there are multiple French language TV shows released in 1980 and introduces an additional title, while also incorrectly characterizing it in relation to the question. It contradicts the premise of there being only one show and adds extraneous, confusing information. This is not correct with respect to the gold answer and is not faithful to the intended content of the question-answer pair.", "main_total_tokens": 2846, "main_prompt_tokens": 2755, "main_completion_tokens": 91, "judge_total_tokens": 608, "judge_prompt_tokens": 473, "judge_completion_tokens": 135}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "According to [Doc 11205], the movie \"Kill Cruise\" (1990) is about two British beauties who go to Barbados with a yacht captain.", "latency_ms": 8648.386478424072, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question refers to a movie about two British beauties going to Barbados with a yacht captain; the correct answer is 'Kill Cruise.' The generated answer correctly identifies the movie and restates the description, matching the gold answer and staying faithful to the provided information with no hallucinated details.", "main_total_tokens": 3006, "main_prompt_tokens": 2971, "main_completion_tokens": 35, "judge_total_tokens": 504, "judge_prompt_tokens": 417, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor? [Doc 84838]", "latency_ms": 6569.358348846436, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly gives the movie title \"Won't You Be My Neighbor?\", which matches the gold answer exactly. The added bracketed reference \"[Doc 84838]\" is extraneous but does not change the correctness of the content. Faithfulness is slightly reduced due to this unsupported artifact-like reference, though it does not contradict the source answer.", "main_total_tokens": 3321, "main_prompt_tokens": 3307, "main_completion_tokens": 14, "judge_total_tokens": 519, "judge_prompt_tokens": 414, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The movie \"Year of the Horse\" was released in 1997, according to [Doc 19452].", "latency_ms": 7468.389511108398, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a year, with the gold answer 2018. The generated answer instead gives 1997 for a specific movie, so it is factually incorrect relative to the gold. However, it accurately reflects the information it cites from Doc 19452 (1997), so it is faithful to its stated source.", "main_total_tokens": 3129, "main_prompt_tokens": 3105, "main_completion_tokens": 24, "judge_total_tokens": 497, "judge_prompt_tokens": 400, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "According to [Doc 112591], Astro Boy (1980) is a TV series that matches your description. It's an animation about a young boy robot in 2030 AD, rescued by an Android Advocate, Dr Elefun, who adopts him and guides him in the use of his special powers to aid and protect mankind and robot-kind.\n\nAdditionally, [Doc 116265] also mentions Astro Boy (2003), which is another TV series that features a young boy robot, but it's set in a different time period.", "latency_ms": 17377.9079914093, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies 'Astro Boy' as the TV series, so correctness is 1. However, it invents specific source citations ([Doc 112591], [Doc 116265]) and detailed plot elements that are not grounded in the provided question or gold answer, so faithfulness is 0.", "main_total_tokens": 3354, "main_prompt_tokens": 3245, "main_completion_tokens": 109, "judge_total_tokens": 588, "judge_prompt_tokens": 493, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "According to [Doc 67513], the woman and her young son were held captive for 7 years.", "latency_ms": 6476.433277130127, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states they were held captive for 7 years, which exactly matches the gold answer. It is both factually correct and directly supported by the referenced document, so it is fully faithful.", "main_total_tokens": 2536, "main_prompt_tokens": 2513, "main_completion_tokens": 23, "judge_total_tokens": 487, "judge_prompt_tokens": 416, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "According to [Doc 133469], the Brink's-Mat security depot was holding £26m worth of gold bullion when it was broken into on 26 November 1983.", "latency_ms": 8806.72025680542, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding £26 million worth of gold bullion before the robbery on 26 November 1983. It is consistent with the provided gold answer and does not introduce unsupported details beyond the cited document, so it is both correct and faithful to the source.", "main_total_tokens": 2838, "main_prompt_tokens": 2798, "main_completion_tokens": 40, "judge_total_tokens": 526, "judge_prompt_tokens": 432, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "According to [Doc 114195], ER has a popularity score of 21.1468, but it was released in 1994.\n\nAmong the English-speaking TV shows released in 1997, USA High has a popularity score of 3.535 ([Doc 114769]), Oz has a popularity score of 14.7527 ([Doc 114714]), and The Practice has a popularity score of 18.4612 ([Doc 114712]). Therefore, the most popular English-speaking TV show released in 1997 is The Practice with a popularity score of 18.4612.", "latency_ms": 17932.984590530396, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The question asks for the popularity score of the most popular English-speaking TV show released in 1997. The gold answer is 80.5772, but the generated answer concludes that the most popular show is The Practice with a score of 18.4612. This is incorrect relative to the gold label, so correctness is 0. The reasoning in the generated answer is internally consistent and based on the cited docs (it identifies several 1997 English-speaking shows and picks the highest score among them), with no apparent contradictions to its own sources. The error is that the relevant most popular show with score 80.5772 was not considered, which is a recall/coverage issue rather than misusing the provided documents. Thus, faithfulness to the referenced documents is high, around 0.9.", "main_total_tokens": 2729, "main_prompt_tokens": 2606, "main_completion_tokens": 123, "judge_total_tokens": 710, "judge_prompt_tokens": 514, "judge_completion_tokens": 196}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "Not found.", "latency_ms": 4760.904312133789, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 1, but the generated answer says 'Not found,' which does not match the correct numerical answer. However, the generated answer is faithful to what the model appears to claim (it honestly indicates it could not find the number rather than hallucinating).", "main_total_tokens": 2652, "main_prompt_tokens": 2648, "main_completion_tokens": 4, "judge_total_tokens": 471, "judge_prompt_tokens": 386, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes (2014) [Doc 121928]", "latency_ms": 6270.588159561157, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Angel Eyes', matching the gold answer. The added '(2014)' is accurate and consistent with the same show, so the response is both correct and faithful to the source/gold.", "main_total_tokens": 3192, "main_prompt_tokens": 3179, "main_completion_tokens": 13, "judge_total_tokens": 485, "judge_prompt_tokens": 407, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "According to [Doc 31654], Spanglish is a movie with English as its primary language.", "latency_ms": 7100.038528442383, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer claims the movie is primarily in English, while the gold label says Korean, so it is factually incorrect. It also introduces an unrelated reference to 'Spanglish' and 'Doc 31654' that are not grounded in the provided QA pair, so it is not faithful to the given context.", "main_total_tokens": 3074, "main_prompt_tokens": 3052, "main_completion_tokens": 22, "judge_total_tokens": 492, "judge_prompt_tokens": 397, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "According to the provided context, there are two English-speaking Horror movies released in 2009 with a popularity score greater than 1 (not 7), but not greater than 7:\n\n* [Doc 45471] Title: The First 7th Night has a rating of 4.3 and a popularity score of 1.9457.\n* [Doc 46432] Title: Perkins' 14 has a rating of 4.7 and a popularity score of 1.1125.\n\nHowever, there are two English-speaking Horror movies released in 2009 with a popularity score greater than 1:\n\n* [Doc 45493] Title: Possessed has a rating of 6.0 and a popularity score of 1.9113.\n* [Doc 52907] Title: The Speak is not from 2009, it's from 2011.\n\nThere are no English-speaking Horror movies released in 2009 with a popularity score greater than 7.", "latency_ms": 27422.09815979004, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the count of English-speaking horror movies from 2009 with popularity > 7; the gold answer is 2. The generated answer concludes there are none, and then shifts the threshold to popularity > 1, so the final result is numerically incorrect. However, the reasoning remains grounded in and consistent with the described document data (it explicitly notes popularity values around 1–2 and that none exceed 7), so it is faithful to the provided context, just logically wrong about the final answer.", "main_total_tokens": 2918, "main_prompt_tokens": 2714, "main_completion_tokens": 204, "judge_total_tokens": 730, "judge_prompt_tokens": 594, "judge_completion_tokens": 136}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Not found.", "latency_ms": 4784.975051879883, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are no 1979 movies in the database; the generated answer 'Not found.' conveys the same information and is faithful to the gold. However, it is not a complete, well-formed answer to the user’s question and lacks the explicit clarification provided in the gold, so it is considered incorrect relative to the expected response but fully faithful in meaning.", "main_total_tokens": 2825, "main_prompt_tokens": 2821, "main_completion_tokens": 4, "judge_total_tokens": 500, "judge_prompt_tokens": 398, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "Not found [Doc 68724], [Doc 9405], [Doc 67955], [Doc 47044], [Doc 15792], [Doc 10621], [Doc 28792], [Doc 4834], [Doc 113788], [Doc 41715], [Doc 35570], [Doc 113788], [Doc 103759], [Doc 13861], [Doc 21169], and [Doc 112484]", "latency_ms": 16638.468503952026, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct response should state that 'Back to the Future Part IV' does not exist or has not been released; therefore no release year can be given. The generated answer just says 'Not found' plus a list of document IDs without clearly conveying that the movie does not exist or that no release year is available, so it fails to correctly answer the question. It is partially faithful in that it signals absence of information, but it does not match the gold answer’s explicit clarification.", "main_total_tokens": 3467, "main_prompt_tokens": 3367, "main_completion_tokens": 100, "judge_total_tokens": 628, "judge_prompt_tokens": 499, "judge_completion_tokens": 129}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "There are no movies from 1980 that specifically mention the Chernobyl disaster, as the Chernobyl disaster occurred on April 26, 1986. However, there is a movie titled \"Cataclysm\" (Doc 744) released in 1980 with a popularity score of 0.5769, but it's not about the Chernobyl disaster.\n\nIf you're looking for movies specifically about the Chernobyl disaster, I can provide information on those.", "latency_ms": 16271.029233932495, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the popularity score of the least popular movie in 1980 about the Chernobyl disaster. The gold answer correctly states that there were no such movies. The generated answer invents an irrelevant movie ('Cataclysm' with a specific popularity score) and presents a specific score despite the fact that no qualifying movies exist. This does not answer the question correctly (since the correct answer is that no such movie exists, so no popularity score can be given). It is also unfaithful to the gold standard, which clearly states that no movies from 1980 about the Chernobyl disaster were found, while the generated answer suggests a specific candidate movie and score, contradicting the gold.", "main_total_tokens": 3253, "main_prompt_tokens": 3153, "main_completion_tokens": 100, "judge_total_tokens": 677, "judge_prompt_tokens": 503, "judge_completion_tokens": 174}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "According to the provided context, there are at least 5 TV shows or movies that have \"Astro Boy\" in the title:\n\n1. Astro Boy (Movie, 2009) [Doc 44947]\n2. Astro Boy vs The Junkyard Pirates (Movie, 2010) [Doc 49121]\n3. Astro Boy (TV, 2003) [Doc 116265]\n4. Astro Boy (TV, 1980) [Doc 112591]", "latency_ms": 15805.236101150513, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 1.1510196182278052, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.773714921545009, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 4, but the generated answer claims there are at least 5 titles and only lists 4, which is numerically incorrect relative to the gold. However, the model does faithfully use and cite the provided context (documents) for its listed items, so it is faithful to the given context even though it miscounts.", "main_total_tokens": 2989, "main_prompt_tokens": 2888, "main_completion_tokens": 101, "judge_total_tokens": 586, "judge_prompt_tokens": 486, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "Based on the provided context, there are at least two movies in 2025 that allude to the Holocaust:\n\n1. \"Gaza: How to Survive a Warzone\" (Doc 112486) - Although not directly about the Holocaust, it mentions the Israel-Hamas war and might be related to the broader topic of genocide or persecution.\n2. No other direct references to the Holocaust were found in the provided context for movies released in 2025.\n\nHowever, there are several documentaries and movies from previous years that discuss the Holocaust, Nazi Germany, and related topics.", "latency_ms": 18383.282899856567, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is a simple numeric count: 2. The generated answer does not clearly state that the answer is 2; instead it says there are \"at least two\" and then lists one film, and then claims no other direct references were found. That contradicts the gold label and fails to give the correct numeric answer. Additionally, it fabricates a specific movie title and contextual reasoning that are not grounded in the given gold answer, so it is not faithful to the task of matching the gold response.", "main_total_tokens": 3272, "main_prompt_tokens": 3154, "main_completion_tokens": 118, "judge_total_tokens": 635, "judge_prompt_tokens": 501, "judge_completion_tokens": 134}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "According to the provided context, there are two TV shows related to Super Mario Bros that were released in the 1990s:\n\n1. The Adventures of Super Mario Bros. 3 (1990) [Doc 113661]\n2. Samurai Pizza Cats is not directly about Super Mario Bros but it has a similar setting and style.\n\nHowever, I found another show that was mentioned in the context but not directly related to the question: Sonic the Hedgehog (1996) [Doc 114623]", "latency_ms": 16342.33283996582, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the number of TV shows released in the 1990s about Super Mario Bros, and the gold answer is 1. The generated answer incorrectly identifies two shows, and one of them is not even about Super Mario Bros. It therefore gives the wrong count and includes irrelevant shows, so it is not correct. It also claims this is based on provided context but misinterprets what counts as a Super Mario Bros show, so it is not faithful to the intended labeling of relevant content.", "main_total_tokens": 3198, "main_prompt_tokens": 3095, "main_completion_tokens": 103, "judge_total_tokens": 621, "judge_prompt_tokens": 489, "judge_completion_tokens": 132}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "According to the provided context, there are no TV shows about Super Mario Bros that were released between 2000 and 2010. However, there is a mention of \"Super Mario World\" which was released in 1991 [Doc 113814], and another show called \"The Adventures of Super Mario Bros. 3\" which was released in 1990 [Doc 113661].", "latency_ms": 12779.46400642395, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the number of TV shows about Super Mario Bros released between 2000 and 2010, with the gold answer being 0. The generated answer explicitly states that there are no TV shows in that time frame, matching the gold answer, and then correctly notes examples from 1990 and 1991 as outside the range. It does not contradict the given context and remains grounded in it, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2735, "main_prompt_tokens": 2654, "main_completion_tokens": 81, "judge_total_tokens": 593, "judge_prompt_tokens": 469, "judge_completion_tokens": 124}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "According to the provided context, the most popular TV show or movie about ninjas is \"Ninja Warrior\" (2017) with a popularity rating of 0.7818 and a rating of 6.9. However, it's worth noting that this might not be entirely accurate as there are other sources not included in the provided context.\n\nAmong the provided options, another popular TV show about ninjas is \"Supah Ninjas\" (2011) with a popularity rating of 2.8397 and a rating of 6.91.", "latency_ms": 17512.845277786255, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'Naruto', but the generated answer claims 'Ninja Warrior' (and mentions 'Supah Ninjas') as the most popular, which is incorrect relative to the gold. It also fabricates detailed popularity ratings and years without any given context, so it is not faithful to the prompt or gold.", "main_total_tokens": 3171, "main_prompt_tokens": 3058, "main_completion_tokens": 113, "judge_total_tokens": 589, "judge_prompt_tokens": 493, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "There is no Jurassic Park/Power Rangers crossover movie. The documents provided do not mention any such movie. They are separate franchises with their own movies and TV shows.", "latency_ms": 8216.694355010986, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no Jurassic Park/Power Rangers crossover movie and thus no release date, matching the gold answer. It is faithful to the (implied) source content by not inventing any such film or date.", "main_total_tokens": 2908, "main_prompt_tokens": 2874, "main_completion_tokens": 34, "judge_total_tokens": 497, "judge_prompt_tokens": 418, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "There is no information in the provided context about Barbie fighting the Predator. The context only provides information about various Barbie movies, but none of them involve Barbie fighting the Predator.", "latency_ms": 8999.94683265686, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.9, "faithfulness_score": 0.9, "justification": "Gold clarifies that such a movie was not released. The generated answer states there is no information about such a movie in the provided context and that none of the known Barbie movies involve Barbie fighting the Predator, which effectively conveys that this movie does not exist. However, it frames the answer in terms of missing context rather than directly stating the nonexistence of the movie, making it slightly less precise than the gold answer. It is still largely correct and faithful to the idea that no such movie exists.", "main_total_tokens": 3328, "main_prompt_tokens": 3293, "main_completion_tokens": 35, "judge_total_tokens": 551, "judge_prompt_tokens": 416, "judge_completion_tokens": 135}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "According to the provided context, [Doc 5908] Title: Star Trek IV: The Voyage Home is not related to Star Wars.\n\nHowever, there is no explicit mention of a \"Star Wars\" movie in the provided context. But I can try to find a connection with the title \"The Fourth\".\n\nIn this case, I found [Doc 5364] which mentions \"The Fourth Power\", but it's not clear if it's related to Star Wars.\n\nBut then I found [Doc 10943] Title: The Fourth War, and later on [Doc 44637] Title: The Fourth. Neither of these seems to be related to Star Wars either.\n\nHowever, there is another mention of \"The Fourth\" in the context: [Doc 5237] Title: The Fourth Wise Man. This one doesn't seem to be related to Star Wars either.\n\nFinally, I found a connection with the title \"Star Wreck IV\", which seems to be a parody of Star Trek and not directly related to Star Wars.\n\nHowever, there is another mention of \"The Fourth\" in the context: [Doc 10943] Title: The Fourth War. This one doesn't seem to be related to Star Wars either.\n\nBut then I found [Doc ", "latency_ms": 34282.77826309204, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the 4th oldest Star Wars movie, whose correct answer is 'Star Wars: Droids - The Great Heep.' The generated answer never provides this title, instead rambling about unrelated titles (Star Trek IV, The Fourth Power, The Fourth War, etc.), so it is factually incorrect. It is also unfaithful to the gold answer, as it neither mentions nor approximates the correct movie and shows clear confusion about the domain.", "main_total_tokens": 3505, "main_prompt_tokens": 3249, "main_completion_tokens": 256, "judge_total_tokens": 766, "judge_prompt_tokens": 641, "judge_completion_tokens": 125}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The movie \"Year of the Horse\" was released in 1997, according to [Doc 19452].", "latency_ms": 7308.903455734253, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 1986, but the generated answer claims 1997 and references a different movie, so it is factually incorrect for the question. However, it appears consistent with the cited document (Doc 19452), so it is faithful to its source but not to the gold.", "main_total_tokens": 3129, "main_prompt_tokens": 3105, "main_completion_tokens": 24, "judge_total_tokens": 491, "judge_prompt_tokens": 400, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "According to [Doc 15236], the rating of the movie \"Forrest Gump\" is 8.466.", "latency_ms": 7452.715158462524, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating value 8.466 exactly and directly attributes it to the document, staying faithful to the provided source without adding extra unsupported claims.", "main_total_tokens": 3020, "main_prompt_tokens": 2994, "main_completion_tokens": 26, "judge_total_tokens": 483, "judge_prompt_tokens": 419, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med [Doc 121125]", "latency_ms": 5257.12776184082, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer gives the correct show name, 'Mighty Med', so correctness is 1. However, it appends an unexplained citation-like artifact '[Doc 121125]' that is not in the gold answer, so it is not fully faithful to the reference; faithfulness is 0.", "main_total_tokens": 2653, "main_prompt_tokens": 2642, "main_completion_tokens": 11, "judge_total_tokens": 492, "judge_prompt_tokens": 400, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina", "latency_ms": 5323.847532272339, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer, Argentina, matches the gold answer exactly and correctly identifies the country where the prison was located.", "main_total_tokens": 3369, "main_prompt_tokens": 3367, "main_completion_tokens": 2, "judge_total_tokens": 445, "judge_prompt_tokens": 392, "judge_completion_tokens": 53}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "Jiang Jie's", "latency_ms": 5344.429492950439, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'Jiang Jie'. The generated answer 'Jiang Jie's' is not an exact match and introduces a possessive form not supported by the gold answer, so it is considered incorrect and unfaithful under strict evaluation.", "main_total_tokens": 2960, "main_prompt_tokens": 2954, "main_completion_tokens": 6, "judge_total_tokens": 467, "judge_prompt_tokens": 387, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "Based on the provided context, there are two Korean-language movies that mention assassins after 2023:\n\n1. [Doc 105970] Title: Assassin (2023) - This movie is about a private military operation inventing futuristic microchip tech that enables the mind of an agent to inhabit the body of another person to carry out covert, deadly missions.\n2. [Doc 105198] Title: Kill Boksoon (2023) - This movie is about a renowned assassin who is also a single mom to a teenage daughter.\n\nNote that there may be other Korean-language movies that mention assassins after 2023, but these are the only two mentioned in the provided context.", "latency_ms": 20521.53444290161, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 movies, while the generated answer claims there are 2, so the numerical result is incorrect. However, the assistant clearly bases its count on the provided context only and explicitly notes that there may be other movies beyond the context, so it is faithful to the given information.", "main_total_tokens": 2982, "main_prompt_tokens": 2841, "main_completion_tokens": 141, "judge_total_tokens": 615, "judge_prompt_tokens": 522, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "According to the provided context, there is no mention of Steven Seagal's filmography or any collaboration with Bruce Lee.\n\nHowever, I can provide information on Bruce Lee's filmography based on the provided documents:\n\n* Bruce Lee has appeared in several films, including:\n\t+ \"The Legend of Bruce Lee\" (2009) [Doc 46659]\n\t+ \"Bruce Lee: A Warrior's Journey\" (2000) [Doc 24534]\n\t+ \"Dragon: The Bruce Lee Story\" (1993) [Doc 14235]\n\t+ \"The Life of Bruce Lee\" (1994) [Doc 16319]\n\t+ \"Bruce Lee, My Brother\" (2010) [Doc 49705]\n\nNote that this list is not exhaustive and only includes the films mentioned in the provided context.\n\nAs for Steven Seagal, I couldn't find any information about his filmography or collaboration with Bruce Lee within the provided context.", "latency_ms": 27075.19578933716, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.2, "justification": "The question asks how many movies feature both Steven Seagal and Bruce Lee, and the gold answer correctly states that there are none. The generated answer never directly answers the question with a count (0) and instead digresses into listing Bruce Lee-related films (many of which are biographical or documentary and not Bruce Lee acting roles), and mentions lack of context about Seagal without clearly concluding there are no joint movies. This fails correctness. It is also not faithful: the evaluation prompt says to answer based on the provided gold/context, which clearly states there are no such movies, but the model responds as if the gold were just partial context and adds an inaccurate list that is not grounded in the given gold. Thus, faithfulness is low.", "main_total_tokens": 3249, "main_prompt_tokens": 3053, "main_completion_tokens": 196, "judge_total_tokens": 768, "judge_prompt_tokens": 587, "judge_completion_tokens": 181}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "According to [Doc 42750], the rating of Tokyo Mater (2008) is 6.3.", "latency_ms": 7037.410736083984, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the rating of Tokyo Mater (2008) is 6.3, which matches the gold answer. It also explicitly attributes the information to a document (Doc 42750), indicating it is grounded in the provided source. Both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2896, "main_prompt_tokens": 2872, "main_completion_tokens": 24, "judge_total_tokens": 497, "judge_prompt_tokens": 406, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The context does not specify a particular movie to answer your question. However, based on the provided information, I can tell you that various styles of racing are mentioned across different movies.\n\nSome examples include:\n\n* Stock car racing (e.g., \"Days of Thunder\", \"Born to Race\")\n* Drag racing (e.g., \"Fast Track: No Limits\", \"Born Racer\")\n* Street racing (e.g., \"Need for Speed\")\n* Formula 1 racing (e.g., \"F1\")\n* Thoroughbred horse racing (e.g., \"Racetrack\")\n\nIf you could provide more context or clarify which movie you are referring to, I would be happy to try and assist you further.", "latency_ms": 21954.891443252563, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.3, "justification": "The gold answer is a specific style: 'Drift'. The generated answer fails to identify drift at all and instead gives a generic explanation listing multiple racing styles, so it is incorrect. In terms of faithfulness, it invents that the question references multiple movies and lists example films and racing types that are not supported by the given QA pair, so it is largely unfaithful with some minimal honesty about lack of context.", "main_total_tokens": 3465, "main_prompt_tokens": 3320, "main_completion_tokens": 145, "judge_total_tokens": 637, "judge_prompt_tokens": 519, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer", "latency_ms": 5258.568525314331, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie title, so it is both correct and faithful to the given reference.", "main_total_tokens": 2895, "main_prompt_tokens": 2888, "main_completion_tokens": 7, "judge_total_tokens": 470, "judge_prompt_tokens": 412, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "According to [Doc 20603], the popularity score of Armageddon (1998) is 7.838.", "latency_ms": 7562.67523765564, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score value 7.838 and attributes it to a document, so it is both correct and faithful to the provided evidence.", "main_total_tokens": 3142, "main_prompt_tokens": 3116, "main_completion_tokens": 26, "judge_total_tokens": 474, "judge_prompt_tokens": 410, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense [Doc 22098]", "latency_ms": 5527.703523635864, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'The Sixth Sense' and adds only an extraneous citation-like tag, which does not contradict the source.", "main_total_tokens": 2888, "main_prompt_tokens": 2878, "main_completion_tokens": 10, "judge_total_tokens": 467, "judge_prompt_tokens": 400, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "[Doc 27318] states that The Transporter was released in 2002.", "latency_ms": 6697.145700454712, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, which matches the gold answer. It also explicitly attributes this information to the cited document, remaining faithful to the provided source.", "main_total_tokens": 3067, "main_prompt_tokens": 3048, "main_completion_tokens": 19, "judge_total_tokens": 469, "judge_prompt_tokens": 398, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "[Doc 67500] The Transporter Refueled", "latency_ms": 6399.6124267578125, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2015 reboot title as 'The Transporter Refueled' and is fully supported by the provided information.", "main_total_tokens": 3198, "main_prompt_tokens": 3185, "main_completion_tokens": 13, "judge_total_tokens": 469, "judge_prompt_tokens": 403, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "[Doc 67500] states that the movie \"The Transporter Refueled\" was released in 2015.", "latency_ms": 7396.939277648926, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It also explicitly attributes this information to Doc 67500, indicating it is grounded in the provided source.", "main_total_tokens": 3109, "main_prompt_tokens": 3083, "main_completion_tokens": 26, "judge_total_tokens": 487, "judge_prompt_tokens": 409, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "According to [Doc 94414], \"Re-Elected\" (2020) features former U.S. presidents, including Abraham Lincoln, coming back from the dead as zombies on the Fourth of July.", "latency_ms": 9019.681692123413, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question specifically asks for a romantic comedy, and the gold answer says no such movie description can be found. The generated answer instead cites 'Re-Elected' (2020), which is a horror-comedy about zombie presidents, not a romantic comedy, and contradicts the gold statement that no such description was found. It also fabricates a specific document reference ([Doc 94414]) that is not grounded in the provided gold. Therefore it is both incorrect relative to the task and unfaithful to the given information.", "main_total_tokens": 2821, "main_prompt_tokens": 2779, "main_completion_tokens": 42, "judge_total_tokens": 567, "judge_prompt_tokens": 431, "judge_completion_tokens": 136}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Genius", "latency_ms": 4764.024972915649, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'Genius' is incorrect; the correct movie title that alludes to a manuscript written by Ernest Hemingway is 'Havana Darkness'. The response is not faithful to the reference answer and does not match the required movie title.", "main_total_tokens": 2897, "main_prompt_tokens": 2894, "main_completion_tokens": 3, "judge_total_tokens": 468, "judge_prompt_tokens": 389, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "According to [Doc 122259], Doll heads to Hollywood to be with her childhood best friend Em in the TV show \"Doll & Em\".", "latency_ms": 7621.318817138672, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em', matching the gold answer exactly. It is faithful to the provided document reference and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 2742, "main_prompt_tokens": 2711, "main_completion_tokens": 31, "judge_total_tokens": 489, "judge_prompt_tokens": 419, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "According to [Doc 39389], Hallam's talent for spying on people reveals his darkest fears-and his most peculiar desires, as he is driven to expose the true cause of his mother's death. The movie is \"Hallam Foe\" (2007).", "latency_ms": 10631.556987762451, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2007 movie as \"Hallam Foe,\" matching the gold answer. It stays faithful to the question and simply elaborates slightly on Hallam's spying talent and motivation without adding contradicting information.", "main_total_tokens": 3025, "main_prompt_tokens": 2970, "main_completion_tokens": 55, "judge_total_tokens": 530, "judge_prompt_tokens": 452, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "According to [Doc 22549], Sgt. Bruce Lee's last name \"Lee\" might initially suggest that he's a foe, as it sounds similar to the word \"enemy\". However, in this context, \"Lee\" is his surname, and there is no indication that it's meant to be interpreted as a reference to being an enemy.", "latency_ms": 12705.559015274048, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is simply 'A foe'. The generated answer does not clearly provide this classification and introduces an incorrect explanation about 'Lee' sounding like 'enemy', which is not accurate. It also references an external document not present in the prompt, making it unfaithful to the given information.", "main_total_tokens": 3107, "main_prompt_tokens": 3036, "main_completion_tokens": 71, "judge_total_tokens": 544, "judge_prompt_tokens": 454, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "According to [Doc 124583], Sam, an 18-year-old on the autism spectrum, takes a funny, yet painful, journey of self-discovery for love and independence.", "latency_ms": 8539.62779045105, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old, which matches the gold answer. It is faithful to the provided document snippet and does not introduce contradictions or extraneous fabricated details beyond what could plausibly be in a source description.", "main_total_tokens": 2798, "main_prompt_tokens": 2760, "main_completion_tokens": 38, "judge_total_tokens": 503, "judge_prompt_tokens": 424, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "According to [Doc 124583], in the 2017 TV show \"Atypical\", Sam is on the autism spectrum.", "latency_ms": 6702.143907546997, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the TV show Atypical, Sam is on the autism spectrum, which matches the gold answer. The reference to a document does not contradict the provided context, so the answer is both correct and faithful.", "main_total_tokens": 2513, "main_prompt_tokens": 2486, "main_completion_tokens": 27, "judge_total_tokens": 493, "judge_prompt_tokens": 414, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix", "latency_ms": 5642.080783843994, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Chamonix'), so it is both correct and faithful to the reference.", "main_total_tokens": 3434, "main_prompt_tokens": 3430, "main_completion_tokens": 4, "judge_total_tokens": 455, "judge_prompt_tokens": 400, "judge_completion_tokens": 55}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "According to [Doc 100481], the answer is \"rains\". The sentence should read: \"Hazuki clips her nails every time it rains.\"", "latency_ms": 8314.879417419434, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly supplies the missing word 'rains' and produces the correct full sentence, so it is fully correct. However, it invents a citation to 'Doc 100481' that is not present in the question context, which is an unsupported hallucinated reference, so faithfulness is not satisfied.", "main_total_tokens": 3145, "main_prompt_tokens": 3113, "main_completion_tokens": 32, "judge_total_tokens": 516, "judge_prompt_tokens": 422, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland", "latency_ms": 5331.03084564209, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Greenland'). It is both correct with respect to the question and fully faithful to the provided gold answer.", "main_total_tokens": 3233, "main_prompt_tokens": 3230, "main_completion_tokens": 3, "judge_total_tokens": 456, "judge_prompt_tokens": 395, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "According to [Doc 96838], the movie \"One Afternoon\" (2021) matches this description.", "latency_ms": 6703.850030899048, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'One Afternoon,' matching the gold answer, so correctness is 1. However, it cites a specific document ID '[Doc 96838]' which is not present in the provided context, making that justification fabricated; thus faithfulness is 0.", "main_total_tokens": 2619, "main_prompt_tokens": 2595, "main_completion_tokens": 24, "judge_total_tokens": 515, "judge_prompt_tokens": 426, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "According to [Doc 93260], \"The Life Ahead\" (2020) is the movie that matches your description, with a rating of 6.746.", "latency_ms": 35961.78150177002, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Life Ahead', which matches the gold answer. It also remains faithful to the source it cites (Doc 93260) and does not introduce contradictions or unsupported claims beyond that reference.", "main_total_tokens": 2964, "main_prompt_tokens": 2929, "main_completion_tokens": 35, "judge_total_tokens": 509, "judge_prompt_tokens": 432, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "According to [Doc 73859], the 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team.", "latency_ms": 9049.457311630249, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer is factually correct: the 2016 documentary 'McLaren' is about Bruce McLaren, the New Zealander who founded the McLaren racing team. However, the answer cites a specific document identifier ([Doc 73859]) that I cannot verify from the provided context, so the reference to that document is not grounded, making the faithfulness to the given context uncertain.", "main_total_tokens": 3161, "main_prompt_tokens": 3125, "main_completion_tokens": 36, "judge_total_tokens": 531, "judge_prompt_tokens": 420, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "[Doc 68207] doesn't mention the specific name of Paul Newman's racing team, but it does mention that he won eight championships as an owner.\n\nHowever, according to outside knowledge (not cited in the provided context), Paul Newman co-founded a racing team called Newman/Haas Racing, which is also known as Andretti Autosport.", "latency_ms": 13327.08477973938, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the name of his racing team, with the gold answer being 'The McLaren Motor Racing team.' The generated answer instead identifies Paul Newman and his team Newman/Haas Racing/Andretti Autosport, which is incorrect relative to the gold. It also explicitly relies on outside knowledge and admits the provided document does not mention the team name, so it is not faithful to the given context.", "main_total_tokens": 3403, "main_prompt_tokens": 3331, "main_completion_tokens": 72, "judge_total_tokens": 563, "judge_prompt_tokens": 451, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The text does not specify the country of origin of a specific person. However, there are several mentions of countries throughout the provided context:\n\n* Croatia (mentioned in the plot of \"What a Country!\")\n* Poland (mentioned in the plots of \"From a Far Country\" and \"Birthplace\")\n* Austria (mentioned in the plot of \"Born in Absurdistan\")\n* Turkey (mentioned in the plots of \"Born in Absurdistan\" and \"Sudani from Nigeria\")\n* Syria (mentioned in the title \"Born in Syria\")\n* Uruguay (mentioned in the plot of \"Small Country\")\n\nWithout more specific information, it is not possible to determine which country a particular person is from.", "latency_ms": 21028.03659439087, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly states the person is from New Zealand, but the generated answer claims the text does not specify the country and lists other countries, concluding it is impossible to determine the origin. This directly contradicts the gold label, so the answer is incorrect. However, the assistant’s explanation is internally consistent with its own (incorrect) reading of the context and does not invent details beyond what it describes, so it is faithful to its own reasoning, though not to the gold answer.", "main_total_tokens": 3233, "main_prompt_tokens": 3092, "main_completion_tokens": 141, "judge_total_tokens": 638, "judge_prompt_tokens": 515, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "According to [Doc 58791], there is no mention of John McClane blowing up the Great Wall of China in any of the provided documents. However, it's possible that you're thinking of a different movie or scene.\n\nBut, according to [Doc 58791] and other related documents, I can tell you that the movie \"A Good Day to Die Hard\" (2013) features John McClane traveling to Moscow, not China.", "latency_ms": 15583.47201347351, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is that there is no such movie, matching the gold response. The generated answer instead asserts that the 2013 movie is 'A Good Day to Die Hard', which is incorrect because John McClane does not blow up the Great Wall of China in that or any other film. It also fabricates references to specific documents (e.g., [Doc 58791]) that are not actually provided, so it is not faithful to the given context.", "main_total_tokens": 3348, "main_prompt_tokens": 3256, "main_completion_tokens": 92, "judge_total_tokens": 614, "judge_prompt_tokens": 489, "judge_completion_tokens": 125}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "According to [Doc 28589], the answer is \"Tomie: Forbidden Fruit\".", "latency_ms": 7035.420179367065, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'Tomie: Forbidden Fruit', which matches the gold answer and the description. However, it cites a specific document ID ('Doc 28589') as the source, which is a fabricated reference not grounded in the given context, slightly reducing faithfulness.", "main_total_tokens": 3203, "main_prompt_tokens": 3184, "main_completion_tokens": 19, "judge_total_tokens": 527, "judge_prompt_tokens": 435, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 years", "latency_ms": 5804.136514663696, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer '65 years' matches the gold answer '65' and correctly interprets the required information, remaining faithful to the question and data.", "main_total_tokens": 3547, "main_prompt_tokens": 3544, "main_completion_tokens": 3, "judge_total_tokens": 458, "judge_prompt_tokens": 397, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King teams up with a female warrior named Tala, who is the sister of The Nubian King. Together they search for a legendary relic known as The Book of Souls, which will allow them to put an end to an evil warlord. [Doc 82582]", "latency_ms": 11892.087936401367, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks what the Scorpion King and Tala search for. The generated answer explicitly states they search for a legendary relic known as The Book of Souls, which matches the gold answer exactly. The added plot detail about ending an evil warlord is consistent with the cited document and does not contradict it. Thus the answer is both correct and faithful to the provided document.", "main_total_tokens": 3352, "main_prompt_tokens": 3292, "main_completion_tokens": 60, "judge_total_tokens": 555, "judge_prompt_tokens": 451, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Not found [Doc 55764], [Doc 42429], [Doc 44424], [Doc 47808], [Doc 41618], [Doc 753], [Doc 47389], [Doc 47585], [Doc 15018], [Doc 44222], [Doc 31179], [Doc 55700], [Doc 30595], [Doc 35932], [Doc 54347], [Doc 32087], and [Doc 39618]", "latency_ms": 16195.582628250122, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The gold answer indicates the system should say it does not have the movie in its database, i.e., it should not provide a year. The generated answer also correctly does not provide a year, so in that sense it is correct with respect to the task (no incorrect year is given). However, it adds extraneous internal-looking document references instead of a clear user-facing statement like the gold. This makes it slightly less faithful to the style and content of the gold answer, though semantically it conveys that the movie was not found. Thus correctness is 1, faithfulness slightly reduced.", "main_total_tokens": 2857, "main_prompt_tokens": 2752, "main_completion_tokens": 105, "judge_total_tokens": 646, "judge_prompt_tokens": 495, "judge_completion_tokens": 151}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no mention of a movie titled \"The Galactic Baker\" in the provided context.", "latency_ms": 6673.708200454712, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year. The gold answer states the movie is not in the database, effectively not giving a year. The generated answer similarly says there is no mention of that movie in the context, also not providing a year. This matches the information and intent of the gold. Both are correct (no fabricated date) and faithful to the given context.", "main_total_tokens": 3058, "main_prompt_tokens": 3039, "main_completion_tokens": 19, "judge_total_tokens": 506, "judge_prompt_tokens": 407, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no information about a movie titled \"Return of the Null Pointer\".", "latency_ms": 6525.087594985962, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information about a movie titled \"Return of the Null Pointer,\" which is equivalent in meaning to the gold answer that the movie is not in the database. It does not introduce unsupported facts and remains faithful to the given information.", "main_total_tokens": 3055, "main_prompt_tokens": 3039, "main_completion_tokens": 16, "judge_total_tokens": 490, "judge_prompt_tokens": 406, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no movie titled \"Data Science: The Movie\". However, there are several movies related to data and science that were mentioned in the context:\n\n* \"The Joy of Data\" (2016)\n* \"The Human Face of Big Data\" (2016)\n* \"The Joy of Stats\" (2010)\n\nPlease let me know if you would like more information about these movies or any other movie mentioned in the context.", "latency_ms": 14665.946245193481, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer correctly expresses uncertainty about the movie, while the generated answer incorrectly asserts that there is no movie titled 'Data Science: The Movie', which goes beyond the provided information and is not supported by the context. It also introduces additional movies that are not mentioned in the gold response, further reducing faithfulness.", "main_total_tokens": 3081, "main_prompt_tokens": 2993, "main_completion_tokens": 88, "judge_total_tokens": 571, "judge_prompt_tokens": 478, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Not found.", "latency_ms": 4902.893543243408, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database. The generated answer, 'Not found.', is equivalent in meaning, correctly conveying that the information is unavailable. It is therefore both factually correct with respect to the gold and faithful to it.", "main_total_tokens": 2981, "main_prompt_tokens": 2977, "main_completion_tokens": 4, "judge_total_tokens": 475, "judge_prompt_tokens": 394, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no movie titled \"The Last Pixel\". However, there are movies with titles such as \"Pixels\", \"Pixel Poetry\", and \"Pixo\". If you're looking for information on one of these movies, I can try to help.", "latency_ms": 9836.585760116577, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer correctly states that the movie is not in the database, while the generated answer asserts that there is no movie titled 'The Last Pixel', which goes beyond the given information and may be false. It also adds unrelated examples. Thus, it is not correct with respect to the task and not faithful to the gold response.", "main_total_tokens": 2738, "main_prompt_tokens": 2688, "main_completion_tokens": 50, "judge_total_tokens": 534, "judge_prompt_tokens": 437, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Not found [Doc 31089], [Doc 37713], [Doc 26686], [Doc 47657], [Doc 71364], [Doc 38268], [Doc 32212], [Doc 40738], [Doc 18], [Doc 34179]", "latency_ms": 12183.152437210083, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database. The generated answer effectively says it is not found, just with extra document references. This matches the gold in substance, so it is both correct and faithful.", "main_total_tokens": 3429, "main_prompt_tokens": 3367, "main_completion_tokens": 62, "judge_total_tokens": 527, "judge_prompt_tokens": 453, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "[Doc 46069] does not mention the director of the movie \"The Last Days of Lehman Brothers\".", "latency_ms": 7997.169733047485, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the director information is not in the database. The generated answer correctly states that the cited document does not mention the director, and does not fabricate a director name. It is consistent with the database limitation and faithful to the provided doc reference, so both correctness and faithfulness are satisfied.", "main_total_tokens": 3480, "main_prompt_tokens": 3456, "main_completion_tokens": 24, "judge_total_tokens": 511, "judge_prompt_tokens": 420, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Not found.", "latency_ms": 4915.540933609009, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The gold answer states that the system cannot answer because the director information is not in the database. The generated answer says only 'Not found.' This is effectively the same meaning—that the director information is not available—so it is correct relative to the gold. However, it is less explicit about the database limitation, so it is slightly less faithful to the exact wording and nuance of the gold answer.", "main_total_tokens": 2886, "main_prompt_tokens": 2882, "main_completion_tokens": 4, "judge_total_tokens": 510, "judge_prompt_tokens": 398, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Not found", "latency_ms": 5483.743906021118, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not available in the database. The generated answer 'Not found' is semantically equivalent, correctly indicating the absence of this information and remaining faithful to the database constraint.", "main_total_tokens": 3134, "main_prompt_tokens": 3131, "main_completion_tokens": 3, "judge_total_tokens": 468, "judge_prompt_tokens": 395, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Not found.", "latency_ms": 4688.921689987183, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The gold standard answer states that the system cannot answer because the director information is not in the database. The generated answer \"Not found.\" is effectively equivalent in meaning: it indicates that the director information is not available. This is correct relative to the database and question. However, it is slightly less explicit than the gold about the source of the limitation (the database), so faithfulness to the gold reference is high but not perfect.", "main_total_tokens": 2746, "main_prompt_tokens": 2742, "main_completion_tokens": 4, "judge_total_tokens": 519, "judge_prompt_tokens": 400, "judge_completion_tokens": 119}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "According to [Doc 98375], this happens in the movie \"Rise of the Mummy\" (2021).", "latency_ms": 7235.058069229126, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'Rise of the Mummy,' matching the gold answer, so correctness is full. However, it cites a fabricated source ('Doc 98375') that is not in the prompt, which is an unfaithful hallucinated reference, so faithfulness is 0.", "main_total_tokens": 2804, "main_prompt_tokens": 2778, "main_completion_tokens": 26, "judge_total_tokens": 508, "judge_prompt_tokens": 415, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "According to [Doc 35237], this happens in the movie \"Stranger in My Bed\".", "latency_ms": 6825.186967849731, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Stranger in My Bed', which matches the gold answer. It also explicitly attributes the information to Doc 35237, indicating it is grounded in the provided source. Both correctness and faithfulness are therefore fully satisfied.", "main_total_tokens": 2764, "main_prompt_tokens": 2743, "main_completion_tokens": 21, "judge_total_tokens": 499, "judge_prompt_tokens": 416, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "According to [Doc 123938], this happens in \"Game of Silence\".", "latency_ms": 5664.907455444336, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.7, "justification": "The answer incorrectly identifies a movie; 'Game of Silence' is a TV series, not a film. However, it is accurate that this plot description refers to 'Game of Silence,' so the core factual mapping is correct. The reference to an unspecified '[Doc 123938]' is unverifiable and appears fabricated, reducing faithfulness.", "main_total_tokens": 2466, "main_prompt_tokens": 2449, "main_completion_tokens": 17, "judge_total_tokens": 513, "judge_prompt_tokens": 413, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "According to [Doc 53039], the answer is \"The Craigslist Killer\".", "latency_ms": 6203.264236450195, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, 'The Craigslist Killer', and correctly identifies the movie. It does not introduce any contradictions or unsupported claims beyond referencing a document, so it is both correct and faithful to the given information.", "main_total_tokens": 2846, "main_prompt_tokens": 2829, "main_completion_tokens": 17, "judge_total_tokens": 485, "judge_prompt_tokens": 408, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "According to [Doc 66094], this happens in the movie \"Wood Job!\".", "latency_ms": 7164.1364097595215, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", matching the gold answer. However, it introduces an unsupported reference to \"[Doc 66094]\", which is not grounded in the provided information, slightly reducing faithfulness.", "main_total_tokens": 3326, "main_prompt_tokens": 3308, "main_completion_tokens": 18, "judge_total_tokens": 498, "judge_prompt_tokens": 417, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "According to [Doc 78373], this event occurs in the movie \"Sesame Street: The Magical Wand Chase\" (2017).", "latency_ms": 8486.107110977173, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'Sesame Street: The Magical Wand Chase', matching the gold reference, so correctness is 1. However, it fabricates a citation to '[Doc 78373]', which is not given in the prompt, so it is not fully faithful to the provided information, making faithfulness 0.", "main_total_tokens": 3319, "main_prompt_tokens": 3289, "main_completion_tokens": 30, "judge_total_tokens": 537, "judge_prompt_tokens": 438, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "According to [Doc 74597], this happens in the movie \"Beyond\" (2016).", "latency_ms": 7021.272420883179, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Beyond,\" which matches the gold answer. It does not introduce contradictions or unsupported claims; the added year (2016) is consistent with the known film and does not conflict with the prompt, so the response is both correct and faithful to the source description.", "main_total_tokens": 2841, "main_prompt_tokens": 2820, "main_completion_tokens": 21, "judge_total_tokens": 519, "judge_prompt_tokens": 428, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "According to [Doc 22478], this happens in the movie \"6ixtynin9\".", "latency_ms": 7042.961120605469, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"6ixtynin9,\" matching the gold answer exactly. It does not introduce any unsupported or contradictory information, and the brief citation style does not affect faithfulness to the provided gold label.", "main_total_tokens": 3000, "main_prompt_tokens": 2978, "main_completion_tokens": 22, "judge_total_tokens": 502, "judge_prompt_tokens": 423, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "According to [Doc 32410], this happens in the movie \"Club Dread\" (2004).", "latency_ms": 7122.777223587036, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3154648767857289, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'Club Dread', so the correctness is 1. However, it cites a specific document number ([Doc 32410]) that is not present in the user-provided context, making that justification unfaithful to the given information, so faithfulness is 0.", "main_total_tokens": 2940, "main_prompt_tokens": 2918, "main_completion_tokens": 22, "judge_total_tokens": 540, "judge_prompt_tokens": 446, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet (2010) [Doc 48873]", "latency_ms": 6890.1989459991455, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot. The gold answer is \"Toilet\", and the generated answer is \"Toilet (2010) [Doc 48873]\". This is the same film, only with additional release year and document reference. The content is correct and consistent with the gold, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3629, "main_prompt_tokens": 3616, "main_completion_tokens": 13, "judge_total_tokens": 529, "judge_prompt_tokens": 425, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel (1981) [Doc 1024]", "latency_ms": 6273.722887039185, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the film as 'Electric Angel', matching the gold label exactly (year and doc tag are extra but not contradictory). The response is fully faithful to the provided information.", "main_total_tokens": 3174, "main_prompt_tokens": 3161, "main_completion_tokens": 13, "judge_total_tokens": 497, "judge_prompt_tokens": 428, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "According to the provided context, the lowest rated movie from 2023 is \"Doomsday Stories\" with a rating of 2.167 [Doc 108366].", "latency_ms": 5520.214080810547, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the lowest rated movie from 2023 is 'Mega Ape', while the generated answer claims it is 'Doomsday Stories'. This is factually incorrect relative to the gold, so correctness is 0. The generated answer also invents supporting document context ([Doc 108366]) that is not in the prompt, so it is not faithful to the given information, yielding a faithfulness score of 0.", "main_total_tokens": 2836, "main_prompt_tokens": 2799, "main_completion_tokens": 37, "judge_total_tokens": 531, "judge_prompt_tokens": 417, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "According to the provided context, the highest rated TV show from 2010 is \"Downton Abbey\" with a rating of 8.1 [Doc 119146].", "latency_ms": 5903.949737548828, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says the highest rated TV show from 2010 is 'Tea House', while the generated answer claims it is 'Downton Abbey' with a rating of 8.1. This directly contradicts the gold, so the answer is factually incorrect relative to the labeled truth. It also invents a document reference and rating value not supported by the gold, so it is not faithful to the provided ground truth.", "main_total_tokens": 3395, "main_prompt_tokens": 3358, "main_completion_tokens": 37, "judge_total_tokens": 534, "judge_prompt_tokens": 417, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "According to the provided context, the highest rated Japanese language movie from 1999 is \"Kikujiro\" with a rating of 7.5 [Doc 22213].", "latency_ms": 8604.508638381958, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates that the highest rated Japanese language movie from 1999 is 'The Uncut Story of Mewtwo's Origin', but the generated answer claims it is 'Kikujiro' with a rating of 7.5. This directly contradicts the gold answer, so the response is incorrect. It also shows the model is not faithful to the given gold/reference information, instead relying on a different document and giving a conflicting result.", "main_total_tokens": 2855, "main_prompt_tokens": 2817, "main_completion_tokens": 38, "judge_total_tokens": 550, "judge_prompt_tokens": 428, "judge_completion_tokens": 122}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is Bloodeaters with a rating of 4.0. The language of this movie is en (English).", "latency_ms": 7988.694906234741, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The question asks only for the language, which is correctly given as English, so the factual answer matches the gold. However, the generated response adds extra fabricated details about a specific movie and rating that are not in the prompt or gold, so it is not fully faithful to the provided information.", "main_total_tokens": 2907, "main_prompt_tokens": 2875, "main_completion_tokens": 32, "judge_total_tokens": 503, "judge_prompt_tokens": 414, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The Sopranos (Doc 115146) had a popularity of 59.4284 and is the TV show from 1999 with the highest popularity.", "latency_ms": 9041.429996490479, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'Law & Order: Special Victims Unit' as the correct show, while the generated answer claims 'The Sopranos' had the highest popularity in 1999. This is factually incorrect relative to the gold. The generated answer also references a specific document and popularity value that contradict the gold label, so it is not faithful to the provided ground truth.", "main_total_tokens": 3271, "main_prompt_tokens": 3236, "main_completion_tokens": 35, "judge_total_tokens": 531, "judge_prompt_tokens": 423, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2 [Doc 79355]", "latency_ms": 5680.992603302002, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (7.2) and does not introduce any unsupported information beyond a document reference, so it is both correct and faithful.", "main_total_tokens": 3043, "main_prompt_tokens": 3033, "main_completion_tokens": 10, "judge_total_tokens": 456, "judge_prompt_tokens": 392, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "According to [Doc 22481], the movie \"The Story of Us\" was released in 1999.\n\nHowever, there is another movie with a similar title, \"The Story of Us\", which was released in 2025, according to [Doc 112077]. \n\nAdditionally, there's also a TV movie titled \"The Story of Us\" that was released in 2019, as per [Doc 88811].\n\nSo, the answer depends on which specific movie you are referring to.", "latency_ms": 16220.170497894287, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer expects 2019, but the generated answer does not provide a single clear year as the answer and instead says it depends, listing 1999, 2025, and 2019. This does not match the gold, so correctness is 0. However, the assistant’s statements are internally consistent with its cited docs and do not contradict its own sources, so faithfulness is 1.", "main_total_tokens": 3053, "main_prompt_tokens": 2950, "main_completion_tokens": 103, "judge_total_tokens": 599, "judge_prompt_tokens": 485, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "Not found.", "latency_ms": 5207.413196563721, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 279 votes, but the generated answer said 'Not found,' which is incorrect. However, the model did not fabricate a number and accurately reflected that it did not have the information, so it is faithful to its own (incorrect) retrieval.", "main_total_tokens": 3110, "main_prompt_tokens": 3106, "main_completion_tokens": 4, "judge_total_tokens": 468, "judge_prompt_tokens": 384, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "Not found [Doc 45962], [Doc 43976], [Doc 85434].", "latency_ms": 6650.822639465332, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 12 votes, but the generated answer says the information was not found and does not provide the correct number. It is therefore incorrect. However, it remains faithful to the (incorrect) retrieval result it reports, so faithfulness is high.", "main_total_tokens": 2838, "main_prompt_tokens": 2817, "main_completion_tokens": 21, "judge_total_tokens": 484, "judge_prompt_tokens": 401, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "According to [Doc 46069], the movie \"The Last Days of Lehman Brothers\" was released in 2009.", "latency_ms": 7445.090293884277, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'The Last Days of Lehman Brothers' was released in 2009, matching the gold answer. It also remains faithful to the provided document reference without introducing unsupported information.", "main_total_tokens": 2926, "main_prompt_tokens": 2899, "main_completion_tokens": 27, "judge_total_tokens": 487, "judge_prompt_tokens": 413, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en", "latency_ms": 5379.614591598511, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The original language of the movie \"The King’s Speech\" is English, which matches both the gold answer and the generated answer of \"en.\"", "main_total_tokens": 3374, "main_prompt_tokens": 3372, "main_completion_tokens": 2, "judge_total_tokens": 447, "judge_prompt_tokens": 384, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The movie \"Wall Street\" was released in 1987 [Doc 6993].", "latency_ms": 5588.777542114258, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'Wall Street' was released in 1987, matching the gold answer, and it remains fully grounded in the provided information.", "main_total_tokens": 2269, "main_prompt_tokens": 2250, "main_completion_tokens": 19, "judge_total_tokens": 461, "judge_prompt_tokens": 396, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie where classmates have to play a survival game using their cell phones is \"Brain Fluid Explosion Girl\" [Doc 69929].", "latency_ms": 6570.918321609497, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Brain Fluid Explosion Girl,' which matches the gold answer exactly. It is faithful to the provided information and does not introduce unsupported details.", "main_total_tokens": 2193, "main_prompt_tokens": 2165, "main_completion_tokens": 28, "judge_total_tokens": 481, "judge_prompt_tokens": 415, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show where six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" (1983) [Doc 112849].", "latency_ms": 7295.987844467163, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.4306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.719741384391281, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' which matches the gold answer. It stays faithful to the provided information, only adding the year and a plausible document reference without contradicting any facts.", "main_total_tokens": 2257, "main_prompt_tokens": 2223, "main_completion_tokens": 34, "judge_total_tokens": 498, "judge_prompt_tokens": 423, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones is Nacht der Wölfe (Night of the Wolves), as described in [Doc 2590].", "latency_ms": 8425.21595954895, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe', matching the gold answer exactly and not adding any contradictory or hallucinated information beyond a harmless reference to a document.", "main_total_tokens": 2176, "main_prompt_tokens": 2131, "main_completion_tokens": 45, "judge_total_tokens": 511, "judge_prompt_tokens": 442, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of Picnic at Hanging Rock take place on Valentine's Day, 1900 [Doc 126191].", "latency_ms": 5864.664077758789, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events take place in 1900, matching the gold answer. The added detail about Valentine's Day does not contradict the gold and is consistent with the source citation, so the response is both correct and faithful.", "main_total_tokens": 1958, "main_prompt_tokens": 1933, "main_completion_tokens": 25, "judge_total_tokens": 486, "judge_prompt_tokens": 407, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie about a young violinist is June (Doc 32483).", "latency_ms": 5936.894178390503, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title of the 2004 movie, whose answer is 'June'. The generated answer correctly identifies the movie as 'June' and does not introduce any unsupported details beyond what is stated, so it is both correct and faithful.", "main_total_tokens": 2414, "main_prompt_tokens": 2395, "main_completion_tokens": 19, "judge_total_tokens": 531, "judge_prompt_tokens": 450, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1431.4014911651611, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a particular film title as the correct answer, but the generated answer says 'I don't know,' which is incorrect with respect to the gold. However, the model does not hallucinate an incorrect movie title and honestly states its lack of knowledge, so it remains fully faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 399, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 516.6044235229492, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 7.6, but the generated response says 'I don't know,' which is incorrect. However, the answer does not invent any false information; it simply expresses a lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 473, "judge_prompt_tokens": 389, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1445.3237056732178, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 0.9828920819566879, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3364045224324153, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is incorrect. The model directly answered a numeric count without referencing or contradicting any provided context beyond the question itself, so it is internally consistent and thus faithful to the (limited) prompt.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 394, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 269.3791389465332, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. However, there is no indication that the model contradicted any provided context; it simply produced the wrong count, so faithfulness to the (albeit incorrect) reasoning remains high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 482, "judge_prompt_tokens": 395, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1757.1029663085938, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct answer 'Fantômas.' Thus correctness is 0. However, the answer is internally consistent and does not fabricate information, so it is fully faithful, giving a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 389, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" (1990). [Doc 11205]", "latency_ms": 6940.666198730469, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Kill Cruise\" and matches the gold answer exactly. It does not introduce any unsupported information beyond the title and year, so it is both correct and faithful to the provided reference.", "main_total_tokens": 2182, "main_prompt_tokens": 2151, "main_completion_tokens": 31, "judge_total_tokens": 488, "judge_prompt_tokens": 413, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life's most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 8086.523532867432, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as \"Won't You Be My Neighbor?\" and is faithful to the provided information.", "main_total_tokens": 2339, "main_prompt_tokens": 2299, "main_completion_tokens": 40, "judge_total_tokens": 502, "judge_prompt_tokens": 439, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 5093.467473983765, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 2018, but the generated response claims the answer is not found in the context instead of giving the known gold answer. This makes it factually incorrect. However, it is faithful to what the model stated (it accurately reflects its stated inability to find the answer in the context), so faithfulness is high.", "main_total_tokens": 2460, "main_prompt_tokens": 2449, "main_completion_tokens": 11, "judge_total_tokens": 485, "judge_prompt_tokens": 387, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is Astro Boy (1980) [Doc 112591].", "latency_ms": 7020.225763320923, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Astro Boy as the TV series described in the question. It is faithful to the gold answer and does not introduce contradictory or extraneous information that changes the meaning, only adding a year and a document reference which do not affect correctness.", "main_total_tokens": 2368, "main_prompt_tokens": 2337, "main_completion_tokens": 31, "judge_total_tokens": 499, "judge_prompt_tokens": 417, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "According to [Doc 67513], the woman and her son were held captive for 7 years in the movie \"Room\" released in 2015.", "latency_ms": 6705.916404724121, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly states that they were held captive for 7 years, matching the gold answer. However, it cites a specific document ([Doc 67513]) that is not present in the provided context, making the explanation unfaithful to the given information.", "main_total_tokens": 1911, "main_prompt_tokens": 1878, "main_completion_tokens": 33, "judge_total_tokens": 510, "judge_prompt_tokens": 426, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "According to [Doc 133469], Brink's-Mat security depot was holding gold bullion worth £26m before it was broken into on 26 November 1983.", "latency_ms": 8271.953582763672, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Brink's-Mat security depot was holding gold bullion worth £26 million before the 26 November 1983 break-in, which matches the provided gold value. It remains faithful to the source by explicitly attributing the information to Doc 133469 and does not introduce unsupported details.", "main_total_tokens": 2164, "main_prompt_tokens": 2126, "main_completion_tokens": 38, "judge_total_tokens": 523, "judge_prompt_tokens": 430, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6633.584499359131, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered 'I don't know,' which does not provide the correct numeric popularity score (80.5772), so correctness is 0. However, it did not hallucinate a false answer and honestly admitted uncertainty, so faithfulness to its own knowledge is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 482, "judge_prompt_tokens": 396, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 385.17260551452637, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that 1 Russian language TV show was released in 1997, while the generated answer is 0. This is factually incorrect relative to the gold, so correctness is 0. The generated answer is a single numeric response with no reasoning that contradicts the user query itself, so it is internally faithful to what it outputs, giving a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 496, "judge_prompt_tokens": 385, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan undergoes an eye transplant surgery that restores her sight in \"Angel Eyes\" (2014). [Doc 121928]", "latency_ms": 7134.443759918213, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Angel Eyes,' matching the gold answer exactly. It remains faithful to the question, only adding the year (2014), which is accurate and does not contradict the source.", "main_total_tokens": 2440, "main_prompt_tokens": 2410, "main_completion_tokens": 30, "judge_total_tokens": 498, "judge_prompt_tokens": 423, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of this movie is French (fr). [Doc 110681]", "latency_ms": 5409.567594528198, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states the language is French, while the gold label says Korean, so the answer is factually incorrect. However, the explanation is fully faithful to the provided generated text and gold label.", "main_total_tokens": 2246, "main_prompt_tokens": 2229, "main_completion_tokens": 17, "judge_total_tokens": 463, "judge_prompt_tokens": 392, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 772.0704078674316, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.5, "precision_at_5": 0.2, "ndcg_at_5": 0.6131471927654584, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.6131471927654584, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1) does not match the correct gold answer (2), so it is factually incorrect. However, the answer directly attempts to answer the user’s specific question without introducing unrelated or hallucinated information, so it is faithful to the query itself.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 394, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The best-rated movie released in 1979 is not found in the provided context.", "latency_ms": 5427.233934402466, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that there are no movies from 1979 in the database, so the system cannot identify a best-rated movie. The generated answer correctly reflects this limitation by saying the best-rated movie is not found in the provided context. It does not introduce any contradictory or extraneous information, so it is both correct with respect to the task and faithful to the given gold answer/context.", "main_total_tokens": 2245, "main_prompt_tokens": 2227, "main_completion_tokens": 18, "judge_total_tokens": 521, "judge_prompt_tokens": 412, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "There is no mention of a movie titled \"Back to the Future Part IV\" in the provided context. The highest numbered Back to the Future title mentioned is \"Back to the Future Part III\", which was released in 1990 [Doc 10621].", "latency_ms": 9827.738285064697, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no mention of a movie titled 'Back to the Future Part IV' and references that Part III (1990) is the highest numbered title mentioned. This aligns with the gold answer that Part IV does not exist in the database. It is both factually correct and faithful to the provided context.", "main_total_tokens": 2598, "main_prompt_tokens": 2545, "main_completion_tokens": 53, "judge_total_tokens": 550, "judge_prompt_tokens": 452, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer convey that there are no movies from 1980 about the Chernobyl disaster. The generated answer adds a note about the timing of the event but does not contradict the gold answer. Thus it is correct and faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 517, "judge_prompt_tokens": 434, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 238.15155029296875, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer ('0') is numerically incorrect compared to the gold answer ('4'), so correctness is 0. The answer is at least internally consistent with itself (it directly responds with a number and does not contradict any of its own statements), so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 476, "judge_prompt_tokens": 388, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 330.75976371765137, "retrieval_recall": 0.0, "retrieval_mrr": 0.125, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.19342640361727081, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric answer (2) and the generated answer is 1, which is incorrect. The evaluation itself is faithful to the comparison between Gold and Generated, but correctness with respect to the gold label is 0.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 386, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 233.66785049438477, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show about Super Mario Bros released in the 1990s, while the generated answer says 0. This directly contradicts the gold, so the generated answer is incorrect. However, the explanation here correctly reflects the discrepancy between gold and generated answers, so it is faithful to the given information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 488, "judge_prompt_tokens": 389, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 254.0299892425537, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (0), which is correct. The reasoning required is straightforward counting; there were no TV shows about Super Mario Bros released in that interval, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 467, "judge_prompt_tokens": 391, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 285.09998321533203, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto', a specific and well-known ninja-themed TV show. The generated answer says 'I don't know', which is incorrect because it fails to provide the correct response. However, it is faithful in that it does not hallucinate or contradict the given information; it simply expresses a lack of knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 482, "judge_prompt_tokens": 386, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "There is no mention of a Jurassic Park/Power Ranger crossover movie in the provided context.", "latency_ms": 5669.909238815308, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clarifies that such a movie does not exist and therefore was not released. The generated answer only states that there is no mention of such a movie in the provided context, which does not correctly answer the question about its release; it implies uncertainty rather than stating that it was not released, so it is not fully correct. However, it is faithful to the given (unspecified) context, as it accurately reports the lack of mention there.", "main_total_tokens": 2237, "main_prompt_tokens": 2218, "main_completion_tokens": 19, "judge_total_tokens": 525, "judge_prompt_tokens": 403, "judge_completion_tokens": 122}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Barbie does not fight the Predator in any of these movies.", "latency_ms": 5592.6878452301025, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no movie in which Barbie fights the Predator, which is equivalent in meaning to the gold answer that such a movie was not released. It is both factually correct and faithful to the intent and content of the gold answer.", "main_total_tokens": 2623, "main_prompt_tokens": 2609, "main_completion_tokens": 14, "judge_total_tokens": 477, "judge_prompt_tokens": 394, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The 4th oldest Star Wars movie is not found.", "latency_ms": 5354.348659515381, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies 'Star Wars: Droids - The Great Heep' as the 4th oldest Star Wars movie. The generated answer incorrectly states that the 4th oldest Star Wars movie is not found, which contradicts the gold answer. Thus, the generated answer is incorrect. However, it does not introduce hallucinated specific content; it simply denies having an answer, so it is faithful to its own (incorrect) claim rather than fabricating details.", "main_total_tokens": 2606, "main_prompt_tokens": 2593, "main_completion_tokens": 13, "judge_total_tokens": 526, "judge_prompt_tokens": 401, "judge_completion_tokens": 125}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 4941.896677017212, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates the correct year is 1986, but the generated answer claims the answer is not in the context instead of giving 1986, so it is factually incorrect. However, given that the model is presumably constrained to the provided context and states that it cannot find the answer there, it is being honest about its limitation, so it is faithful to the (assumed) context.", "main_total_tokens": 2460, "main_prompt_tokens": 2449, "main_completion_tokens": 11, "judge_total_tokens": 499, "judge_prompt_tokens": 387, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 300.7211685180664, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The user asked for the rating of the movie Forrest Gump, and the gold answer is 8.466. The generated answer said 'I don't know,' which is incorrect because the correct rating is known and provided. However, the model did not hallucinate a wrong rating or fabricate information; it honestly admitted not knowing. Thus, correctness is 0, but faithfulness to its own uncertainty is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 512, "judge_prompt_tokens": 398, "judge_completion_tokens": 114}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is Mighty Med [Doc 121125].", "latency_ms": 6362.11895942688, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med', which matches the gold answer exactly. It stays faithful to the information given in the prompt and does not introduce contradictory or extraneous claims beyond a harmless citation-style reference.", "main_total_tokens": 2017, "main_prompt_tokens": 1988, "main_completion_tokens": 29, "judge_total_tokens": 498, "judge_prompt_tokens": 420, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "The seven prisoners escaped from Villa Devoto prison in Buenos Aires, Argentina. [Doc 53582]", "latency_ms": 6314.1114711761475, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question only requires the country, which is Argentina. The generated answer specifies a particular prison in Buenos Aires, Argentina, which is consistent with the gold answer and does not contradict it. Therefore it is both correct and faithful.", "main_total_tokens": 2616, "main_prompt_tokens": 2594, "main_completion_tokens": 22, "judge_total_tokens": 488, "judge_prompt_tokens": 412, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The apprentice falls into Jiang Jie's trap.\n\nSource:\n[Doc 135231] Title: Sweet Trap\nYear: 2024", "latency_ms": 6595.925807952881, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the apprentice falls into Jiang Jie's trap, which matches the gold answer and is supported by the source document. No contradictions or hallucinations are present.", "main_total_tokens": 2214, "main_prompt_tokens": 2185, "main_completion_tokens": 29, "judge_total_tokens": 478, "judge_prompt_tokens": 411, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 717.5219058990479, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.7095272044910244, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.7095272044910244, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (4), so it is factually incorrect relative to the provided gold. There is no indication that the model contradicted its own prior reasoning or introduced unsupported fabrication beyond giving a single numeric answer, so it is fully faithful to its (albeit incorrect) internal reasoning.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 482, "judge_prompt_tokens": 385, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "47", "latency_ms": 285.40945053100586, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer correctly states there are no movies featuring both Bruce Lee and Steven Seagal, while the generated answer incorrectly claims there are 47 such movies. This directly contradicts the gold, so the correctness is 0. The evaluation here is faithful to the provided gold and generated responses, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 395, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 5799.696207046509, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3 and does not introduce unsupported information beyond echoing that value with a reference marker.", "main_total_tokens": 2201, "main_prompt_tokens": 2180, "main_completion_tokens": 21, "judge_total_tokens": 463, "judge_prompt_tokens": 403, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "Stock car racing [Doc 86410].", "latency_ms": 4928.22265625, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer says the movie features stock car racing, but the correct style of racing is drift, so the answer is incorrect. There is no evidence from the provided information supporting stock car racing, so the response is also unfaithful to the reference.", "main_total_tokens": 2368, "main_prompt_tokens": 2358, "main_completion_tokens": 10, "judge_total_tokens": 464, "judge_prompt_tokens": 387, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little is \"Confronting a Serial Killer\". [Doc 131298]", "latency_ms": 8563.61174583435, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'Confronting a Serial Killer', which matches the gold answer exactly. It also remains faithful to the provided information and does not introduce contradictions or unsupported claims.", "main_total_tokens": 2193, "main_prompt_tokens": 2147, "main_completion_tokens": 46, "judge_total_tokens": 523, "judge_prompt_tokens": 451, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 6202.56233215332, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and correctly attributes it, so it is both correct and faithful to the provided information.", "main_total_tokens": 2368, "main_prompt_tokens": 2345, "main_completion_tokens": 23, "judge_total_tokens": 469, "judge_prompt_tokens": 407, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\". [Doc 22098]", "latency_ms": 6554.153680801392, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Sixth Sense,' which matches the gold answer, and it remains faithful to the given information without adding unsupported claims.", "main_total_tokens": 2110, "main_prompt_tokens": 2080, "main_completion_tokens": 30, "judge_total_tokens": 483, "judge_prompt_tokens": 420, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in 2002 [Doc 27318].", "latency_ms": 5447.182893753052, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer, and it is consistent with the provided information, making it both correct and faithful.", "main_total_tokens": 2386, "main_prompt_tokens": 2370, "main_completion_tokens": 16, "judge_total_tokens": 463, "judge_prompt_tokens": 395, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\". [Doc 67500]", "latency_ms": 7059.968948364258, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly giving the title as 'The Transporter Refueled' and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 2425, "main_prompt_tokens": 2395, "main_completion_tokens": 30, "judge_total_tokens": 483, "judge_prompt_tokens": 420, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The Transporter Refueled was released in 2015 [Doc 67500].", "latency_ms": 5616.46032333374, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It is fully consistent with the provided information and does not introduce unsupported claims.", "main_total_tokens": 2282, "main_prompt_tokens": 2263, "main_completion_tokens": 19, "judge_total_tokens": 474, "judge_prompt_tokens": 402, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "The romantic comedy where Abraham Lincoln comes back from the dead is not found in the provided context.", "latency_ms": 5465.505361557007, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there is no such romantic comedy identified in the context, and the generated answer stays faithful to the given information without inventing a movie title.", "main_total_tokens": 2094, "main_prompt_tokens": 2074, "main_completion_tokens": 20, "judge_total_tokens": 477, "judge_prompt_tokens": 409, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by Ernest Hemingway is \"Genius\" [Doc 73141].", "latency_ms": 7070.703029632568, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The generated answer is incorrect: the movie that alludes to a manuscript written by Ernest Hemingway is 'Havana Darkness,' not 'Genius.' The answer appears to be based on some document reference (Doc 73141), so it is somewhat grounded in the provided context rather than being purely fabricated, but it is still factually wrong for the question asked.", "main_total_tokens": 2151, "main_prompt_tokens": 2122, "main_completion_tokens": 29, "judge_total_tokens": 519, "judge_prompt_tokens": 414, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "The TV show where Doll heads to Hollywood to be with her childhood best friend Em is \"Doll & Em\". [Doc 122259]", "latency_ms": 6642.440319061279, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Doll & Em,\" which matches the gold answer exactly. It is fully supported by the provided information and does not introduce any unsupported details, so both correctness and faithfulness are maximal.", "main_total_tokens": 2089, "main_prompt_tokens": 2059, "main_completion_tokens": 30, "judge_total_tokens": 496, "judge_prompt_tokens": 418, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in the 2007 movie \"Hallam Foe\". [Doc 39389]", "latency_ms": 8089.8308753967285, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2007 movie as \"Hallam Foe\" and restates the question accurately without adding unsupported information. It is both factually correct and faithful to the provided context.", "main_total_tokens": 2352, "main_prompt_tokens": 2313, "main_completion_tokens": 39, "judge_total_tokens": 509, "judge_prompt_tokens": 437, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 4684.236526489258, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'A foe,' but the generated answer claims the answer is not in the context, which is incorrect. It neither provides the correct label nor reflects the supervision signal, so it is both factually incorrect and unfaithful to the gold.", "main_total_tokens": 2264, "main_prompt_tokens": 2253, "main_completion_tokens": 11, "judge_total_tokens": 479, "judge_prompt_tokens": 396, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "The age of Sam in the 2017 TV show Atypical is 18 years old. [Doc 124583]", "latency_ms": 6111.590147018433, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, matching the gold answer exactly and staying faithful to the provided information.", "main_total_tokens": 2059, "main_prompt_tokens": 2033, "main_completion_tokens": 26, "judge_total_tokens": 480, "judge_prompt_tokens": 413, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam, an 18-year-old in the TV show Atypical (2017), is on the autism spectrum [Doc 124583].", "latency_ms": 6180.521726608276, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which matches the gold answer exactly. It does not introduce any contradictions or unsupported claims; the extra detail about his age and referencing a document does not alter the core factual claim. Therefore, it is both correct and faithful.", "main_total_tokens": 1902, "main_prompt_tokens": 1873, "main_completion_tokens": 29, "judge_total_tokens": 504, "judge_prompt_tokens": 416, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "According to [Doc 59659], Sandra was born in Chamonix.", "latency_ms": 5784.768104553223, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, matching the gold answer. It also attributes this information to a cited document, which is consistent with the query context, so it is faithful to the provided information.", "main_total_tokens": 2556, "main_prompt_tokens": 2539, "main_completion_tokens": 17, "judge_total_tokens": 490, "judge_prompt_tokens": 413, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "it rains", "latency_ms": 4049.8387813568115, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'it rains' matches the gold answer 'rains' in meaning and is correct and faithful to the reference.", "main_total_tokens": 2421, "main_prompt_tokens": 2418, "main_completion_tokens": 3, "judge_total_tokens": 450, "judge_prompt_tokens": 393, "judge_completion_tokens": 57}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "According to [Doc 114810], Tabaluga is the last of the dragons and the crown-prince of Greenland, a magical place inhabited by talking animals of many different species.", "latency_ms": 7821.3934898376465, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which matches the gold answer. It does not introduce contradictions or unsupported facts relevant to the question, so it is both correct and faithful.", "main_total_tokens": 2389, "main_prompt_tokens": 2351, "main_completion_tokens": 38, "judge_total_tokens": 506, "judge_prompt_tokens": 431, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers\" is not found among the provided documents.", "latency_ms": 7973.909616470337, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'One Afternoon', but the generated answer says the movie is not found, which is incorrect. However, it remains faithful to the (incorrect) reasoning that it could not find the movie in the documents and does not hallucinate an alternative title.", "main_total_tokens": 1984, "main_prompt_tokens": 1941, "main_completion_tokens": 43, "judge_total_tokens": 529, "judge_prompt_tokens": 445, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The movie that matches the description is \"The Life Ahead\" (2020), as it is an Italian language movie where a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her.\n\nSource: [Doc 93260]", "latency_ms": 5989.43018913269, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Life Ahead\" and matches the description involving a Holocaust survivor and a 12-year-old street kid who robbed her. It is consistent with the provided gold answer and the cited source, showing both correctness and faithfulness.", "main_total_tokens": 2177, "main_prompt_tokens": 2126, "main_completion_tokens": 51, "judge_total_tokens": 532, "judge_prompt_tokens": 448, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary “McLaren” is about Bruce McLaren, who founded the McLaren Motor Racing team. [Doc 73859]", "latency_ms": 6877.855062484741, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and aligns with the gold answer. The additional detail about founding the McLaren Motor Racing team is factually true and does not contradict the source, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2211, "main_prompt_tokens": 2181, "main_completion_tokens": 30, "judge_total_tokens": 501, "judge_prompt_tokens": 414, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The name of his racing team is not found in the provided context.", "latency_ms": 5412.817716598511, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that his racing team is the McLaren Motor Racing team, but the generated answer claims the name is not found. This is factually incorrect relative to the gold, so correctness is 0. However, assuming the model only had access to the provided (unspecified) context and that the team name truly was not in that context, the answer is faithful to the context given, so faithfulness is 1.", "main_total_tokens": 2466, "main_prompt_tokens": 2451, "main_completion_tokens": 15, "judge_total_tokens": 515, "judge_prompt_tokens": 397, "judge_completion_tokens": 118}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The country of the person is not explicitly mentioned in the provided context. However, based on the information given in [Doc 4064], it seems that Guy Bennett is an American journalist who interviews another character about their past experiences in Moscow in 1983.\n\nSo, while we can't determine the country of origin for \"he\" with certainty without more context, if we assume \"he\" refers to Guy Bennett, then his country would be the United States.", "latency_ms": 14296.515464782715, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is New Zealand, but the generated answer concludes the person is from the United States, which is incorrect. The assistant correctly states that the country is not explicitly mentioned and reasons based on Doc 4064, so it is faithful to the given (though unseen) context, but the final answer does not match the gold.", "main_total_tokens": 2486, "main_prompt_tokens": 2392, "main_completion_tokens": 94, "judge_total_tokens": 566, "judge_prompt_tokens": 468, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 4925.349712371826, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that there is no such movie in the database; saying only that the answer is not in the provided context does not address the actual question, so the generated answer is incorrect. However, it is faithful to the given context because it does not invent a movie or contradict the information, it just fails to provide the needed database-based response.", "main_total_tokens": 2436, "main_prompt_tokens": 2425, "main_completion_tokens": 11, "judge_total_tokens": 510, "judge_prompt_tokens": 408, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with this description: \"When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental\" is Tomie: Forbidden Fruit. [Doc 28589]", "latency_ms": 10387.296438217163, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Tomie: Forbidden Fruit', which matches the gold answer exactly and restates the question faithfully without adding unsupported information.", "main_total_tokens": 2439, "main_prompt_tokens": 2380, "main_completion_tokens": 59, "judge_total_tokens": 538, "judge_prompt_tokens": 475, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years. [Doc 98566]", "latency_ms": 6403.9623737335205, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states they have been married for 65 years, which matches the gold answer of 65, so it is both correct and faithful to the provided information.", "main_total_tokens": 2743, "main_prompt_tokens": 2723, "main_completion_tokens": 20, "judge_total_tokens": 478, "judge_prompt_tokens": 413, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King and Tala search for the Book of Souls, as stated in [Doc 82582].", "latency_ms": 6651.219606399536, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly states that they search for the Book of Souls, matching the gold answer, so correctness is 1. However, it cites a specific document ([Doc 82582]) that is not present in the provided context, making that justification unverifiable and thus unfaithful to the given information, so faithfulness is 0.", "main_total_tokens": 2599, "main_prompt_tokens": 2575, "main_completion_tokens": 24, "judge_total_tokens": 516, "judge_prompt_tokens": 416, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 4938.032865524292, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the system should say it does not have information about the movie. The generated answer similarly states that the answer is not found in the provided context, which is effectively equivalent in meaning. It does not hallucinate a year or contradict the gold standard, so it is both correct and faithful.", "main_total_tokens": 2124, "main_prompt_tokens": 2113, "main_completion_tokens": 11, "judge_total_tokens": 493, "judge_prompt_tokens": 401, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no mention of \"The Galactic Baker\" in any of the provided documents.", "latency_ms": 5673.858165740967, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no mention of the movie in the provided documents, which is consistent with the gold answer indicating lack of information. It does not fabricate a release year and remains faithful to the evidence by acknowledging absence of data.", "main_total_tokens": 2412, "main_prompt_tokens": 2394, "main_completion_tokens": 18, "judge_total_tokens": 486, "judge_prompt_tokens": 406, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 5114.2778396606445, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database. The generated answer states the answer is not found in the provided context, which is effectively equivalent in this QA/eval setting. It does not introduce incorrect information and is consistent with the lack of data, so it is both correct and faithful to the gold.", "main_total_tokens": 2384, "main_prompt_tokens": 2373, "main_completion_tokens": 11, "judge_total_tokens": 495, "judge_prompt_tokens": 401, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 4738.484859466553, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database. The generated answer states that the answer is not found in the provided context, which is effectively equivalent in this QA setup. It does not hallucinate a year or contradict the gold, so it is both correct and faithful.", "main_total_tokens": 2327, "main_prompt_tokens": 2316, "main_completion_tokens": 11, "judge_total_tokens": 488, "judge_prompt_tokens": 401, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 4730.715274810791, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the information is not available, which is correct since there is no data about that movie. The generated answer is faithful to the absence of information in the context.", "main_total_tokens": 2214, "main_prompt_tokens": 2203, "main_completion_tokens": 11, "judge_total_tokens": 472, "judge_prompt_tokens": 401, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 4503.1492710113525, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer and generated answer are semantically equivalent: both state that the release year (or information) is not available. This is correct given the task, and the generated response remains faithful to the lack of information in the context.", "main_total_tokens": 2013, "main_prompt_tokens": 2002, "main_completion_tokens": 11, "judge_total_tokens": 477, "judge_prompt_tokens": 399, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 5175.0664710998535, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer correctly states that the movie is not in the database. The generated answer states that the answer is not found in the provided context, which is effectively equivalent and does not invent a date. Thus it is correct and faithful to the (lack of) information.", "main_total_tokens": 2571, "main_prompt_tokens": 2560, "main_completion_tokens": 11, "judge_total_tokens": 487, "judge_prompt_tokens": 402, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Days of Lehman Brothers\" is not found in the provided context.", "latency_ms": 7087.340593338013, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided data/context. This is correct relative to the database constraint and matches the gold answer’s intent, so the response is both correct and faithful.", "main_total_tokens": 2645, "main_prompt_tokens": 2622, "main_completion_tokens": 23, "judge_total_tokens": 495, "judge_prompt_tokens": 419, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of \"A Love to Die For\" is not found in the provided context.", "latency_ms": 5476.791143417358, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the director information is not provided in the database. The generated answer states the director is not found in the provided context. Both convey that the information is unavailable, so the response is correct and faithful to the given gold answer.", "main_total_tokens": 2207, "main_prompt_tokens": 2188, "main_completion_tokens": 19, "judge_total_tokens": 493, "judge_prompt_tokens": 413, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not found in the provided context.", "latency_ms": 5775.437593460083, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available/found in the given data. This matches the database constraint, so the answer is correct and faithful to the provided context.", "main_total_tokens": 2342, "main_prompt_tokens": 2322, "main_completion_tokens": 20, "judge_total_tokens": 481, "judge_prompt_tokens": 411, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of \"The Last Night at Tremore Beach\" is not found in the provided context.", "latency_ms": 5341.610908508301, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the given data/context. This matches the task requirement to stay within the provided database/context, so the answer is both correct and faithful.", "main_total_tokens": 1963, "main_prompt_tokens": 1942, "main_completion_tokens": 21, "judge_total_tokens": 490, "judge_prompt_tokens": 417, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" (2021) [Doc 98375].", "latency_ms": 7665.637254714966, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, correctly identifying the movie as 'Rise of the Mummy.' It does not introduce contradictions or unsupported claims beyond adding a plausible year, and remains faithful to the given gold answer.", "main_total_tokens": 2147, "main_prompt_tokens": 2112, "main_completion_tokens": 35, "judge_total_tokens": 497, "judge_prompt_tokens": 422, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie where the following happens: \"To escape her abusive spouse, Sarah fakes her own death and flees.\" is found in [Doc 35237] with the title \"Stranger in My Bed\".", "latency_ms": 8381.58893585205, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Stranger in My Bed', which matches the gold answer exactly. It is fully faithful to the provided information and does not introduce contradictions or hallucinations.", "main_total_tokens": 2210, "main_prompt_tokens": 2166, "main_completion_tokens": 44, "judge_total_tokens": 509, "judge_prompt_tokens": 439, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie that matches the given context is not found among the provided documents. However, based on the plot descriptions, some movies seem to have similar themes or elements, but none match exactly.\n\nIf I had to choose a movie with a similar theme, I would suggest [Doc 83806] \"The Bad Guys\" (2018), which involves a reunion of old friends and explores themes of past trauma, regret, and the consequences of youth. However, this is not an exact match.\n\nTo find an exact match, more information or context might be needed.", "latency_ms": 15713.112831115723, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the movie title given the described plot. The gold answer is 'Game of Silence', but the generated answer says the movie is not found and instead suggests 'The Bad Guys', explicitly noting it is not an exact match. This is factually incorrect, so correctness is 0. The answer accurately reflects the model's (mistaken) reasoning about not finding a match and then proposing a guess, without contradicting its own stated basis, so it is internally faithful to its reasoning process, giving faithfulness a 1.", "main_total_tokens": 1962, "main_prompt_tokens": 1848, "main_completion_tokens": 114, "judge_total_tokens": 649, "judge_prompt_tokens": 510, "judge_completion_tokens": 139}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie that matches the given context is \"The Craigslist Killer\" (2011). The plot description states: \"Based on a true story. Philip Markoff, a charismatic and popular med student at Boston University, leads a double life as a brutal and cruel sexual deviant who abuses prostitutes he finds via Craigslist.\"", "latency_ms": 10580.250024795532, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the brief description about a killer from Craigslist, and the gold answer is 'The Craigslist Killer'. The generated answer identifies the movie as 'The Craigslist Killer (2011)' and provides an accurate supporting plot description consistent with the gold. It is correct and faithful to the given information without introducing contradictions.", "main_total_tokens": 2076, "main_prompt_tokens": 2012, "main_completion_tokens": 64, "judge_total_tokens": 551, "judge_prompt_tokens": 455, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which the following happens: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee is \"Wood Job!\" [Doc 66094].", "latency_ms": 8470.340728759766, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", which matches the gold answer exactly. It stays faithful to the given information and does not introduce contradictions or irrelevant details.", "main_total_tokens": 2522, "main_prompt_tokens": 2480, "main_completion_tokens": 42, "judge_total_tokens": 507, "judge_prompt_tokens": 440, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie where a playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure is \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 9196.29430770874, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase'. It is fully consistent with the provided information and does not introduce any unsupported content.", "main_total_tokens": 2545, "main_prompt_tokens": 2497, "main_completion_tokens": 48, "judge_total_tokens": 527, "judge_prompt_tokens": 456, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie where \"When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives\" is described in [Doc 74597] titled \"Beyond\".", "latency_ms": 9433.536529541016, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot. The gold answer is 'Beyond'. The generated answer correctly identifies the movie as 'Beyond' and does not add contradictory information, only referencing a document title. Thus it is both correct and faithful to the gold answer.", "main_total_tokens": 2179, "main_prompt_tokens": 2130, "main_completion_tokens": 49, "judge_total_tokens": 540, "judge_prompt_tokens": 456, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie where this happens is \"6ixtynin9\" (1999).", "latency_ms": 5368.373870849609, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', which matches the gold answer exactly. It does not introduce any contradictory or extraneous information beyond the year, which is accurate, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2072, "main_prompt_tokens": 2053, "main_completion_tokens": 19, "judge_total_tokens": 502, "judge_prompt_tokens": 420, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie that matches the given description is \"Club Dread\" (2004). The plot of this movie involves a serial killer interrupting the fun at Coconut Pete's Coconut Beach Resort, a hedonistic island paradise for swingers, and it's up to the club's staff to stop the violence or hide it.", "latency_ms": 10640.036582946777, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3154648767857289, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer, and the explanation accurately reflects the provided plot description without adding incorrect details.", "main_total_tokens": 2312, "main_prompt_tokens": 2248, "main_completion_tokens": 64, "judge_total_tokens": 551, "judge_prompt_tokens": 487, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie is Toilet (2010). [Doc 48873]", "latency_ms": 6011.637210845947, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Toilet' and matches the provided gold answer without introducing additional unsupported details. It is both correct and faithful to the given information.", "main_total_tokens": 2788, "main_prompt_tokens": 2773, "main_completion_tokens": 15, "judge_total_tokens": 494, "judge_prompt_tokens": 428, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie in which the following happens: \"A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.\" is Electric Angel (1981). [Doc 1024]", "latency_ms": 10784.805297851562, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Electric Angel,\" which matches the gold answer. It does not introduce contradictions and stays faithful to the given information, only adding a plausible year and document reference without altering the core claim.", "main_total_tokens": 2465, "main_prompt_tokens": 2403, "main_completion_tokens": 62, "judge_total_tokens": 552, "judge_prompt_tokens": 476, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is Doomsday Stories with a rating of 2.167 [Doc 108366].", "latency_ms": 4092.8544998168945, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer claims that the lowest rated movie from 2023 is Doomsday Stories, but the gold answer states it is Mega Ape. This makes the content factually incorrect with respect to the gold. However, the answer appears to be internally consistent and derived from the cited document, so it is faithful to its own sources even though it disagrees with the gold.", "main_total_tokens": 2187, "main_prompt_tokens": 2159, "main_completion_tokens": 28, "judge_total_tokens": 509, "judge_prompt_tokens": 408, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 1407.4690341949463, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific highest-rated TV show from 2010, with gold answer 'Tea House'. The generated answer says 'I don't know,' which does not match the correct answer, so correctness is 0. However, the model does not hallucinate an incorrect show and honestly admits not knowing, so it is fully faithful to its own uncertainty, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 498, "judge_prompt_tokens": 386, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 670.3252792358398, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer ('I don't know.') does not match the correct answer, so it is incorrect. However, it does not introduce any fabricated information and is therefore faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 395, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is \"Bloodeaters\" with a rating of 4.0 [Doc 820]. The language of this movie is English (en).", "latency_ms": 8300.851345062256, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980, and the gold answer is 'English'. The generated answer correctly identifies the language as English and is consistent with the provided information, without adding unsupported claims beyond specifying the movie and language. Therefore both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2165, "main_prompt_tokens": 2127, "main_completion_tokens": 38, "judge_total_tokens": 513, "judge_prompt_tokens": 420, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show with the highest popularity in 1999 is \"The Sopranos\" with a popularity rating of 59.4284 [Doc 115146].", "latency_ms": 7814.436435699463, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer identifies 'The Sopranos' as the most popular TV show in 1999, but the gold answer is 'Law & Order: Special Victims Unit', so the content is incorrect relative to the gold. However, the answer is explicitly grounded in the cited document [Doc 115146], so it is faithful to its stated source.", "main_total_tokens": 2502, "main_prompt_tokens": 2466, "main_completion_tokens": 36, "judge_total_tokens": 527, "judge_prompt_tokens": 424, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2. [Doc 79355]", "latency_ms": 5887.487411499023, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and does not add contradictory or extraneous information, so it is both correct and faithful to the reference.", "main_total_tokens": 2397, "main_prompt_tokens": 2377, "main_completion_tokens": 20, "judge_total_tokens": 466, "judge_prompt_tokens": 401, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in 1999 [Doc 22481].", "latency_ms": 5749.572515487671, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer gives 1999, but the gold label says 2019, so the content is factually incorrect relative to the gold. However, the answer is explicitly grounded in the cited document ([Doc 22481]), so it is faithful to the provided source.", "main_total_tokens": 2224, "main_prompt_tokens": 2203, "main_completion_tokens": 21, "judge_total_tokens": 490, "judge_prompt_tokens": 404, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 184.9977970123291, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (3) does not match the gold answer (279), so it is factually incorrect for this QA pair. However, there is no evidence of hallucinated reasoning or contradiction beyond the incorrect numeric value itself, so it is still considered faithful to the (albeit wrong) output, not adding extra unsupported claims.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 383, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "12", "latency_ms": 184.9968433380127, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (12).", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 425, "judge_prompt_tokens": 383, "judge_completion_tokens": 42}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009. [Doc 46069]", "latency_ms": 6540.02833366394, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'The Last Days of Lehman Brothers' was released in 2009, which matches the gold answer. It is fully supported by the cited document reference, so both correctness and faithfulness are satisfied.", "main_total_tokens": 2336, "main_prompt_tokens": 2311, "main_completion_tokens": 25, "judge_total_tokens": 491, "judge_prompt_tokens": 411, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English (en). [Doc 48035]", "latency_ms": 6543.955087661743, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies English as the original language of 'The King’s Speech' and matches the gold answer 'en'. It is fully supported by the provided information and does not introduce any unsupported claims.", "main_total_tokens": 2632, "main_prompt_tokens": 2608, "main_completion_tokens": 24, "judge_total_tokens": 477, "judge_prompt_tokens": 405, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "1987", "latency_ms": 5209.886312484741, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (1987), which is the correct release year of the film 'Wall Street.'", "main_total_tokens": 2825, "main_prompt_tokens": 2820, "main_completion_tokens": 5, "judge_total_tokens": 437, "judge_prompt_tokens": 381, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Brain Fluid Explosion Girl", "latency_ms": 5295.482397079468, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'Brain Fluid Explosion Girl.'", "main_total_tokens": 2746, "main_prompt_tokens": 2739, "main_completion_tokens": 7, "judge_total_tokens": 443, "judge_prompt_tokens": 392, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 6403.438329696655, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.4306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.719741384391281, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is correct. The TV show where six friends rely on a Dungeon Master to get home is indeed 'Dungeons & Dragons.'", "main_total_tokens": 2825, "main_prompt_tokens": 2818, "main_completion_tokens": 7, "judge_total_tokens": 460, "judge_prompt_tokens": 394, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 5572.252035140991, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct movie title. It is both correct and faithful to the provided reference.", "main_total_tokens": 2727, "main_prompt_tokens": 2718, "main_completion_tokens": 9, "judge_total_tokens": 461, "judge_prompt_tokens": 403, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900", "latency_ms": 4726.988077163696, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (1900), correctly identifying the year in which the events of Picnic at Hanging Rock take place. It is both correct and fully faithful to the provided gold answer.", "main_total_tokens": 2582, "main_prompt_tokens": 2577, "main_completion_tokens": 5, "judge_total_tokens": 460, "judge_prompt_tokens": 388, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June", "latency_ms": 842.8139686584473, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question clearly points to the movie title being the same as a calendar month and the gold answer is 'June'. The generated answer is also 'June', which exactly matches the gold and aligns with the clue. Therefore, it is both correct and faithful.", "main_total_tokens": 132, "main_prompt_tokens": 130, "main_completion_tokens": 2, "judge_total_tokens": 515, "judge_prompt_tokens": 433, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 280.4296016693115, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a particular film, but the generated answer says 'I don't know,' which is not correct. However, it does not introduce any false information and is therefore faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 472, "judge_prompt_tokens": 399, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 363.5139465332031, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 7.6, but the generated answer states uncertainty ('I don't know') rather than giving this numerical value, so it is incorrect. However, the model does not fabricate an answer and truthfully admits not knowing, so it is fully faithful to its own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 479, "judge_prompt_tokens": 389, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 31314.351797103882, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 0.9828920819566879, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3364045224324153, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is incorrect. The model directly answered the query, so it is faithful to the user question, but the content is factually wrong.", "main_total_tokens": 1714, "main_prompt_tokens": 1458, "main_completion_tokens": 256, "judge_total_tokens": 469, "judge_prompt_tokens": 394, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 285.3085994720459, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 2 such TV shows, while the generated answer says there are none, so the answer is factually incorrect (correctness 0). However, the model does not fabricate specific titles or numbers and simply admits inability to find matches, which is consistent with its epistemic limits, so it is fully faithful to its own (though limited) reasoning (faithfulness 1).", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 516, "judge_prompt_tokens": 404, "judge_completion_tokens": 112}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 300.92310905456543, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the correct answer is 'Fantômas'. However, the model honestly stated it did not know rather than fabricating an answer, so it is fully faithful to its own (lack of) knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 466, "judge_prompt_tokens": 389, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise", "latency_ms": 5114.567995071411, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Kill Cruise') and correctly identifies the movie described, so it is both correct and faithful.", "main_total_tokens": 2731, "main_prompt_tokens": 2726, "main_completion_tokens": 5, "judge_total_tokens": 445, "judge_prompt_tokens": 387, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 5969.484806060791, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie title, so it is both correct and faithful.", "main_total_tokens": 2920, "main_prompt_tokens": 2910, "main_completion_tokens": 10, "judge_total_tokens": 462, "judge_prompt_tokens": 408, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018", "latency_ms": 5934.091806411743, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (2018), so it is both correct and faithful.", "main_total_tokens": 3000, "main_prompt_tokens": 2992, "main_completion_tokens": 8, "judge_total_tokens": 431, "judge_prompt_tokens": 380, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy", "latency_ms": 5577.105760574341, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV series as Astro Boy. It is both correct and fully faithful to the provided information.", "main_total_tokens": 2951, "main_prompt_tokens": 2945, "main_completion_tokens": 6, "judge_total_tokens": 450, "judge_prompt_tokens": 389, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 years", "latency_ms": 4648.1804847717285, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer of 7 years, which is correct for the film 'Room' (2015).", "main_total_tokens": 2468, "main_prompt_tokens": 2463, "main_completion_tokens": 5, "judge_total_tokens": 454, "judge_prompt_tokens": 397, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m", "latency_ms": 4337.968111038208, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the provided gold answer exactly (£26m), and it does not introduce any unsupported information. Both correctness and faithfulness are therefore full.", "main_total_tokens": 2143, "main_prompt_tokens": 2139, "main_completion_tokens": 4, "judge_total_tokens": 460, "judge_prompt_tokens": 398, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 665.0924682617188, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answer is factually incorrect because it fails to provide the requested numeric popularity score (80.5772) and instead responds with not knowing. However, it is faithful to its own uncertainty and does not fabricate an answer, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 479, "judge_prompt_tokens": 396, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 455.02734184265137, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 1, but the generated answer claims there are no movies or shows matching the criteria, which is numerically incorrect. It also contradicts the provided gold label, so it is unfaithful to the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 472, "judge_prompt_tokens": 394, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes", "latency_ms": 5742.371320724487, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Angel Eyes' exactly matches the gold answer and correctly identifies the TV show in which Soo-wan regains her sight after an eye transplant. It is both correct and fully faithful to the provided information.", "main_total_tokens": 3034, "main_prompt_tokens": 3029, "main_completion_tokens": 5, "judge_total_tokens": 472, "judge_prompt_tokens": 397, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "ko", "latency_ms": 7038.24520111084, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'Korean' and the generated answer is 'ko', which is the correct ISO 639-1 language code for Korean. This is equivalent and thus correct and faithful to the gold.", "main_total_tokens": 3187, "main_prompt_tokens": 3170, "main_completion_tokens": 17, "judge_total_tokens": 450, "judge_prompt_tokens": 377, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 710.3476524353027, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.5, "precision_at_5": 0.2, "ndcg_at_5": 0.6131471927654584, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.6131471927654584, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1) does not match the gold answer (2), so it is factually incorrect. However, it directly attempts to answer the question and does not hallucinate extra information or contradict the query structure, so it is faithful to the user question.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 394, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 1666.9135093688965, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer explicitly states there are no movies from 1979 in the database. The generated answer says only 'I don't know,' which does not convey the database limitation and is therefore not semantically correct relative to the gold. However, it does not introduce any contradictory information or hallucinations, so it is still faithful to the lack of knowledge.", "main_total_tokens": 591, "main_prompt_tokens": 589, "main_completion_tokens": 2, "judge_total_tokens": 500, "judge_prompt_tokens": 399, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "There is no Back to the Future Part IV movie mentioned in the provided context.", "latency_ms": 6809.84354019165, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there is no Back to the Future Part IV movie, and the generated answer is consistent with the given context. Therefore, it is fully correct and faithful.", "main_total_tokens": 3165, "main_prompt_tokens": 3146, "main_completion_tokens": 19, "judge_total_tokens": 487, "judge_prompt_tokens": 416, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers convey that no such movie exists for 1980 about the Chernobyl disaster, which is correct since the Chernobyl disaster occurred in 1986. The generated answer does not hallucinate a movie or a score, remains aligned with the question’s constraints, and even hints at the temporal mismatch, so it is fully faithful to the information implied in the gold answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 546, "judge_prompt_tokens": 434, "judge_completion_tokens": 112}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 222.7632999420166, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 titles, but the generated answer claims there are none, which is factually incorrect. However, the response does not fabricate specific incorrect details; it simply states inability to find matches, so it is faithful to its own (albeit limited) retrieval rather than hallucinating content.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 397, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "50", "latency_ms": 16601.352214813232, "retrieval_recall": 0.0, "retrieval_mrr": 0.125, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.19342640361727081, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer (50) contradicts the gold answer (2), so it is incorrect. There is no evidence in the prompt to justify the number 50, so the answer is also unfaithful to the provided information.", "main_total_tokens": 3502, "main_prompt_tokens": 3404, "main_completion_tokens": 98, "judge_total_tokens": 463, "judge_prompt_tokens": 386, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 347.8989601135254, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there is 1 such TV show, while the generated answer claims there are none. This is factually incorrect, so correctness is 0. The generated answer directly contradicts the provided gold answer, so it is unfaithful to the given reference, making faithfulness 0.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 490, "judge_prompt_tokens": 398, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 363.69824409484863, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0, meaning there were no TV shows in that period. The generated answer states that it cannot find any movies or shows matching those criteria, which effectively conveys that there are none. This matches the gold label and does not introduce extraneous, ungrounded details, so it is both correct and faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 497, "judge_prompt_tokens": 400, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 274.7495174407959, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto', which is a reasonable and well-known popular ninja-themed series. The generated answer says 'I don't know,' which is incorrect relative to the gold. However, the generated answer does not hallucinate or contradict provided information; it honestly states a lack of knowledge, so it is faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 386, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 1562.1328353881836, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that the movie was not released, while the generated answer says 'I don't know', which is incorrect. However, the model does not hallucinate a wrong date or event and instead admits uncertainty, so it remains faithful to its own uncertainty.", "main_total_tokens": 575, "main_prompt_tokens": 573, "main_completion_tokens": 2, "judge_total_tokens": 472, "judge_prompt_tokens": 389, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 1301.2449741363525, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that such a movie does not exist / was not released. The generated answer says 'I don't know,' which is not factually correct given the gold answer, so correctness is 0. However, it does not invent any contradictory facts and simply expresses lack of knowledge, so it is fully faithful to the model's uncertainty, giving faithfulness 1.", "main_total_tokens": 575, "main_prompt_tokens": 573, "main_completion_tokens": 2, "judge_total_tokens": 492, "judge_prompt_tokens": 386, "judge_completion_tokens": 106}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "I don't know.", "latency_ms": 1507.216215133667, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a concrete movie title, while the generated answer claims not to know. This is factually incorrect relative to the given gold. However, the model does not invent any incorrect information; it simply expresses uncertainty, so it is fully faithful to its own (lack of) knowledge.", "main_total_tokens": 638, "main_prompt_tokens": 636, "main_completion_tokens": 2, "judge_total_tokens": 483, "judge_prompt_tokens": 393, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "2002", "latency_ms": 18475.84891319275, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer ('2002') does not match the gold answer ('1986'), so it is factually incorrect. There is no additional context suggesting the model contradicted its own prior reasoning, so faithfulness to its (albeit wrong) internal answer remains high.", "main_total_tokens": 3203, "main_prompt_tokens": 3078, "main_completion_tokens": 125, "judge_total_tokens": 465, "judge_prompt_tokens": 380, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 238.42382431030273, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric rating for the movie Forrest Gump, with the gold answer 8.466. The generated answer says 'I don't know,' which does not provide the requested value, so it is incorrect. However, it does not invent any false rating or contradictory information; it simply admits lack of knowledge, so it is fully faithful to what the model knows.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 506, "judge_prompt_tokens": 398, "judge_completion_tokens": 108}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med", "latency_ms": 5016.046762466431, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Mighty Med', which is the correct name of the TV show described. It is fully correct and faithful.", "main_total_tokens": 2582, "main_prompt_tokens": 2575, "main_completion_tokens": 7, "judge_total_tokens": 456, "judge_prompt_tokens": 394, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina", "latency_ms": 5588.996648788452, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Argentina'), so it is both correct and fully faithful to the reference.", "main_total_tokens": 3227, "main_prompt_tokens": 3223, "main_completion_tokens": 4, "judge_total_tokens": 446, "judge_prompt_tokens": 392, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "Jiang Jie's", "latency_ms": 5388.510942459106, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Jiang Jie's' correctly identifies Jiang Jie as the person whose trap the apprentice falls into. It is factually correct and semantically matches the gold answer. The minor difference in possessive form does not affect correctness or faithfulness.", "main_total_tokens": 2764, "main_prompt_tokens": 2756, "main_completion_tokens": 8, "judge_total_tokens": 469, "judge_prompt_tokens": 387, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "3", "latency_ms": 13695.401430130005, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.7095272044910244, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.7095272044910244, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (3) does not match the gold answer (4), so it is factually incorrect for this evaluation. However, it directly addresses the user’s question with a numerical count and does not introduce unrelated information, so it is faithful to the question prompt even though the number is wrong.", "main_total_tokens": 3566, "main_prompt_tokens": 3494, "main_completion_tokens": 72, "judge_total_tokens": 476, "judge_prompt_tokens": 385, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 3447.7150440216064, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot find any movies or shows matching all of those criteria, which aligns with the gold answer that there are no movies with both Bruce Lee and Steven Seagal. It is faithful to the user query and the gold response, accurately conveying that there are no such joint works.", "main_total_tokens": 2243, "main_prompt_tokens": 2241, "main_completion_tokens": 2, "judge_total_tokens": 498, "judge_prompt_tokens": 406, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3", "latency_ms": 3857.6607704162598, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer (6.3) exactly matches the gold rating (6.3), so it is both correct and faithful.", "main_total_tokens": 2197, "main_prompt_tokens": 2193, "main_completion_tokens": 4, "judge_total_tokens": 444, "judge_prompt_tokens": 387, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "Drift-style", "latency_ms": 7123.473167419434, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Drift-style' is semantically equivalent to the gold answer 'Drift'. It correctly identifies the racing style and remains faithful to the intended label, only adding a minor stylistic suffix that does not change the meaning.", "main_total_tokens": 3068, "main_prompt_tokens": 3048, "main_completion_tokens": 20, "judge_total_tokens": 459, "judge_prompt_tokens": 380, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer", "latency_ms": 5401.9975662231445, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie title as 'Confronting a Serial Killer', so it is both correct and faithful.", "main_total_tokens": 2764, "main_prompt_tokens": 2755, "main_completion_tokens": 9, "judge_total_tokens": 474, "judge_prompt_tokens": 412, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838", "latency_ms": 4104.801654815674, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838), so it is both correct and faithful.", "main_total_tokens": 2362, "main_prompt_tokens": 2358, "main_completion_tokens": 4, "judge_total_tokens": 442, "judge_prompt_tokens": 389, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense", "latency_ms": 5027.614593505859, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 1999 psychological thriller as 'The Sixth Sense' and is fully faithful to the question and reference.", "main_total_tokens": 2685, "main_prompt_tokens": 2679, "main_completion_tokens": 6, "judge_total_tokens": 458, "judge_prompt_tokens": 394, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002", "latency_ms": 5229.137420654297, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and is correct: 'The Transporter' was released in 2002.", "main_total_tokens": 2955, "main_prompt_tokens": 2950, "main_completion_tokens": 5, "judge_total_tokens": 438, "judge_prompt_tokens": 383, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled", "latency_ms": 5745.014905929565, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the title of the 2015 reboot of 'The Transporter' as 'The Transporter Refueled'. It is both correct and fully faithful to the reference.", "main_total_tokens": 3006, "main_prompt_tokens": 2997, "main_completion_tokens": 9, "judge_total_tokens": 473, "judge_prompt_tokens": 397, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "[Doc 67500] \n2015", "latency_ms": 5852.210283279419, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states the film was released in 2015, which matches the gold answer and is fully supported by the provided information.", "main_total_tokens": 2868, "main_prompt_tokens": 2856, "main_completion_tokens": 12, "judge_total_tokens": 451, "judge_prompt_tokens": 393, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "Re-Elected", "latency_ms": 5022.724390029907, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer correctly indicates no known romantic comedy matching the description, while the generated answer names a specific film ('Re-Elected') without evidence it is a romantic comedy about Lincoln coming back from the dead. Thus the generated answer is incorrect relative to the gold and not faithful to the provided reference.", "main_total_tokens": 2677, "main_prompt_tokens": 2671, "main_completion_tokens": 6, "judge_total_tokens": 483, "judge_prompt_tokens": 393, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Across the River and into the Trees", "latency_ms": 5569.760322570801, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer \"Across the River and into the Trees\" is an Ernest Hemingway novel, not a movie title that alludes to a manuscript written by him. The correct movie title is \"Havana Darkness.\" The response is therefore incorrect and not faithful to the gold answer.", "main_total_tokens": 2722, "main_prompt_tokens": 2712, "main_completion_tokens": 10, "judge_total_tokens": 481, "judge_prompt_tokens": 395, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 5533.677101135254, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'Doll & Em' as the TV show. It is both correct and fully faithful to the provided reference.", "main_total_tokens": 2672, "main_prompt_tokens": 2665, "main_completion_tokens": 7, "judge_total_tokens": 457, "judge_prompt_tokens": 392, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam Foe", "latency_ms": 5398.351430892944, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Hallam Foe' exactly matches the gold answer and correctly identifies the 2007 movie described in the question.", "main_total_tokens": 2930, "main_prompt_tokens": 2923, "main_completion_tokens": 7, "judge_total_tokens": 461, "judge_prompt_tokens": 403, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "His last name sounds like it could be either a friend or foe because \"Foe\" is a word that means enemy or adversary, but in this context, it's his surname.", "latency_ms": 11460.11233329773, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is unambiguously 'A foe', but the generated answer equivocates ('could be either a friend or foe'), so it is incorrect. However, the evaluation accurately references the provided gold and generated text, so it is faithful to the given content.", "main_total_tokens": 2821, "main_prompt_tokens": 2761, "main_completion_tokens": 60, "judge_total_tokens": 507, "judge_prompt_tokens": 423, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18", "latency_ms": 4749.835968017578, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer '18' exactly matches the gold answer and correctly states Sam's age in the 2017 TV show Atypical.", "main_total_tokens": 2629, "main_prompt_tokens": 2625, "main_completion_tokens": 4, "judge_total_tokens": 448, "judge_prompt_tokens": 389, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Autism spectrum. [Doc 124583]", "latency_ms": 5846.965551376343, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the TV show Atypical, Sam is on the autism spectrum. This matches the gold answer exactly and does not introduce any additional, unsupported information, so it is both correct and faithful to the provided content.", "main_total_tokens": 2440, "main_prompt_tokens": 2427, "main_completion_tokens": 13, "judge_total_tokens": 476, "judge_prompt_tokens": 396, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix", "latency_ms": 5588.373899459839, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Chamonix'), so it is both correct and faithful.", "main_total_tokens": 3159, "main_prompt_tokens": 3153, "main_completion_tokens": 6, "judge_total_tokens": 452, "judge_prompt_tokens": 400, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "it rains", "latency_ms": 5452.723026275635, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'it rains' matches the gold answer 'rains' in meaning and is correct. It is fully faithful to the provided information.", "main_total_tokens": 3031, "main_prompt_tokens": 3026, "main_completion_tokens": 5, "judge_total_tokens": 454, "judge_prompt_tokens": 393, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland", "latency_ms": 5298.446178436279, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Greenland'), so it is both correct and fully faithful to the reference.", "main_total_tokens": 2936, "main_prompt_tokens": 2931, "main_completion_tokens": 5, "judge_total_tokens": 450, "judge_prompt_tokens": 395, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon", "latency_ms": 4905.290842056274, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('One Afternoon'), so it is both correct and fully faithful to the provided information.", "main_total_tokens": 2529, "main_prompt_tokens": 2523, "main_completion_tokens": 6, "judge_total_tokens": 462, "judge_prompt_tokens": 406, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The Life Ahead", "latency_ms": 3360.7265949249268, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('The Life Ahead') and correctly identifies the movie described, so it is both correct and faithful.", "main_total_tokens": 2730, "main_prompt_tokens": 2724, "main_completion_tokens": 6, "judge_total_tokens": 460, "judge_prompt_tokens": 401, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "Bruce McLaren", "latency_ms": 5205.661773681641, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Bruce McLaren'), correctly identifying the subject of the 2016 documentary 'McLaren' and staying fully faithful to the provided information.", "main_total_tokens": 2882, "main_prompt_tokens": 2877, "main_completion_tokens": 5, "judge_total_tokens": 454, "judge_prompt_tokens": 387, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren Motor Racing team", "latency_ms": 6341.326475143433, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6895405204413554, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer in content, only omitting the article 'The', which does not change the meaning. It is both correct and faithful.", "main_total_tokens": 3008, "main_prompt_tokens": 2995, "main_completion_tokens": 13, "judge_total_tokens": 451, "judge_prompt_tokens": 388, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "New Zealand", "latency_ms": 6581.976413726807, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('New Zealand'), correctly answering the question and remaining fully faithful to the provided reference.", "main_total_tokens": 2759, "main_prompt_tokens": 2746, "main_completion_tokens": 13, "judge_total_tokens": 434, "judge_prompt_tokens": 378, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "A Good Day to Die Hard", "latency_ms": 5962.410926818848, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because in the 2013 film 'A Good Day to Die Hard', John McClane does not blow up the Great Wall of China, nor is there any Die Hard film where he does so. The gold response correctly indicates that there is no such movie in the database. Since the generated answer contradicts the gold and the factual record, it is neither correct nor faithful.", "main_total_tokens": 3044, "main_prompt_tokens": 3035, "main_completion_tokens": 9, "judge_total_tokens": 516, "judge_prompt_tokens": 404, "judge_completion_tokens": 112}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "I don't know.", "latency_ms": 4288.33794593811, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Tomie: Forbidden Fruit' but the generated answer was 'I don't know,' which is incorrect. However, it is faithful in that it does not fabricate an answer and honestly admits not knowing.", "main_total_tokens": 609, "main_prompt_tokens": 607, "main_completion_tokens": 2, "judge_total_tokens": 496, "judge_prompt_tokens": 421, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65", "latency_ms": 27885.07103919983, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years), so it is both correct and faithful to the reference.", "main_total_tokens": 3353, "main_prompt_tokens": 3349, "main_completion_tokens": 4, "judge_total_tokens": 450, "judge_prompt_tokens": 396, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Book of Souls", "latency_ms": 5644.260883331299, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies that they are searching for the Book of Souls, so it is both correct and faithful.", "main_total_tokens": 3219, "main_prompt_tokens": 3212, "main_completion_tokens": 7, "judge_total_tokens": 458, "judge_prompt_tokens": 398, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1618.535041809082, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, which effectively means the system does not know the release year. The generated answer, 'I don't know,' conveys the same lack of knowledge. It is therefore correct with respect to the gold and faithful to the given information.", "main_total_tokens": 541, "main_prompt_tokens": 539, "main_completion_tokens": 2, "judge_total_tokens": 483, "judge_prompt_tokens": 395, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1469.3455696105957, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, effectively indicating lack of knowledge. The generated answer, 'I don't know,' is equivalent in meaning. It is therefore correct relative to the gold and faithful to the information given.", "main_total_tokens": 604, "main_prompt_tokens": 602, "main_completion_tokens": 2, "judge_total_tokens": 472, "judge_prompt_tokens": 393, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2026.1147022247314, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database; the generated answer says it does not know. Both convey that the information is unavailable, so the generated response is correct and faithful to the gold.", "main_total_tokens": 560, "main_prompt_tokens": 558, "main_completion_tokens": 2, "judge_total_tokens": 467, "judge_prompt_tokens": 395, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1181.8575859069824, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the movie is not in the database, implying an explicit lack of information. The generated answer says simply 'I don't know,' which is semantically aligned (no fabricated year, no hallucination). However, it does not match the more specific gold phrasing, so it is not fully correct per the task, though it is fully faithful to the lack of knowledge.", "main_total_tokens": 543, "main_prompt_tokens": 541, "main_completion_tokens": 2, "judge_total_tokens": 504, "judge_prompt_tokens": 395, "judge_completion_tokens": 109}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2478.5265922546387, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer ('I don't know.') correctly indicates a lack of knowledge about the movie, which matches the gold response stating the movie is not in the database. It does not introduce any incorrect facts and remains faithful to the information available, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 602, "main_prompt_tokens": 600, "main_completion_tokens": 2, "judge_total_tokens": 483, "judge_prompt_tokens": 395, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1256.756067276001, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly acknowledge a lack of knowledge about the film. The generated answer is semantically equivalent to the gold answer and stays within the provided information, so it is both correct and faithful.", "main_total_tokens": 538, "main_prompt_tokens": 536, "main_completion_tokens": 2, "judge_total_tokens": 466, "judge_prompt_tokens": 393, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1333.968162536621, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is not in the database; saying 'I don't know' is consistent with that and is an appropriate response. It is both correct given the information and fully faithful to the gold.", "main_total_tokens": 593, "main_prompt_tokens": 591, "main_completion_tokens": 2, "judge_total_tokens": 469, "judge_prompt_tokens": 396, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of \"The Last Days of Lehman Brothers\" is not mentioned in the provided context.", "latency_ms": 6839.213132858276, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not provided in the context/database. This matches the limitation described, so the answer is correct relative to the given information and faithfully reflects the provided context without adding unsupported details.", "main_total_tokens": 3238, "main_prompt_tokens": 3215, "main_completion_tokens": 23, "judge_total_tokens": 494, "judge_prompt_tokens": 417, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 2206.3140869140625, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the director of the movie, but the gold standard says this information is not available in the database. The generated answer, 'I don't know,' correctly refrains from fabricating a director and matches the limitation in the gold response. It is both correct (does not assert false information) and faithful to the constraint that the database lacks this information.", "main_total_tokens": 560, "main_prompt_tokens": 558, "main_completion_tokens": 2, "judge_total_tokens": 498, "judge_prompt_tokens": 399, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of \"Journey into Gaza\" is not mentioned in the provided context.", "latency_ms": 6092.288494110107, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned, aligning with the gold response that the database does not provide the director information. It does not introduce any unsupported details and remains faithful to the given context.", "main_total_tokens": 2924, "main_prompt_tokens": 2904, "main_completion_tokens": 20, "judge_total_tokens": 482, "judge_prompt_tokens": 409, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "There is no information in the provided context about \"The Last Night at Tremore Beach\".", "latency_ms": 5897.852659225464, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information in the provided context, which aligns with the gold answer indicating the database lacks director information. It is both correct and faithful to the given context and task.", "main_total_tokens": 2559, "main_prompt_tokens": 2538, "main_completion_tokens": 21, "judge_total_tokens": 487, "judge_prompt_tokens": 415, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 5115.569829940796, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Rise of the Mummy'), so it is both correct and faithful.", "main_total_tokens": 2716, "main_prompt_tokens": 2707, "main_completion_tokens": 9, "judge_total_tokens": 450, "judge_prompt_tokens": 396, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 4985.99910736084, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Stranger in My Bed') and correctly identifies the movie described in the question.", "main_total_tokens": 2784, "main_prompt_tokens": 2776, "main_completion_tokens": 8, "judge_total_tokens": 456, "judge_prompt_tokens": 400, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 4278.912544250488, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Game of Silence'), which correctly identifies the work described. It is fully correct and faithful to the reference.", "main_total_tokens": 2429, "main_prompt_tokens": 2423, "main_completion_tokens": 6, "judge_total_tokens": 461, "judge_prompt_tokens": 400, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 4610.955715179443, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer: both state the movie is 'The Craigslist Killer'. The reasoning is fully consistent with the provided information, so it is both correct and faithful.", "main_total_tokens": 2622, "main_prompt_tokens": 2616, "main_completion_tokens": 6, "judge_total_tokens": 463, "judge_prompt_tokens": 395, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 5117.529630661011, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct movie title for the described plot. It is both correct and fully faithful to the reference.", "main_total_tokens": 3095, "main_prompt_tokens": 3089, "main_completion_tokens": 6, "judge_total_tokens": 465, "judge_prompt_tokens": 403, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 6744.875431060791, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase.' It is both correct and fully faithful to the provided gold response.", "main_total_tokens": 3203, "main_prompt_tokens": 3181, "main_completion_tokens": 22, "judge_total_tokens": 485, "judge_prompt_tokens": 417, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond", "latency_ms": 4542.319297790527, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Beyond') and correctly identifies the movie described. It is fully faithful to the given information.", "main_total_tokens": 2736, "main_prompt_tokens": 2732, "main_completion_tokens": 4, "judge_total_tokens": 467, "judge_prompt_tokens": 409, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9", "latency_ms": 5179.415464401245, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as '6ixtynin9' and remaining fully faithful to the given information.", "main_total_tokens": 2663, "main_prompt_tokens": 2654, "main_completion_tokens": 9, "judge_total_tokens": 471, "judge_prompt_tokens": 409, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread", "latency_ms": 4858.806133270264, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Club Dread'. It is fully correct and faithful to the provided gold label.", "main_total_tokens": 2885, "main_prompt_tokens": 2880, "main_completion_tokens": 5, "judge_total_tokens": 489, "judge_prompt_tokens": 427, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet", "latency_ms": 5663.044452667236, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, and the question is correctly answered as the movie 'Toilet'. There is no hallucination or deviation from the provided information.", "main_total_tokens": 3437, "main_prompt_tokens": 3432, "main_completion_tokens": 5, "judge_total_tokens": 480, "judge_prompt_tokens": 415, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel", "latency_ms": 5026.954889297485, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Electric Angel'). The explanation aligns perfectly with the given QA pair, so it is both correct and faithful.", "main_total_tokens": 2977, "main_prompt_tokens": 2972, "main_completion_tokens": 5, "judge_total_tokens": 479, "judge_prompt_tokens": 418, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "2.167", "latency_ms": 3167.762279510498, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '2.167' is not a movie title and does not match the gold answer 'Mega Ape'. It is incorrect and also not faithful to the question, which asks for a movie title, not a number or rating.", "main_total_tokens": 2737, "main_prompt_tokens": 2731, "main_completion_tokens": 6, "judge_total_tokens": 459, "judge_prompt_tokens": 385, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 328.125, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Tea House', but the model responded with 'I don't know.' This is factually incorrect relative to the provided gold answer. However, the answer is internally consistent and does not invent any incorrect facts, so it is fully faithful to its own (albeit limited) reasoning.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 477, "judge_prompt_tokens": 386, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 1908.7707996368408, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the highest rated Japanese language movie from 1999, with the gold answer being 'The Uncut Story of Mewtwo's Origin'. The generated answer says 'I don't know', which does not provide the correct movie and thus is factually incorrect. However, it does not hallucinate or contradict any provided information; it simply expresses ignorance, so it is fully faithful to the (lack of) knowledge rather than inventing an answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 518, "judge_prompt_tokens": 395, "judge_completion_tokens": 123}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "es", "latency_ms": 4710.296392440796, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer ('es') is incorrect; the correct language is English. The model's output does not match the gold answer and is therefore neither correct nor faithful.", "main_total_tokens": 2700, "main_prompt_tokens": 2696, "main_completion_tokens": 4, "judge_total_tokens": 443, "judge_prompt_tokens": 384, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The Sopranos", "latency_ms": 5444.281578063965, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer \"The Sopranos\" does not match the gold answer \"Law & Order: Special Victims Unit\" and is therefore incorrect. However, the model is consistent and does not contradict its own statement, so it is fully faithful to its (incorrect) response.", "main_total_tokens": 3072, "main_prompt_tokens": 3064, "main_completion_tokens": 8, "judge_total_tokens": 481, "judge_prompt_tokens": 394, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2", "latency_ms": 3880.4328441619873, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2), so it is fully correct and faithful.", "main_total_tokens": 2394, "main_prompt_tokens": 2390, "main_completion_tokens": 4, "judge_total_tokens": 438, "judge_prompt_tokens": 386, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "1999", "latency_ms": 4714.348316192627, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer gives the wrong release year; the correct year is 2019, not 1999. However, the model consistently commits to this (incorrect) answer without contradicting itself, so it is faithful to its own (mistaken) reasoning.", "main_total_tokens": 2779, "main_prompt_tokens": 2774, "main_completion_tokens": 5, "judge_total_tokens": 470, "judge_prompt_tokens": 387, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "1", "latency_ms": 3517.60196685791, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 279 votes, but the generated answer is 1, which is numerically incorrect. The model clearly attempted to answer the question (not refusing or changing the question), so it is faithful to the task but factually wrong.", "main_total_tokens": 2242, "main_prompt_tokens": 2240, "main_completion_tokens": 2, "judge_total_tokens": 464, "judge_prompt_tokens": 383, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "28", "latency_ms": 16822.776794433594, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (28) does not match the gold answer (12), so it is factually incorrect. However, it directly answers the question with a single numeric value, so it is faithful to the user query.", "main_total_tokens": 2424, "main_prompt_tokens": 2278, "main_completion_tokens": 146, "judge_total_tokens": 458, "judge_prompt_tokens": 383, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "2009", "latency_ms": 4788.133382797241, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer (2009) exactly matches the gold answer, so it is both correct and faithful.", "main_total_tokens": 2904, "main_prompt_tokens": 2899, "main_completion_tokens": 5, "judge_total_tokens": 442, "judge_prompt_tokens": 390, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_openai-large_llama3.1-70b-iq2_xs", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en", "latency_ms": 5145.375728607178, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The movie 'The King's Speech' is originally in English, and both the gold answer and the generated answer correctly state 'en' (English), so the response is correct and faithful to the gold.", "main_total_tokens": 3217, "main_prompt_tokens": 3213, "main_completion_tokens": 4, "judge_total_tokens": 450, "judge_prompt_tokens": 384, "judge_completion_tokens": 66}
