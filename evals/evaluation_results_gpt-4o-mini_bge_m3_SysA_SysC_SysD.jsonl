{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The context does not provide information about a movie or show titled \"Wall Street.\" Therefore, I cannot answer your question.", "latency_ms": 6047.560214996338, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1987, but the generated answer claims it cannot answer, which is factually incorrect. However, given its stated limitation that the context does not provide the information, it remains faithful to that context.", "main_total_tokens": 2942, "main_prompt_tokens": 2918, "main_completion_tokens": 24, "judge_total_tokens": 478, "judge_prompt_tokens": 402, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "In the movie \"Signal 100,\" a group of high school students are forced into a game where they are hypnotized to commit suicide on an unknown command, which involves a survival aspect [Doc 93745]. Additionally, in \"Naraka 19,\" a university student becomes trapped in a terrifying mobile phone game from hell [Doc 41254].", "latency_ms": 1824.3885040283203, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the specific movie where classmates must play a survival game using their cell phones, and the gold answer is 'Brain Fluid Explosion Girl.' The generated answer instead mentions 'Signal 100' and 'Naraka 19,' which are different films and do not match the required title, so it is incorrect. However, it appears to accurately describe those other movies, presumably reflecting the referenced documents, so it is faithful to its own sources even though it does not answer the question correctly.", "main_total_tokens": 2697, "main_prompt_tokens": 2626, "main_completion_tokens": 71, "judge_total_tokens": 589, "judge_prompt_tokens": 459, "judge_completion_tokens": 130}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show in which six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons,\" which aired in 1983 [Doc 112849].", "latency_ms": 1290.2588844299316, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the TV show as \"Dungeons & Dragons,\" matching the gold answer, so correctness is 1. The added detail about it airing in 1983 is accurate, but the reference to a specific document [Doc 112849] is unverifiable from the prompt, slightly reducing faithfulness.", "main_total_tokens": 2456, "main_prompt_tokens": 2420, "main_completion_tokens": 36, "judge_total_tokens": 523, "judge_prompt_tokens": 426, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The context does not provide any information about a movie that features a story about two rival gangs in Munich, one composed of German teens and the other of Turkish ones. Therefore, I cannot provide an answer based on the given context.", "latency_ms": 1123.507022857666, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Nacht der Wölfe', but the generated answer claims it cannot provide an answer. This is factually incorrect. However, the answer is faithful to the (assumed) lack of information in the provided context, so it is faithful but incorrect.", "main_total_tokens": 2984, "main_prompt_tokens": 2938, "main_completion_tokens": 46, "judge_total_tokens": 531, "judge_prompt_tokens": 444, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of \"Picnic at Hanging Rock\" take place in the year 1900 [Doc 126191].", "latency_ms": 950.5953788757324, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of 'Picnic at Hanging Rock' take place in 1900, matching the gold answer. It is also faithful to the provided information, directly reflecting the source without adding unsupported details.", "main_total_tokens": 2668, "main_prompt_tokens": 2644, "main_completion_tokens": 24, "judge_total_tokens": 486, "judge_prompt_tokens": 409, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The movie you are referring to is not found in the provided context.", "latency_ms": 726.193904876709, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'June', which correctly identifies the movie. The generated answer incorrectly states that the movie is not found, so it is factually wrong. However, it is faithful to the given (limited) context since the model did not fabricate a movie title and instead admitted lack of information.", "main_total_tokens": 2924, "main_prompt_tokens": 2910, "main_completion_tokens": 14, "judge_total_tokens": 537, "judge_prompt_tokens": 446, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "The highest rated Russian-language movie released in 1980 is \"Moscow Does Not Believe in Tears,\" which has a rating of 7.6 [Doc 61].", "latency_ms": 1341.2432670593262, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger' as the highest rated Russian-language movie released in 1980, while the generated answer claims it is 'Moscow Does Not Believe in Tears.' This directly conflicts with the gold, so the generated answer is incorrect. Additionally, the generated answer cites a specific rating and document reference that contradict the given gold label, so it is not faithful with respect to the provided ground truth.", "main_total_tokens": 2660, "main_prompt_tokens": 2625, "main_completion_tokens": 35, "judge_total_tokens": 555, "judge_prompt_tokens": 430, "judge_completion_tokens": 125}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest Russian-language movie rating in 1999 is 7.4, for the movie \"Leila\" [Doc 22998].", "latency_ms": 818.0973529815674, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the highest Russian-language movie rating in 1999 is 7.6, while the generated answer claims it is 7.4 and attributes it to a specific movie. This directly contradicts the gold, so the correctness is 0. Additionally, the generated answer introduces a specific film and rating not supported by the gold, indicating it is not faithful, so faithfulness is 0.", "main_total_tokens": 2745, "main_prompt_tokens": 2716, "main_completion_tokens": 29, "judge_total_tokens": 528, "judge_prompt_tokens": 414, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "There are two movies released in 1987 that have a rating over 7 and are in Chinese:\n\n1. **A Chinese Ghost Story** - Rating: 7.3 [Doc 7064]\n2. **Last Eunuch In China** - Rating: 7.8 [Doc 7754]", "latency_ms": 1955.2302360534668, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 4 movies, but the generated answer states there are 2, so the numerical result is incorrect. However, the assistant consistently reasons from the provided documents and does not invent conflicting information beyond miscounting, so it is faithful to its cited evidence.", "main_total_tokens": 2641, "main_prompt_tokens": 2577, "main_completion_tokens": 64, "judge_total_tokens": 544, "judge_prompt_tokens": 456, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "There are no TV shows released in 1987 that have a rating over 7 and are in Chinese based on the provided context.", "latency_ms": 1807.2631359100342, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The gold answer is 2, but the generated answer says there are none, so it is factually incorrect. It is somewhat faithful to the (implied) context by referencing constraints, but since the context actually supports a different count, the claim that there are no such shows is not faithful to the underlying data.", "main_total_tokens": 2992, "main_prompt_tokens": 2965, "main_completion_tokens": 27, "judge_total_tokens": 517, "judge_prompt_tokens": 420, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "The only French language TV show released in 1980 is \"Dom\" [Doc 112605].", "latency_ms": 919.0402030944824, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because the gold label indicates that the only French language TV show released in 1980 is 'Fantômas', not 'Dom'. However, the answer appears to be faithful to the cited document [Doc 112605], so it is faithful to its given source but not correct with respect to the gold standard.", "main_total_tokens": 2494, "main_prompt_tokens": 2473, "main_completion_tokens": 21, "judge_total_tokens": 507, "judge_prompt_tokens": 406, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is \"Kill Cruise,\" released in 1990 [Doc 11205].", "latency_ms": 1073.115587234497, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Kill Cruise,\" which matches the gold answer exactly and stays faithful to the provided information without adding unsupported claims.", "main_total_tokens": 2629, "main_prompt_tokens": 2600, "main_completion_tokens": 29, "judge_total_tokens": 475, "judge_prompt_tokens": 414, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 1172.7967262268066, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, \"Won't You Be My Neighbor?\", and clearly identifies it as the movie about Fred Rogers’ TV series. It is correct and consistent with the provided information.", "main_total_tokens": 2930, "main_prompt_tokens": 2891, "main_completion_tokens": 39, "judge_total_tokens": 513, "judge_prompt_tokens": 440, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The year of release varies depending on the title you are referring to. Here are the years for the movies listed in the context:\n\n- The movie titled \"It\" (2017) was released in 2017 [Doc 76823].\n- The movie titled \"It\" (1990) was released in 1990 [Doc 113633].\n- The movie titled \"What Time Is It?\" was released in 1989 [Doc 10351].\n- The movie titled \"What Time Is It There?\" was released in 2001 [Doc 25850].\n- The movie titled \"Album\" was released in 2016 [Doc 73431].\n- The movie titled \"Light It Up\" was released in 1999 [Doc 22768].\n- The movie titled \"Hiding Out\" was released in 1987 [Doc 7631].\n- The movie titled \"Hello, It's Me\" was released in 2015 [Doc 69838].\n- The movie titled \"That's It\" was released in 2015 [Doc 70563].\n- The movie titled \"His Secret Past\" was released in 2016 [Doc 74305].\n\nIf you have a specific title in mind, please specify, and I can provide the exact year of release.", "latency_ms": 4935.121059417725, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.2, "justification": "The question expects the single answer '2018'. The generated answer does not provide 2018 anywhere, instead listing multiple unrelated titles and years, so it is incorrect. It is also not faithful to the gold answer because it does not commit to or even mention 2018, instead deflecting by asking for a specific title; thus it fails to answer the given question as scoped by the gold.", "main_total_tokens": 2950, "main_prompt_tokens": 2687, "main_completion_tokens": 263, "judge_total_tokens": 754, "judge_prompt_tokens": 640, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The context does not provide information about a TV series specifically featuring a young boy robot set in 2030 AD. Therefore, I cannot provide an answer based on the given context.", "latency_ms": 1135.3144645690918, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Astro Boy'. The generated answer incorrectly states that it cannot provide an answer, so it is factually incorrect. However, given the assumption that the model only had the referenced context and not the gold answer, its statement about lacking information is consistent with that context, so it is faithful to its own reasoning context.", "main_total_tokens": 2519, "main_prompt_tokens": 2483, "main_completion_tokens": 36, "judge_total_tokens": 522, "judge_prompt_tokens": 423, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room,\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 1366.7104244232178, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: they were held captive for 7 years. It is correct and directly grounded in the provided information.", "main_total_tokens": 2301, "main_prompt_tokens": 2271, "main_completion_tokens": 30, "judge_total_tokens": 484, "judge_prompt_tokens": 424, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 1489.6149635314941, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983, which matches the provided gold answer exactly. It is faithful to the given information and does not introduce unsupported details beyond what is stated.", "main_total_tokens": 3029, "main_prompt_tokens": 2996, "main_completion_tokens": 33, "judge_total_tokens": 519, "judge_prompt_tokens": 428, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "The most popular English-speaking TV show released in 1997 is \"USA High,\" which has a popularity score of 3.535 [Doc 114769].", "latency_ms": 1051.119089126587, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the popularity score of the most popular English-speaking TV show released in 1997, whose gold answer is 80.5772. The generated answer instead states that the most popular show is 'USA High' with a score of 3.535, which does not match the gold value and is therefore incorrect. However, the generated answer is internally consistent and clearly grounded in a cited document [Doc 114769], with no hallucinated contradictions relative to its own source, so it is faithful to the provided document even though the document itself must not describe the true most popular show.", "main_total_tokens": 2533, "main_prompt_tokens": 2500, "main_completion_tokens": 33, "judge_total_tokens": 577, "judge_prompt_tokens": 425, "judge_completion_tokens": 152}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "There are no Russian language TV shows listed in the provided context for the year 1997.", "latency_ms": 734.1928482055664, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer indicates there was 1 Russian language TV show released in 1997, but the generated answer claims there were none, so it is factually incorrect. It is partially faithful in that it references the provided context and does not fabricate a specific different number, but it incorrectly infers absence from the context rather than giving the correct answer.", "main_total_tokens": 2580, "main_prompt_tokens": 2561, "main_completion_tokens": 19, "judge_total_tokens": 505, "judge_prompt_tokens": 402, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 911.8325710296631, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the TV show as 'Angel Eyes'. It is fully correct and consistent with the provided information.", "main_total_tokens": 3429, "main_prompt_tokens": 3394, "main_completion_tokens": 35, "judge_total_tokens": 489, "judge_prompt_tokens": 429, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"What Is This Film Called Love?\" is English [Doc 58692].", "latency_ms": 933.0108165740967, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer incorrectly states that the movie is in English, while the gold label says Korean. The answer is therefore factually incorrect. However, it is consistent with the cited document snippet, so it is faithful to the provided source.", "main_total_tokens": 2503, "main_prompt_tokens": 2482, "main_completion_tokens": 21, "judge_total_tokens": 475, "judge_prompt_tokens": 397, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "Based on the provided context, there are no English-speaking horror movies released in 2009 that have a popularity score greater than 7. The only movie listed with a popularity score above 7 is \"Triangle,\" which has a score of 4.9214 [Doc 44860].", "latency_ms": 1769.47021484375, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The gold answer is 2, but the generated answer says there are no such movies, so it is factually incorrect (correctness 0). The assistant cites the given context and uses it consistently, but misreads or misinterprets the popularity score of 'Triangle' (4.9214 is not above 7) and concludes none match; while it stays within the document, it draws an incorrect inference about the presence/absence of other movies in the unseen parts of the dataset, so faithfulness is not perfect but mostly grounded in the snippet provided.", "main_total_tokens": 2794, "main_prompt_tokens": 2735, "main_completion_tokens": 59, "judge_total_tokens": 597, "judge_prompt_tokens": 451, "judge_completion_tokens": 146}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The context provided does not contain any information about movies released in 1979, so I cannot answer that question.", "latency_ms": 825.9987831115723, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Given the gold answer that the database has no movies from 1979, the generated answer correctly states it cannot answer based on the provided context. This matches the gold response and does not fabricate information, so it is both correct and faithful to the given context.", "main_total_tokens": 2838, "main_prompt_tokens": 2815, "main_completion_tokens": 23, "judge_total_tokens": 501, "judge_prompt_tokens": 418, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "There is no information available about a movie titled \"Back to the Future Part IV\" in the provided context.", "latency_ms": 950.127124786377, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both answers correctly indicate that there is no known movie titled 'Back to the Future Part IV' and thus no release year. The generated answer is consistent with the gold answer in meaning and does not introduce incorrect information.", "main_total_tokens": 3293, "main_prompt_tokens": 3271, "main_completion_tokens": 22, "judge_total_tokens": 496, "judge_prompt_tokens": 422, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "There is no movie about the nuclear Chernobyl disaster mentioned in the provided context for the year 1980. Therefore, I cannot provide a popularity score for such a movie.", "latency_ms": 1119.8334693908691, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no movies from 1980 related to the Chernobyl disaster could be found. The generated answer correctly notes that there is no such movie in the provided context and therefore a popularity score cannot be given. This matches the gold answer in substance: both indicate the non-existence of such a movie in the context and the consequent impossibility of giving a popularity score. The answer is both correct and faithful to the gold.", "main_total_tokens": 2910, "main_prompt_tokens": 2874, "main_completion_tokens": 36, "judge_total_tokens": 562, "judge_prompt_tokens": 442, "judge_completion_tokens": 120}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are three entries with 'Astro Boy' in the title:\n\n1. Astro Boy (2003) - TV [Doc 116265]\n2. Astro Boy (1980) - TV [Doc 112591]\n3. Astro Boy (2009) - Movie [Doc 44947]", "latency_ms": 1628.063440322876, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.8318724637288826, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.8318724637288826, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 4 titles with 'Astro Boy' in the title, but the generated answer claims there are three and lists only three entries. This contradicts the gold, so the correctness is 0. The reasoning and counts in the generated answer are internally consistent with what it lists, so it is faithful to its own content and thus gets a faithfulness score of 1.", "main_total_tokens": 2787, "main_prompt_tokens": 2725, "main_completion_tokens": 62, "judge_total_tokens": 560, "judge_prompt_tokens": 448, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "The context provided does not mention any movies in 2025 that allude to the Holocaust. Therefore, I cannot provide an answer based on the given information.", "latency_ms": 1003.5994052886963, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is a specific numeric value (2). The generated answer instead claims the information is not available and does not provide the correct number, so it is factually incorrect relative to the gold. However, it is faithful to the context it assumes (lack of information) and does not invent data, so it is fully faithful to its own stated limitations.", "main_total_tokens": 2688, "main_prompt_tokens": 2656, "main_completion_tokens": 32, "judge_total_tokens": 519, "judge_prompt_tokens": 416, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "There were three TV shows released in the 1990s about Super Mario Bros:\n\n1. The Super Mario Bros. Super Show! (1989) - [Doc 113517]\n2. The Adventures of Super Mario Bros. 3 (1990) - [Doc 113661]\n3. Super Mario World (1991) - [Doc 113814]", "latency_ms": 1837.8825187683105, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show released in the 1990s about Super Mario Bros. The generated answer says there were three, and includes The Super Mario Bros. Super Show!, which was released in 1989, not the 1990s. It therefore miscounts and is incorrect. However, it appears to be consistent with its cited documents and does not contradict its own sources, so it is faithful to them even though the overall answer is wrong relative to the gold.", "main_total_tokens": 3306, "main_prompt_tokens": 3230, "main_completion_tokens": 76, "judge_total_tokens": 594, "judge_prompt_tokens": 463, "judge_completion_tokens": 131}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "The context provided does not mention any TV shows about Super Mario Bros released between 2000 and 2010. Therefore, I cannot provide an answer to your question.", "latency_ms": 1150.615930557251, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The gold answer is 0, but the generated answer says it cannot provide an answer, which is incorrect. Thus correctness is 0. The answer is partially faithful to the (unstated) context by noting the context does not mention such TV shows and refraining from hallucinating a specific number, but it should have inferred 0 from the information given in the evaluation setup. Therefore faithfulness is relatively high but not perfect.", "main_total_tokens": 2911, "main_prompt_tokens": 2877, "main_completion_tokens": 34, "judge_total_tokens": 542, "judge_prompt_tokens": 423, "judge_completion_tokens": 119}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular TV show or movie about ninjas, based on the provided context, is \"UNDER NINJA,\" which has a popularity rating of 3.1982 [Doc 133621].", "latency_ms": 1766.3257122039795, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates that the expected response is 'Naruto' as the most popular ninja TV show. The generated answer instead claims 'UNDER NINJA' is the most popular, which contradicts the gold and is factually incorrect in this QA context. However, it is faithful to the (incorrectly used) provided context it cites, so faithfulness to context is high even though the answer is wrong relative to the gold.", "main_total_tokens": 2975, "main_prompt_tokens": 2934, "main_completion_tokens": 41, "judge_total_tokens": 541, "judge_prompt_tokens": 423, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "There is no mention of a Jurassic Park/Power Ranger crossover movie in the provided context. Therefore, I cannot provide a release date for such a movie.", "latency_ms": 1038.074254989624, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold answer: movie does not exist / not released. Generated answer: states there is no mention of such a crossover and cannot provide a release date. This is effectively equivalent to saying there is no such released movie. Correct and consistent with the gold.", "main_total_tokens": 2807, "main_prompt_tokens": 2776, "main_completion_tokens": 31, "judge_total_tokens": 497, "judge_prompt_tokens": 416, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "There is no movie in the provided context where Barbie fights the Predator. The closest mention is \"Killer Barbys vs. Dracula,\" but it does not involve the Predator [Doc 28703].", "latency_ms": 1775.456190109253, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The gold answer clarifies that such a movie was not released. The generated answer correctly states that there is no such movie, so it is correct relative to the question. However, it incorrectly cites a specific document and context (\"Doc 28703\") that are not actually present in the prompt, so it is not fully faithful to the given information.", "main_total_tokens": 3032, "main_prompt_tokens": 2992, "main_completion_tokens": 40, "judge_total_tokens": 524, "judge_prompt_tokens": 422, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The 4th oldest Star Wars movie based on the provided context is \"The Story of Star Wars,\" which was released in 2004 [Doc 32101].", "latency_ms": 1082.453727722168, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct answer per the gold label is 'Star Wars: Droids - The Great Heep', but the generated answer gives 'The Story of Star Wars' (2004), which is incorrect, so correctness is 0. The answer references a specific document ID, implying it is grounded in the provided context, but that context appears to be wrong relative to the gold. Since we only judge whether it follows the given context, not external truth, faithfulness is partial: it cites the provided doc and follows it, but we cannot fully verify alignment, so 0.5 is appropriate.", "main_total_tokens": 3128, "main_prompt_tokens": 3094, "main_completion_tokens": 34, "judge_total_tokens": 575, "judge_prompt_tokens": 423, "judge_completion_tokens": 152}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The release year varies depending on the title you are referring to. Here are some examples:\n\n- \"It\" (2017) was released in 2017 [Doc 76823].\n- \"It\" (1990) was released in 1990 [Doc 113633].\n- \"What Time Is It There?\" was released in 2001 [Doc 25850].\n- \"Album\" was released in 2016 [Doc 73431].\n\nIf you have a specific title in mind, please specify, and I can provide the release year for that movie.", "latency_ms": 2641.4718627929688, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.3, "justification": "The question clearly expects a single year answer (1986). The generated answer instead lists multiple possible titles and years and does not provide 1986, so it is incorrect. It is also not faithful to the provided gold answer, as it does not use or align with the given information and introduces unrelated titles and years.", "main_total_tokens": 2804, "main_prompt_tokens": 2687, "main_completion_tokens": 117, "judge_total_tokens": 591, "judge_prompt_tokens": 494, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The rating of the movie \"Forrest Gump\" is not provided in the context. Therefore, I cannot answer that question.", "latency_ms": 941.0762786865234, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the rating as 8.466, but the generated answer claims the rating is not provided and does not give the correct value. This makes the answer incorrect. However, the model did not hallucinate a rating; it accurately stated that the context did not provide one, so it is faithful to the given context.", "main_total_tokens": 2857, "main_prompt_tokens": 2831, "main_completion_tokens": 26, "judge_total_tokens": 518, "judge_prompt_tokens": 420, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 1104.158878326416, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the show as 'Mighty Med' and directly matches the gold answer. It is fully supported by the prompt and does not introduce any unsupported information.", "main_total_tokens": 2194, "main_prompt_tokens": 2163, "main_completion_tokens": 31, "judge_total_tokens": 489, "judge_prompt_tokens": 423, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In \"Túnel de los huesos\" (2011), seven prisoners escaped from a prison in Argentina [Doc 53582].", "latency_ms": 1057.4736595153809, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the prisoners escaped from a prison in Argentina, which matches the gold answer. It is consistent with the provided information and does not introduce unsupported details beyond a reference marker, so it is both correct and faithful.", "main_total_tokens": 2772, "main_prompt_tokens": 2744, "main_completion_tokens": 28, "judge_total_tokens": 497, "judge_prompt_tokens": 419, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The information about \"Sweet Trap\" (2024) is not found in the provided context.", "latency_ms": 1006.2181949615479, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Jiang Jie', but the generated answer claims the information is not found in the context instead of providing the correct name, so it is factually incorrect. However, it accurately reflects that the context did not contain the needed information, so it is faithful to the provided context.", "main_total_tokens": 2693, "main_prompt_tokens": 2674, "main_completion_tokens": 19, "judge_total_tokens": 494, "judge_prompt_tokens": 403, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "There are two Korean-language movies that mention assassins and were released in 2023:\n\n1. **The Assassin** - An action historical drama depicting the story of Joseon's best swordsman Yi Nan in front of an irreversible fate [Doc 106219].\n2. **Kill Boksoon** - A film about a renowned assassin who is also a single mom to a teenage daughter, highlighting the challenges of balancing her work and parenting [Doc 105198].", "latency_ms": 1744.1871166229248, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the number of Korean-language movies that mention assassins after 2023, with a gold answer of 4. The generated answer instead lists two specific movies and gives the count implicitly as two, not addressing the correct number or the correct time frame (it refers to movies released in 2023, not after 2023). Thus the numerical answer is incorrect. In addition, the response fabricates specific film details and document references without tying them to any provided source context, so it is not faithful to the given information.", "main_total_tokens": 2602, "main_prompt_tokens": 2510, "main_completion_tokens": 92, "judge_total_tokens": 615, "judge_prompt_tokens": 475, "judge_completion_tokens": 140}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "The context provided does not contain any information about Steven Seagal or the number of movies he has. It only includes details about various movies related to Bruce Lee. Therefore, I cannot provide an answer to your question.", "latency_ms": 1451.9398212432861, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.8, "faithfulness_score": 0.8, "justification": "Gold says explicitly there are no movies with both Bruce Lee and Steven Seagal. The generated answer instead claims it cannot provide an answer due to lack of context, which is slightly different but still effectively does not assert a wrong number. It's mostly correct in that it does not fabricate a number but misses the specific fact that there are none. Faithfulness is high because it stays within the given context and makes no hallucinations, but it incorrectly states there is no information about Steven Seagal when the gold implicitly provides that information, so it is not fully faithful to the gold answer.", "main_total_tokens": 3304, "main_prompt_tokens": 3261, "main_completion_tokens": 43, "judge_total_tokens": 586, "judge_prompt_tokens": 436, "judge_completion_tokens": 150}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 803.4725189208984, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (6.3) and stays consistent with the provided information, including the citation.", "main_total_tokens": 2785, "main_prompt_tokens": 2765, "main_completion_tokens": 20, "judge_total_tokens": 459, "judge_prompt_tokens": 403, "judge_completion_tokens": 56}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The movie \"Fast Track: No Limits\" centers around street racing, as it is described as an action movie focused on a group of street racers [Doc 41929].", "latency_ms": 988.1834983825684, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the racing style as Drift, while the generated answer claims it is street racing. These are not equivalent, so the generated answer is factually incorrect relative to the gold. However, the generated answer is faithful to the provided document citation, which describes the movie as focusing on street racers, so faithfulness to the cited source is high.", "main_total_tokens": 2972, "main_prompt_tokens": 2937, "main_completion_tokens": 35, "judge_total_tokens": 515, "judge_prompt_tokens": 413, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 1142.0447826385498, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Confronting a Serial Killer,' and it does not introduce any contradictions or unsupported claims.", "main_total_tokens": 2642, "main_prompt_tokens": 2622, "main_completion_tokens": 20, "judge_total_tokens": 491, "judge_prompt_tokens": 426, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 633.3639621734619, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and clearly attributes it, so it is both correct and faithful.", "main_total_tokens": 3017, "main_prompt_tokens": 2995, "main_completion_tokens": 22, "judge_total_tokens": 465, "judge_prompt_tokens": 407, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 988.3873462677002, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'The Sixth Sense,' which matches the gold answer exactly. It remains faithful to the question and does not introduce any incorrect or extraneous information beyond a harmless citation marker.", "main_total_tokens": 2753, "main_prompt_tokens": 2724, "main_completion_tokens": 29, "judge_total_tokens": 494, "judge_prompt_tokens": 420, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 815.6886100769043, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer, and it directly reflects the information from the cited document, so it is both correct and faithful.", "main_total_tokens": 2956, "main_prompt_tokens": 2939, "main_completion_tokens": 17, "judge_total_tokens": 468, "judge_prompt_tokens": 397, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is 'The Transporter Refueled' [Doc 67500].", "latency_ms": 988.2049560546875, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly states the title of the 2015 reboot as 'The Transporter Refueled'. It is fully correct and consistent with the given information.", "main_total_tokens": 3018, "main_prompt_tokens": 2989, "main_completion_tokens": 29, "judge_total_tokens": 490, "judge_prompt_tokens": 420, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in 2015 [Doc 67500].", "latency_ms": 856.2955856323242, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It is fully faithful to the given information and does not introduce contradictions or unsupported claims.", "main_total_tokens": 3002, "main_prompt_tokens": 2982, "main_completion_tokens": 20, "judge_total_tokens": 478, "judge_prompt_tokens": 404, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "The context provided does not contain any information about a romantic comedy featuring Abraham Lincoln coming back from the dead. Therefore, I cannot provide an answer based on the given context.", "latency_ms": 1056.7936897277832, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that there is no found movie description where Abraham Lincoln comes back from the dead in a romantic comedy. The generated answer similarly says the context does not contain such information and thus cannot provide an answer. This is correct given the premise and is fully faithful to the provided context and question constraints.", "main_total_tokens": 2887, "main_prompt_tokens": 2853, "main_completion_tokens": 34, "judge_total_tokens": 516, "judge_prompt_tokens": 424, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The context provided does not contain any information about a movie that alludes to a manuscript written by Ernest Hemingway. Therefore, I cannot provide an answer based on the given context.", "latency_ms": 1552.863597869873, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Havana Darkness'. The generated answer incorrectly states that it cannot provide an answer, so it is factually incorrect. However, it is faithful to the (implied) context by not hallucinating an answer, so its faithfulness is high.", "main_total_tokens": 2692, "main_prompt_tokens": 2657, "main_completion_tokens": 35, "judge_total_tokens": 508, "judge_prompt_tokens": 423, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "In the TV show \"Doll & Em,\" Doll heads to Hollywood to be with her childhood best friend Em, who is now a successful actress [Doc 122259].", "latency_ms": 974.8141765594482, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Doll & Em\" and matches the information given in the gold answer and cited document. It does not introduce unsupported details beyond noting Em is now a successful actress, which is consistent with the doc reference, so it is both correct and faithful.", "main_total_tokens": 2660, "main_prompt_tokens": 2625, "main_completion_tokens": 35, "judge_total_tokens": 514, "judge_prompt_tokens": 424, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The movie you are referring to is \"Cover\" from 2007, where the character Valerie Maas, who is connected to a murder investigation, faces challenges related to her husband's double life, which ties into themes of spying and uncovering truths [Doc 41101]. However, the specific character Hallam and the plot about spying on people to find the cause of his mother's death is not explicitly mentioned in the provided documents. Therefore, I cannot confirm the exact movie based on the context given.", "latency_ms": 2123.615503311157, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct answer is 'Hallam Foe'. The generated answer incorrectly identifies the movie as 'Cover' and provides an unrelated plot description, so it is factually wrong (correctness 0). It does acknowledge lack of confirmation from the provided documents, showing some faithfulness to the cited source, but then still speculates and gives a definitive but incorrect movie title, so faithfulness is partial (0.5).", "main_total_tokens": 2755, "main_prompt_tokens": 2656, "main_completion_tokens": 99, "judge_total_tokens": 615, "judge_prompt_tokens": 498, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The context does not provide information about a character's last name or whether it sounds like a friend or a foe. Therefore, I cannot answer that question.", "latency_ms": 919.3949699401855, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is \"A foe,\" indicating the question is answerable and the correct response is that his last name sounds like a foe. The generated answer claims the context does not provide the information and refuses to answer, which contradicts the gold. Thus, it is incorrect. However, it remains faithful to the (assumed) given context, as it does not introduce unsupported details, only states lack of information, so faithfulness is high.", "main_total_tokens": 2341, "main_prompt_tokens": 2310, "main_completion_tokens": 31, "judge_total_tokens": 538, "judge_prompt_tokens": 417, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "In the 2017 TV show Atypical, Sam is 18 years old [Doc 124583].", "latency_ms": 950.690746307373, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, which matches the gold answer exactly. It is fully faithful to the given information without adding unsupported details.", "main_total_tokens": 2855, "main_prompt_tokens": 2832, "main_completion_tokens": 23, "judge_total_tokens": 484, "judge_prompt_tokens": 410, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 903.2409191131592, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the 2017 TV show 'Atypical', Sam is on the autism spectrum, which matches the gold answer exactly. It is fully consistent and faithful to the provided information.", "main_total_tokens": 2758, "main_prompt_tokens": 2735, "main_completion_tokens": 23, "judge_total_tokens": 484, "judge_prompt_tokens": 410, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "In the movie À corde tendue, Sandra was born in Chamonix [Doc 59659].", "latency_ms": 888.1101608276367, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, which matches the gold answer. It does not introduce contradictions or extra unsupported details beyond a citation, so it is fully faithful to the provided information.", "main_total_tokens": 3223, "main_prompt_tokens": 3202, "main_completion_tokens": 21, "judge_total_tokens": 492, "judge_prompt_tokens": 418, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In this Japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 1304.903507232666, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and faithfully reflects the content described, with no added incorrect information.", "main_total_tokens": 3802, "main_prompt_tokens": 3781, "main_completion_tokens": 21, "judge_total_tokens": 468, "judge_prompt_tokens": 412, "judge_completion_tokens": 56}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the low-rated German language animation \"Tabaluga,\" the character is the last of the dragons and the crown-prince of Greenland, a magical place inhabited by talking animals of many different species [Doc 114810].", "latency_ms": 1072.9351043701172, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, matching the gold answer exactly. It stays faithful to the provided information without adding contradictory details, only slightly elaborating with non-essential but non-conflicting context.", "main_total_tokens": 3091, "main_prompt_tokens": 3046, "main_completion_tokens": 45, "judge_total_tokens": 518, "judge_prompt_tokens": 439, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is titled \"One Afternoon,\" released in 2021 [Doc 96838].", "latency_ms": 1543.0622100830078, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as \"One Afternoon,\" matching the gold answer. However, it adds specific release year information and a document reference (\"released in 2021 [Doc 96838]\") that are not present in the question or gold label, so there is a slight faithfulness deduction for including extra, unverified detail.", "main_total_tokens": 2442, "main_prompt_tokens": 2392, "main_completion_tokens": 50, "judge_total_tokens": 557, "judge_prompt_tokens": 454, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The information about a low-rated Italian language movie featuring a holocaust survivor taking care of a 12-year-old street kid who recently robbed her is not found in the provided context.", "latency_ms": 2966.0866260528564, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "recall_at_10": 0.0, "precision_at_5": 0.0, "precision_at_10": 0.0, "ndcg_at_5": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_5": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'The Life Ahead'. The generated answer incorrectly states that the information is not found, rather than providing the correct movie title, so it is factually incorrect. However, given only the (incorrectly assumed) context, it does not hallucinate a wrong movie title and instead admits lack of information, so it remains faithful to its stated context and limitations.", "main_total_tokens": 2757, "main_prompt_tokens": 2721, "main_completion_tokens": 36, "judge_total_tokens": 540, "judge_prompt_tokens": 434, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team, showcasing how a man of humble beginnings could take on the elite of motor racing and win [Doc 73859].", "latency_ms": 1706.2253952026367, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.7, "justification": "The generated answer correctly identifies that the 2016 documentary 'McLaren' is about Bruce McLaren, matching the gold answer, so correctness is full. However, it adds extra descriptive detail and a reference tag '[Doc 73859]' that appears fabricated or not grounded in the provided information, reducing faithfulness.", "main_total_tokens": 2940, "main_prompt_tokens": 2889, "main_completion_tokens": 51, "judge_total_tokens": 531, "judge_prompt_tokens": 435, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The name of the racing team is Newman-Haas (Andretti) [Doc 20456].", "latency_ms": 1141.057014465332, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the correct team is the McLaren Motor Racing team, not Newman-Haas (Andretti). It is also unfaithful to the provided gold answer, giving a completely different team name with an unrelated citation.", "main_total_tokens": 2579, "main_prompt_tokens": 2559, "main_completion_tokens": 20, "judge_total_tokens": 477, "judge_prompt_tokens": 403, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context does not provide information about the country of origin for the character or subject referred to as \"he.\" Therefore, I cannot answer that question.", "latency_ms": 2207.9672813415527, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates the correct country is New Zealand. The generated answer says the question cannot be answered from the context, which is incorrect relative to the gold. However, it is faithful to the information it claims to use (the provided context), so correctness is 0 and faithfulness is 1.", "main_total_tokens": 2535, "main_prompt_tokens": 2505, "main_completion_tokens": 30, "judge_total_tokens": 498, "judge_prompt_tokens": 406, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "The context provided does not mention any movie in which John McClane blows up the Great Wall of China. Therefore, I cannot provide an answer based on the given information.", "latency_ms": 950.200080871582, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct answer is that there is no such movie; the gold answer explicitly says this. The generated answer instead suggests it merely lacks context, implying the answer might exist elsewhere, so it is factually incorrect. It is partially faithful in that it acknowledges missing information, but it fails to follow the gold answer's core conclusion, so faithfulness is low.", "main_total_tokens": 2621, "main_prompt_tokens": 2586, "main_completion_tokens": 35, "judge_total_tokens": 537, "judge_prompt_tokens": 433, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The movie you are referring to is \"Tomie: Forbidden Fruit,\" which was released in 2002. It is a Japanese language horror film with the plot description you provided [Doc 28589].", "latency_ms": 912.3919010162354, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2002 Japanese horror film matching the description as 'Tomie: Forbidden Fruit,' which matches the gold answer exactly. It does not introduce contradictions or unsupported claims beyond a neutral citation-style reference, so it is fully faithful to the provided information.", "main_total_tokens": 2782, "main_prompt_tokens": 2741, "main_completion_tokens": 41, "judge_total_tokens": 544, "judge_prompt_tokens": 458, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 1210.4806900024414, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, which matches the gold answer. It does not introduce any contradictions or unsupported claims and appears fully grounded in the provided information.", "main_total_tokens": 3190, "main_prompt_tokens": 3161, "main_completion_tokens": 29, "judge_total_tokens": 497, "judge_prompt_tokens": 423, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The context provided does not contain any information about \"Scorpion King: Book of Souls\" (2018) or what The Scorpion King and Tala search for in that movie. Therefore, I cannot provide an answer.", "latency_ms": 987.4708652496338, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a clear factual answer: they search for the Book of Souls. The generated answer incorrectly claims it cannot provide an answer, so it is factually incorrect, yielding a correctness score of 0. However, given the assistant states its inability based on lack of context and does not fabricate information, it remains faithful to its stated knowledge limits, so faithfulness is 1.", "main_total_tokens": 2535, "main_prompt_tokens": 2491, "main_completion_tokens": 44, "judge_total_tokens": 546, "judge_prompt_tokens": 438, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The title \"Zombies on the Moon\" does not appear in the provided context. Therefore, I cannot provide the year of its release.", "latency_ms": 1182.1517944335938, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the system cannot provide the release year because the movie/title is not in the known information/context. This is correct, as there is no evidence given about a release year, and the model avoids fabricating one. The generated answer is faithful to the (implied) context and does not introduce unsupported details.", "main_total_tokens": 3086, "main_prompt_tokens": 3058, "main_completion_tokens": 28, "judge_total_tokens": 519, "judge_prompt_tokens": 419, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"The Galactic Baker\" was released is not found in the provided context.", "latency_ms": 1273.6902236938477, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer and generated answer both state that the release year is not available / not found. This is correct given no information provided about such a movie and is faithful to the (lack of) context.", "main_total_tokens": 3364, "main_prompt_tokens": 3347, "main_completion_tokens": 17, "judge_total_tokens": 478, "judge_prompt_tokens": 406, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The title \"Return of the Null Pointer\" is not found in the provided context. Therefore, I cannot provide the year of its release.", "latency_ms": 903.0218124389648, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the movie is not found and thus the year cannot be provided. This is correct given the lack of information, and the generated answer does not introduce any unsupported facts, so it is both correct and faithful to the (missing) context.", "main_total_tokens": 2615, "main_prompt_tokens": 2587, "main_completion_tokens": 28, "judge_total_tokens": 505, "judge_prompt_tokens": 419, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year of release for \"Data Science: The Movie\" is not found in the provided context.", "latency_ms": 835.8376026153564, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available/known. The generated answer correctly notes the absence of information in the context, which is faithful to the prompt and equivalent in substance to the gold response.", "main_total_tokens": 2735, "main_prompt_tokens": 2715, "main_completion_tokens": 20, "judge_total_tokens": 487, "judge_prompt_tokens": 411, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The title \"Python's Revenge 2\" does not appear in the provided context. Therefore, I cannot provide the year of its release.", "latency_ms": 1371.1059093475342, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the release year cannot be provided because the movie is not in the available information/context. The generated answer is faithful to the information given and does not hallucinate a release year.", "main_total_tokens": 2908, "main_prompt_tokens": 2880, "main_completion_tokens": 28, "judge_total_tokens": 494, "judge_prompt_tokens": 419, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year of release for \"The Last Pixel\" is not found in the provided context.", "latency_ms": 670.6070899963379, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available / not found. There is no hallucinated year or contradiction. The generated answer is correct relative to the gold and faithful to the given information.", "main_total_tokens": 2362, "main_prompt_tokens": 2344, "main_completion_tokens": 18, "judge_total_tokens": 481, "judge_prompt_tokens": 407, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The information about \"The Man Who Ate the Sun\" is not found in the provided context.", "latency_ms": 837.5875949859619, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, and the generated answer similarly says the information is not found in the provided context. Both effectively refrain from inventing a release year and are consistent with the lack of information, so the response is correct and faithful to the gold.", "main_total_tokens": 2740, "main_prompt_tokens": 2721, "main_completion_tokens": 19, "judge_total_tokens": 500, "judge_prompt_tokens": 411, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The information regarding the director of \"The Last Days of Lehman Brothers\" is not provided in the context.", "latency_ms": 820.2922344207764, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided data/context. The generated answer is correct with respect to the given database/context and is faithful to it by not hallucinating a director name.", "main_total_tokens": 2692, "main_prompt_tokens": 2670, "main_completion_tokens": 22, "judge_total_tokens": 495, "judge_prompt_tokens": 419, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The information regarding the director of the movie \"A Love to Die For\" is not found in the provided context.", "latency_ms": 877.5904178619385, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director information for the movie is not available and aligns with the gold response, which also says the database does not provide the director. It does not add unsupported details or contradict the given information, so it is both correct and faithful to the context.", "main_total_tokens": 2967, "main_prompt_tokens": 2944, "main_completion_tokens": 23, "judge_total_tokens": 505, "judge_prompt_tokens": 418, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not mentioned in the provided context.", "latency_ms": 755.8164596557617, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned, matching the gold answer that the information is not provided in the database. It is both correct and faithful to the given context.", "main_total_tokens": 3397, "main_prompt_tokens": 3379, "main_completion_tokens": 18, "judge_total_tokens": 479, "judge_prompt_tokens": 411, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of \"The Last Night at Tremore Beach\" is not mentioned in the provided context.", "latency_ms": 856.0540676116943, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director is not provided/mentioned in the available information. This matches the database constraint and does not hallucinate a director, so the answer is both correct and faithful to the context.", "main_total_tokens": 2384, "main_prompt_tokens": 2364, "main_completion_tokens": 20, "judge_total_tokens": 493, "judge_prompt_tokens": 417, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" (2021) [Doc 98375].", "latency_ms": 903.1739234924316, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Rise of the Mummy', which matches the gold answer exactly. It does not introduce contradictory or extraneous claims beyond a plausible release year and a document reference, so it remains faithful to the information implied by the gold label.", "main_total_tokens": 2561, "main_prompt_tokens": 2530, "main_completion_tokens": 31, "judge_total_tokens": 507, "judge_prompt_tokens": 422, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse is \"Stranger in My Bed\" (2005) [Doc 35237].", "latency_ms": 886.9092464447021, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Stranger in My Bed'. It does not introduce any contradictions or fabrications relative to the provided information, so it is both correct and faithful.", "main_total_tokens": 2605, "main_prompt_tokens": 2572, "main_completion_tokens": 33, "judge_total_tokens": 504, "judge_prompt_tokens": 429, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie you are referring to is \"Game of Silence\" from 2016. In this story, a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years, leading to the resurfacing of a dark secret from their past [Doc 123938].", "latency_ms": 1304.7525882720947, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a movie, while the correct answer is the TV series 'Game of Silence'. The generated answer also incorrectly calls it a movie instead of a TV series. Therefore, it is not correct. Additionally, the answer references an unspecified document [Doc 123938] that is not provided in the prompt, so the justification is not fully grounded in the given context, reducing faithfulness.", "main_total_tokens": 2229, "main_prompt_tokens": 2172, "main_completion_tokens": 57, "judge_total_tokens": 565, "judge_prompt_tokens": 454, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie you are referring to is \"The Craigslist Killer,\" which is based on the true story of Philip Markoff, a med student who leads a double life as a brutal sexual deviant abusing prostitutes he finds via Craigslist. It was released in 2011 [Doc 53039].", "latency_ms": 1853.6601066589355, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer' and accurately describes it as based on Philip Markoff, a med student who attacked women he met via Craigslist, which matches the gold answer and real-world facts, so correctness is high. However, it references an external source '[Doc 53039]' that is not present in the provided context, making that part of the answer ungrounded relative to the given information, so faithfulness must be scored lower.", "main_total_tokens": 2636, "main_prompt_tokens": 2578, "main_completion_tokens": 58, "judge_total_tokens": 576, "judge_prompt_tokens": 450, "judge_completion_tokens": 126}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which Yuki Hirano sets his eyes on the role of a forestry trainee after failing his university entrance exam is \"Wood Job!\" [Doc 66094].", "latency_ms": 1104.1157245635986, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", which matches the gold answer exactly. It is fully faithful to the given information and does not introduce any contradictions or unsupported claims, only adding a citation-style note that does not alter the content.", "main_total_tokens": 3712, "main_prompt_tokens": 3677, "main_completion_tokens": 35, "judge_total_tokens": 517, "judge_prompt_tokens": 435, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure, is titled \"Sesame Street: The Magical Wand Chase\" from the year 2017 [Doc 78373].", "latency_ms": 1342.9107666015625, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Sesame Street: The Magical Wand Chase,' which matches the gold reference exactly. The additional mention of the 2017 release year and a document reference does not contradict the gold answer and appears consistent with known information. Therefore, the response is both correct and faithful to the given data.", "main_total_tokens": 3425, "main_prompt_tokens": 3368, "main_completion_tokens": 57, "judge_total_tokens": 564, "judge_prompt_tokens": 467, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a 7-year-old boy must find the hero within him to save his parents trapped inside a burning car due to a rock fall is titled \"Beyond,\" released in 2016 [Doc 74597].", "latency_ms": 1272.2861766815186, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Beyond,\" which matches the gold answer, and does not introduce conflicting or unsupported information beyond specifying a plausible release year.", "main_total_tokens": 2474, "main_prompt_tokens": 2428, "main_completion_tokens": 46, "judge_total_tokens": 517, "judge_prompt_tokens": 454, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie you are referring to is \"6ixtynin9\" from the year 1999. In this film, a woman is fired from a financial corporation during the Asia crisis and returns home with no money [Doc 22478].", "latency_ms": 1913.7344360351562, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9,' which matches the gold answer exactly. It restates the plot detail about a woman fired from a financial corporation during the Asia crisis returning home with no money, consistent with the query and the cited document. There are no contradictions or extraneous fabricated details, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2447, "main_prompt_tokens": 2397, "main_completion_tokens": 50, "judge_total_tokens": 559, "judge_prompt_tokens": 452, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie you are referring to is \"Club Dread,\" which was released in 2004. [Doc 32410]", "latency_ms": 1041.7413711547852, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' matching the gold answer exactly. It also adds accurate release year information. There is no contradiction or fabrication relative to the given gold, so both correctness and faithfulness are full.", "main_total_tokens": 2811, "main_prompt_tokens": 2785, "main_completion_tokens": 26, "judge_total_tokens": 529, "judge_prompt_tokens": 450, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie you are referring to is \"Toilet,\" which was released in 2010. In this film, a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral [Doc 48873].", "latency_ms": 1606.0328483581543, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" matching the gold answer exactly, and the description aligns with the provided information without introducing contradictions or extraneous claims. Therefore, it is both correct and faithful to the source.", "main_total_tokens": 3085, "main_prompt_tokens": 3020, "main_completion_tokens": 65, "judge_total_tokens": 556, "judge_prompt_tokens": 479, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie you are referring to is \"Electric Angel,\" which is a typical experimental film that uses a variety of audiovisual techniques to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century [Doc 1024].", "latency_ms": 1467.6225185394287, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Electric Angel,' which matches the gold answer exactly. It stays faithful to the provided information and does not introduce contradictory or extraneous claims beyond rephrasing the description.", "main_total_tokens": 3040, "main_prompt_tokens": 2987, "main_completion_tokens": 53, "judge_total_tokens": 542, "judge_prompt_tokens": 469, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is \"Fall,\" which has a rating of 4.0 [Doc 106810].", "latency_ms": 830.4939270019531, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is factually incorrect with respect to the gold answer: the gold states that the lowest rated movie from 2023 is 'Mega Ape', while the generated response claims it is 'Fall' with a 4.0 rating. This directly contradicts the gold. Additionally, the answer is not faithful to the provided information (the gold label) and instead introduces a different movie title and rating, making it unfaithful to the source.", "main_total_tokens": 2844, "main_prompt_tokens": 2817, "main_completion_tokens": 27, "judge_total_tokens": 530, "judge_prompt_tokens": 408, "judge_completion_tokens": 122}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "The highest rated TV show from 2010 is \"Tea House,\" which has a rating of 9.2 [Doc 119448].", "latency_ms": 608.6475849151611, "retrieval_recall": 1.0, "retrieval_mrr": 0.08333333333333333, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies \"Tea House\" as the highest rated TV show from 2010, matching the gold answer. It also stays faithful to the presumed source by specifying the rating and document reference without adding unsupported claims.", "main_total_tokens": 3138, "main_prompt_tokens": 3109, "main_completion_tokens": 29, "judge_total_tokens": 487, "judge_prompt_tokens": 411, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest rated Japanese language movie from 1999 is \"Yu☆Gi☆Oh!\" with a rating of 7.0 [Doc 22216].", "latency_ms": 1087.3539447784424, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states that the highest rated Japanese language movie from 1999 is 'Yu☆Gi☆Oh!' with a rating of 7.0, but the gold answer is 'The Uncut Story of Mewtwo's Origin.' This directly contradicts the gold, so the answer is factually incorrect relative to the provided ground truth. Additionally, the model cites a document ID and rating that do not match the gold answer, indicating it is not being faithful to the reference. Therefore, both correctness and faithfulness are scored 0.", "main_total_tokens": 2775, "main_prompt_tokens": 2743, "main_completion_tokens": 32, "judge_total_tokens": 564, "judge_prompt_tokens": 423, "judge_completion_tokens": 141}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is \"Cataclysm,\" which has a rating of 3.889. The language of this movie is English [Doc 744].", "latency_ms": 1119.9455261230469, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The question only asks for the language, and the generated answer correctly states that it is English, matching the gold answer. However, the generated answer adds extra factual details (movie title and rating) that are not present in the gold reference; while likely correct, these specifics are not directly required, so there is slight uncertainty about faithfulness to the evaluated context.", "main_total_tokens": 3274, "main_prompt_tokens": 3237, "main_completion_tokens": 37, "judge_total_tokens": 525, "judge_prompt_tokens": 420, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was \"Popular,\" which had a popularity score of 4.5113 [Doc 115196].", "latency_ms": 818.3267116546631, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect: the gold label indicates that the TV show with the highest popularity in 1999 was 'Law & Order: Special Victims Unit', but the generated response claims it was 'Popular'. Additionally, the generated answer cites a specific document and popularity score that contradict the gold answer, indicating it is not faithful to the provided ground truth.", "main_total_tokens": 3362, "main_prompt_tokens": 3330, "main_completion_tokens": 32, "judge_total_tokens": 525, "judge_prompt_tokens": 422, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 856.5471172332764, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and explicitly attributes it to a document, so it is both correct and faithful.", "main_total_tokens": 2823, "main_prompt_tokens": 2805, "main_completion_tokens": 18, "judge_total_tokens": 460, "judge_prompt_tokens": 400, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "There are two movies titled \"The Story of Us.\" One was released in 1999 [Doc 22481], and the other in 2019 [Doc 88811].", "latency_ms": 1067.0454502105713, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states a single correct year: 2019. The generated answer introduces a 1999 movie titled 'The Story of Us' that is not part of the gold reference and frames the question as involving two movies. This conflicts with the evaluated gold, so it is incorrect with respect to the task. It is also unfaithful because it adds unsupported information (the 1999 film) beyond the gold answer and the evaluation context.", "main_total_tokens": 2855, "main_prompt_tokens": 2818, "main_completion_tokens": 37, "judge_total_tokens": 542, "judge_prompt_tokens": 421, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The context does not provide information about the number of votes \"My Dog Skip\" received. Therefore, I cannot answer that question.", "latency_ms": 871.9396591186523, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 279 votes, but the generated answer claims it cannot be answered. This is incorrect relative to the gold, so correctness is 0. However, the model accurately reflects that, given its stated context, it does not know the number of votes instead of hallucinating a value, so its answer is faithful to the provided context, yielding a faithfulness score of 1.", "main_total_tokens": 2835, "main_prompt_tokens": 2809, "main_completion_tokens": 26, "judge_total_tokens": 512, "judge_prompt_tokens": 407, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The context does not provide information about the number of votes received by \"Oh My God!\" or any of the other movies listed. Therefore, I cannot answer that question.", "latency_ms": 1282.883882522583, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates the movie received 12 votes, but the generated answer claims the context does not provide this information and does not give the correct number. This makes the answer factually incorrect. However, it is faithful to the model's stated understanding of the context (it does not fabricate a number), so faithfulness is high.", "main_total_tokens": 2898, "main_prompt_tokens": 2864, "main_completion_tokens": 34, "judge_total_tokens": 513, "judge_prompt_tokens": 415, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The year of release for the movie \"The Last Days of Lehman Brothers\" is not found in the provided context.", "latency_ms": 950.2472877502441, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states the film was released in 2009. The generated answer claims the release year is not found, which is incorrect relative to the gold. However, it remains faithful to the provided (unspecified) context by not inventing a year, so it is fully faithful but factually incorrect.", "main_total_tokens": 2601, "main_prompt_tokens": 2577, "main_completion_tokens": 24, "judge_total_tokens": 504, "judge_prompt_tokens": 411, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o-mini", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 702.4807929992676, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the original language is English, and the generated answer correctly states that the original language of 'The King’s Speech' is English. The reference citation does not contradict the content, so the answer is both correct and faithful.", "main_total_tokens": 2882, "main_prompt_tokens": 2863, "main_completion_tokens": 19, "judge_total_tokens": 481, "judge_prompt_tokens": 402, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The release year of \"Wall Street\" is not found in the provided context.", "latency_ms": 1070.596694946289, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1987, but the generated answer claims the release year is not in the context instead of providing 1987, so it is factually incorrect. However, it accurately reflects the (hypothetical) provided context as described, so it is faithful to the context given in the prompt.", "main_total_tokens": 2092, "main_prompt_tokens": 2076, "main_completion_tokens": 16, "judge_total_tokens": 488, "judge_prompt_tokens": 394, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie in which classmates have to play a survival game using their cell phones is \"Tokyo Gore School\" (2009). In this film, a mysterious violent game is being spread virally via the students’ mobile phones, leading to a situation where victims become senseless killing machines, and the protagonist must fight for survival and uncover the game's terrifying secret [Doc 45096].", "latency_ms": 1343.141794204712, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Brain Fluid Explosion Girl,' but the generated answer claims it is 'Tokyo Gore School,' which is incorrect. However, the answer remains faithful to the provided document citation (Doc 45096), so it is faithful but not correct.", "main_total_tokens": 1994, "main_prompt_tokens": 1918, "main_completion_tokens": 76, "judge_total_tokens": 546, "judge_prompt_tokens": 464, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show in which six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons,\" which aired in 1983. In this show, the friends are transported into the Dungeons & Dragons realm and must find a way home with the help of their guide, the Dungeon Master [Doc 112849].", "latency_ms": 1773.3242511749268, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' matching the gold answer exactly. The additional detail about the premise and year does not contradict the gold and remains consistent with known information, so the response is both correct and faithful.", "main_total_tokens": 1733, "main_prompt_tokens": 1665, "main_completion_tokens": 68, "judge_total_tokens": 539, "judge_prompt_tokens": 458, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The context provided does not contain information about a movie that tells the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones. Therefore, I cannot provide an answer based on the given documents.", "latency_ms": 1203.9012908935547, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the title of a specific movie, and the gold answer is 'Nacht der Wölfe'. The generated answer claims it cannot provide an answer, which is incorrect. However, it does not hallucinate or provide incorrect movie titles; it only states a limitation based on context, so it remains faithful to its (assumed) context but is factually incorrect with respect to the gold answer.", "main_total_tokens": 2059, "main_prompt_tokens": 2013, "main_completion_tokens": 46, "judge_total_tokens": 558, "judge_prompt_tokens": 444, "judge_completion_tokens": 114}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of \"Picnic at Hanging Rock\" take place on Valentine's Day, 1900 [Doc 126191].", "latency_ms": 1003.6032199859619, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the year as 1900 and aligns fully with the provided gold answer and source document.", "main_total_tokens": 1935, "main_prompt_tokens": 1910, "main_completion_tokens": 25, "judge_total_tokens": 464, "judge_prompt_tokens": 410, "judge_completion_tokens": 54}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie you are referring to is \"April Rain.\" It features a young violinist who spends her days working in a quirky Venice coffee shop, rehearsing for a professional music career at night, and daydreaming about her destined love. The character's name, April, corresponds to the name of a calendar month. However, this specific title does not appear in the provided documents, so I cannot cite a source for it.", "latency_ms": 2053.0614852905273, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.3, "justification": "The correct answer is 'June', but the generated response gives 'April Rain', so it is factually incorrect. It also fabricates a movie title and plot details not supported by any source, so faithfulness is low, though it does admit lack of citation, giving it a slightly higher score than zero.", "main_total_tokens": 2345, "main_prompt_tokens": 2256, "main_completion_tokens": 89, "judge_total_tokens": 616, "judge_prompt_tokens": 521, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1187.1023178100586, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific movie title, whereas the generated answer says 'I don't know.' This is factually incorrect relative to the gold. However, the generated answer does not hallucinate or contradict given information; it simply admits lack of knowledge, so it is fully faithful to its own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 399, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 286.93294525146484, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 7.6, but the generated answer says 'I don't know,' which is incorrect. However, the response is faithful to the model's uncertainty and does not fabricate an answer, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 467, "judge_prompt_tokens": 389, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1069.4959163665771, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3535124404757273, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 4, but the generated answer is 18, which is numerically incorrect. The evaluation itself correctly identifies the mismatch without adding unfounded details, so it is faithful to the provided information but the underlying answer is not correct.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 394, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 185.90688705444336, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the count of TV shows matching certain criteria, and the gold answer is 2. The generated answer is 0, which is incorrect, so correctness is 0. There is no indication the model contradicted its own reasoning or context; it simply produced the wrong count, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 395, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1504.0030479431152, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model explicitly says it does not know, which is incorrect because the correct answer is Fantômas. However, the response is faithful in that it does not invent an incorrect answer or hallucinate information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 461, "judge_prompt_tokens": 389, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is titled **Kill Cruise**. It was released in 1990 and is described as a thriller where the yacht captain does not know what he's in for when he takes the two British beauties on their trip [Doc 11205].", "latency_ms": 1419.4095134735107, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise', which matches the gold answer exactly. The added detail about it being a 1990 thriller with a yacht captain taking two British beauties to Barbados is consistent with the described question and does not contradict the gold. Therefore, both correctness and faithfulness are fully satisfied.", "main_total_tokens": 1935, "main_prompt_tokens": 1875, "main_completion_tokens": 60, "judge_total_tokens": 540, "judge_prompt_tokens": 445, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 1572.4754333496094, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as \"Won't You Be My Neighbor?\", which matches the gold answer exactly. It is fully faithful, directly restating the information without adding unsupported claims.", "main_total_tokens": 2137, "main_prompt_tokens": 2098, "main_completion_tokens": 39, "judge_total_tokens": 511, "judge_prompt_tokens": 440, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The year of release for the movie titled \"It\" is 2017 [Doc 76823].", "latency_ms": 839.6258354187012, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says the correct year is 2018, but the generated answer states 2017, so it is factually incorrect relative to the gold. However, the answer is self-consistent and does not contradict its own cited document, so it is fully faithful to its own reasoning and reference.", "main_total_tokens": 1973, "main_prompt_tokens": 1952, "main_completion_tokens": 21, "judge_total_tokens": 490, "judge_prompt_tokens": 398, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The context does not provide information about a specific TV series that features a young boy robot in 2030 AD. Therefore, I cannot answer your question based on the provided documents.", "latency_ms": 808.3233833312988, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question clearly refers to the animated TV series 'Astro Boy', but the generated answer claims it cannot answer and does not provide 'Astro Boy'. This is incorrect. It is also unfaithful to the gold answer, which explicitly gives 'Astro Boy'.", "main_total_tokens": 1972, "main_prompt_tokens": 1936, "main_completion_tokens": 36, "judge_total_tokens": 509, "judge_prompt_tokens": 423, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room,\" released in 2015, the woman and her son were held captive for 7 years in an enclosed space [Doc 67513].", "latency_ms": 1040.5333042144775, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer. It is fully consistent with the provided information and does not introduce contradictions or unsupported details.", "main_total_tokens": 1674, "main_prompt_tokens": 1640, "main_completion_tokens": 34, "judge_total_tokens": 501, "judge_prompt_tokens": 428, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 1757.7521800994873, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before the 26 November 1983 break-in, matching the provided gold value. It stays faithful to the given information and does not introduce contradictions or unsupported claims.", "main_total_tokens": 2089, "main_prompt_tokens": 2056, "main_completion_tokens": 33, "judge_total_tokens": 513, "judge_prompt_tokens": 428, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6316.128253936768, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score (80.5772) for the most popular English-speaking TV show released in 1997. The generated answer says, 'I don't know,' which is incorrect because the value is known and provided in the gold answer. However, the response is faithful in that it does not fabricate any information; it accurately reflects uncertainty instead of guessing.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 504, "judge_prompt_tokens": 396, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 319.3995952606201, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1 but the generated answer is 0, so the response is factually incorrect. However, the model directly answered the question without adding unsupported details, so it is faithful to its (incorrect) internal claim.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 463, "judge_prompt_tokens": 385, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 1343.4786796569824, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Angel Eyes' and appears to be supported by the cited document reference, so it is both correct and faithful.", "main_total_tokens": 2500, "main_prompt_tokens": 2465, "main_completion_tokens": 35, "judge_total_tokens": 498, "judge_prompt_tokens": 429, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"What Is This Film Called Love?\" is English [Doc 58692].", "latency_ms": 678.1337261199951, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The model answered that the language is English, but the gold label is Korean, so the answer is factually incorrect. It also contradicts the provided gold answer, so it is not faithful to the given reference.", "main_total_tokens": 1845, "main_prompt_tokens": 1824, "main_completion_tokens": 21, "judge_total_tokens": 471, "judge_prompt_tokens": 397, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 690.7272338867188, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is factually incorrect. However, there is no indication that the model contradicted its own reasoning or used unsupported assumptions beyond giving the wrong number, so it is fully faithful to its (implicit) reasoning process.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 484, "judge_prompt_tokens": 394, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The context provided does not contain any information about movies released in 1979. Therefore, I cannot answer the question regarding the best-rated movie released in that year.", "latency_ms": 1169.7180271148682, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Given the gold answer, the database/context has no movies from 1979. The generated answer correctly states it cannot answer because the context has no information on movies released in 1979, which matches the gold answer’s limitation. It does not fabricate any movie or rating and is fully consistent with the provided context.", "main_total_tokens": 2188, "main_prompt_tokens": 2155, "main_completion_tokens": 33, "judge_total_tokens": 522, "judge_prompt_tokens": 428, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The movie \"Back to the Future Part IV\" was not mentioned in the provided context, and therefore, I cannot provide a release year for it.", "latency_ms": 936.3992214202881, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer correctly states that 'Back to the Future Part IV' does not exist in the database, implicitly that no release year can be given. The generated answer instead says it was not mentioned in the provided context, implying that if it had been mentioned, a year could be given. This is incorrect with respect to the underlying fact that such a movie does not exist. It is partially faithful because it at least acknowledges inability to provide a year, but it changes the reason from nonexistence to lack of mention in context.", "main_total_tokens": 2682, "main_prompt_tokens": 2652, "main_completion_tokens": 30, "judge_total_tokens": 569, "judge_prompt_tokens": 430, "judge_completion_tokens": 139}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot find any movies or shows matching the criteria, which is consistent with the gold answer that there were no movies from 1980 about the Chernobyl disaster (which occurred later, in 1986). The response does not introduce incorrect facts and remains faithful to the question and the underlying information limits.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 533, "judge_prompt_tokens": 434, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 144.12832260131836, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer '0' is factually incorrect given the gold answer '4', so correctness is 0. The answer is a single numeric value without reasoning or reference to external context, so it is internally consistent and thus faithful to itself, giving faithfulness 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 474, "judge_prompt_tokens": 388, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 167.67287254333496, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. However, it is consistent with itself and does not invent any supporting details, so it is fully faithful to its own content.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 462, "judge_prompt_tokens": 386, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 138.01169395446777, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the number of TV shows released in the 1990s about Super Mario Bros, and the gold answer is 1. The generated answer is 0, which is incorrect. The response directly answers the question with a single number and does not contradict or add any extraneous information beyond that, so it is fully faithful to its own content but factually wrong relative to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 501, "judge_prompt_tokens": 389, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 130.77044486999512, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (0) and is fully supported by the given information, so it is both correct and faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 449, "judge_prompt_tokens": 391, "judge_completion_tokens": 58}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 132.01189041137695, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Naruto'. The generated answer says 'I don't know', which is incorrect. However, the response is faithful to the model's uncertainty and does not fabricate information, so faithfulness is high even though correctness is zero.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 466, "judge_prompt_tokens": 386, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The context provided does not mention a Jurassic Park/Power Ranger crossover movie, nor does it provide a release date for such a film. Therefore, I cannot provide an answer to your question.", "latency_ms": 1126.533031463623, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is that the movie was not released. The generated answer implies uncertainty and claims inability to answer rather than stating that it does not exist, so it is factually incorrect. It is also not faithful to the gold answer, which explicitly asserts that the movie was not released.", "main_total_tokens": 2085, "main_prompt_tokens": 2047, "main_completion_tokens": 38, "judge_total_tokens": 511, "judge_prompt_tokens": 423, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "The context provided does not mention any movie in which Barbie fights the Predator. Therefore, I cannot provide an answer based on the given information.", "latency_ms": 901.7605781555176, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The correct answer is that such a movie does not exist / was not released. The generated answer instead says it cannot answer based on the context, which is incorrect with respect to the gold. However, it is mostly faithful to the (implied) context in that it does not hallucinate a nonexistent movie, hence a relatively high faithfulness score.", "main_total_tokens": 2208, "main_prompt_tokens": 2180, "main_completion_tokens": 28, "judge_total_tokens": 513, "judge_prompt_tokens": 410, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The context provided does not specify the release years of the main Star Wars films, so I cannot determine which is the 4th oldest Star Wars movie based on the information given.", "latency_ms": 1114.6392822265625, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the 4th oldest Star Wars movie, and the gold answer is a specific title: 'Star Wars: Droids - The Great Heep'. The generated answer instead claims it cannot determine the answer from context and does not provide the correct movie title. This is incorrect with respect to the gold answer. However, given the assumption that the model saw no additional context, its statement about lacking information is consistent with that limited context, so it is faithful to its own reasoning, but not correct relative to the task.", "main_total_tokens": 2397, "main_prompt_tokens": 2361, "main_completion_tokens": 36, "judge_total_tokens": 563, "judge_prompt_tokens": 425, "judge_completion_tokens": 138}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The year of release for the movie titled \"It\" is 2017 [Doc 76823].", "latency_ms": 728.7218570709229, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 1986 but the generated answer states 2017, so it is factually incorrect. It also contradicts the provided gold answer and is therefore unfaithful to the reference.", "main_total_tokens": 1973, "main_prompt_tokens": 1952, "main_completion_tokens": 21, "judge_total_tokens": 470, "judge_prompt_tokens": 398, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 92.30327606201172, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a precise numeric rating (8.466). The generated answer says \"I don't know,\" which does not match the gold value, so it is incorrect (0 for correctness). However, the model does not hallucinate or give a wrong rating; it truthfully expresses uncertainty, so it is fully faithful to its own stated knowledge (1 for faithfulness).", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 504, "judge_prompt_tokens": 398, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 1123.5473155975342, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med,' which matches the gold answer exactly. It is fully faithful to the given information and does not introduce contradictions or extraneous claims.", "main_total_tokens": 1697, "main_prompt_tokens": 1666, "main_completion_tokens": 31, "judge_total_tokens": 493, "judge_prompt_tokens": 423, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In \"Túnel de los huesos\" (2011), seven prisoners escaped from a prison in Argentina, specifically from the Villa Devoto prison in Buenos Aires [Doc 53582].", "latency_ms": 1205.9035301208496, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the country as Argentina, matching the gold answer. It adds extra detail about Villa Devoto prison in Buenos Aires, which is accurate but not supported by the provided gold snippet; since the evaluation is relative to the given context, this additional unsourced detail slightly lowers faithfulness.", "main_total_tokens": 2095, "main_prompt_tokens": 2056, "main_completion_tokens": 39, "judge_total_tokens": 523, "judge_prompt_tokens": 430, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The context provided does not contain any information about a movie titled \"Sweet Trap\" (2024) or details regarding an apprentice falling into someone's trap. Therefore, I cannot provide an answer to your question.", "latency_ms": 1467.261552810669, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Jiang Jie'. The generated answer claims the context does not contain information and refuses to answer. While this is faithful to the (lack of) context, it is incorrect relative to the gold answer, so correctness is 0 and faithfulness is 1.", "main_total_tokens": 2146, "main_prompt_tokens": 2105, "main_completion_tokens": 41, "judge_total_tokens": 513, "judge_prompt_tokens": 425, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 429.4285774230957, "retrieval_recall": 0.0, "retrieval_mrr": 0.25, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.16812753627111746, "hit_rate_at_5": 1.0, "recall_at_10": 0.25, "precision_at_10": 0.1, "ndcg_at_10": 0.16812753627111746, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (4), so it is factually incorrect. However, the response is simple and does not introduce any extra, ungrounded content beyond the incorrect number, so it is fully faithful to what was generated.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 470, "judge_prompt_tokens": 385, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "35", "latency_ms": 92.03648567199707, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '35' is incorrect because the gold answer states that there are no movies featuring both Bruce Lee and Steven Seagal together. The model also did not reflect the uncertainty or lack of evidence indicated in the gold answer, so it is not faithful to the provided reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 482, "judge_prompt_tokens": 395, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of \"Tokyo Mater\" (2008) is 6.3 [Doc 42750].", "latency_ms": 961.2233638763428, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (6.3) and attributes it to a document, so it is both correct and faithful to the provided information.", "main_total_tokens": 2032, "main_prompt_tokens": 2010, "main_completion_tokens": 22, "judge_total_tokens": 468, "judge_prompt_tokens": 405, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in the movie \"Fast Track: No Limits\" is centered around street racing, as it focuses on a group of street racers [Doc 41929].", "latency_ms": 900.9268283843994, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.4, "justification": "The gold answer specifies the style of racing as 'Drift', while the generated answer says the movie centers on 'street racing', which is a different and more general style. Therefore, the generated answer is incorrect with respect to the labeled gold. It is partially faithful to its cited document (mentioning street racers as from [Doc 41929]), but it does not address or align with the required drift style, so faithfulness is limited.", "main_total_tokens": 2275, "main_prompt_tokens": 2240, "main_completion_tokens": 35, "judge_total_tokens": 535, "judge_prompt_tokens": 413, "judge_completion_tokens": 122}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 1449.5823383331299, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Confronting a Serial Killer' and is consistent with the provided document reference, so it is both correct and faithful.", "main_total_tokens": 2053, "main_prompt_tokens": 2007, "main_completion_tokens": 46, "judge_total_tokens": 523, "judge_prompt_tokens": 452, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of \"Armageddon\" (1998) is 7.838 [Doc 20603].", "latency_ms": 925.2340793609619, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and does not introduce information beyond what is implied by the question and gold answer, so it is both correct and faithful.", "main_total_tokens": 2325, "main_prompt_tokens": 2301, "main_completion_tokens": 24, "judge_total_tokens": 478, "judge_prompt_tokens": 409, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 834.3992233276367, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Sixth Sense' and does not introduce any unsupported or contradictory information. It is both correct and faithful to the given gold answer.", "main_total_tokens": 2104, "main_prompt_tokens": 2075, "main_completion_tokens": 29, "judge_total_tokens": 486, "judge_prompt_tokens": 420, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 626.5811920166016, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that The Transporter was released in 2002, which matches the gold answer. It does not introduce contradictions or unsupported claims beyond a generic citation placeholder, so it is both correct and faithful.", "main_total_tokens": 2235, "main_prompt_tokens": 2218, "main_completion_tokens": 17, "judge_total_tokens": 471, "judge_prompt_tokens": 397, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is 'The Transporter Refueled' [Doc 67500].", "latency_ms": 1619.5838451385498, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, providing the correct title 'The Transporter Refueled' without adding unsupported information. It is both correct and faithful.", "main_total_tokens": 2291, "main_prompt_tokens": 2262, "main_completion_tokens": 29, "judge_total_tokens": 484, "judge_prompt_tokens": 420, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in the year 2015 [Doc 67500].", "latency_ms": 757.1349143981934, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, which matches the gold answer. It also does not introduce unsupported information beyond citing a document, so it is fully faithful.", "main_total_tokens": 2226, "main_prompt_tokens": 2204, "main_completion_tokens": 22, "judge_total_tokens": 482, "judge_prompt_tokens": 406, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "The context provided does not mention any romantic comedy in which Abraham Lincoln comes back from the dead. Therefore, I cannot provide an answer based on the given information.", "latency_ms": 1284.2905521392822, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says there is no such movie description found, and the generated answer essentially restates that it cannot provide an answer based on the given context. This matches the information and does not invent a movie, so it is both correct and faithful to the given context.", "main_total_tokens": 2144, "main_prompt_tokens": 2112, "main_completion_tokens": 32, "judge_total_tokens": 507, "judge_prompt_tokens": 422, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The context provided does not mention any movie that alludes to a manuscript written by Ernest Hemingway. Therefore, I cannot provide an answer based on the given information.", "latency_ms": 776.6830921173096, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Havana Darkness'. The generated answer claims it cannot provide an answer, which is incorrect given the gold label. However, it is faithful to the (incorrect) assumption that the context lacks that information, so it does not hallucinate and remains faithful to its own stated constraints.", "main_total_tokens": 2097, "main_prompt_tokens": 2065, "main_completion_tokens": 32, "judge_total_tokens": 511, "judge_prompt_tokens": 420, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "The TV show in which Doll heads to Hollywood to be with her childhood best friend Em is titled \"Doll & Em\" [Doc 122259].", "latency_ms": 836.0016345977783, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em' and directly matches the gold answer without adding unsupported information.", "main_total_tokens": 1953, "main_prompt_tokens": 1922, "main_completion_tokens": 31, "judge_total_tokens": 477, "judge_prompt_tokens": 420, "judge_completion_tokens": 57}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The movie you are referring to is not found in the provided context. None of the listed films from 2007 mention a character named Hallam or a plot involving spying on people to find the cause of a mother's death.", "latency_ms": 1635.9496116638184, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The correct answer to the question is 'Hallam Foe.' The generated answer incorrectly claims the movie is not found, so it is factually wrong, yielding a correctness score of 0. However, the answer does faithfully describe its own (incorrect) reasoning based on the supposed context and does not introduce extraneous fabricated facts, so faithfulness to its stated basis is relatively high, though not perfect because it implies limitations about the context without evidence.", "main_total_tokens": 2060, "main_prompt_tokens": 2015, "main_completion_tokens": 45, "judge_total_tokens": 567, "judge_prompt_tokens": 444, "judge_completion_tokens": 123}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The context does not provide information about a specific last name or whether it sounds like a friend or a foe. Therefore, I cannot answer the question based on the provided documents.", "latency_ms": 1163.9225482940674, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is a specific label ('A foe'). The generated answer declines to answer, claiming insufficient information, which does not match the gold. So it is incorrect. However, given only the brief Q/A pair and no original passage, we cannot see evidence that the model contradicted the provided context; it simply stated that the context lacks the needed information. Thus it is faithful to the visible context but not correct relative to the gold.", "main_total_tokens": 1808, "main_prompt_tokens": 1773, "main_completion_tokens": 35, "judge_total_tokens": 540, "judge_prompt_tokens": 421, "judge_completion_tokens": 119}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "In the 2017 TV show *Atypical*, Sam is 18 years old [Doc 124583].", "latency_ms": 708.0698013305664, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old and matches the gold answer exactly. It is fully consistent with the provided information and does not introduce any unsupported claims.", "main_total_tokens": 2190, "main_prompt_tokens": 2166, "main_completion_tokens": 24, "judge_total_tokens": 477, "judge_prompt_tokens": 411, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show *Atypical*, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 789.6568775177002, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the TV show 'Atypical', Sam is on the autism spectrum, which matches the gold answer exactly. It does not introduce any contradictions or unsupported claims, so it is both correct and faithful.", "main_total_tokens": 2130, "main_prompt_tokens": 2106, "main_completion_tokens": 24, "judge_total_tokens": 489, "judge_prompt_tokens": 411, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "In the movie À corde tendue, Sandra was born in Chamonix [Doc 59659].", "latency_ms": 695.746660232544, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that Sandra was born in Chamonix, which matches the gold answer exactly. It is correct and stays fully consistent with the provided information without introducing unsupported details.", "main_total_tokens": 2435, "main_prompt_tokens": 2414, "main_completion_tokens": 21, "judge_total_tokens": 485, "judge_prompt_tokens": 418, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In the movie \"When the Rain Falls,\" Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 885.5283260345459, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The gold answer is simply 'rains'. The generated answer includes that word correctly, so it is correct for the fill-in-the-blank. However, it hallucinates extra details: the movie title 'When the Rain Falls' and a document reference '[Doc 100481]', which are not supported by the given information. Thus correctness is 1, but faithfulness is slightly reduced for unnecessary invented context.", "main_total_tokens": 2848, "main_prompt_tokens": 2824, "main_completion_tokens": 24, "judge_total_tokens": 531, "judge_prompt_tokens": 415, "judge_completion_tokens": 116}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the low-rated German language animation \"Tabaluga,\" the character Tabaluga is the last of the dragons and the crown-prince of Greenland, a magical place inhabited by talking animals of many different species [Doc 114810].", "latency_ms": 1193.1354999542236, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Greenland as the place where Tabaluga is crown prince, which matches the gold answer. It stays faithful to the information, only adding a brief description that does not contradict the source.", "main_total_tokens": 2412, "main_prompt_tokens": 2364, "main_completion_tokens": 48, "judge_total_tokens": 515, "judge_prompt_tokens": 442, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie that matches the description of \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers\" is **One Afternoon** from the year 2021 [Doc 96838].", "latency_ms": 1218.4679508209229, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'One Afternoon', which matches the gold answer. It does not contradict the given description and remains faithful, only adding the year and a document reference that are plausible metadata rather than conflicting content.", "main_total_tokens": 1800, "main_prompt_tokens": 1748, "main_completion_tokens": 52, "judge_total_tokens": 533, "judge_prompt_tokens": 456, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is not found in the provided context.", "latency_ms": 1684.9687099456787, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "recall_at_10": 0.0, "precision_at_5": 0.0, "precision_at_10": 0.0, "ndcg_at_5": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_5": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'The Life Ahead'. The generated answer claims that the movie is not found, which is factually incorrect. However, it does not invent an alternative movie or contradict the given context beyond stating non-availability, so it remains faithful to the (incorrect) reasoning based on the supposed context.", "main_total_tokens": 2213, "main_prompt_tokens": 2180, "main_completion_tokens": 33, "judge_total_tokens": 524, "judge_prompt_tokens": 431, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team. The film illustrates how he demonstrated that a man of humble beginnings could take on the elite of motor racing and win [Doc 73859].", "latency_ms": 1757.274866104126, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and stays aligned with the given gold answer. The extra contextual detail about him founding the McLaren racing team is accurate and does not contradict the gold answer, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2292, "main_prompt_tokens": 2236, "main_completion_tokens": 56, "judge_total_tokens": 529, "judge_prompt_tokens": 440, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The context provided does not contain any information about a racing team or its name. Therefore, I cannot provide an answer to your question.", "latency_ms": 1090.014934539795, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates that the correct response is 'The McLaren Motor Racing team.' The generated answer says the context does not contain information and refuses to answer, which is factually incorrect relative to the gold, so correctness is 0. However, given the model only had the (apparently missing) context it refers to, it does not invent a team name and stays faithful to the stated lack of information, so faithfulness is 1.", "main_total_tokens": 1924, "main_prompt_tokens": 1897, "main_completion_tokens": 27, "judge_total_tokens": 531, "judge_prompt_tokens": 410, "judge_completion_tokens": 121}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context provided does not specify the country of origin for \"he.\" Therefore, I cannot provide an answer based on the available information.", "latency_ms": 771.0916996002197, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'New Zealand', but the model said it could not determine the country, so the answer is incorrect. However, given the model’s stated assumption that the context did not specify the country, its response was internally consistent with that mistaken belief, so it is faithful to its own reasoning about the (assumed) context.", "main_total_tokens": 1827, "main_prompt_tokens": 1800, "main_completion_tokens": 27, "judge_total_tokens": 502, "judge_prompt_tokens": 403, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "The context provided does not mention any 2013 movie in which John McClane blows up the Great Wall of China. Therefore, I cannot provide an answer based on the given information.", "latency_ms": 957.8917026519775, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer correctly states that there is no such movie in the database, while the generated answer incorrectly frames the limitation as being due only to the provided context, implying that such a movie might exist but is merely not mentioned. The question is factual, and the correct response is to deny the existence of such a movie rather than to appeal to missing context. Thus, the generated answer is not correct with respect to the task and is not faithful to the gold answer’s intent.", "main_total_tokens": 1916, "main_prompt_tokens": 1878, "main_completion_tokens": 38, "judge_total_tokens": 562, "judge_prompt_tokens": 436, "judge_completion_tokens": 126}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie that matches the description is **\"Tomie: Forbidden Fruit.\"** The plot involves a lonely high school girl who befriends a strange, mysterious, and beautiful girl, and it becomes clear that their meeting was not purely accidental [Doc 28589].", "latency_ms": 1745.3577518463135, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Tomie: Forbidden Fruit,' which matches the gold answer. It does not introduce contradictions or unsupported claims beyond a brief restatement of the plot, so it is both correct and faithful.", "main_total_tokens": 2211, "main_prompt_tokens": 2152, "main_completion_tokens": 59, "judge_total_tokens": 553, "judge_prompt_tokens": 476, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie \"Lei mi parla ancora,\" Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 1034.4288349151611, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (65 years) and stays within the information given, only rephrasing it with minor formatting and a bracketed source reference. Thus it is both correct and faithful.", "main_total_tokens": 2435, "main_prompt_tokens": 2405, "main_completion_tokens": 30, "judge_total_tokens": 497, "judge_prompt_tokens": 424, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The context provided does not contain any information about \"Scorpion King: Book of Souls\" (2018) or what The Scorpion King and Tala are searching for. Therefore, I cannot provide an answer to your question.", "latency_ms": 1316.2391185760498, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks what The Scorpion King and Tala search for in 'Scorpion King: Book of Souls (2018)'. The correct answer is 'The Book of Souls'. The generated answer claims there is no information in the context and thus does not answer the question, which is incorrect with respect to the gold answer. However, it is faithful to the (stated) lack of context, so it is fully faithful but factually incorrect regarding the movie plot.", "main_total_tokens": 1949, "main_prompt_tokens": 1904, "main_completion_tokens": 45, "judge_total_tokens": 564, "judge_prompt_tokens": 439, "judge_completion_tokens": 125}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"Zombies on the Moon\" was released is not found in the provided context.", "latency_ms": 1552.7467727661133, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year is not found in the provided context, which matches the gold response indicating the movie is not in the database. It is correct relative to the given information and faithful to the context limitations.", "main_total_tokens": 2263, "main_prompt_tokens": 2244, "main_completion_tokens": 19, "judge_total_tokens": 487, "judge_prompt_tokens": 410, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"The Galactic Baker\" was released is not found in the provided context.", "latency_ms": 662.1763706207275, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available / not found, which is correct given the lack of information. The generated answer is faithful to the (implicit) context and does not hallucinate a year.", "main_total_tokens": 2486, "main_prompt_tokens": 2469, "main_completion_tokens": 17, "judge_total_tokens": 483, "judge_prompt_tokens": 406, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"Return of the Null Pointer\" was released is not found in the provided context.", "latency_ms": 853.945255279541, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both answers correctly state that the release year is not available / not found. The generated answer is consistent with the gold answer and does not introduce unsupported information.", "main_total_tokens": 2069, "main_prompt_tokens": 2050, "main_completion_tokens": 19, "judge_total_tokens": 472, "judge_prompt_tokens": 410, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"Data Science: The Movie\" was released is not found in the provided context.", "latency_ms": 1034.8033905029297, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available (not in the database / not in the context). This is correct given the information and faithful to the prompt, with no hallucinated year provided.", "main_total_tokens": 2022, "main_prompt_tokens": 2003, "main_completion_tokens": 19, "judge_total_tokens": 485, "judge_prompt_tokens": 410, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year of \"Python's Revenge 2\" is not found in the provided context.", "latency_ms": 909.7025394439697, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database. The generated answer states that the release year is not found in the provided context, which is effectively equivalent: it does not hallucinate a year and acknowledges lack of information. This is correct and faithful to the given information.", "main_total_tokens": 2073, "main_prompt_tokens": 2054, "main_completion_tokens": 19, "judge_total_tokens": 498, "judge_prompt_tokens": 410, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"The Last Pixel\" was released is not found in the provided context.", "latency_ms": 686.7251396179199, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available / not found. This is correct given the context and faithful to it, though the wording differs slightly.", "main_total_tokens": 1789, "main_prompt_tokens": 1772, "main_completion_tokens": 17, "judge_total_tokens": 472, "judge_prompt_tokens": 406, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"The Man Who Ate the Sun\" was released is not found in the provided context.", "latency_ms": 898.1342315673828, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year or information about the movie is not available in the given data/context. This is correct given the lack of information and is faithful to the provided context.", "main_total_tokens": 1976, "main_prompt_tokens": 1956, "main_completion_tokens": 20, "judge_total_tokens": 484, "judge_prompt_tokens": 412, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was directed by **Michael Samuels** [Doc 46069].", "latency_ms": 1024.5239734649658, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the director information is not in the database, so the system should say it cannot answer. The generated answer names Michael Samuels as the director and cites a document, introducing information not supported by the provided database context. This contradicts the gold standard and hallucinates beyond the database, making it both incorrect and unfaithful.", "main_total_tokens": 2167, "main_prompt_tokens": 2142, "main_completion_tokens": 25, "judge_total_tokens": 524, "judge_prompt_tokens": 422, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not contain any information about a movie titled \"A Love to Die For\" or its director. Therefore, I cannot provide an answer to your question.", "latency_ms": 995.9192276000977, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot provide the director because the context/database does not contain information about the movie or its director, which matches the gold answer in meaning. It is both correct and faithful to the given context.", "main_total_tokens": 2290, "main_prompt_tokens": 2256, "main_completion_tokens": 34, "judge_total_tokens": 505, "judge_prompt_tokens": 429, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not specified in the provided context. Therefore, I cannot provide that information.", "latency_ms": 1730.7324409484863, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not specified in the provided context and that it cannot provide the information, which matches the gold answer. It does not invent a director and remains faithful to the given database limitation.", "main_total_tokens": 2423, "main_prompt_tokens": 2397, "main_completion_tokens": 26, "judge_total_tokens": 494, "judge_prompt_tokens": 419, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Night at Tremore Beach\" is not mentioned in the provided context. Therefore, I cannot provide that information.", "latency_ms": 1227.2765636444092, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned and cannot be provided, which matches the gold answer. It does not introduce any unsupported information and remains faithful to the given context and database limitations.", "main_total_tokens": 1816, "main_prompt_tokens": 1786, "main_completion_tokens": 30, "judge_total_tokens": 498, "judge_prompt_tokens": 427, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy,\" released in 2021 [Doc 98375].", "latency_ms": 1807.39426612854, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as \"Rise of the Mummy.\" It does not add any contradictory or fabricated information; the mention of release year and document reference does not conflict with the gold and is plausible, so the response is both correct and faithful.", "main_total_tokens": 1929, "main_prompt_tokens": 1897, "main_completion_tokens": 32, "judge_total_tokens": 512, "judge_prompt_tokens": 423, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse is titled \"Stranger in My Bed,\" released in 2005 [Doc 35237].", "latency_ms": 1070.8661079406738, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Stranger in My Bed,\" which matches the gold answer. It does not introduce contradictory or unsupported information; the year and citation style do not conflict with any given constraints, so the response is fully faithful to the provided information.", "main_total_tokens": 1963, "main_prompt_tokens": 1928, "main_completion_tokens": 35, "judge_total_tokens": 516, "judge_prompt_tokens": 431, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie you are referring to is \"Game of Silence,\" which is a TV series from 2016. The plot involves a successful Atlanta attorney whose long-lost childhood friends unexpectedly reappear after 25 years, leading to the resurfacing of a dark secret from their past [Doc 123938].", "latency_ms": 1438.9050006866455, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The question asks for the title of the movie/production, and the generated answer correctly identifies it as \"Game of Silence,\" matching the gold answer. However, the answer adds that it is a TV series from 2016 and references a document ID, which goes beyond the original question. While that extra detail is factually correct, it is not grounded in the provided QA pair itself, so faithfulness is slightly reduced.", "main_total_tokens": 1705, "main_prompt_tokens": 1644, "main_completion_tokens": 61, "judge_total_tokens": 576, "judge_prompt_tokens": 458, "judge_completion_tokens": 118}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie you are referring to is \"The Craigslist Killer,\" which was released in 2011. It is based on the true story of Philip Markoff, a med student who led a double life as a brutal sexual deviant, abusing prostitutes he found via Craigslist [Doc 53039].", "latency_ms": 1423.858642578125, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer,' which matches the gold answer and the described plot, so correctness is 1. The slight elaboration about Philip Markoff and his background is consistent with the real story and likely with the referenced document, but since I cannot directly verify Doc 53039, I assign faithfulness 0.9 rather than 1.", "main_total_tokens": 1994, "main_prompt_tokens": 1935, "main_completion_tokens": 59, "judge_total_tokens": 561, "judge_prompt_tokens": 451, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which Yuki Hirano sets his eyes on the role of a forestry trainee after failing his university entrance exam is titled **Wood Job!**. It was released in 2014 and falls under the genres of drama and comedy. The plot follows Yuki as he embarks on a training course and eventually works in Kamusari Village, where he learns to appreciate nature and the local community [Doc 66094].", "latency_ms": 1989.7618293762207, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.88, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", matching the gold answer, so correctness is 1. It also accurately describes the setup that he becomes a forestry trainee after failing his exam. The added details (2014 release, drama/comedy, Kamusari Village, appreciating nature and local community) are consistent with known information about the film, but the reference to a specific document [Doc 66094] is unverifiable here and slightly reduces faithfulness to the provided context, so faithfulness is slightly reduced.", "main_total_tokens": 2882, "main_prompt_tokens": 2795, "main_completion_tokens": 87, "judge_total_tokens": 627, "judge_prompt_tokens": 487, "judge_completion_tokens": 140}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure, is **\"Sesame Street: The Magical Wand Chase\"** from 2017 [Doc 78373].", "latency_ms": 1560.514211654663, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Sesame Street: The Magical Wand Chase,' which matches the gold answer exactly and adheres to the described plot. The added year and citation do not contradict the known information and are consistent with the source, so the response is both correct and faithful to the reference document.", "main_total_tokens": 2365, "main_prompt_tokens": 2309, "main_completion_tokens": 56, "judge_total_tokens": 560, "judge_prompt_tokens": 466, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a 7-year-old boy must find the hero within him to save his parents trapped inside a burning car due to a rock fall is titled **Beyond**, released in **2016** [Doc 74597].", "latency_ms": 1013.4034156799316, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title given the plot description. The gold answer is \"Beyond\". The generated answer correctly identifies the movie as \"Beyond\" and does not contradict the gold. The extra mention of the release year does not conflict with the given information, so the response is both correct and faithful to the gold answer.", "main_total_tokens": 1884, "main_prompt_tokens": 1837, "main_completion_tokens": 47, "judge_total_tokens": 552, "judge_prompt_tokens": 455, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman is fired from a financial corporation during the Asia crisis and returns home with no money is **6ixtynin9** (1999) [Doc 22478].", "latency_ms": 1012.6159191131592, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9,' which matches the gold answer. It stays fully consistent with the provided information, simply restating and slightly elaborating on the plot description without introducing contradictions or unsupported claims.", "main_total_tokens": 1887, "main_prompt_tokens": 1846, "main_completion_tokens": 41, "judge_total_tokens": 523, "judge_prompt_tokens": 443, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie you are referring to is **Club Dread** (2004). In this film, a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort, a hedonistic island paradise for swingers, and it is up to the club's staff to stop the violence or at least hide it [Doc 32410].", "latency_ms": 1590.1105403900146, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' matching the gold answer exactly. It remains faithful to the provided information, simply restating the plot summary without adding incorrect details.", "main_total_tokens": 2229, "main_prompt_tokens": 2159, "main_completion_tokens": 70, "judge_total_tokens": 563, "judge_prompt_tokens": 494, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie you are referring to is titled \"Toilet,\" which was released in 2010. The plot involves a 30-something Gundam fan and engineer who is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral [Doc 48873].", "latency_ms": 1842.3097133636475, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" matching the gold answer exactly. It also aligns with the provided plot description and adds a plausible release year without contradicting the source. Therefore, it is both factually correct and faithful to the given information.", "main_total_tokens": 2426, "main_prompt_tokens": 2360, "main_completion_tokens": 66, "judge_total_tokens": 564, "judge_prompt_tokens": 480, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie you are referring to is \"Electric Angel,\" released in 1981. It is described as a typical experimental film that employs a variety of audiovisual techniques to evoke a sense of polymorphic eroticism, drawing from European and Mediterranean cinematography of the 20th century. The film combines animation and live action, featuring a loose structure that enhances its open-ended character, with rhythm being a key element [Doc 1024].", "latency_ms": 2305.882453918457, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.92, "justification": "The question expects only the movie title, which is correctly given as 'Electric Angel'. However, the evaluation here is about the generated answer relative to the task style in these datasets, where adding extra descriptive content is treated as incorrect even if the title matches. Therefore, correctness is scored as 0. The description of the film in the generated answer is consistent with the prompt text and does not invent contradictory details, so it is largely faithful to the provided information, though it slightly elaborates beyond it, so faithfulness is high but not perfect.", "main_total_tokens": 2282, "main_prompt_tokens": 2195, "main_completion_tokens": 87, "judge_total_tokens": 645, "judge_prompt_tokens": 503, "judge_completion_tokens": 142}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is \"Fall,\" which has a rating of 4.0 [Doc 106810].", "latency_ms": 780.31325340271, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect: it claims the lowest-rated 2023 movie is 'Fall' with a 4.0 rating, while the gold answer indicates it is 'Mega Ape'. Additionally, the answer fabricates a specific rating and citation without grounding in the provided gold information, so it is not faithful.", "main_total_tokens": 2126, "main_prompt_tokens": 2099, "main_completion_tokens": 27, "judge_total_tokens": 502, "judge_prompt_tokens": 408, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 131.00194931030273, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Tea House', but the model responded with 'I don't know', which is incorrect. However, the response does not introduce any incorrect facts; it simply admits lack of knowledge, so it is fully faithful to its own (stated) uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 471, "judge_prompt_tokens": 386, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 532.3569774627686, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct movie ('The Uncut Story of Mewtwo's Origin'), so it is incorrect. However, it does not introduce any hallucinated facts and honestly admits lack of knowledge, so it is fully faithful to the model's stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 488, "judge_prompt_tokens": 395, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is \"Cataclysm,\" which has a rating of 3.889. The language of this movie is English (en) [Doc 744].", "latency_ms": 1195.1110363006592, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980. The gold answer is 'English'. The generated answer states that the lowest-rated movie is 'Cataclysm' with rating 3.889 and explicitly says its language is English (en). The language matches the gold answer, so correctness is 1. The generated answer correctly cites document 744 as the source and does not contradict the provided gold information, so faithfulness is also 1.", "main_total_tokens": 2335, "main_prompt_tokens": 2295, "main_completion_tokens": 40, "judge_total_tokens": 550, "judge_prompt_tokens": 423, "judge_completion_tokens": 127}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was \"Popular,\" which had a popularity score of 4.5113 [Doc 115196].", "latency_ms": 1472.5348949432373, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect: the gold answer is 'Law & Order: Special Victims Unit,' but the model answered 'Popular.' It is also unfaithful to the provided gold reference, so both correctness and faithfulness are 0.", "main_total_tokens": 2312, "main_prompt_tokens": 2280, "main_completion_tokens": 32, "judge_total_tokens": 502, "judge_prompt_tokens": 422, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 923.5310554504395, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 and does not introduce any unsupported information beyond what is implied by the reference doc ID. It is both correct and faithful.", "main_total_tokens": 2213, "main_prompt_tokens": 2195, "main_completion_tokens": 18, "judge_total_tokens": 468, "judge_prompt_tokens": 400, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in the year 1999 [Doc 22481].", "latency_ms": 877.2256374359131, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states the movie was released in 1999, but the gold label says 2019, so the content is factually incorrect relative to the gold. However, the answer is internally consistent with its cited document ([Doc 22481]) and does not contradict its own source, so faithfulness to the referenced document is high.", "main_total_tokens": 2156, "main_prompt_tokens": 2134, "main_completion_tokens": 22, "judge_total_tokens": 506, "judge_prompt_tokens": 406, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "1", "latency_ms": 92.0712947845459, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 279, but the generated answer was 1, so it is incorrect. The answer is fully based on the provided gold/reference information, so it is faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 451, "judge_prompt_tokens": 383, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "7", "latency_ms": 96.0090160369873, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (7) does not match the gold answer (12), so it is incorrect. The answer is directly addressing the question without adding unsupported information, so it is faithful to the prompt but factually wrong relative to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 462, "judge_prompt_tokens": 383, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The year of release for the movie \"The Last Days of Lehman Brothers\" is not found in the provided context.", "latency_ms": 1044.609785079956, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 2009, but the generated response incorrectly claims the release year is not found, so it is factually wrong. However, it is faithful to the (assumed) provided context by not hallucinating a specific year and instead stating that the information is not available.", "main_total_tokens": 2043, "main_prompt_tokens": 2019, "main_completion_tokens": 24, "judge_total_tokens": 500, "judge_prompt_tokens": 411, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 623.8486766815186, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the original language of 'The King\\'s Speech' as English, matching the gold label 'en', and does not introduce unsupported information beyond that.", "main_total_tokens": 2189, "main_prompt_tokens": 2170, "main_completion_tokens": 19, "judge_total_tokens": 469, "judge_prompt_tokens": 402, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "I don't know.", "latency_ms": 1329.9062252044678, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 1987, but the model responded with 'I don't know,' which is incorrect, so correctness is 0. However, saying 'I don't know' does not introduce any fabricated or contradictory information relative to the question, so the answer is fully faithful to the model's stated uncertainty, yielding a faithfulness score of 1.", "main_total_tokens": 546, "main_prompt_tokens": 545, "main_completion_tokens": 1, "judge_total_tokens": 484, "judge_prompt_tokens": 382, "judge_completion_tokens": 102}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Tokyo Gore School", "latency_ms": 885.629415512085, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'Tokyo Gore School' is incorrect. The correct movie is 'Brain Fluid Explosion Girl,' as stated in the gold answer. The generated response does not match the gold and is not supported by it, so it is unfaithful.", "main_total_tokens": 2483, "main_prompt_tokens": 2479, "main_completion_tokens": 4, "judge_total_tokens": 472, "judge_prompt_tokens": 391, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 1461.2622261047363, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Dungeons & Dragons.'", "main_total_tokens": 2178, "main_prompt_tokens": 2173, "main_completion_tokens": 5, "judge_total_tokens": 446, "judge_prompt_tokens": 394, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "I don't know.", "latency_ms": 426.0754585266113, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer identifies the movie as 'Nacht der Wölfe'. The generated answer says 'I don't know', which is incorrect. However, it does not introduce any fabricated or contradictory information; it simply admits lack of knowledge, so it is fully faithful though incorrect.", "main_total_tokens": 545, "main_prompt_tokens": 544, "main_completion_tokens": 1, "judge_total_tokens": 488, "judge_prompt_tokens": 402, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900", "latency_ms": 1439.2266273498535, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (1900), so it is both correct and faithful.", "main_total_tokens": 2493, "main_prompt_tokens": 2490, "main_completion_tokens": 3, "judge_total_tokens": 439, "judge_prompt_tokens": 388, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "Playing Mona Lisa", "latency_ms": 463.79995346069336, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question clearly points to the 2004 movie 'June,' matching the description of a young violinist named after a month. The generated answer 'Playing Mona Lisa' is a different film (from 2000, about a pianist), so it is factually incorrect. It is also unfaithful to the gold answer, which explicitly states the correct movie is 'June.'", "main_total_tokens": 159, "main_prompt_tokens": 156, "main_completion_tokens": 3, "judge_total_tokens": 542, "judge_prompt_tokens": 435, "judge_completion_tokens": 107}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 168.3061122894287, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific movie title, but the generated answer simply says 'I don't know,' which does not match the correct response and is therefore incorrect. However, the generated answer does not hallucinate or contradict known facts; it accurately reflects a lack of knowledge, so it is fully faithful to the model's own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 496, "judge_prompt_tokens": 399, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 176.74016952514648, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 7.6, but the model said it did not know, so the answer is incorrect. However, the response is faithful because the model did not fabricate an answer and honestly stated its lack of knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 466, "judge_prompt_tokens": 389, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "12", "latency_ms": 1594.2986011505127, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3535124404757273, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (12) does not match the gold answer (4), so it is incorrect. However, it is a direct numeric response to the question and does not introduce any extraneous or fabricated reasoning, so it is faithful to the user query.", "main_total_tokens": 1465, "main_prompt_tokens": 1442, "main_completion_tokens": 23, "judge_total_tokens": 476, "judge_prompt_tokens": 394, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 167.02675819396973, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer contradicts the gold answer. The gold states there are 2 such TV shows, but the model claims there are none. This makes the answer factually incorrect. However, the answer does stay within the bounds of the information it claims to have (it does not fabricate specific shows or numbers), so it is faithful to its own stated limitations.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 507, "judge_prompt_tokens": 404, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 216.0499095916748, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered 'I don't know,' which does not provide the correct answer 'Fantômas,' so the correctness is 0. However, the response is internally consistent and does not contradict any provided information, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 389, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise", "latency_ms": 1416.308879852295, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, and the movie 'Kill Cruise' is indeed about two British women who sail to Barbados with a yacht captain. Therefore it is both correct and faithful.", "main_total_tokens": 2444, "main_prompt_tokens": 2441, "main_completion_tokens": 3, "judge_total_tokens": 457, "judge_prompt_tokens": 387, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 1211.9412422180176, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie title, so it is both correct and faithful.", "main_total_tokens": 2706, "main_prompt_tokens": 2698, "main_completion_tokens": 8, "judge_total_tokens": 462, "judge_prompt_tokens": 408, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018", "latency_ms": 1520.3912258148193, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('2018'), so it is both correct and faithful.", "main_total_tokens": 2860, "main_prompt_tokens": 2844, "main_completion_tokens": 16, "judge_total_tokens": 431, "judge_prompt_tokens": 380, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Murderbot", "latency_ms": 3657.4575901031494, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer 'Murderbot' is incorrect; the correct TV series is 'Astro Boy'. However, the model did respond to the question, so it is faithful to the prompt but factually wrong.", "main_total_tokens": 2683, "main_prompt_tokens": 2609, "main_completion_tokens": 74, "judge_total_tokens": 463, "judge_prompt_tokens": 389, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 years [Doc 67513]", "latency_ms": 933.2232475280762, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer. The answer is fully supported by the provided information and does not introduce any unsupported details.", "main_total_tokens": 2185, "main_prompt_tokens": 2176, "main_completion_tokens": 9, "judge_total_tokens": 476, "judge_prompt_tokens": 403, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m", "latency_ms": 637.6228332519531, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The Brink's-Mat robbery involved about £26 million in gold bullion, diamonds, and cash; stating £26m for the gold holding matches the commonly cited figure and the generated answer correctly repeats it without alteration.", "main_total_tokens": 2072, "main_prompt_tokens": 2069, "main_completion_tokens": 3, "judge_total_tokens": 472, "judge_prompt_tokens": 398, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 496.25515937805176, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score, and the gold answer is 80.5772. The generated answer says 'I don't know,' which does not provide the correct value, so correctness is 0. However, stating uncertainty rather than fabricating an answer is faithful to the model's knowledge limits, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 497, "judge_prompt_tokens": 396, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 180.02700805664062, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1, but the generated answer claims there are none that match the criteria, which is factually incorrect. However, the model does not invent any specific incorrect shows or data; it simply states inability to find matches. Thus, it is entirely incorrect in result but remains faithful to its own (stated) uncertainty and does not hallucinate details.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 494, "judge_prompt_tokens": 394, "judge_completion_tokens": 100}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes", "latency_ms": 888.5788917541504, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Angel Eyes' exactly matches the gold answer and correctly identifies the TV show in which Soo-wan regains her sight after an eye transplant. It is both correct and fully faithful to the provided gold answer.", "main_total_tokens": 3078, "main_prompt_tokens": 3075, "main_completion_tokens": 3, "judge_total_tokens": 473, "judge_prompt_tokens": 397, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "en", "latency_ms": 1627.2857189178467, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'en' is incorrect because the gold label specifies the language is Korean. The answer is also unfaithful to the provided gold label, directly contradicting it.", "main_total_tokens": 2909, "main_prompt_tokens": 2899, "main_completion_tokens": 10, "judge_total_tokens": 444, "judge_prompt_tokens": 377, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 558.4254264831543, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 2 such movies, while the generated answer claims there are none. This is factually incorrect, so correctness is 0. However, the model does not hallucinate any specific data and simply admits inability to find matches, which is faithful to its stated limitations, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 500, "judge_prompt_tokens": 403, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 558.2540035247803, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that there are no movies from 1979 in the database. The generated answer says only 'I don't know,' which fails to convey that specific limitation and is therefore not correct relative to the gold. However, it does not contradict the gold or invent information; it expresses ignorance, which is consistent with lacking the data, so it is fully faithful.", "main_total_tokens": 545, "main_prompt_tokens": 544, "main_completion_tokens": 1, "judge_total_tokens": 504, "judge_prompt_tokens": 399, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The movie Back to the Future Part IV was not released in any year, as there is no record of such a film existing in the provided context. The last installment of the original trilogy is Back to the Future Part III, released in 1990 [Doc 10621].", "latency_ms": 1810.4560375213623, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Back to the Future Part IV was never released and clarifies that the trilogy ended with Part III in 1990. This is accurate and consistent with the gold answer, and it stays faithful to the given context without introducing contradictions.", "main_total_tokens": 3308, "main_prompt_tokens": 3251, "main_completion_tokens": 57, "judge_total_tokens": 539, "judge_prompt_tokens": 456, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both gold and generated answers correctly state that there are no movies in 1980 about the Chernobyl disaster, which is historically accurate since the disaster occurred in 1986. The generated answer is faithful to the question and does not introduce fabricated specifics; it simply notes the absence of matching movies and suggests reconsidering the timeframe.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 531, "judge_prompt_tokens": 434, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 176.4993667602539, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 4, but the generated answer says there are none, so it is factually incorrect. However, the model does not fabricate information beyond stating it cannot find any matches, so it remains faithful to its (incorrect) internal retrieval.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 479, "judge_prompt_tokens": 397, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "3", "latency_ms": 553.678035736084, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (3) does not match the gold answer (2), so it is factually incorrect with respect to the labeled ground truth. However, it is directly answering the user’s question without adding unrelated or fabricated context beyond the numeric response, so it is faithful to the prompt in form.", "main_total_tokens": 3299, "main_prompt_tokens": 3294, "main_completion_tokens": 5, "judge_total_tokens": 477, "judge_prompt_tokens": 386, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 131.99925422668457, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The gold answer is 1, but the generated answer says there are none, so it is factually incorrect. It does, however, stay on topic and addresses the question directly without adding unrelated information, so it is mostly but not perfectly faithful to the query and context.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 486, "judge_prompt_tokens": 398, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 135.0233554840088, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that there were 0 TV shows released between 2000 and 2010 about Super Mario Bros. The generated answer says it cannot find any movies or shows matching those criteria, which is effectively consistent with there being none. Thus, it aligns with the gold (zero results) and does not hallucinate, so it is both correct relative to the gold and faithful to the query.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 512, "judge_prompt_tokens": 400, "judge_completion_tokens": 112}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 132.01069831848145, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the most popular TV show or movie about ninjas, and the gold answer is 'Naruto'. The generated answer says 'I don't know,' which is factually incorrect because a well-known and widely accepted answer exists. However, the model does not fabricate or contradict the prompt; it simply expresses ignorance, so it remains faithful to the given information and does not introduce hallucinations.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 497, "judge_prompt_tokens": 386, "judge_completion_tokens": 111}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "There is no information available in the provided context regarding a Jurassic Park/Power Ranger crossover movie.", "latency_ms": 1351.593017578125, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the movie was not released, which is a definite, correct resolution to the question. The generated answer only says there is no information in the provided context, which neither affirms nor denies the existence or release of such a movie. This is incorrect relative to the gold. It is also unfaithful because the model should commit to the fact that the movie was not released, rather than deflecting to lack of context.", "main_total_tokens": 2615, "main_prompt_tokens": 2595, "main_completion_tokens": 20, "judge_total_tokens": 525, "judge_prompt_tokens": 404, "judge_completion_tokens": 121}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 436.0308647155762, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is that such a movie does not exist or was not released. The generated answer \"I don't know\" is not correct with respect to the gold answer. It is also not faithful to the gold answer since it fails to convey that the movie was not released.", "main_total_tokens": 538, "main_prompt_tokens": 537, "main_completion_tokens": 1, "judge_total_tokens": 472, "judge_prompt_tokens": 386, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The 4th oldest Star Wars movie is \"The Story of Star Wars\" from 2004 [Doc 32101].", "latency_ms": 1366.239309310913, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect: the 4th oldest Star Wars movie is given in the gold as 'Star Wars: Droids - The Great Heep', not 'The Story of Star Wars' from 2004. However, the model does not contradict or misrepresent any provided source in the prompt itself, so it is fully faithful to its own cited document reference, even though that content is factually wrong relative to the gold.", "main_total_tokens": 2954, "main_prompt_tokens": 2927, "main_completion_tokens": 27, "judge_total_tokens": 534, "judge_prompt_tokens": 415, "judge_completion_tokens": 119}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "2004", "latency_ms": 1839.2119407653809, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 2004, but the correct year is 1986, so it is factually incorrect (score 0 for correctness). However, it does directly answer the user question without adding unrelated content, so it is faithful to the prompt format (score 1 for faithfulness).", "main_total_tokens": 3008, "main_prompt_tokens": 2993, "main_completion_tokens": 15, "judge_total_tokens": 471, "judge_prompt_tokens": 380, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 142.70496368408203, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The true rating is 8.466, but the generated answer says 'I don't know,' which is incorrect. However, the answer does not fabricate any information; it simply expresses a lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 398, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med", "latency_ms": 749.028205871582, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Mighty Med' exactly matches the gold answer and correctly identifies the TV show described, so it is both correct and faithful.", "main_total_tokens": 2200, "main_prompt_tokens": 2196, "main_completion_tokens": 4, "judge_total_tokens": 454, "judge_prompt_tokens": 394, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina", "latency_ms": 848.9429950714111, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: both state that the prison was in Argentina. This is fully correct and faithful to the provided reference.", "main_total_tokens": 2634, "main_prompt_tokens": 2632, "main_completion_tokens": 2, "judge_total_tokens": 452, "judge_prompt_tokens": 392, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The apprentice falls into the trap of a \"sexual sorceress\" in Sex Files: Sexually Bewitched [Doc 25258].", "latency_ms": 1224.7827053070068, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect relative to the gold answer (it should be Jiang Jie, not a 'sexual sorceress' or anything related to 'Sex Files'). It also introduces unrelated content and a different work entirely, so it is not faithful to the original question context.", "main_total_tokens": 2715, "main_prompt_tokens": 2686, "main_completion_tokens": 29, "judge_total_tokens": 498, "judge_prompt_tokens": 412, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "6", "latency_ms": 752.4960041046143, "retrieval_recall": 0.0, "retrieval_mrr": 0.25, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.16812753627111746, "hit_rate_at_5": 1.0, "recall_at_10": 0.25, "precision_at_10": 0.1, "ndcg_at_10": 0.16812753627111746, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (6) does not match the gold answer (4), so it is incorrect. However, the model directly answered the quantitative question without adding extra fabricated details or reasoning, so it is fully faithful to its own (albeit wrong) prediction.", "main_total_tokens": 3397, "main_prompt_tokens": 3386, "main_completion_tokens": 11, "judge_total_tokens": 468, "judge_prompt_tokens": 385, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 898.2980251312256, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that no movies (or shows) matching the requested combination can be found, which aligns with the gold answer that there are no movies with both Bruce Lee and Steven Seagal. It is fully faithful to the gold response in meaning, only adding a minor extension ('or shows') that does not contradict or change the core information.", "main_total_tokens": 2231, "main_prompt_tokens": 2230, "main_completion_tokens": 1, "judge_total_tokens": 507, "judge_prompt_tokens": 406, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3", "latency_ms": 438.4465217590332, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3, so it is both correct and faithful.", "main_total_tokens": 2026, "main_prompt_tokens": 2023, "main_completion_tokens": 3, "judge_total_tokens": 440, "judge_prompt_tokens": 387, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "Drift-style racing [Doc 42750].", "latency_ms": 1686.1693859100342, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the style of racing as drift, which matches the gold label. Adding '-style racing' does not change the meaning, and the bracketed citation does not contradict the gold answer. Thus it is both correct and faithful.", "main_total_tokens": 2597, "main_prompt_tokens": 2574, "main_completion_tokens": 23, "judge_total_tokens": 467, "judge_prompt_tokens": 387, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer", "latency_ms": 1057.011604309082, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct title of the movie. It is fully correct and faithful to the reference.", "main_total_tokens": 2615, "main_prompt_tokens": 2608, "main_completion_tokens": 7, "judge_total_tokens": 471, "judge_prompt_tokens": 412, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838", "latency_ms": 640.8641338348389, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838), so it is both correct and faithful.", "main_total_tokens": 2317, "main_prompt_tokens": 2314, "main_completion_tokens": 3, "judge_total_tokens": 442, "judge_prompt_tokens": 389, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense", "latency_ms": 909.6181392669678, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'The Sixth Sense' as the 1999 psychological thriller that alludes to an extra sense. It is fully correct and faithful to the reference.", "main_total_tokens": 2643, "main_prompt_tokens": 2639, "main_completion_tokens": 4, "judge_total_tokens": 466, "judge_prompt_tokens": 394, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002", "latency_ms": 1371.09375, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer ('2002') exactly matches the gold answer ('2002'), so it is both correct and faithful.", "main_total_tokens": 2782, "main_prompt_tokens": 2779, "main_completion_tokens": 3, "judge_total_tokens": 438, "judge_prompt_tokens": 383, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled", "latency_ms": 5293.429374694824, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly giving the title of the 2015 reboot as 'The Transporter Refueled' and remains fully faithful to the question.", "main_total_tokens": 2852, "main_prompt_tokens": 2845, "main_completion_tokens": 7, "judge_total_tokens": 464, "judge_prompt_tokens": 397, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015", "latency_ms": 938.6467933654785, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer '2015' exactly matches the gold answer and correctly states the film's release year, showing both correctness and faithfulness.", "main_total_tokens": 2790, "main_prompt_tokens": 2787, "main_completion_tokens": 3, "judge_total_tokens": 446, "judge_prompt_tokens": 387, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "I don't know.", "latency_ms": 313.2014274597168, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says no such movie description can be found; the generated answer admits not knowing. Given the task, this matches the gold and is correct. The model faithfully reflects the lack of knowledge without inventing a movie.", "main_total_tokens": 561, "main_prompt_tokens": 560, "main_completion_tokens": 1, "judge_total_tokens": 470, "judge_prompt_tokens": 394, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "I don't know.", "latency_ms": 852.9605865478516, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the correct movie title is 'Havana Darkness.' However, the response 'I don't know.' does not introduce any fabricated information; it simply expresses lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 570, "main_prompt_tokens": 569, "main_completion_tokens": 1, "judge_total_tokens": 473, "judge_prompt_tokens": 392, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 1868.579626083374, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Doll & Em.'", "main_total_tokens": 2586, "main_prompt_tokens": 2552, "main_completion_tokens": 34, "judge_total_tokens": 444, "judge_prompt_tokens": 392, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "I don't know.", "latency_ms": 340.7747745513916, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the 2007 movie where Hallam spies on people and investigates his mother’s death. The correct answer is “Hallam Foe,” but the model responded with “I don't know,” which is incorrect. However, the answer is faithful in that it does not invent or hallucinate a wrong movie title; it simply admits not knowing.", "main_total_tokens": 574, "main_prompt_tokens": 573, "main_completion_tokens": 1, "judge_total_tokens": 501, "judge_prompt_tokens": 403, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "His last name sounds at first like he’s a foe. [Doc 22549]", "latency_ms": 2187.621831893921, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer in meaning and directly responds to the question without adding unsupported information.", "main_total_tokens": 2683, "main_prompt_tokens": 2646, "main_completion_tokens": 37, "judge_total_tokens": 456, "judge_prompt_tokens": 404, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18", "latency_ms": 665.9276485443115, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, and it is consistent with the factual age of Sam in the 2017 TV show 'Atypical'.", "main_total_tokens": 2745, "main_prompt_tokens": 2743, "main_completion_tokens": 2, "judge_total_tokens": 452, "judge_prompt_tokens": 389, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam is on the autism spectrum [Doc 124583].", "latency_ms": 976.3834476470947, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the TV show 'Atypical', Sam is on the autism spectrum. This matches the gold answer exactly and is faithful to the provided information, with no unsupported additions beyond a harmless document reference tag.", "main_total_tokens": 2707, "main_prompt_tokens": 2694, "main_completion_tokens": 13, "judge_total_tokens": 477, "judge_prompt_tokens": 399, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix", "latency_ms": 771.7630863189697, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Chamonix'), so it is correct and faithful.", "main_total_tokens": 3007, "main_prompt_tokens": 3003, "main_completion_tokens": 4, "judge_total_tokens": 451, "judge_prompt_tokens": 400, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "Hazuki clips her nails every time it rains. [Doc 100481]", "latency_ms": 1533.3681106567383, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold completion ('rains') and faithfully uses only the provided information.", "main_total_tokens": 3455, "main_prompt_tokens": 3438, "main_completion_tokens": 17, "judge_total_tokens": 458, "judge_prompt_tokens": 407, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 1361.767292022705, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which matches the gold answer exactly. It is fully supported by the provided information and adds only a citation, so it is both correct and faithful.", "main_total_tokens": 2954, "main_prompt_tokens": 2931, "main_completion_tokens": 23, "judge_total_tokens": 493, "judge_prompt_tokens": 416, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon", "latency_ms": 783.9701175689697, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('One Afternoon'), so it is both correct and faithful.", "main_total_tokens": 2310, "main_prompt_tokens": 2307, "main_completion_tokens": 3, "judge_total_tokens": 457, "judge_prompt_tokens": 406, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "I don't know.", "latency_ms": 262.2091770172119, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "recall_at_10": 0.0, "precision_at_5": 0.0, "precision_at_10": 0.0, "ndcg_at_5": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_5": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the title of a specific Italian-language movie, and the correct answer is 'The Life Ahead.' The generated answer says 'I don't know,' which is incorrect. However, it does not introduce any fabricated information or contradictions; it simply admits lack of knowledge. Therefore, correctness is 0, but faithfulness is 1 because the answer stays within the model's knowledge without hallucinating.", "main_total_tokens": 580, "main_prompt_tokens": 579, "main_completion_tokens": 1, "judge_total_tokens": 509, "judge_prompt_tokens": 402, "judge_completion_tokens": 107}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary “McLaren” is about Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 2182.307243347168, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and matches the gold answer. The added detail about him founding the McLaren Motor Racing team is accurate and does not contradict the source, so the response is both correct and faithful.", "main_total_tokens": 2854, "main_prompt_tokens": 2824, "main_completion_tokens": 30, "judge_total_tokens": 497, "judge_prompt_tokens": 413, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren", "latency_ms": 2114.750862121582, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer \"McLaren\" is essentially correct and refers to the same entity as the gold answer \"The McLaren Motor Racing team.\" While less specific, it is a faithful and accurate shorthand for the team name.", "main_total_tokens": 2881, "main_prompt_tokens": 2866, "main_completion_tokens": 15, "judge_total_tokens": 460, "judge_prompt_tokens": 385, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "I don't know.", "latency_ms": 1350.7030010223389, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'New Zealand', but the model answered 'I don't know', which is incorrect. However, the model did not hallucinate a wrong country and explicitly admitted uncertainty, so it is fully faithful to its own stated knowledge limits.", "main_total_tokens": 651, "main_prompt_tokens": 642, "main_completion_tokens": 9, "judge_total_tokens": 460, "judge_prompt_tokens": 380, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "I don't know.", "latency_ms": 423.18010330200195, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks about a 2013 movie where John McClane blows up the Great Wall of China. There is no such movie; this does not exist in standard film databases. The gold answer correctly indicates the information is not in the database. The generated answer, 'I don't know,' is also correct in that it does not fabricate an answer and aligns with the lack of such a movie. It is faithful to the prompt and does not introduce hallucinations.", "main_total_tokens": 546, "main_prompt_tokens": 545, "main_completion_tokens": 1, "judge_total_tokens": 526, "judge_prompt_tokens": 402, "judge_completion_tokens": 124}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit", "latency_ms": 888.1783485412598, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2002 Japanese-language horror movie as 'Tomie: Forbidden Fruit.'", "main_total_tokens": 2796, "main_prompt_tokens": 2790, "main_completion_tokens": 6, "judge_total_tokens": 480, "judge_prompt_tokens": 422, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65", "latency_ms": 832.0753574371338, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (65 years), so it is both correct and faithful to the reference.", "main_total_tokens": 2990, "main_prompt_tokens": 2988, "main_completion_tokens": 2, "judge_total_tokens": 450, "judge_prompt_tokens": 396, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "I don't know.", "latency_ms": 546.4529991149902, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states they search for The Book of Souls. The generated answer says 'I don't know,' which is incorrect. However, it is faithful because it does not introduce any incorrect fabricated information; it simply admits lack of knowledge.", "main_total_tokens": 581, "main_prompt_tokens": 580, "main_completion_tokens": 1, "judge_total_tokens": 476, "judge_prompt_tokens": 398, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"Zombies on the Moon\" was released is not provided in the context.", "latency_ms": 1355.7031154632568, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold standard answer is that the movie is not in the database, so the model should not fabricate a year. The generated answer correctly states that the release year is not provided, which aligns with the gold and does not introduce unsupported information.", "main_total_tokens": 2836, "main_prompt_tokens": 2817, "main_completion_tokens": 19, "judge_total_tokens": 488, "judge_prompt_tokens": 409, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 324.0649700164795, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database, effectively not providing the year. The generated answer says 'I don't know,' which is equivalent in meaning to not having the information. This matches the gold response in intent and does not introduce any incorrect information.", "main_total_tokens": 569, "main_prompt_tokens": 568, "main_completion_tokens": 1, "judge_total_tokens": 477, "judge_prompt_tokens": 393, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1047.0013618469238, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, effectively indicating lack of knowledge. The generated answer 'I don't know.' is equivalent in meaning and appropriately refrains from fabricating a date. It is correct relative to the gold and fully faithful to the (lack of) information.", "main_total_tokens": 558, "main_prompt_tokens": 557, "main_completion_tokens": 1, "judge_total_tokens": 485, "judge_prompt_tokens": 395, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 497.7998733520508, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer correctly states lack of knowledge; the generated answer also expresses not knowing, which is effectively equivalent. It does not fabricate a year and stays within the information available, so it is both correct and faithful to the gold.", "main_total_tokens": 550, "main_prompt_tokens": 549, "main_completion_tokens": 1, "judge_total_tokens": 472, "judge_prompt_tokens": 395, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "2002", "latency_ms": 842.9856300354004, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer asserts a specific release year (2002) for a movie that the gold answer states is not in the database. This is incorrect because it fabricates information, and unfaithful because it contradicts the gold response instead of aligning with its acknowledged lack of data.", "main_total_tokens": 2637, "main_prompt_tokens": 2634, "main_completion_tokens": 3, "judge_total_tokens": 481, "judge_prompt_tokens": 394, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 384.99927520751953, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database; the generated answer admits not knowing. Both effectively convey lack of information, so the response is correct and faithful.", "main_total_tokens": 521, "main_prompt_tokens": 520, "main_completion_tokens": 1, "judge_total_tokens": 458, "judge_prompt_tokens": 393, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 609.6100807189941, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers express a lack of knowledge about the movie. The generated answer is correct (we have no information) and faithful to the gold intent of not providing fabricated details.", "main_total_tokens": 558, "main_prompt_tokens": 557, "main_completion_tokens": 1, "judge_total_tokens": 465, "judge_prompt_tokens": 396, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was directed by Adrian Sitaru [Doc 46069].", "latency_ms": 2327.2345066070557, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer names Adrian Sitaru as the director, which is factually incorrect; the film was directed by Michael Samuels. It also contradicts the gold response, which states that the director information is not available in the database, so the model is not being faithful to the given context or its limitations.", "main_total_tokens": 2733, "main_prompt_tokens": 2708, "main_completion_tokens": 25, "judge_total_tokens": 510, "judge_prompt_tokens": 421, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 407.0000648498535, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not available in the database and thus cannot be answered. The generated answer says \"I don't know,\" which is equivalent in meaning: it does not fabricate a director and correctly reflects the lack of information. Therefore it is both correct and faithful to the gold response.", "main_total_tokens": 565, "main_prompt_tokens": 564, "main_completion_tokens": 1, "judge_total_tokens": 491, "judge_prompt_tokens": 399, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The movie \"Journey into Gaza\" was directed by an unspecified individual in the provided context.", "latency_ms": 1345.7701206207275, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer correctly states that the director information is not available, so the model should admit it cannot answer. The generated answer instead invents that the movie was directed by an unspecified individual, which is both incorrect (it does not identify a real director) and unfaithful to the context (which explicitly says the information is not provided).", "main_total_tokens": 2970, "main_prompt_tokens": 2951, "main_completion_tokens": 19, "judge_total_tokens": 510, "judge_prompt_tokens": 411, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of \"The Last Night at Tremore Beach\" is not specified in the provided context.", "latency_ms": 1793.7250137329102, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not provided in the database. The generated answer states that the director is not specified in the provided context. Both convey the same limitation of the available information and do not hallucinate a director, so the response is both correct and faithful to the gold.", "main_total_tokens": 2358, "main_prompt_tokens": 2337, "main_completion_tokens": 21, "judge_total_tokens": 507, "judge_prompt_tokens": 417, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 1154.1073322296143, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy' and remains fully faithful to the provided information.", "main_total_tokens": 2500, "main_prompt_tokens": 2494, "main_completion_tokens": 6, "judge_total_tokens": 457, "judge_prompt_tokens": 396, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 1394.0625190734863, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'Stranger in My Bed'. The response is fully correct and faithful to the reference.", "main_total_tokens": 2521, "main_prompt_tokens": 2515, "main_completion_tokens": 6, "judge_total_tokens": 463, "judge_prompt_tokens": 400, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 659.9829196929932, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Game of Silence') and correctly identifies the work in question, so it is both correct and faithful.", "main_total_tokens": 2194, "main_prompt_tokens": 2190, "main_completion_tokens": 4, "judge_total_tokens": 460, "judge_prompt_tokens": 400, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 1458.1284523010254, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'The Craigslist Killer'. It is both correct and fully faithful to the given information.", "main_total_tokens": 2548, "main_prompt_tokens": 2530, "main_completion_tokens": 18, "judge_total_tokens": 457, "judge_prompt_tokens": 395, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 1085.921287536621, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Wood Job!') and correctly identifies the movie, so it is both correct and faithful.", "main_total_tokens": 3401, "main_prompt_tokens": 3397, "main_completion_tokens": 4, "judge_total_tokens": 461, "judge_prompt_tokens": 403, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 1596.2929725646973, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase.'", "main_total_tokens": 2987, "main_prompt_tokens": 2970, "main_completion_tokens": 17, "judge_total_tokens": 472, "judge_prompt_tokens": 417, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond", "latency_ms": 837.4209403991699, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The predicted answer 'Beyond' exactly matches the gold answer 'Beyond'. The evaluation is fully faithful to the provided data and correct with respect to the gold label.", "main_total_tokens": 2424, "main_prompt_tokens": 2422, "main_completion_tokens": 2, "judge_total_tokens": 472, "judge_prompt_tokens": 409, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9", "latency_ms": 1118.4885501861572, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (both are '6ixtynin9'), correctly identifying the movie described. The response is fully correct and faithful to the reference.", "main_total_tokens": 2432, "main_prompt_tokens": 2425, "main_completion_tokens": 7, "judge_total_tokens": 476, "judge_prompt_tokens": 409, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread", "latency_ms": 1414.4432544708252, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the correct movie 'Club Dread'.", "main_total_tokens": 2768, "main_prompt_tokens": 2764, "main_completion_tokens": 4, "judge_total_tokens": 476, "judge_prompt_tokens": 427, "judge_completion_tokens": 49}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet", "latency_ms": 1096.9393253326416, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Toilet') and correctly identifies the movie based on the described plot. It is fully faithful to the given gold reference.", "main_total_tokens": 2958, "main_prompt_tokens": 2955, "main_completion_tokens": 3, "judge_total_tokens": 479, "judge_prompt_tokens": 415, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel", "latency_ms": 782.2349071502686, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Electric Angel'), so it is fully correct. It is also perfectly faithful to the given information, with no additions or deviations.", "main_total_tokens": 2735, "main_prompt_tokens": 2732, "main_completion_tokens": 3, "judge_total_tokens": 483, "judge_prompt_tokens": 418, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "4.0", "latency_ms": 944.401741027832, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the lowest rated movie from 2023, and the gold answer is 'Mega Ape'. The generated answer '4.0' is not a movie title and does not match the gold answer, so it is incorrect. It is also not faithful to the gold reference as it does not provide the required movie name.", "main_total_tokens": 2635, "main_prompt_tokens": 2631, "main_completion_tokens": 4, "judge_total_tokens": 483, "judge_prompt_tokens": 385, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 169.03209686279297, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the highest rated TV show from 2010 is 'Tea House'. The generated answer says 'I don't know,' which does not provide the correct show, so it is factually incorrect relative to the gold. However, the model is honest about its uncertainty and does not invent an answer, so it is fully faithful to its own stated knowledge limits.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 386, "judge_completion_tokens": 106}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 291.776180267334, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answer is incorrect because the highest rated Japanese language movie from 1999 is given as 'The Uncut Story of Mewtwo's Origin,' whereas the model responded with 'I don't know.' However, the response does not invent any incorrect facts; it simply admits lack of knowledge, so it is faithful to the model's stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 496, "judge_prompt_tokens": 395, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The language of the lowest-rated movie from 1980 is en [Doc 744].", "latency_ms": 1441.925048828125, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer specifies the language as 'English', while the generated answer uses the code 'en'. The task expects the explicit language name, so the generated answer is incorrect. Additionally, the mention of a specific document ID without explicit support in the prompt context makes the answer unfaithful to the provided information.", "main_total_tokens": 2899, "main_prompt_tokens": 2880, "main_completion_tokens": 19, "judge_total_tokens": 494, "judge_prompt_tokens": 401, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was \"Popular\" with a popularity score of 4.5113 [Doc 115196].", "latency_ms": 2390.1824951171875, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The generated answer is incorrect: the gold label indicates that 'Law & Order: Special Victims Unit' was the TV show in 1999 with the highest popularity, while the model answered 'Popular'. Therefore correctness is 0. In terms of faithfulness to the cited document, the answer references 'Popular' and includes a specific popularity score and document ID, suggesting it is based on that source, but since we are not given the document contents here, and the statement contradicts the gold answer, we cannot fully trust that the doc actually supports the global claim that it was the highest overall. The answer appears partially grounded in some document but overgeneralizes its scope, so faithfulness is partial.", "main_total_tokens": 2885, "main_prompt_tokens": 2853, "main_completion_tokens": 32, "judge_total_tokens": 595, "judge_prompt_tokens": 421, "judge_completion_tokens": 174}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2", "latency_ms": 570.9984302520752, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2, so it is both correct and faithful.", "main_total_tokens": 2211, "main_prompt_tokens": 2208, "main_completion_tokens": 3, "judge_total_tokens": 439, "judge_prompt_tokens": 386, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "1999", "latency_ms": 1027.8162956237793, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1999) does not match the gold answer (2019), so it is factually incorrect relative to the specified target, yielding a correctness score of 0. However, the model directly answered the question without adding unsupported information or hallucinated details, so it is fully faithful to the user query and its own reasoning, giving a faithfulness score of 1.", "main_total_tokens": 2712, "main_prompt_tokens": 2709, "main_completion_tokens": 3, "judge_total_tokens": 495, "judge_prompt_tokens": 387, "judge_completion_tokens": 108}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "1", "latency_ms": 485.781192779541, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 279 votes, but the generated answer is 1, which is incorrect. The model stayed on topic (it gave a numeric vote count) but the value is wrong, so correctness is 0 while faithfulness to the question type and content is 1.", "main_total_tokens": 2188, "main_prompt_tokens": 2187, "main_completion_tokens": 1, "judge_total_tokens": 466, "judge_prompt_tokens": 383, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 464.0240669250488, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the number of votes and the gold answer is 12. The generated answer claims it cannot find any movies or shows matching the criteria, which is incorrect with respect to the gold answer, so correctness is 0. However, the response itself is internally consistent and not hallucinated given its stated inability to find the item, so faithfulness is 1.", "main_total_tokens": 2237, "main_prompt_tokens": 2236, "main_completion_tokens": 1, "judge_total_tokens": 498, "judge_prompt_tokens": 392, "judge_completion_tokens": 106}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "I don't know.", "latency_ms": 717.7996635437012, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2009, but the model said it did not know, which is incorrect. However, the model did not hallucinate an incorrect year; it simply declined to answer, so it is fully faithful to its own (lack of) knowledge.", "main_total_tokens": 526, "main_prompt_tokens": 525, "main_completion_tokens": 1, "judge_total_tokens": 475, "judge_prompt_tokens": 391, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o-mini", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en", "latency_ms": 1440.375804901123, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer and the generated answer both correctly give the original language of the movie as English (\"en\"). The response is accurate and matches the gold label, so it is both correct and faithful.", "main_total_tokens": 2750, "main_prompt_tokens": 2748, "main_completion_tokens": 2, "judge_total_tokens": 456, "judge_prompt_tokens": 384, "judge_completion_tokens": 72}
