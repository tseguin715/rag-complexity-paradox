{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The movie \"Wall Street\" was released in 1987 [Doc 6993].", "latency_ms": 6507.943391799927, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that \"Wall Street\" was released in 1987, which matches the gold answer exactly and aligns with the cited document. It is both correct and faithful to the provided evidence.", "main_total_tokens": 2916, "main_prompt_tokens": 2898, "main_completion_tokens": 18, "judge_total_tokens": 467, "judge_prompt_tokens": 396, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie where classmates have to play a survival game using their cell phones is \"Brain Fluid Explosion Girl\" [Doc 69929].", "latency_ms": 771.3172435760498, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Brain Fluid Explosion Girl,' which matches the gold answer exactly. It does not introduce conflicting or unsupported information relative to the given QA pair, so it is both correct and faithful.", "main_total_tokens": 2858, "main_prompt_tokens": 2831, "main_completion_tokens": 27, "judge_total_tokens": 490, "judge_prompt_tokens": 415, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "In the TV show \"Dungeons & Dragons\" from 1983, six friends are transported into the Dungeons & Dragons realm and must try to find a way home with the help of their guide 'Dungeon Master' [Doc 112849].", "latency_ms": 1256.615400314331, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.4306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.719741384391281, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Dungeons & Dragons\" and accurately describes the premise that six friends are transported to a fantasy realm and must rely on the Dungeon Master to find a way home. It matches the gold answer and remains faithful to the provided document reference without adding unsupported claims.", "main_total_tokens": 3056, "main_prompt_tokens": 3006, "main_completion_tokens": 50, "judge_total_tokens": 531, "judge_prompt_tokens": 440, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie \"Nacht der Wölfe\" is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones [Doc 2590].", "latency_ms": 1906.2509536743164, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Nacht der Wölfe') and correctly restates the movie description. It is both correct and faithful to the provided information.", "main_total_tokens": 2833, "main_prompt_tokens": 2795, "main_completion_tokens": 38, "judge_total_tokens": 502, "judge_prompt_tokens": 436, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of \"Picnic at Hanging Rock\" take place on Valentine's Day, 1900 [Doc 126191].", "latency_ms": 950.1368999481201, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events take place in 1900, matching the gold answer, and does not introduce any contradictions or unsupported claims beyond specifying Valentine's Day, which is accurate to the source.", "main_total_tokens": 2642, "main_prompt_tokens": 2617, "main_completion_tokens": 25, "judge_total_tokens": 482, "judge_prompt_tokens": 410, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie you are referring to is \"June.\" It is about a young, innocent, and quirky violinist named June, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man destined just for her [Doc 32483].", "latency_ms": 1118.870496749878, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as \"June,\" matching the gold answer and accurately restating the plot details from the prompt, so correctness is 1. However, it adds an inline citation [Doc 32483] that appears fabricated given only the user-provided text and no visible document context, so faithfulness is slightly reduced to 0.9.", "main_total_tokens": 3130, "main_prompt_tokens": 3061, "main_completion_tokens": 69, "judge_total_tokens": 607, "judge_prompt_tokens": 501, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "The highest rated Russian-language movie released in 1980 is \"Moscow Does Not Believe in Tears,\" with a rating of 7.6 [Doc 61].", "latency_ms": 919.2416667938232, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the highest rated Russian-language movie released in 1980 per the gold standard is 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger', not 'Moscow Does Not Believe in Tears'. However, the answer is faithful to the cited document, as it accurately reports the movie and rating given in [Doc 61].", "main_total_tokens": 2872, "main_prompt_tokens": 2838, "main_completion_tokens": 34, "judge_total_tokens": 533, "judge_prompt_tokens": 429, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest-rated Russian-language movie in 1999 is \"The Rifleman of the Voroshilov Regiment\" with a rating of 7.117 [Doc 22123].", "latency_ms": 802.2406101226807, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks specifically for the highest Russian-language movie rating in 1999, and the gold answer is simply the numeric value 7.6. The generated answer instead gives a different numeric rating (7.117) and adds a specific movie title and citation that are not present in the gold answer. Thus, the numeric result is incorrect relative to the gold, and the added movie/title and document reference are hallucinated beyond the provided information, so the answer is not faithful.", "main_total_tokens": 2739, "main_prompt_tokens": 2702, "main_completion_tokens": 37, "judge_total_tokens": 549, "judge_prompt_tokens": 422, "judge_completion_tokens": 127}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "There are three movies released in 1987, have a rating over 7, and are in Chinese:\n\n1. \"A Chinese Ghost Story\" with a rating of 7.3 [Doc 7064].\n2. \"Last Eunuch In China\" with a rating of 7.8 [Doc 7754].\n3. \"Prison on Fire\" with a rating of 7.0 [Doc 7372].", "latency_ms": 1405.4388999938965, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer is 4, but the generated answer states there are three movies, so the core numerical result is incorrect. It partially uses the given documents faithfully for the listed movies and ratings, but it omits at least one qualifying movie, so it is not fully faithful to the underlying data distribution implied by the gold label.", "main_total_tokens": 2900, "main_prompt_tokens": 2811, "main_completion_tokens": 89, "judge_total_tokens": 580, "judge_prompt_tokens": 481, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "There are no TV shows released in 1987 with a rating over 7 that are in Chinese.", "latency_ms": 724.5280742645264, "retrieval_recall": 0.0, "retrieval_mrr": 0.07142857142857142, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer contradicts the gold answer. The gold indicates there are 2 such TV shows, but the generated answer says there are none. The assistant is internally consistent and faithful to its own reasoning, but factually incorrect relative to the gold.", "main_total_tokens": 2708, "main_prompt_tokens": 2687, "main_completion_tokens": 21, "judge_total_tokens": 495, "judge_prompt_tokens": 414, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "The only French language TV show released in 1980 is \"Fantômas\" [Doc 112642].", "latency_ms": 681.0545921325684, "retrieval_recall": 1.0, "retrieval_mrr": 0.2, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.38685280723454163, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.38685280723454163, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies \"Fantômas\" as the only French language TV show released in 1980, exactly matching the gold answer. It does not introduce any contradictions or unsupported claims beyond a benign document reference, so it is both correct and faithful.", "main_total_tokens": 2687, "main_prompt_tokens": 2664, "main_completion_tokens": 23, "judge_total_tokens": 491, "judge_prompt_tokens": 408, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" [Doc 11205].", "latency_ms": 831.636905670166, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' which matches the gold answer. It does not introduce unsupported information and stays faithful to the given content.", "main_total_tokens": 2922, "main_prompt_tokens": 2898, "main_completion_tokens": 24, "judge_total_tokens": 472, "judge_prompt_tokens": 409, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 671.0503101348877, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title \"Won't You Be My Neighbor?\" and stays faithful to the provided information without adding unsupported details.", "main_total_tokens": 3259, "main_prompt_tokens": 3239, "main_completion_tokens": 20, "judge_total_tokens": 481, "judge_prompt_tokens": 421, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The document does not specify which movie you are referring to. Please provide the title of the movie for which you want the release year.", "latency_ms": 1467.017412185669, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates the correct response is the year 2018, implying the context made the referenced item clear. The generated answer instead asks for clarification and does not provide the correct year, so it is factually incorrect relative to the gold. However, it is faithful to the (assumed) lack of explicit movie title in the immediate question, so no hallucinated year is given.", "main_total_tokens": 3080, "main_prompt_tokens": 3053, "main_completion_tokens": 27, "judge_total_tokens": 513, "judge_prompt_tokens": 404, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series \"Astro Boy\" from 1980 is an animation about a young boy robot in 2030 AD. In this series, after being abandoned by his creator, the young boy robot is rescued by an Android Advocate, Dr. Elefun, who adopts him and guides him in using his special powers to aid and protect mankind and robot-kind [Doc 112591].", "latency_ms": 1553.734302520752, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as \"Astro Boy,\" which matches the gold answer. The additional descriptive details about the 1980 series and its plot are plausible and consistent with typical descriptions of Astro Boy, and there is no indication of contradiction or hallucination relative to the given information. Therefore, both correctness and faithfulness are high.", "main_total_tokens": 3280, "main_prompt_tokens": 3202, "main_completion_tokens": 78, "judge_total_tokens": 566, "judge_prompt_tokens": 465, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 648.7767696380615, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer. It is also faithful to the given information and does not introduce unsupported details.", "main_total_tokens": 2486, "main_prompt_tokens": 2456, "main_completion_tokens": 30, "judge_total_tokens": 495, "judge_prompt_tokens": 424, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 1427.649736404419, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before the 26 November 1983 break-in, matching the provided gold value and staying faithful to it.", "main_total_tokens": 2772, "main_prompt_tokens": 2739, "main_completion_tokens": 33, "judge_total_tokens": 502, "judge_prompt_tokens": 428, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "The most popular English-speaking TV show released in 1997 is \"Just Shoot Me!\" with a popularity score of 7.771 [Doc 114728].", "latency_ms": 670.8662509918213, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the popularity score of the most popular English-speaking TV show released in 1997, with a gold answer of 80.5772. The generated answer instead gives a specific show (\"Just Shoot Me!\") with a score of 7.771, which does not match the gold value and is therefore incorrect. It also fabricates a document reference ([Doc 114728]) and specific details not grounded in the provided information, so it is not faithful.", "main_total_tokens": 2577, "main_prompt_tokens": 2544, "main_completion_tokens": 33, "judge_total_tokens": 553, "judge_prompt_tokens": 425, "judge_completion_tokens": 128}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "There are no Russian language TV shows released in 1997 mentioned in the provided documents.", "latency_ms": 834.0673446655273, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 Russian language TV show released in 1997, but the generated answer claims there are none. This contradicts the gold and is therefore incorrect. However, the generated answer is faithful to the provided documents as described, since it explicitly refers to what is mentioned in the documents and does not introduce external information.", "main_total_tokens": 2598, "main_prompt_tokens": 2580, "main_completion_tokens": 18, "judge_total_tokens": 501, "judge_prompt_tokens": 401, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 1320.192575454712, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, identifying the TV show as 'Angel Eyes'. It is correct and consistent with the provided information.", "main_total_tokens": 3139, "main_prompt_tokens": 3104, "main_completion_tokens": 35, "judge_total_tokens": 488, "judge_prompt_tokens": 429, "judge_completion_tokens": 59}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "Please specify the title of the movie you are referring to, so I can provide the language information.", "latency_ms": 1289.0422344207764, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates the movie language is Korean. The generated answer does not provide the language and instead asks for clarification, so it is incorrect. However, it does not introduce any hallucinated facts; it simply requests more information, so it is faithful to the (limited) context given.", "main_total_tokens": 2997, "main_prompt_tokens": 2977, "main_completion_tokens": 20, "judge_total_tokens": 485, "judge_prompt_tokens": 396, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "None of the English-speaking Horror movies released in 2009 have a popularity score greater than 7. The highest popularity score among them is 2.0469 for the movie \"Missing\" [Doc 45413].", "latency_ms": 1103.4400463104248, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there are 2 such movies, but the generated answer claims there are none, so it is factually incorrect relative to the gold. However, the generated answer is internally consistent with the cited document and does not invent unsupported details, so it is faithful to its own cited evidence.", "main_total_tokens": 2719, "main_prompt_tokens": 2674, "main_completion_tokens": 45, "judge_total_tokens": 528, "judge_prompt_tokens": 437, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The documents provided do not contain information about any movies released in 1979.", "latency_ms": 728.1641960144043, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the provided documents lack information about movies released in 1979, which aligns with the gold response that the database has no such movies. It does not introduce unsupported details and remains faithful to the given information.", "main_total_tokens": 2786, "main_prompt_tokens": 2770, "main_completion_tokens": 16, "judge_total_tokens": 489, "judge_prompt_tokens": 411, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "There is no mention of a movie titled \"Back to the Future Part IV\" in the provided documents.", "latency_ms": 771.6958522796631, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there is no information about a movie titled 'Back to the Future Part IV' in either the database or the provided documents. Since such a movie does not exist, the generated answer is factually correct and stays faithful to the given context (the provided documents).", "main_total_tokens": 3327, "main_prompt_tokens": 3306, "main_completion_tokens": 21, "judge_total_tokens": 513, "judge_prompt_tokens": 421, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "There is no movie from 1980 about the nuclear Chernobyl disaster in the provided documents. The Chernobyl disaster occurred in 1986, so any related films would be from that year or later.", "latency_ms": 1737.339735031128, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no movies from 1980 related to the Chernobyl disaster were found. The generated answer correctly notes that there is no such movie in the provided documents and adds the historically accurate reason that the Chernobyl disaster occurred in 1986, so related films would be later. This does not contradict the gold answer or the prompt context. Both correctness (answering that there is no such movie) and faithfulness (staying within the given information and plausible background knowledge) are satisfied.", "main_total_tokens": 3148, "main_prompt_tokens": 3105, "main_completion_tokens": 43, "judge_total_tokens": 583, "judge_prompt_tokens": 449, "judge_completion_tokens": 134}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are four TV shows or movies with 'Astro Boy' in the title:\n\n1. Astro Boy (Movie, 2009) [Doc 44947]\n2. Astro Boy (TV, 2003) [Doc 116265]\n3. Astro Boy (TV, 1980) [Doc 112591]\n4. Astro Boy vs The Junkyard Pirates (Movie, 2010) [Doc 49121]", "latency_ms": 1288.3000373840332, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 1.1510196182278052, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.773714921545009, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that there are four titles with 'Astro Boy' in the title and lists four items, which matches the gold answer of 4. The reasoning is consistent with the question and does not introduce contradictions or unfounded claims beyond the provided information.", "main_total_tokens": 2930, "main_prompt_tokens": 2840, "main_completion_tokens": 90, "judge_total_tokens": 560, "judge_prompt_tokens": 476, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "None of the movies from 2025 in the provided documents allude to the Holocaust. The movies listed for 2025 are \"Enigma\" [Doc 112426], \"Gaza: How to Survive a Warzone\" [Doc 112486], \"Shadow of God\" [Doc 111520], and \"Zelensky\" [Doc 112575], and none of their descriptions mention the Holocaust.", "latency_ms": 1319.8492527008057, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there are 2 movies in 2025 that allude to the Holocaust, while the generated answer asserts that none of the listed 2025 movies allude to the Holocaust. This directly contradicts the gold answer, so the correctness is 0. Additionally, the generated answer references specific documents and their contents, but in this evaluation setting the assistant must treat the gold answer as ground truth; by denying any Holocaust allusions, it is not faithful to that ground truth, so faithfulness is also 0.", "main_total_tokens": 3201, "main_prompt_tokens": 3115, "main_completion_tokens": 86, "judge_total_tokens": 608, "judge_prompt_tokens": 470, "judge_completion_tokens": 138}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "There were three TV shows released in the 1990s about Super Mario Bros:\n\n1. \"The Adventures of Super Mario Bros. 3\" (1990) [Doc 113661].\n2. \"Super Mario World\" (1991) [Doc 113814].\n3. \"Super Mario Bros.\" (1993) [Doc 14139].", "latency_ms": 1220.2873229980469, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there was 1 TV show, while the generated answer claims there were three. It incorrectly counts two additional titles and even includes a 1993 theatrical film as a TV show. This contradicts the gold answer and misidentifies the nature/number of shows, so it is neither correct nor faithful to the reference.", "main_total_tokens": 3105, "main_prompt_tokens": 3031, "main_completion_tokens": 74, "judge_total_tokens": 560, "judge_prompt_tokens": 461, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "There are no TV shows about Super Mario Bros released between 2000 and 2010 mentioned in the provided documents.", "latency_ms": 764.5049095153809, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0, meaning there were no such TV shows in that period. The generated answer states that there are no TV shows about Super Mario Bros released between 2000 and 2010 mentioned in the provided documents, which aligns with the gold. It is both correct and faithful to the given information.", "main_total_tokens": 2608, "main_prompt_tokens": 2584, "main_completion_tokens": 24, "judge_total_tokens": 508, "judge_prompt_tokens": 413, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular TV show or movie about ninjas, based on the provided data, is \"Ninja Assassin\" with a popularity score of 5.8597 [Doc 44828].", "latency_ms": 1520.5738544464111, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates that the correct response is 'Naruto', but the generated answer claims 'Ninja Assassin' is the most popular, which is incorrect relative to the gold. However, the generated answer is faithful to its cited document, as it directly uses the popularity score from [Doc 44828].", "main_total_tokens": 3052, "main_prompt_tokens": 3013, "main_completion_tokens": 39, "judge_total_tokens": 514, "judge_prompt_tokens": 421, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "There is no information available about a Jurassic Park/Power Ranger crossover movie in the provided documents.", "latency_ms": 702.1629810333252, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states explicitly that the movie was not released. The generated answer only says there is no information in the provided documents, which does not correctly address the factual status of the movie. It neither gives the correct answer nor faithfully matches the gold explanation.", "main_total_tokens": 2858, "main_prompt_tokens": 2839, "main_completion_tokens": 19, "judge_total_tokens": 486, "judge_prompt_tokens": 404, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "The context provided does not mention any movie where Barbie fights the Predator.", "latency_ms": 729.2294502258301, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The gold answer clarifies that such a movie does not exist or was not released. The generated answer instead says the context does not mention any movie where Barbie fights the Predator, which does not explicitly answer the question and is therefore incorrect. However, it does not invent any false facts beyond the given context, so it is largely faithful to the provided information.", "main_total_tokens": 3230, "main_prompt_tokens": 3216, "main_completion_tokens": 14, "judge_total_tokens": 500, "judge_prompt_tokens": 396, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The context does not provide information about Star Wars movies. Therefore, the 4th oldest Star Wars movie cannot be determined from the given documents.", "latency_ms": 833.1592082977295, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the 4th oldest Star Wars movie, and the gold answer specifies a particular title. The generated answer instead claims the information is not in the context and refuses to answer. This is incorrect with respect to the task (it should name the movie) but it is faithful to the absence of information in the provided context, so correctness is 0 and faithfulness is 1.", "main_total_tokens": 3228, "main_prompt_tokens": 3199, "main_completion_tokens": 29, "judge_total_tokens": 529, "judge_prompt_tokens": 418, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The document does not specify which movie you are referring to. Please provide the title of the movie for which you want the release year.", "latency_ms": 1474.1497039794922, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly expects the year 1986, indicating that in the original context the referenced item was known and its release year is 1986. The generated answer instead claims lack of context and asks for the movie title, failing to provide the correct year; thus it is incorrect. However, it is faithful to the ambiguous user question considered in isolation, since the question alone does not specify a movie. It does not hallucinate a year or movie; it simply requests clarification, so faithfulness is high.", "main_total_tokens": 3080, "main_prompt_tokens": 3053, "main_completion_tokens": 27, "judge_total_tokens": 538, "judge_prompt_tokens": 404, "judge_completion_tokens": 134}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The rating of the movie \"Forrest Gump\" is 8.466 [Doc 15236].", "latency_ms": 802.8700351715088, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating value 8.466 and correctly refers to the movie \"Forrest Gump\" (not the documentary), with an appropriate citation placeholder. It is both correct and faithful to the provided gold answer.", "main_total_tokens": 2956, "main_prompt_tokens": 2934, "main_completion_tokens": 22, "judge_total_tokens": 495, "judge_prompt_tokens": 416, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show is \"Mighty Med\" [Doc 121125].", "latency_ms": 687.4544620513916, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med' and matches the gold answer exactly. It does not introduce any unsupported information and remains faithful to the provided content.", "main_total_tokens": 2591, "main_prompt_tokens": 2573, "main_completion_tokens": 18, "judge_total_tokens": 477, "judge_prompt_tokens": 410, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In \"Túnel de los huesos\" (2011), seven prisoners escaped from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 834.8963260650635, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies that the prison is in Buenos Aires, Argentina, matching the gold answer of Argentina. The additional detail about Villa Devoto prison is accurate and does not contradict the source, so the response is both correct and faithful.", "main_total_tokens": 3336, "main_prompt_tokens": 3302, "main_completion_tokens": 34, "judge_total_tokens": 504, "judge_prompt_tokens": 425, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice Li Nai falls into a trap designed by Jiang Jie, the \"Chef Assassin\" [Doc 135231].", "latency_ms": 1103.193759918213, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The gold answer is simply 'Jiang Jie', and the generated answer correctly identifies that the apprentice falls into Jiang Jie's trap, so it is correct. The added detail that the apprentice is Li Nai and that Jiang Jie is the 'Chef Assassin' is extra content not present in the gold; while it might be true, we cannot verify all of it from the gold alone, so faithfulness is slightly reduced.", "main_total_tokens": 2916, "main_prompt_tokens": 2883, "main_completion_tokens": 33, "judge_total_tokens": 533, "judge_prompt_tokens": 417, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "There are no Korean-language movies mentioning assassins after 2023 in the provided documents.", "latency_ms": 949.9249458312988, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there are 4 such movies, but the generated answer states there are none, which is factually incorrect. However, the generated answer is faithful to the (implicit) information in the provided documents as described in the prompt, since it only makes a claim based on what is said to be in the documents and does not add hallucinated specific movies or counts beyond that. It incorrectly infers zero rather than admitting uncertainty, so correctness is 0, but it does not contradict the described context content itself, so faithfulness is relatively high.", "main_total_tokens": 2816, "main_prompt_tokens": 2798, "main_completion_tokens": 18, "judge_total_tokens": 544, "judge_prompt_tokens": 401, "judge_completion_tokens": 143}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "The provided documents do not contain information about movies featuring Steven Seagal and Bruce Lee together.", "latency_ms": 734.2054843902588, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not contain information about movies featuring Steven Seagal and Bruce Lee together, which aligns with the gold answer that no such movies can be found. It does not add unsupported claims and stays faithful to the given information.", "main_total_tokens": 3025, "main_prompt_tokens": 3007, "main_completion_tokens": 18, "judge_total_tokens": 493, "judge_prompt_tokens": 411, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of \"Tokyo Mater\" (2008) is 6.3 [Doc 42750].", "latency_ms": 717.5905704498291, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (6.3) and stays aligned with the provided information, citing the same value without adding unsupported details.", "main_total_tokens": 2841, "main_prompt_tokens": 2819, "main_completion_tokens": 22, "judge_total_tokens": 466, "judge_prompt_tokens": 405, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The movie \"The Last Race\" is centered around stock car racing at a small town track [Doc 86410].", "latency_ms": 1072.4773406982422, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The question asks specifically for the style of racing in the movie, and the gold answer is 'Drift.' The generated answer instead describes stock car racing at a small town track, which is incorrect for this question, so correctness is 0. The answer does appear consistent with the cited document snippet and does not invent unsupported details beyond that, so faithfulness is relatively high.", "main_total_tokens": 3261, "main_prompt_tokens": 3237, "main_completion_tokens": 24, "judge_total_tokens": 510, "judge_prompt_tokens": 402, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the TV show about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 1088.6309146881104, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The question asks for the title of the movie; the gold answer is \"Confronting a Serial Killer.\" The generated answer correctly provides that exact title, so it is factually correct. However, it describes it as a TV show rather than a movie, introducing a slight discrepancy from the prompt, so it is very slightly unfaithful to the question's framing.", "main_total_tokens": 2891, "main_prompt_tokens": 2844, "main_completion_tokens": 47, "judge_total_tokens": 560, "judge_prompt_tokens": 453, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 1320.3015327453613, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and correctly attributes it, so it is both correct and faithful to the provided information.", "main_total_tokens": 3088, "main_prompt_tokens": 3066, "main_completion_tokens": 22, "judge_total_tokens": 469, "judge_prompt_tokens": 407, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 1050.4496097564697, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Sixth Sense' and stays fully consistent with the provided information.", "main_total_tokens": 2868, "main_prompt_tokens": 2839, "main_completion_tokens": 29, "judge_total_tokens": 473, "judge_prompt_tokens": 420, "judge_completion_tokens": 53}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 972.3963737487793, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer, and it does not introduce any unsupported information beyond a document reference.", "main_total_tokens": 3036, "main_prompt_tokens": 3019, "main_completion_tokens": 17, "judge_total_tokens": 462, "judge_prompt_tokens": 397, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 717.8208827972412, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2015 reboot as 'The Transporter Refueled' and does not introduce any unsupported information.", "main_total_tokens": 3183, "main_prompt_tokens": 3154, "main_completion_tokens": 29, "judge_total_tokens": 484, "judge_prompt_tokens": 420, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The movie 'The Transporter Refueled' was released in the year 2015 [Doc 67500].", "latency_ms": 849.3611812591553, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It does not introduce any contradictions or unsupported claims, and the citation style does not affect faithfulness here.", "main_total_tokens": 3074, "main_prompt_tokens": 3050, "main_completion_tokens": 24, "judge_total_tokens": 486, "judge_prompt_tokens": 408, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "There is no romantic comedy listed in the provided documents where Abraham Lincoln comes back from the dead. However, there is a horror-comedy movie titled \"Re-Elected\" where former U.S. presidents, including Abraham Lincoln, come back from the dead as zombies [Doc 94414].", "latency_ms": 1057.5087070465088, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a romantic comedy where Abraham Lincoln comes back from the dead, and the gold answer says no such movie description can be found. The generated answer correctly states that no romantic comedy in the provided documents features this premise and then mentions a different genre film, 'Re-Elected,' as a horror-comedy with resurrected presidents. This does not contradict the gold answer and is consistent with the constraint of using the provided documents. Both correctness and faithfulness are satisfied.", "main_total_tokens": 2769, "main_prompt_tokens": 2711, "main_completion_tokens": 58, "judge_total_tokens": 574, "judge_prompt_tokens": 448, "judge_completion_tokens": 126}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of the movie that alludes to a manuscript written by Ernest Hemingway is \"The Old Man and the Sea\" [Doc 11238].", "latency_ms": 819.089412689209, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Havana Darkness', but the generated answer gives 'The Old Man and the Sea', which is incorrect. However, the answer is internally consistent with its own (though incorrect) premise, so it is faithful to the provided generation but not to the gold answer.", "main_total_tokens": 2785, "main_prompt_tokens": 2755, "main_completion_tokens": 30, "judge_total_tokens": 506, "judge_prompt_tokens": 418, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "In the TV show \"Doll & Em,\" Doll heads to Hollywood to be with her childhood best friend Em, who's now a successful actress [Doc 122259].", "latency_ms": 1073.6124515533447, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the show as \"Doll & Em\" and matches the gold answer. It is also faithful to the provided document reference, adding only that Em is a successful actress, which is consistent with the source.", "main_total_tokens": 2688, "main_prompt_tokens": 2654, "main_completion_tokens": 34, "judge_total_tokens": 500, "judge_prompt_tokens": 423, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in the 2007 movie \"Hallam Foe\" [Doc 39389].", "latency_ms": 872.2274303436279, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2007 movie as \"Hallam Foe\" and matches the gold answer exactly. It does not introduce any incorrect or unsupported information and remains faithful to the given content.", "main_total_tokens": 2942, "main_prompt_tokens": 2904, "main_completion_tokens": 38, "judge_total_tokens": 509, "judge_prompt_tokens": 437, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The context does not provide information about a character's last name that sounds like \"friend\" or \"foe.\" Therefore, the answer is not found.", "latency_ms": 1182.3606491088867, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is \"A foe\". The generated answer says the information is not found, which is incorrect relative to the gold, so correctness is 0. However, given only the user question and no supporting context, the model’s statement that the context does not provide the information is internally consistent and not hallucinated about the missing context, so faithfulness is 1.", "main_total_tokens": 2994, "main_prompt_tokens": 2963, "main_completion_tokens": 31, "judge_total_tokens": 524, "judge_prompt_tokens": 417, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "In the 2017 TV show \"Atypical,\" Sam is 18 years old [Doc 124583].", "latency_ms": 718.3742523193359, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Sam’s age as 18, matching the gold answer exactly. It does not introduce any contradictions or extraneous fabricated details, so it is both correct and faithful.", "main_total_tokens": 2724, "main_prompt_tokens": 2700, "main_completion_tokens": 24, "judge_total_tokens": 480, "judge_prompt_tokens": 411, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show \"Atypical,\" Sam is on the autism spectrum [Doc 124583].", "latency_ms": 673.2730865478516, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the 2017 TV show 'Atypical,' Sam is on the autism spectrum, which matches the gold answer exactly. It is fully faithful to the given gold reference without adding contradictory or extraneous information.", "main_total_tokens": 2462, "main_prompt_tokens": 2438, "main_completion_tokens": 24, "judge_total_tokens": 491, "judge_prompt_tokens": 411, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Sandra was born in Chamonix [Doc 59659].", "latency_ms": 849.700927734375, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, which matches the gold answer exactly and is fully supported by the provided information.", "main_total_tokens": 3348, "main_prompt_tokens": 3335, "main_completion_tokens": 13, "judge_total_tokens": 470, "judge_prompt_tokens": 410, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In this Japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 802.1495342254639, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly fills in the blank with 'rains', matching the gold answer exactly, and it restates the sentence faithfully without adding incorrect information. The citation-style tag does not affect the factual content.", "main_total_tokens": 3073, "main_prompt_tokens": 3052, "main_completion_tokens": 21, "judge_total_tokens": 485, "judge_prompt_tokens": 412, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the German language animation \"Tabaluga,\" Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 988.5389804840088, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies that in the German animation \"Tabaluga,\" he is the crown-prince of Greenland, which matches the gold answer exactly. It is fully supported by the provided information and contains no extraneous or hallucinated content beyond restating the source, so both correctness and faithfulness are maximal.", "main_total_tokens": 3208, "main_prompt_tokens": 3176, "main_completion_tokens": 32, "judge_total_tokens": 520, "judge_prompt_tokens": 426, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is \"One Afternoon\" [Doc 96838].", "latency_ms": 1019.2093849182129, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'One Afternoon'. It does not introduce contradictions or unsupported claims; the bracketed document reference does not affect the factual alignment with the provided gold answer.", "main_total_tokens": 2594, "main_prompt_tokens": 2550, "main_completion_tokens": 44, "judge_total_tokens": 523, "judge_prompt_tokens": 448, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" from 2020, with a rating of 6.746 [Doc 93260].", "latency_ms": 6377.395868301392, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The answer correctly identifies the movie as \"The Life Ahead,\" matching the gold answer, so correctness is full. However, it introduces an unsupported and likely incorrect detail from the cited document: the specific rating value of 6.746, and asserts it as part of the answer. Since that rating detail is not present in the user-provided gold and is treated as documentary evidence, the response is not fully faithful to the given context, reducing faithfulness.", "main_total_tokens": 2901, "main_prompt_tokens": 2851, "main_completion_tokens": 50, "judge_total_tokens": 570, "judge_prompt_tokens": 448, "judge_completion_tokens": 122}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 1034.9068641662598, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the 2016 documentary 'McLaren' is about Bruce McLaren, matching the gold answer. It also accurately describes him as a New Zealander who founded the McLaren motor racing team. There is no contradiction or hallucination relative to the provided information, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3124, "main_prompt_tokens": 3091, "main_completion_tokens": 33, "judge_total_tokens": 517, "judge_prompt_tokens": 417, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The name of Sonny Hayes' racing team is not mentioned in the provided documents. [Doc 111503]", "latency_ms": 897.1109390258789, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the team is the McLaren Motor Racing team, so the correct answer is known from the provided materials. The generated answer incorrectly claims the team name is not mentioned, making it factually wrong. However, it remains faithful to its own cited document context (Doc 111503) by saying the name is not mentioned there, so it is faithful but incorrect overall with respect to the gold.", "main_total_tokens": 3297, "main_prompt_tokens": 3275, "main_completion_tokens": 22, "judge_total_tokens": 519, "judge_prompt_tokens": 405, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context does not provide information about the country of origin for any specific individual.", "latency_ms": 1182.682752609253, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'New Zealand'; the generated answer incorrectly claims the context does not provide this information, so it is factually wrong relative to the gold. However, it is faithful to the (implicit) claim about the context, not hallucinating any country, so faithfulness is high.", "main_total_tokens": 3058, "main_prompt_tokens": 3042, "main_completion_tokens": 16, "judge_total_tokens": 482, "judge_prompt_tokens": 392, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "The context provided does not mention any 2013 movie in which John McClane blows up the Great Wall of China.", "latency_ms": 834.6257209777832, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is that there is no such movie, matching the gold reference. The generated answer incorrectly claims limitation to the provided context rather than clearly stating that no such movie exists, so it is not factually answering the question and is not faithful to the intended answer.", "main_total_tokens": 3270, "main_prompt_tokens": 3245, "main_completion_tokens": 25, "judge_total_tokens": 508, "judge_prompt_tokens": 423, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with that description is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 884.1047286987305, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Tomie: Forbidden Fruit,' which matches the gold answer exactly. It does not introduce any contradictions or unsupported claims and remains faithful to the information implied in the question and gold reference.", "main_total_tokens": 3161, "main_prompt_tokens": 3136, "main_completion_tokens": 25, "judge_total_tokens": 518, "judge_prompt_tokens": 442, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie \"Lei mi parla ancora,\" Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 919.1043376922607, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years). It directly uses the information without contradiction or unsupported claims, so it is both correct and faithful.", "main_total_tokens": 3487, "main_prompt_tokens": 3457, "main_completion_tokens": 30, "judge_total_tokens": 486, "judge_prompt_tokens": 424, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In \"Scorpion King: Book of Souls\" (2018), The Scorpion King teams up with Tala to search for a legendary relic known as The Book of Souls, which will allow them to put an end to an evil warlord [Doc 82582].", "latency_ms": 1187.1588230133057, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies that in 'Scorpion King: Book of Souls' (2018), the Scorpion King and Tala are searching for the Book of Souls, matching the gold answer. However, it adds extra narrative detail (about stopping an evil warlord and citing a document reference) that is not present in the provided gold answer, so it is not fully faithful to the given reference.", "main_total_tokens": 3275, "main_prompt_tokens": 3221, "main_completion_tokens": 54, "judge_total_tokens": 559, "judge_prompt_tokens": 448, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Zombies on the Moon\" is not found in the provided documents.", "latency_ms": 999.2668628692627, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database; the generated answer says the release year is not found in the provided documents. Both convey that the information is unavailable, without fabricating a date. This is correct and faithful to the gold.", "main_total_tokens": 2720, "main_prompt_tokens": 2701, "main_completion_tokens": 19, "judge_total_tokens": 491, "judge_prompt_tokens": 410, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Galactic baker\" is not found in the provided documents.", "latency_ms": 987.9369735717773, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available; the generated answer is correct and faithful to the given information.", "main_total_tokens": 3008, "main_prompt_tokens": 2991, "main_completion_tokens": 17, "judge_total_tokens": 464, "judge_prompt_tokens": 406, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Return of the Null Pointer\" is not found in the provided documents.", "latency_ms": 988.5380268096924, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available/known from the given information. The generated answer correctly notes that the release year is not found in the provided documents, which is faithful to the premise that the movie is not in the database. There is no fabricated information, and it matches the gold answer in substance.", "main_total_tokens": 2995, "main_prompt_tokens": 2976, "main_completion_tokens": 19, "judge_total_tokens": 509, "judge_prompt_tokens": 410, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Data Science: The Movie\" is not found in the provided documents.", "latency_ms": 1132.173776626587, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about the release year of a movie. The gold answer says the movie is not in the database, effectively not providing a year. The generated answer similarly states that the release year is not found in the provided documents, which is consistent with having no information. It does not fabricate a date and remains faithful to the implied context (limited to provided documents). Therefore it is both correct and faithful.", "main_total_tokens": 2967, "main_prompt_tokens": 2948, "main_completion_tokens": 19, "judge_total_tokens": 522, "judge_prompt_tokens": 410, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document does not provide information about a movie titled \"Python's Revenge 2.\"", "latency_ms": 910.226583480835, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database; the generated answer correctly says the document does not provide information about that movie. Both avoid fabricating a release year and are aligned in content and intent.", "main_total_tokens": 2935, "main_prompt_tokens": 2918, "main_completion_tokens": 17, "judge_total_tokens": 481, "judge_prompt_tokens": 408, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Last Pixel\" is not found in the provided documents.", "latency_ms": 1220.743179321289, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold answer says the movie is not in the database; generated answer says the release year is not found in the provided documents. Both convey that the information is unavailable, without hallucinating a year. This is correct and faithful to the gold.", "main_total_tokens": 2665, "main_prompt_tokens": 2648, "main_completion_tokens": 17, "judge_total_tokens": 485, "judge_prompt_tokens": 406, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The man who ate the sun\" is not found in the provided documents.", "latency_ms": 971.6396331787109, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer admits the movie is not in the database; the generated answer correctly states that the release year is not found in the provided documents. Both effectively convey the same limitation without adding unsupported details, so the response is both correct and faithful to the given context.", "main_total_tokens": 3331, "main_prompt_tokens": 3311, "main_completion_tokens": 20, "judge_total_tokens": 496, "judge_prompt_tokens": 412, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Days of Lehman Brothers\" is not mentioned in the provided documents.", "latency_ms": 903.5642147064209, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director is not provided/mentioned in the available information. This matches the database limitation and does not introduce unsupported details, so the response is both correct and faithful to the source.", "main_total_tokens": 3408, "main_prompt_tokens": 3386, "main_completion_tokens": 22, "judge_total_tokens": 494, "judge_prompt_tokens": 419, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The document does not provide information about a movie titled \"A Love to Die For.\" Therefore, the director of such a movie is not mentioned.", "latency_ms": 902.7194976806641, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer is effectively correct given the database context: it states that the director is not mentioned, which aligns with the gold answer that the director information is not provided. However, it slightly diverges in wording by claiming the document does not provide information about the movie at all, whereas the gold only states that the director information is not provided. This is a minor faithfulness issue about what is or is not in the database, though it does not affect the ultimate conclusion about the director.", "main_total_tokens": 2874, "main_prompt_tokens": 2845, "main_completion_tokens": 29, "judge_total_tokens": 555, "judge_prompt_tokens": 424, "judge_completion_tokens": 131}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not mentioned in the provided documents.", "latency_ms": 821.0675716400146, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned, aligning with the gold answer that the director information is not provided in the database. It does not introduce any unsupported facts and remains faithful to the given information.", "main_total_tokens": 3111, "main_prompt_tokens": 3093, "main_completion_tokens": 18, "judge_total_tokens": 485, "judge_prompt_tokens": 411, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Night at Tremore Beach\" is not mentioned in the provided documents. [Doc 135098]", "latency_ms": 966.1638736724854, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director is not specified in the available information. The generated answer stays within the provided documents and does not hallucinate a director name, so it is both correct and faithful.", "main_total_tokens": 2708, "main_prompt_tokens": 2680, "main_completion_tokens": 28, "judge_total_tokens": 499, "judge_prompt_tokens": 425, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 1276.7915725708008, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy'. It does not introduce contradictions or unsupported claims beyond a harmless document reference, so it is both correct and faithful.", "main_total_tokens": 2744, "main_prompt_tokens": 2717, "main_completion_tokens": 27, "judge_total_tokens": 492, "judge_prompt_tokens": 418, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse is \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 857.5897216796875, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Stranger in My Bed,\" which matches the gold answer exactly. It stays fully faithful to the provided information without adding contradictory or extraneous claims.", "main_total_tokens": 2720, "main_prompt_tokens": 2691, "main_completion_tokens": 29, "judge_total_tokens": 494, "judge_prompt_tokens": 425, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie you are referring to is not a movie, but a TV show titled \"Game of Silence\" from the year 2016 [Doc 123938].", "latency_ms": 1030.9712886810303, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question explicitly asks for a movie, and the gold label is 'Game of Silence.' The generated answer incorrectly asserts that it is not a movie but a TV show, directly contradicting the gold answer and the task, so it is both incorrect and unfaithful to the provided gold.", "main_total_tokens": 2412, "main_prompt_tokens": 2379, "main_completion_tokens": 33, "judge_total_tokens": 519, "judge_prompt_tokens": 430, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie in which a true story about a killer from the site Craigslist is depicted is \"The Craigslist Killer\" [Doc 53039].", "latency_ms": 1077.3255825042725, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer,' which exactly matches the gold answer. It remains faithful to the given information without adding any conflicting or hallucinated details, aside from a harmless document citation marker.", "main_total_tokens": 2819, "main_prompt_tokens": 2791, "main_completion_tokens": 28, "judge_total_tokens": 495, "judge_prompt_tokens": 420, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which Yuki Hirano sets his eyes on the role of a forestry trainee after failing his university entrance exam is \"Wood Job!\" [Doc 66094].", "latency_ms": 720.5924987792969, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Wood Job!' and matches the key details from the question without introducing contradictions or extraneous fabricated information. It is fully consistent with the provided gold answer.", "main_total_tokens": 3274, "main_prompt_tokens": 3239, "main_completion_tokens": 35, "judge_total_tokens": 505, "judge_prompt_tokens": 435, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure is \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 972.7330207824707, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Sesame Street: The Magical Wand Chase' and correctly restates the scenario. It does not introduce contradictions or extraneous claims, so it is both correct and faithful.", "main_total_tokens": 3247, "main_prompt_tokens": 3198, "main_completion_tokens": 49, "judge_total_tokens": 533, "judge_prompt_tokens": 459, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a rock fall traps a boy's parents inside a burning car, and the 7-year-old boy must find the hero within him to save their lives, is \"Beyond\" [Doc 74597].", "latency_ms": 1274.4419574737549, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Beyond,' which matches the gold answer. It does not introduce any contradictions or hallucinations beyond a reference tag, so it is both correct and faithful.", "main_total_tokens": 2819, "main_prompt_tokens": 2774, "main_completion_tokens": 45, "judge_total_tokens": 523, "judge_prompt_tokens": 453, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman, fired from a financial corporation during the Asia crisis, returns home with no money is \"6ixtynin9\" [Doc 22478].", "latency_ms": 1582.8633308410645, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', which matches the gold answer. It stays faithful to the question and does not introduce unsupported details beyond restating the correct title.", "main_total_tokens": 2951, "main_prompt_tokens": 2914, "main_completion_tokens": 37, "judge_total_tokens": 511, "judge_prompt_tokens": 439, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie in which a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort is \"Club Dread\" [Doc 32410].", "latency_ms": 817.8956508636475, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer exactly. It does not introduce contradictions or extraneous, unsupported claims. Therefore, it is both correct and faithful to the given information.", "main_total_tokens": 2895, "main_prompt_tokens": 2863, "main_completion_tokens": 32, "judge_total_tokens": 532, "judge_prompt_tokens": 456, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is \"Toilet\" [Doc 48873].", "latency_ms": 1109.576940536499, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" which matches the gold label. It also stays fully grounded in the provided information without adding unsupported details, so both correctness and faithfulness are maximal.", "main_total_tokens": 3581, "main_prompt_tokens": 3530, "main_completion_tokens": 51, "judge_total_tokens": 537, "judge_prompt_tokens": 465, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century is \"Electric Angel\" [Doc 1024].", "latency_ms": 1010.0865364074707, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Electric Angel' and does not introduce any contradictory information. It is both correct and faithful to the provided gold response.", "main_total_tokens": 3158, "main_prompt_tokens": 3113, "main_completion_tokens": 45, "judge_total_tokens": 530, "judge_prompt_tokens": 461, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is \"Doomsday Stories,\" with a rating of 2.167 [Doc 108366].", "latency_ms": 1506.6356658935547, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The gold answer says the lowest rated 2023 movie is 'Mega Ape', while the generated answer claims it is 'Doomsday Stories' with a specific rating. These are contradictory, so the generated answer is incorrect relative to the gold. However, the generated answer is internally consistent and clearly grounded in a cited document [Doc 108366], so it remains largely faithful to its source, even though that conflicts with the gold label.", "main_total_tokens": 2775, "main_prompt_tokens": 2745, "main_completion_tokens": 30, "judge_total_tokens": 528, "judge_prompt_tokens": 411, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "The highest rated TV show from 2010 is \"Downton Abbey\" with a rating of 8.1 [Doc 119146].", "latency_ms": 688.1444454193115, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the highest rated TV show from 2010, and the gold answer is 'Tea House'. The generated answer instead states that the show is 'Downton Abbey' with a specific rating, which directly contradicts the gold answer, so it is factually incorrect relative to the supervision. Additionally, the generated answer fabricates a document reference and rating ([Doc 119146], 8.1) with no provided source context, so it is not faithful to any given evidence.", "main_total_tokens": 3309, "main_prompt_tokens": 3280, "main_completion_tokens": 29, "judge_total_tokens": 537, "judge_prompt_tokens": 411, "judge_completion_tokens": 126}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest rated Japanese language movie from 1999 is \"The Geisha House\" with a rating of 7.4 [Doc 23076].", "latency_ms": 655.2815437316895, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer does not match the gold answer; the correct movie is 'The Uncut Story of Mewtwo's Origin', not 'The Geisha House'. There is no evidence in the prompt that supports the claim about 'The Geisha House', so the response is both factually incorrect with respect to the task and unfaithful to the provided gold reference.", "main_total_tokens": 2791, "main_prompt_tokens": 2760, "main_completion_tokens": 31, "judge_total_tokens": 527, "judge_prompt_tokens": 422, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is \"Cataclysm,\" with a rating of 3.889. The language of this movie is English [Doc 744].", "latency_ms": 734.0133190155029, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The question asks only for the language, and the generated answer correctly identifies it as English, matching the gold. However, the answer adds specific details about the movie title, year, rating, and a document reference that are not present in the provided gold answer, so it is not fully faithful to the gold.", "main_total_tokens": 2845, "main_prompt_tokens": 2809, "main_completion_tokens": 36, "judge_total_tokens": 512, "judge_prompt_tokens": 419, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was \"The Sopranos,\" with a popularity score of 59.4284 [Doc 115146].", "latency_ms": 1335.4661464691162, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates that the correct TV show is 'Law & Order: Special Victims Unit,' but the generated answer states 'The Sopranos' with a specific popularity score. This is factually incorrect relative to the gold. The reference to a particular document and score is not supported by the given gold answer, so the response is also unfaithful to the provided information.", "main_total_tokens": 3136, "main_prompt_tokens": 3102, "main_completion_tokens": 34, "judge_total_tokens": 532, "judge_prompt_tokens": 424, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 718.6670303344727, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and correctly attributes the value to the movie 'Bluefin'. It is both correct and faithful to the provided gold data.", "main_total_tokens": 2998, "main_prompt_tokens": 2980, "main_completion_tokens": 18, "judge_total_tokens": 468, "judge_prompt_tokens": 400, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in three different years: 1999, 2019, and 2025 [Doc 22481] [Doc 88811] [Doc 112077].", "latency_ms": 1490.344762802124, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer specifies a single correct year, 2019. The generated answer lists three different years (1999, 2019, and 2025) as release years, which is incorrect for the question asked. It is therefore not correct. It also introduces unsupported years (1999 and 2025) without grounding in the gold, so it is not faithful to the reference answer.", "main_total_tokens": 2924, "main_prompt_tokens": 2879, "main_completion_tokens": 45, "judge_total_tokens": 540, "judge_prompt_tokens": 429, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The number of votes \"My Dog Skip\" received is not provided in the document. [Doc 24114]", "latency_ms": 950.9479999542236, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that \"My Dog Skip\" received 279 votes, but the generated answer claims the number of votes is not provided. This directly contradicts the gold answer, so the response is factually incorrect. However, the model is internally consistent with the cited document and does not hallucinate a number; it faithfully reports that the document lacks this information, so it is faithful to the provided context.", "main_total_tokens": 3055, "main_prompt_tokens": 3032, "main_completion_tokens": 23, "judge_total_tokens": 512, "judge_prompt_tokens": 404, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The documents provided do not contain information about the number of votes received by \"Oh My God!\" in any of its versions.", "latency_ms": 887.0303630828857, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that \"Oh My God!\" received 12 votes, while the generated answer claims that the documents do not contain information about the number of votes. This contradicts the gold, so the answer is factually incorrect relative to the labeled truth (correctness 0). However, the generated response is internally consistent with its stated premise that the documents lack that information and does not hallucinate a specific number; it acknowledges absence of data instead of fabricating, so it is faithful to its own stated evidence context (faithfulness 1).", "main_total_tokens": 2785, "main_prompt_tokens": 2760, "main_completion_tokens": 25, "judge_total_tokens": 548, "judge_prompt_tokens": 406, "judge_completion_tokens": 142}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in the year 2009 [Doc 46069].", "latency_ms": 695.1448917388916, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'The Last Days of Lehman Brothers' was released in 2009, which matches the gold answer. It is also faithful to the provided information and does not introduce unsupported claims.", "main_total_tokens": 2873, "main_prompt_tokens": 2848, "main_completion_tokens": 25, "judge_total_tokens": 488, "judge_prompt_tokens": 412, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 1166.5995121002197, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the original language of 'The King’s Speech' is English, which matches the gold answer 'en' and does not add any contradictory information. Reference to a document ID does not change faithfulness to the provided gold fact.", "main_total_tokens": 3309, "main_prompt_tokens": 3290, "main_completion_tokens": 19, "judge_total_tokens": 487, "judge_prompt_tokens": 402, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The movie \"Wall Street\" was released in the year 1987 [Doc 6993].", "latency_ms": 1305.5810928344727, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer. It does not introduce any unsupported information and attributes the fact to a document, so it is both correct and faithful.", "main_total_tokens": 2222, "main_prompt_tokens": 2202, "main_completion_tokens": 20, "judge_total_tokens": 472, "judge_prompt_tokens": 398, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie in which classmates have to play a survival game using their cell phones is \"Brain Fluid Explosion Girl\" [Doc 69929].", "latency_ms": 666.6915416717529, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Brain Fluid Explosion Girl,' matching the gold answer exactly. It does not introduce any extraneous or contradictory information and directly answers the question, so it is both correct and faithful to the provided content.", "main_total_tokens": 2162, "main_prompt_tokens": 2134, "main_completion_tokens": 28, "judge_total_tokens": 495, "judge_prompt_tokens": 416, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show in which six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" from 1983 [Doc 112849].", "latency_ms": 1188.227891921997, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.4306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.719741384391281, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the show as 'Dungeons & Dragons', matching the gold answer. It does not introduce incorrect information; mentioning the year 1983 and document reference does not contradict the question and stays faithful to the implied context.", "main_total_tokens": 2228, "main_prompt_tokens": 2194, "main_completion_tokens": 34, "judge_total_tokens": 503, "judge_prompt_tokens": 424, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is \"Nacht der Wölfe\" [Doc 2590].", "latency_ms": 1082.3884010314941, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe', which matches the gold answer exactly. It is fully faithful to the information provided, even citing the document reference.", "main_total_tokens": 2114, "main_prompt_tokens": 2075, "main_completion_tokens": 39, "judge_total_tokens": 506, "judge_prompt_tokens": 437, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of \"Picnic at Hanging Rock\" take place in the year 1900 [Doc 126191].", "latency_ms": 872.3533153533936, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of 'Picnic at Hanging Rock' take place in 1900, which matches the gold answer, and it does not introduce any unsupported information.", "main_total_tokens": 1949, "main_prompt_tokens": 1925, "main_completion_tokens": 24, "judge_total_tokens": 478, "judge_prompt_tokens": 409, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie about a young violinist named June, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man destined for her, is titled \"June\" [Doc 32483].", "latency_ms": 1668.3449745178223, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The question asks for the movie title; the gold answer is simply 'June'. The generated answer correctly identifies the title as 'June', so it is correct. However, it adds an unsupported citation to a specific document ([Doc 32483]), which we cannot verify from the prompt and thus is not grounded in the provided information. Therefore, correctness is 1, but faithfulness is 0.", "main_total_tokens": 2393, "main_prompt_tokens": 2334, "main_completion_tokens": 59, "judge_total_tokens": 602, "judge_prompt_tokens": 491, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 283.83374214172363, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer admits not knowing and does not provide an incorrect movie title, so it is faithful to the model's stated uncertainty. However, it fails to provide the correct answer given in the gold standard, so correctness is 0.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 477, "judge_prompt_tokens": 399, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 284.8796844482422, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a concrete rating (7.6), while the model responded with not knowing the answer. This is incorrect with respect to the gold, though it is faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 461, "judge_prompt_tokens": 389, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 857.0461273193359, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 0.9828920819566879, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3364045224324153, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is factually incorrect. The question is simple counting and the model’s numeric output is unsupported by the given gold label, hence correctness is 0. However, there is no evidence of internal contradiction or fabrication beyond the incorrect number itself, so faithfulness to its (albeit wrong) reasoning is considered maximal under this rubric.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 507, "judge_prompt_tokens": 394, "judge_completion_tokens": 113}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 370.0385093688965, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. However, the evaluation itself is faithful to the provided gold and generated values.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 461, "judge_prompt_tokens": 395, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 486.0513210296631, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer admits not knowing and does not assert any incorrect fact. It is therefore factually wrong relative to the gold answer (Fantômas) but fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 459, "judge_prompt_tokens": 389, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" [Doc 11205].", "latency_ms": 787.0988845825195, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise', which matches the gold answer exactly. It does not introduce any unsupported information beyond referencing the document, so it is both correct and faithful to the provided gold answer.", "main_total_tokens": 2115, "main_prompt_tokens": 2091, "main_completion_tokens": 24, "judge_total_tokens": 484, "judge_prompt_tokens": 409, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 1652.7118682861328, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as \"Won't You Be My Neighbor?\" and does not introduce unsupported information beyond a harmless document reference.", "main_total_tokens": 2285, "main_prompt_tokens": 2246, "main_completion_tokens": 39, "judge_total_tokens": 507, "judge_prompt_tokens": 440, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The movie \"Year of the Horse\" was released in 1997 [Doc 19452].", "latency_ms": 739.7081851959229, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states the release year is 2018, while the generated answer claims a movie titled 'Year of the Horse' was released in 1997. This directly contradicts the correct year, so the generated answer is factually incorrect with respect to the gold. However, the answer appears internally consistent with its own cited document and does not introduce contradictions relative to its own source, so it is faithful to the provided context but not correct to the gold label.", "main_total_tokens": 2428, "main_prompt_tokens": 2408, "main_completion_tokens": 20, "judge_total_tokens": 522, "judge_prompt_tokens": 397, "judge_completion_tokens": 125}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is \"Astro Boy\" from 1980. In this series, a young boy robot is abandoned by his creator and later rescued by an Android Advocate, Dr. Elefun, who adopts him and guides him in using his special powers to aid and protect mankind and robot-kind [Doc 112591].", "latency_ms": 971.332311630249, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as \"Astro Boy,\" which matches the gold answer. The additional descriptive details about the 1980 series and its plot are plausible and do not contradict the core fact, and there is no evidence they misrepresent the referenced document. Thus, the response is both correct and faithful.", "main_total_tokens": 2380, "main_prompt_tokens": 2301, "main_completion_tokens": 79, "judge_total_tokens": 562, "judge_prompt_tokens": 466, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 670.7558631896973, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer of 7 years, and it does not introduce any contradictions or unsupported claims. It is fully correct and faithful.", "main_total_tokens": 1863, "main_prompt_tokens": 1833, "main_completion_tokens": 30, "judge_total_tokens": 486, "judge_prompt_tokens": 424, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 940.59157371521, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983, which matches the gold value provided in the question. It does not introduce contradictions or unsupported claims and appears fully grounded in the given information.", "main_total_tokens": 2108, "main_prompt_tokens": 2075, "main_completion_tokens": 33, "judge_total_tokens": 520, "judge_prompt_tokens": 428, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 663.9678478240967, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score. The gold answer provides this value (80.5772). The generated answer says, 'I don't know,' which does not provide the requested score, so it is incorrect. However, saying 'I don't know' does not introduce any fabrication or contradiction with the given information, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 503, "judge_prompt_tokens": 396, "judge_completion_tokens": 107}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 369.7981834411621, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 Russian language TV show released in 1997, while the generated answer states 0. This is factually incorrect, so correctness is 0. The generated answer is a direct numeric response to the question (no extra fabrication beyond the incorrect value), so faithfulness to the user query format is high, but since faithfulness here is about matching the gold/reference, which it does not, the faithfulness score is set to 1 only in the sense that the model did not hallucinate additional information; however, the core factual content is wrong, hence correctness is 0.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 540, "judge_prompt_tokens": 385, "judge_completion_tokens": 155}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 848.9482402801514, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Angel Eyes,' which matches the gold answer exactly. It remains faithful to the provided information and does not introduce contradictions or unsupported claims.", "main_total_tokens": 2386, "main_prompt_tokens": 2351, "main_completion_tokens": 35, "judge_total_tokens": 496, "judge_prompt_tokens": 429, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French [Doc 110681].", "latency_ms": 770.8444595336914, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer claims the movie language is French, but the gold label says it is Korean, so the answer is factually incorrect. It also contradicts the given gold information, so it is unfaithful to the source.", "main_total_tokens": 2189, "main_prompt_tokens": 2172, "main_completion_tokens": 17, "judge_total_tokens": 470, "judge_prompt_tokens": 393, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 727.6818752288818, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.5, "precision_at_5": 0.2, "ndcg_at_5": 0.6131471927654584, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.6131471927654584, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1) does not match the gold answer (2), so it is incorrect. However, it directly addresses the question and remains faithful to the task without adding unsupported details or changing the question, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 473, "judge_prompt_tokens": 394, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The context provided does not include any information about movies released in 1979, so I cannot determine the best-rated movie from that year based on the given documents.", "latency_ms": 1418.2817935943604, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not provide information about movies released in 1979, so it cannot determine the best-rated movie. This aligns with the gold answer, which also indicates the system cannot provide such a movie due to database limitations. The response does not introduce any unsupported facts and is faithful to the absence of relevant information in the context.", "main_total_tokens": 2219, "main_prompt_tokens": 2186, "main_completion_tokens": 33, "judge_total_tokens": 531, "judge_prompt_tokens": 428, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The context provided does not mention a movie titled \"Back to the Future Part IV\" or its release year. Therefore, based on the given documents, there is no information about the release year of \"Back to the Future Part IV.\"", "latency_ms": 1017.7993774414062, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information about a movie titled 'Back to the Future Part IV' or its release year, aligning with the gold answer that such a movie does not exist in the database. It does not invent a release year or contradict the context, so it is both correct and faithful to the provided information.", "main_total_tokens": 2543, "main_prompt_tokens": 2496, "main_completion_tokens": 47, "judge_total_tokens": 545, "judge_prompt_tokens": 447, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly indicate that there are no movies from 1980 about the Chernobyl disaster (which occurred in 1986), so a popularity score cannot be provided. The generated answer is consistent with the gold and does not introduce contradictory or fabricated information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 521, "judge_prompt_tokens": 434, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 300.6117343902588, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says there are 0 TV shows or movies with 'Astro Boy' in the title, while the gold answer is 4, so the result is factually incorrect. However, the assistant does not contradict or ignore any provided context or instructions beyond giving the wrong count, so it is still faithful to the (very limited) prompt.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 490, "judge_prompt_tokens": 388, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 317.704439163208, "retrieval_recall": 0.0, "retrieval_mrr": 0.125, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.19342640361727081, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2 while the generated answer is 1, so the generated answer is factually incorrect relative to the gold. However, the model directly answered the question and did not introduce any unrelated or hallucinated content beyond the numerical response, so it is faithful to the (albeit incorrect) single fact it presents.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 386, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 332.3392868041992, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show about Super Mario Bros released in the 1990s, while the generated answer claims there were 0. This directly contradicts the gold, so the generated answer is factually incorrect relative to the labeled ground truth. However, the generated answer is internally consistent and does not contradict itself or the question, so it is fully faithful to its own reasoning and the prompt.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 504, "judge_prompt_tokens": 389, "judge_completion_tokens": 115}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 253.6928653717041, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (0), which is correct for the number of TV shows about Super Mario Bros released between 2000 and 2010. The model directly reflects the gold label without adding unsupported details, so it is both correct and faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 391, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 247.15089797973633, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto', which is a well-known and widely recognized popular ninja-themed show. The generated answer says 'I don't know', which is incorrect with respect to the gold. However, the model is honest about its uncertainty and does not hallucinate an incorrect answer, so it is fully faithful to its own stated knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 386, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The documents provided do not mention a Jurassic Park/Power Ranger crossover movie. Therefore, there is no information available on the release of such a movie.", "latency_ms": 1149.0652561187744, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that the movie was not released. The generated answer implies uncertainty and says there is no information available, rather than clearly stating that it was never released, so it is incorrect. However, it is faithful to the given documents and does not introduce fabricated details.", "main_total_tokens": 2220, "main_prompt_tokens": 2190, "main_completion_tokens": 30, "judge_total_tokens": 501, "judge_prompt_tokens": 415, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "The context provided does not mention any movie where Barbie fights the Predator. None of the listed Barbie movies involve a crossover with the Predator franchise. [Doc 68094], [Doc 44920], [Doc 78562], [Doc 73104], [Doc 25632], [Doc 72290], [Doc 36330], [Doc 52809], [Doc 65585], [Doc 94538], [Doc 82000], [Doc 48119].", "latency_ms": 1673.7208366394043, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.5, "faithfulness_score": 1, "justification": "Gold says explicitly that the movie was not released, implying such a movie was planned or proposed. The generated answer instead states that no such movie exists in the provided context and that none of the listed Barbie movies involve a crossover. This is faithful to the given context but only partially correct relative to the gold answer, since it fails to mention that a movie featuring Barbie fighting the Predator was planned but never released.", "main_total_tokens": 2641, "main_prompt_tokens": 2541, "main_completion_tokens": 100, "judge_total_tokens": 596, "judge_prompt_tokens": 482, "judge_completion_tokens": 114}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The context provided does not list any Star Wars movies, so it is not possible to determine the 4th oldest Star Wars movie from the given information.", "latency_ms": 821.02370262146, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The user question is factual and has a specific known answer (according to the provided gold label, the 4th oldest Star Wars movie is 'Star Wars: Droids - The Great Heep'). The generated answer claims it is not possible to determine the 4th oldest movie because the context does not list any movies. This is factually incorrect with respect to the task being evaluated (the correct answer is known and does not depend on additional context). Thus correctness is 0. However, with respect to the assistant's own stated constraint (relying only on given context), the answer is internally consistent and does not hallucinate another title; it truthfully notes lack of information. Therefore the faithfulness to its own accessible context is high, though not perfect because the evaluation setting expects knowledge beyond the immediate context.", "main_total_tokens": 2582, "main_prompt_tokens": 2551, "main_completion_tokens": 31, "judge_total_tokens": 616, "judge_prompt_tokens": 420, "judge_completion_tokens": 196}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The movie \"Year of the Horse\" was released in 1997 [Doc 19452].", "latency_ms": 585.9909057617188, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer gives a completely different movie and year (1997 for 'Year of the Horse') instead of the correct year 1986, so it is factually incorrect and not faithful to the gold answer.", "main_total_tokens": 2428, "main_prompt_tokens": 2408, "main_completion_tokens": 20, "judge_total_tokens": 471, "judge_prompt_tokens": 397, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 281.7215919494629, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model said it did not know the rating, while the gold answer specifies a rating of 8.466. This makes the response factually incorrect. However, the response is faithful to the model’s stated uncertainty and does not fabricate an answer, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 398, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 771.3561058044434, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med,' which matches the gold answer exactly. It is fully supported by the question and does not introduce any contradictions or unsupported claims, so it is both correct and faithful.", "main_total_tokens": 1969, "main_prompt_tokens": 1938, "main_completion_tokens": 31, "judge_total_tokens": 500, "judge_prompt_tokens": 423, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In \"Túnel de los huesos\" (2011), seven prisoners escaped from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 644.3929672241211, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks only for the country. The gold answer is Argentina. The generated answer specifies Villa Devoto prison in Buenos Aires, Argentina, which correctly includes the country and is consistent with the gold answer. No hallucinated or contradictory information relative to the question; extra detail (prison name, city) is accurate and does not conflict with the gold.", "main_total_tokens": 2577, "main_prompt_tokens": 2543, "main_completion_tokens": 34, "judge_total_tokens": 527, "judge_prompt_tokens": 425, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice Li Nai falls into a trap designed by Jiang Jie [Doc 135231].", "latency_ms": 868.6425685882568, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks whose trap the apprentice falls into. The gold answer is 'Jiang Jie,' and the generated answer states that the apprentice falls into a trap designed by Jiang Jie. The extra name 'Li Nai' is plausible added detail and does not contradict the gold. Therefore, the answer is both correct and faithful to the provided information.", "main_total_tokens": 2161, "main_prompt_tokens": 2134, "main_completion_tokens": 27, "judge_total_tokens": 510, "judge_prompt_tokens": 411, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 354.198694229126, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.7095272044910244, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.7095272044910244, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the number of Korean-language movies mentioning assassins after 2023. The gold answer is 4, while the generated answer is 0, which is factually incorrect, so correctness is 0. The generated answer is a direct numeric response to the question (no fabrication beyond the wrong value), so it is faithful to the user query in form, though incorrect in content, thus faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 503, "judge_prompt_tokens": 385, "judge_completion_tokens": 118}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "47", "latency_ms": 153.64885330200195, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '47' is factually incorrect given the gold statement that no movies can be found featuring both Bruce Lee and Steven Seagal together. It is also unfaithful to the gold answer, which explicitly states there are no such movies, while the generated response introduces a specific number with no support.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 488, "judge_prompt_tokens": 395, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of \"Tokyo Mater\" (2008) is 6.3 [Doc 42750].", "latency_ms": 1188.1859302520752, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states the rating of 'Tokyo Mater' (2008) as 6.3, which matches the provided gold answer. It also appears directly grounded in the cited document, so both correctness and faithfulness are full.", "main_total_tokens": 2163, "main_prompt_tokens": 2141, "main_completion_tokens": 22, "judge_total_tokens": 484, "judge_prompt_tokens": 405, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in the movie \"The Last Race\" is stock car racing. The film is a documentary that portrays a small town stock car track and the community of drivers associated with it [Doc 86410].", "latency_ms": 1064.5594596862793, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the style of racing is 'Drift', but the generated answer claims it is stock car racing in a documentary called 'The Last Race'. This contradicts the gold answer, so it is factually incorrect for this QA pair. It also does not remain faithful to the provided gold label or question context, instead introducing a different movie and racing style, so faithfulness is also low.", "main_total_tokens": 2356, "main_prompt_tokens": 2312, "main_completion_tokens": 44, "judge_total_tokens": 534, "judge_prompt_tokens": 422, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 886.0564231872559, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Confronting a Serial Killer' and does not introduce any unsupported information beyond restating the question context.", "main_total_tokens": 2161, "main_prompt_tokens": 2115, "main_completion_tokens": 46, "judge_total_tokens": 520, "judge_prompt_tokens": 452, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of \"Armageddon\" (1998) is 7.838 [Doc 20603].", "latency_ms": 769.0780162811279, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and correctly attributes it to the specified movie, so it is both correct and faithful to the provided information.", "main_total_tokens": 2330, "main_prompt_tokens": 2306, "main_completion_tokens": 24, "judge_total_tokens": 470, "judge_prompt_tokens": 409, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 737.210750579834, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the film as 'The Sixth Sense' and does not introduce any contradictory or extraneous information beyond a benign citation placeholder.", "main_total_tokens": 2078, "main_prompt_tokens": 2049, "main_completion_tokens": 29, "judge_total_tokens": 486, "judge_prompt_tokens": 420, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 964.8747444152832, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that The Transporter was released in 2002, which matches the gold answer exactly. It does not introduce any contradictions or extraneous claims beyond citing a document, so it is both correct and faithful to the provided information.", "main_total_tokens": 2369, "main_prompt_tokens": 2352, "main_completion_tokens": 17, "judge_total_tokens": 477, "judge_prompt_tokens": 397, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 786.4727973937988, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly giving the title as 'The Transporter Refueled' and does not introduce any ungrounded information beyond a benign document reference.", "main_total_tokens": 2402, "main_prompt_tokens": 2373, "main_completion_tokens": 29, "judge_total_tokens": 488, "judge_prompt_tokens": 420, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "\"The Transporter Refueled\" was released in the year 2015 [Doc 67500].", "latency_ms": 1256.537914276123, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold label. It is fully supported by the referenced document, so it is both correct and faithful.", "main_total_tokens": 2259, "main_prompt_tokens": 2238, "main_completion_tokens": 21, "judge_total_tokens": 480, "judge_prompt_tokens": 406, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "The context provided does not mention any romantic comedy where Abraham Lincoln comes back from the dead. The movies listed that involve Abraham Lincoln, such as \"Abraham Lincoln vs. Zombies\" and \"Abraham Lincoln: Vampire Hunter,\" are categorized under genres like horror, action, and thriller, but not romance or comedy [Doc 57213] [Doc 54943].", "latency_ms": 1365.4990196228027, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says there is no such romantic comedy, and the generated answer essentially states the same: that the context does not mention any romantic comedy where Abraham Lincoln comes back from the dead, noting only horror/action films instead. This correctly addresses the question (none found) and is fully grounded in the provided context, so both correctness and faithfulness are high.", "main_total_tokens": 2094, "main_prompt_tokens": 2020, "main_completion_tokens": 74, "judge_total_tokens": 567, "judge_prompt_tokens": 464, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of the movie that alludes to a manuscript written by Ernest Hemingway is \"Garden of Eden\" [Doc 43581].", "latency_ms": 933.6130619049072, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates the correct movie title is 'Havana Darkness', but the generated answer incorrectly gives 'Garden of Eden', so it is factually wrong. The answer is, however, fully based on the provided document reference and does not invent extra information, so it is faithful to its cited source.", "main_total_tokens": 2047, "main_prompt_tokens": 2020, "main_completion_tokens": 27, "judge_total_tokens": 507, "judge_prompt_tokens": 415, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show \"Doll & Em\" [Doc 122259].", "latency_ms": 795.6023216247559, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, correctly identifying the TV show as 'Doll & Em' and is fully supported by the cited document reference.", "main_total_tokens": 2045, "main_prompt_tokens": 2016, "main_completion_tokens": 29, "judge_total_tokens": 479, "judge_prompt_tokens": 417, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The 2007 movie in which Hallam has a talent for spying on people and attempts to find the cause of his mother’s death is \"Hallam Foe\" [Doc 39389].", "latency_ms": 1303.5211563110352, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the film as \"Hallam Foe,\" which matches the gold answer exactly. It does not introduce any contradictions or hallucinations beyond restating the question context, so it is both correct and faithful.", "main_total_tokens": 2299, "main_prompt_tokens": 2259, "main_completion_tokens": 40, "judge_total_tokens": 515, "judge_prompt_tokens": 439, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The last name \"Lee\" in the context of the movie \"He is My Enemy, Partner and Father-In-Law\" [Doc 22549] does not inherently suggest whether he is a friend or a foe. The perception of the name would depend on the viewer's personal associations with the name \"Lee,\" which is a common surname and does not have an intrinsic connotation of friendship or enmity.", "latency_ms": 1961.2758159637451, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is unambiguously 'A foe,' indicating that the last name clearly sounds like that of an enemy. The generated answer claims the name does not inherently suggest friend or foe and makes it subjective, directly contradicting the gold label, so it is incorrect. However, the response is internally consistent and based only on the given information without fabricating external facts, so it is faithful.", "main_total_tokens": 2282, "main_prompt_tokens": 2199, "main_completion_tokens": 83, "judge_total_tokens": 579, "judge_prompt_tokens": 469, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "In the 2017 TV show \"Atypical,\" Sam is 18 years old [Doc 124583].", "latency_ms": 764.4186019897461, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show 'Atypical,' which matches the gold answer exactly. It is faithful to the provided information without adding unsupported details.", "main_total_tokens": 2017, "main_prompt_tokens": 1993, "main_completion_tokens": 24, "judge_total_tokens": 485, "judge_prompt_tokens": 411, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show \"Atypical,\" Sam is on the autism spectrum [Doc 124583].", "latency_ms": 819.9024200439453, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the 2017 TV show 'Atypical,' Sam is on the autism spectrum, which matches the gold answer exactly. It is fully faithful to the provided information and does not add unsupported claims.", "main_total_tokens": 1860, "main_prompt_tokens": 1836, "main_completion_tokens": 24, "judge_total_tokens": 489, "judge_prompt_tokens": 411, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Sandra was born in Chamonix [Doc 59659].", "latency_ms": 648.4556198120117, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, which matches the gold answer exactly. It is also fully grounded in the provided information, referencing the same detail, so both correctness and faithfulness are maximal.", "main_total_tokens": 2486, "main_prompt_tokens": 2473, "main_completion_tokens": 13, "judge_total_tokens": 486, "judge_prompt_tokens": 410, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In the Japanese language movie \"When the Rain Falls,\" Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 755.1531791687012, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The blank is correctly filled with 'rains', matching the gold answer. However, the generated response adds specific, likely fabricated details about the movie title ('When the Rain Falls') and a document reference, which are not supported by the provided information, so faithfulness is slightly reduced.", "main_total_tokens": 2396, "main_prompt_tokens": 2370, "main_completion_tokens": 26, "judge_total_tokens": 507, "judge_prompt_tokens": 417, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the animated series \"Tabaluga,\" Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 781.0356616973877, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the place where Tabaluga is crown-prince. The gold answer is Greenland. The generated answer states that he is the crown-prince of Greenland, matching the gold exactly. It does not introduce contradictions or irrelevant information, so it is both correct and faithful.", "main_total_tokens": 2343, "main_prompt_tokens": 2312, "main_completion_tokens": 31, "judge_total_tokens": 513, "judge_prompt_tokens": 425, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers\" is \"One Afternoon\" [Doc 96838].", "latency_ms": 977.5979518890381, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'One Afternoon'. It does not introduce any contradictions or fabricate information beyond restating the correct title with a plausible citation marker, so it is both correct and faithful.", "main_total_tokens": 1949, "main_prompt_tokens": 1905, "main_completion_tokens": 44, "judge_total_tokens": 526, "judge_prompt_tokens": 448, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" [Doc 93260].", "latency_ms": 748.8408088684082, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Life Ahead,\" which matches the gold answer exactly. It does not introduce contradictory or unsupported information and remains faithful to the provided content, only adding a benign citation-style reference.", "main_total_tokens": 2111, "main_prompt_tokens": 2074, "main_completion_tokens": 37, "judge_total_tokens": 510, "judge_prompt_tokens": 435, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 981.4305305480957, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and aligns with the gold answer. The additional detail that he is a New Zealander and founder of the McLaren Motor Racing team is accurate and does not contradict the source, so the response is both correct and faithful.", "main_total_tokens": 2317, "main_prompt_tokens": 2284, "main_completion_tokens": 33, "judge_total_tokens": 510, "judge_prompt_tokens": 417, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The name of Sonny Hayes' racing team is not mentioned in the provided documents. [Doc 111503]", "latency_ms": 805.0041198730469, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer clearly states that the team is the McLaren Motor Racing team, so the generated response is factually incorrect. It also contradicts the provided gold information by claiming the name is not mentioned, so it is unfaithful to the reference.", "main_total_tokens": 2428, "main_prompt_tokens": 2406, "main_completion_tokens": 22, "judge_total_tokens": 487, "judge_prompt_tokens": 405, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context provided does not specify the country of origin for any individual named \"he.\" Therefore, the answer is not found in the given documents.", "latency_ms": 964.4978046417236, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The gold answer clearly states the person is from New Zealand, so the correct answer is New Zealand. The model instead claimed the information was not specified, which is factually incorrect relative to the gold. It is mostly faithful to its own stated reasoning about the (assumed) context, but that reasoning conflicts with the gold answer, so correctness is 0. Faithfulness is slightly reduced because it should have considered that the question likely refers to the gold, but it consistently reasons from its (incorrect) premise that the context lacks that info.", "main_total_tokens": 2382, "main_prompt_tokens": 2353, "main_completion_tokens": 29, "judge_total_tokens": 546, "judge_prompt_tokens": 405, "judge_completion_tokens": 141}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "The context provided does not mention any 2013 movie in which John McClane blows up the Great Wall of China. The movies listed, such as \"A Good Day to Die Hard\" [Doc 58791], do not include any plot details involving the Great Wall of China.", "latency_ms": 1150.0544548034668, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.7, "faithfulness_score": 0.7, "justification": "Gold answer correctly states that there is no such movie in the database, implicitly that the premise is false. The generated answer instead focuses on the given context not mentioning such a movie, leaving open the possibility that it exists elsewhere, so it is less direct and not fully aligned with the gold. It is partially correct (it notes no such movie is in the listed context) but does not clearly answer the question. Faithfulness is partial because it adheres to context but fails to mirror the gold answer’s explicit refusal and database framing.", "main_total_tokens": 2479, "main_prompt_tokens": 2421, "main_completion_tokens": 58, "judge_total_tokens": 600, "judge_prompt_tokens": 456, "judge_completion_tokens": 144}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with the description \"When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental\" is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 901.637077331543, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Tomie: Forbidden Fruit' and correctly restates the question. It does not introduce contradictions or unsupported claims and remains faithful to the provided information.", "main_total_tokens": 2401, "main_prompt_tokens": 2343, "main_completion_tokens": 58, "judge_total_tokens": 544, "judge_prompt_tokens": 475, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie \"Lei mi parla ancora,\" Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 761.5573406219482, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years) and stays consistent with the provided information without introducing contradictions or unsupported claims. It is both correct and faithful.", "main_total_tokens": 2685, "main_prompt_tokens": 2655, "main_completion_tokens": 30, "judge_total_tokens": 488, "judge_prompt_tokens": 424, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In \"Scorpion King: Book of Souls\" (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls, which will allow them to put an end to an evil warlord [Doc 82582].", "latency_ms": 1387.4964714050293, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.95, "justification": "The generated answer correctly states that in 'Scorpion King: Book of Souls' (2018), the Scorpion King and Tala are searching for the Book of Souls, matching the gold answer. The extra detail about it being a legendary relic that can end an evil warlord is broadly accurate to the film’s plot, but the specific citation '[Doc 82582]' is fabricated, slightly reducing faithfulness.", "main_total_tokens": 2573, "main_prompt_tokens": 2522, "main_completion_tokens": 51, "judge_total_tokens": 560, "judge_prompt_tokens": 445, "judge_completion_tokens": 115}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Zombies on the Moon\" is not found in the provided documents.", "latency_ms": 871.3943958282471, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated responses state that the release year is not available/not found. This is correct given the context and faithful to the (lack of) information in the provided documents.", "main_total_tokens": 2094, "main_prompt_tokens": 2075, "main_completion_tokens": 19, "judge_total_tokens": 478, "judge_prompt_tokens": 410, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Galactic Baker\" is not found in the provided documents.", "latency_ms": 1187.23726272583, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database; the generated answer similarly says the release year is not found in the provided documents. Both convey that the information is unavailable, which is correct and faithful to the given context.", "main_total_tokens": 2370, "main_prompt_tokens": 2353, "main_completion_tokens": 17, "judge_total_tokens": 483, "judge_prompt_tokens": 406, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year of release for \"Return of the Null Pointer\" is not found in the provided documents.", "latency_ms": 616.7457103729248, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer and the generated answer are semantically equivalent: both state that the release year is not available / not found, and neither hallucinates a year or extra details. The generated answer remains grounded in the stated evidence (\"provided documents\") and does not contradict the gold.", "main_total_tokens": 2343, "main_prompt_tokens": 2323, "main_completion_tokens": 20, "judge_total_tokens": 499, "judge_prompt_tokens": 411, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year of release for \"Data Science: The Movie\" is not found in the provided documents.", "latency_ms": 987.1330261230469, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database; the generated answer states the year is not found in the provided documents. Both convey that the information is unavailable and do not fabricate a year, so the response is both correct and faithful to the given information.", "main_total_tokens": 2303, "main_prompt_tokens": 2283, "main_completion_tokens": 20, "judge_total_tokens": 495, "judge_prompt_tokens": 411, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Python's Revenge 2\" is not found in the provided documents.", "latency_ms": 679.1398525238037, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available / not found, which is correct given the context. The generated answer is faithful to the information in the provided documents and does not hallucinate a release year.", "main_total_tokens": 2172, "main_prompt_tokens": 2153, "main_completion_tokens": 19, "judge_total_tokens": 487, "judge_prompt_tokens": 410, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Last Pixel\" is not provided in the given documents.", "latency_ms": 863.9206886291504, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie that is not in the available data. Both the gold and generated answers correctly indicate that the information is not available rather than fabricating a year. The generated answer is consistent with the premise that it is limited to given documents and does not contradict them, so it is both correct and faithful.", "main_total_tokens": 1987, "main_prompt_tokens": 1970, "main_completion_tokens": 17, "judge_total_tokens": 505, "judge_prompt_tokens": 406, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Man Who Ate the Sun\" is not provided in the given documents.", "latency_ms": 779.9901962280273, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers convey that the release year is unknown / not available in the provided information. The generated answer correctly states that the year is not in the given documents, which is faithful to the prompt and correctly aligned with the gold response.", "main_total_tokens": 2532, "main_prompt_tokens": 2512, "main_completion_tokens": 20, "judge_total_tokens": 493, "judge_prompt_tokens": 412, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Days of Lehman Brothers\" is not mentioned in the provided documents.", "latency_ms": 830.007791519165, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the director information is not in the database. The generated answer similarly states that the director is not mentioned in the provided documents. This matches the limitation of the source and does not hallucinate a director name, so it is both correct relative to the task and faithful to the given information.", "main_total_tokens": 2594, "main_prompt_tokens": 2572, "main_completion_tokens": 22, "judge_total_tokens": 511, "judge_prompt_tokens": 419, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"A Love to Die For\" is not mentioned in the provided documents.", "latency_ms": 944.2822933197021, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not provided in the database. The generated answer states that the director is not mentioned in the provided documents. Both convey the same limitation and avoid fabricating a director name, so the response is correct relative to the evidence and faithful to the given context.", "main_total_tokens": 2179, "main_prompt_tokens": 2159, "main_completion_tokens": 20, "judge_total_tokens": 505, "judge_prompt_tokens": 415, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not mentioned in the provided documents. [Doc 111360]", "latency_ms": 887.312650680542, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned and ties this to the provided documents, which aligns with the gold answer that the database does not provide the director information. No unsupported information is added, so it is both correct and faithful to the source context.", "main_total_tokens": 2321, "main_prompt_tokens": 2297, "main_completion_tokens": 24, "judge_total_tokens": 501, "judge_prompt_tokens": 417, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Night at Tremore Beach\" is not mentioned in the provided documents. [Doc 135098]", "latency_ms": 902.1496772766113, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director is not mentioned/provided in the available information. The generated answer is correct relative to the given context and does not hallucinate a director, so it is fully faithful.", "main_total_tokens": 1922, "main_prompt_tokens": 1894, "main_completion_tokens": 28, "judge_total_tokens": 500, "judge_prompt_tokens": 425, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 648.2434272766113, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'Rise of the Mummy', matching the gold answer, so the correctness is 1. However, it fabricates a citation '[Doc 98375]' which is not grounded in any provided source, so the faithfulness score is 0.", "main_total_tokens": 2090, "main_prompt_tokens": 2063, "main_completion_tokens": 27, "judge_total_tokens": 507, "judge_prompt_tokens": 418, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse is \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 1184.1404438018799, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, correctly identifying the movie as 'Stranger in My Bed' and does not introduce contradictions or unsupported claims.", "main_total_tokens": 2157, "main_prompt_tokens": 2128, "main_completion_tokens": 29, "judge_total_tokens": 486, "judge_prompt_tokens": 425, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The scenario where a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years occurs in the TV show \"Game of Silence\" [Doc 123938].", "latency_ms": 1217.3256874084473, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The question explicitly asks for a movie, but both the gold and the generated answer correctly identify 'Game of Silence,' which is a TV show, not a movie. So while it matches the gold, it is not strictly correct with respect to the question. The answer is faithful to the provided information, but slightly overstates by calling it a TV show without clear grounding from the quoted doc, so not perfectly faithful.", "main_total_tokens": 1834, "main_prompt_tokens": 1798, "main_completion_tokens": 36, "judge_total_tokens": 549, "judge_prompt_tokens": 433, "judge_completion_tokens": 116}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie in which the plot involves a killer from the site Craigslist is \"The Craigslist Killer\" [Doc 53039].", "latency_ms": 1304.0411472320557, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'The Craigslist Killer' and stays faithful to the given information.", "main_total_tokens": 2007, "main_prompt_tokens": 1982, "main_completion_tokens": 25, "judge_total_tokens": 475, "judge_prompt_tokens": 417, "judge_completion_tokens": 58}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which Yuki Hirano, after failing his university entrance exam, sets his eyes on the role of a forestry trainee is \"Wood Job!\" [Doc 66094].", "latency_ms": 1079.5106887817383, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", which exactly matches the gold answer, and does not introduce any contradictions or unsupported details. It is both correct and faithful.", "main_total_tokens": 2457, "main_prompt_tokens": 2420, "main_completion_tokens": 37, "judge_total_tokens": 505, "judge_prompt_tokens": 437, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure is \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 938.363790512085, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase' and does not introduce any contradictory or extraneous information.", "main_total_tokens": 2472, "main_prompt_tokens": 2423, "main_completion_tokens": 49, "judge_total_tokens": 525, "judge_prompt_tokens": 459, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a 7-year-old boy must find the hero within him to save his parents trapped inside a burning car due to a rock fall is \"Beyond\" [Doc 74597].", "latency_ms": 1025.8362293243408, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Beyond,\" matching the gold answer exactly, and does so without adding unsupported details. It is both correct and faithful to the provided information.", "main_total_tokens": 2133, "main_prompt_tokens": 2093, "main_completion_tokens": 40, "judge_total_tokens": 515, "judge_prompt_tokens": 448, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman, fired from a financial corporation during the Asia crisis, returns home with no money is \"6ixtynin9\" [Doc 22478].", "latency_ms": 909.6786975860596, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', which matches the gold answer. It is fully consistent with the provided information without adding extraneous or conflicting details.", "main_total_tokens": 2042, "main_prompt_tokens": 2005, "main_completion_tokens": 37, "judge_total_tokens": 508, "judge_prompt_tokens": 439, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie in which a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort, a hedonistic island paradise for swingers, and it's up to the club's staff to stop the violence or at least hide it, is \"Club Dread\" [Doc 32410].", "latency_ms": 1187.971591949463, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer exactly. It remains faithful to the provided information without adding unsupported details, only restating and slightly elaborating the prompt description.", "main_total_tokens": 2264, "main_prompt_tokens": 2204, "main_completion_tokens": 60, "judge_total_tokens": 558, "judge_prompt_tokens": 484, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is \"Toilet\" [Doc 48873].", "latency_ms": 833.6772918701172, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" matching the gold answer exactly. It remains faithful to the provided information without adding unsupported details.", "main_total_tokens": 2765, "main_prompt_tokens": 2714, "main_completion_tokens": 51, "judge_total_tokens": 526, "judge_prompt_tokens": 465, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century is \"Electric Angel\" [Doc 1024].", "latency_ms": 1032.57155418396, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot. The gold answer is 'Electric Angel.' The generated answer explicitly identifies the movie as 'Electric Angel,' matching the gold exactly. There is no contradiction or hallucination relative to the provided information. Therefore, the response is both correct and faithful.", "main_total_tokens": 2408, "main_prompt_tokens": 2363, "main_completion_tokens": 45, "judge_total_tokens": 550, "judge_prompt_tokens": 461, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is \"Doomsday Stories\" with a rating of 2.167 [Doc 108366].", "latency_ms": 702.6803493499756, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates that the lowest rated movie from 2023 is 'Mega Ape', while the generated answer claims it is 'Doomsday Stories' with a rating of 2.167. This directly contradicts the gold answer, so the response is incorrect. Additionally, the mention of a specific document ID and rating appears to be fabricated or at least not grounded in the provided gold information, so it is not faithful.", "main_total_tokens": 2140, "main_prompt_tokens": 2110, "main_completion_tokens": 30, "judge_total_tokens": 529, "judge_prompt_tokens": 411, "judge_completion_tokens": 118}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 279.13641929626465, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Tea House', but the model responded with 'I don't know,' which does not match the gold answer, so correctness is 0. However, the model did not invent an incorrect fact and honestly stated lack of knowledge, so the answer is fully faithful to its own uncertainty, giving faithfulness 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 386, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 285.0162982940674, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not match the gold answer ('The Uncut Story of Mewtwo's Origin') and thus is incorrect. However, it does not hallucinate or provide any incorrect alternative; it honestly expresses uncertainty, so it is fully faithful to the model's own stated knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 490, "judge_prompt_tokens": 395, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The language of the lowest-rated movie from 1980 is English. The movie is \"Cataclysm,\" which has a rating of 3.889 [Doc 744].", "latency_ms": 1133.3339214324951, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly states that the language of the lowest-rated movie from 1980 is English, matching the gold answer. However, it adds the specific movie title and rating (\"Cataclysm\" with rating 3.889 [Doc 744]), which may not be directly supported or necessary for the question and could be hallucinated relative to the referenced docs, slightly reducing faithfulness.", "main_total_tokens": 2122, "main_prompt_tokens": 2085, "main_completion_tokens": 37, "judge_total_tokens": 533, "judge_prompt_tokens": 420, "judge_completion_tokens": 113}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show from 1999 with the highest popularity was \"The Sopranos,\" with a popularity score of 59.4284 [Doc 115146].", "latency_ms": 1102.250337600708, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect: the gold label indicates the most popular 1999 TV show is 'Law & Order: Special Victims Unit,' but the model answered 'The Sopranos.' Additionally, the reference to a specific document and numeric popularity score is fabricated in this context, so the answer is not faithful to the provided gold information.", "main_total_tokens": 2457, "main_prompt_tokens": 2423, "main_completion_tokens": 34, "judge_total_tokens": 524, "judge_prompt_tokens": 424, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 824.3420124053955, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and correctly attributes it, so it is both correct and faithful.", "main_total_tokens": 2351, "main_prompt_tokens": 2333, "main_completion_tokens": 18, "judge_total_tokens": 457, "judge_prompt_tokens": 400, "judge_completion_tokens": 57}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in the year 1999 [Doc 22481].", "latency_ms": 717.4229621887207, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states that the movie was released in 1999, but the gold answer is 2019, so the response is factually incorrect relative to the gold. However, it is faithful to its cited document ([Doc 22481]) and does not contradict its own source, so faithfulness is high.", "main_total_tokens": 2173, "main_prompt_tokens": 2151, "main_completion_tokens": 22, "judge_total_tokens": 501, "judge_prompt_tokens": 406, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 169.27671432495117, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (3) does not match the gold answer (279), so it is incorrect. However, it is at least a direct attempt to answer the question without introducing unrelated content, so it is faithful to the user query.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 461, "judge_prompt_tokens": 383, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "12", "latency_ms": 238.3899688720703, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (12), so it is both correct and faithful to the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 436, "judge_prompt_tokens": 383, "judge_completion_tokens": 53}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in the year 2009 [Doc 46069].", "latency_ms": 786.489725112915, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie was released in 2009, matching the gold answer. It also remains faithful to the provided information without adding unsupported details.", "main_total_tokens": 2295, "main_prompt_tokens": 2270, "main_completion_tokens": 25, "judge_total_tokens": 476, "judge_prompt_tokens": 412, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 988.3589744567871, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the original language of 'The King's Speech' is English, which matches the gold answer 'en'. It stays faithful to the given information and does not introduce contradictions.", "main_total_tokens": 2561, "main_prompt_tokens": 2542, "main_completion_tokens": 19, "judge_total_tokens": 472, "judge_prompt_tokens": 402, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "1987 [Doc 6993].", "latency_ms": 1773.0467319488525, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer exactly. The citation does not contradict the answer and appears to support it, so the response is both correct and faithful to the source.", "main_total_tokens": 2767, "main_prompt_tokens": 2758, "main_completion_tokens": 9, "judge_total_tokens": 464, "judge_prompt_tokens": 387, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Brain Fluid Explosion Girl", "latency_ms": 871.502161026001, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which correctly identifies 'Brain Fluid Explosion Girl' as the movie where classmates must play a survival game using their cell phones. No discrepancies or hallucinations are present.", "main_total_tokens": 2703, "main_prompt_tokens": 2698, "main_completion_tokens": 5, "judge_total_tokens": 464, "judge_prompt_tokens": 392, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 1203.822135925293, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.4306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.719741384391281, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Dungeons & Dragons.'", "main_total_tokens": 2838, "main_prompt_tokens": 2823, "main_completion_tokens": 15, "judge_total_tokens": 446, "judge_prompt_tokens": 394, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 1039.9713516235352, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Nacht der Wölfe') and correctly identifies the movie described in the question.", "main_total_tokens": 2650, "main_prompt_tokens": 2643, "main_completion_tokens": 7, "judge_total_tokens": 460, "judge_prompt_tokens": 403, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900", "latency_ms": 848.9339351654053, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (1900) and correctly identifies the year in which the events of Picnic at Hanging Rock take place.", "main_total_tokens": 2507, "main_prompt_tokens": 2504, "main_completion_tokens": 3, "judge_total_tokens": 448, "judge_prompt_tokens": 388, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June", "latency_ms": 570.0511932373047, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question clearly points to the movie 'June' and both the gold and generated answers match exactly. The response is fully correct and faithful to the gold label.", "main_total_tokens": 126, "main_prompt_tokens": 125, "main_completion_tokens": 1, "judge_total_tokens": 496, "judge_prompt_tokens": 433, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 338.55557441711426, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a specific known answer, but the generated response says 'I don't know,' which is incorrect. However, the model does not hallucinate or invent an answer, so it is fully faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 399, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 285.1688861846924, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric value (the highest Russian-language movie rating in 1999). The gold answer is 7.6, but the generated answer says 'I don't know,' which is incorrect. However, it does not hallucinate or provide fabricated information; it simply admits lack of knowledge, so it is fully faithful but not correct.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 389, "judge_completion_tokens": 102}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "12", "latency_ms": 1534.2490673065186, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 0.9828920819566879, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3364045224324153, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model output (12) does not match the gold answer (4), so it is factually incorrect. However, it directly attempts to answer the question posed using the given information, so it is faithful to the user query and not hallucinating about a different task.", "main_total_tokens": 1465, "main_prompt_tokens": 1442, "main_completion_tokens": 23, "judge_total_tokens": 479, "judge_prompt_tokens": 394, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 305.45973777770996, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer claims there are no matching items, which is factually incorrect, so correctness is 0. The model clearly and directly answers the question in line with its own (albeit wrong) reasoning without adding fabricated details, so faithfulness to its internal response is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 499, "judge_prompt_tokens": 404, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 338.56892585754395, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct specific show 'Fantômas,' so it is incorrect. However, it does not introduce any fabricated information or contradictions with the provided gold answer, so it is faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 389, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise", "latency_ms": 1209.6259593963623, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the movie as 'Kill Cruise'. The response is fully correct and faithful to the reference.", "main_total_tokens": 2653, "main_prompt_tokens": 2650, "main_completion_tokens": 3, "judge_total_tokens": 447, "judge_prompt_tokens": 387, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 1056.0119152069092, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the title of the movie about the Fred Rogers TV series.", "main_total_tokens": 2851, "main_prompt_tokens": 2843, "main_completion_tokens": 8, "judge_total_tokens": 462, "judge_prompt_tokens": 408, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018", "latency_ms": 1387.6776695251465, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (2018), so it is both correct and faithful to the reference.", "main_total_tokens": 2896, "main_prompt_tokens": 2880, "main_completion_tokens": 16, "judge_total_tokens": 434, "judge_prompt_tokens": 380, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy [Doc 112591].", "latency_ms": 971.0805416107178, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 'Astro Boy' as the animated TV series about a young boy robot in 2030 AD and stays faithful to the information implied by the question and gold answer. The extra bracketed note does not change the correctness or introduce unfaithful content.", "main_total_tokens": 2910, "main_prompt_tokens": 2900, "main_completion_tokens": 10, "judge_total_tokens": 482, "judge_prompt_tokens": 395, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 [Doc 67513]", "latency_ms": 1249.631404876709, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The correct answer is that they were held captive for 7 years. The generated answer \"7\" matches the gold answer exactly and does not introduce any unsupported details.", "main_total_tokens": 2416, "main_prompt_tokens": 2408, "main_completion_tokens": 8, "judge_total_tokens": 466, "judge_prompt_tokens": 402, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26 million [Doc 133469].", "latency_ms": 1334.2299461364746, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer in substance, differing only by writing out 'million' instead of 'm'. This is numerically and semantically equivalent and supported by the cited document, so it is both correct and faithful.", "main_total_tokens": 2097, "main_prompt_tokens": 2088, "main_completion_tokens": 9, "judge_total_tokens": 481, "judge_prompt_tokens": 404, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 573.9870071411133, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score; the gold answer is 80.5772. The generated answer says only 'I don't know,' which does not provide the correct value, so correctness is 0. However, it does not fabricate any information or contradict known facts; it honestly expresses uncertainty, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 496, "judge_prompt_tokens": 396, "judge_completion_tokens": 100}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 259.0341567993164, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 1, indicating that there was one Russian language TV show released in 1997. The generated answer claims it cannot find any matching movies or shows, which contradicts the gold. Thus the answer is incorrect. It is also unfaithful to the reference since it directly conflicts with the gold label.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 490, "judge_prompt_tokens": 394, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes", "latency_ms": 1185.5294704437256, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Angel Eyes' exactly matches the gold answer and correctly identifies the TV show where Soo-wan regains her sight after an eye transplant surgery.", "main_total_tokens": 2959, "main_prompt_tokens": 2956, "main_completion_tokens": 3, "judge_total_tokens": 460, "judge_prompt_tokens": 397, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "en", "latency_ms": 1418.928623199463, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.7461414348591218, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer says the language is 'en' (English), but the gold label is Korean. This is factually incorrect, so correctness is 0. It also directly contradicts the provided gold answer, so faithfulness is 0.", "main_total_tokens": 2841, "main_prompt_tokens": 2829, "main_completion_tokens": 12, "judge_total_tokens": 457, "judge_prompt_tokens": 377, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 598.0193614959717, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.5, "precision_at_5": 0.2, "ndcg_at_5": 0.6131471927654584, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.6131471927654584, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1) does not match the gold answer (2), so it is incorrect. However, the model did not contradict the given information; it simply produced the wrong count, so faithfulness to the (limited) provided context is maintained.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 476, "judge_prompt_tokens": 394, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 485.6901168823242, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer explicitly states that there are no movies from 1979 in the database, while the generated answer just says 'I don't know' without mentioning the database limitation. This makes the generated answer less informative and not aligned with the specified reference behavior. It is also effectively incorrect relative to the gold response's content. It is partially faithful only in that it expresses lack of knowledge, but it omits the key constraint about the database.", "main_total_tokens": 579, "main_prompt_tokens": 578, "main_completion_tokens": 1, "judge_total_tokens": 515, "judge_prompt_tokens": 399, "judge_completion_tokens": 116}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "I don't know.", "latency_ms": 802.6809692382812, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.0, "faithfulness_score": 0.5, "justification": "The question asks for the release year of a non-existent movie. The gold answer correctly identifies that 'Back to the Future Part IV' does not exist. The generated answer, 'I don't know,' does not explicitly state that the movie does not exist and thus is less informative and not fully correct, so correctness is low. However, it does not introduce false information and expresses uncertainty, so it is partially faithful to the lack of knowledge, warranting a moderate faithfulness score.", "main_total_tokens": 583, "main_prompt_tokens": 582, "main_completion_tokens": 1, "judge_total_tokens": 535, "judge_prompt_tokens": 404, "judge_completion_tokens": 131}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot find any movies or shows matching the given criteria, which aligns with the gold answer that there were no movies from 1980 related to the Chernobyl disaster. It is faithful to the question and does not introduce fabricated details; it merely suggests verifying the event timing, which is reasonable and consistent with the known fact that Chernobyl occurred in 1986.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 546, "judge_prompt_tokens": 434, "judge_completion_tokens": 112}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 285.1269245147705, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 4 titles with 'Astro Boy' in the title, but the generated answer claims there are none, which is factually incorrect. However, the model does not fabricate a specific but wrong number; it honestly states it cannot find any, so it is still faithful to its own (albeit limited) retrieval and reasoning process.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 501, "judge_prompt_tokens": 397, "judge_completion_tokens": 104}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "3", "latency_ms": 819.6966648101807, "retrieval_recall": 0.0, "retrieval_mrr": 0.125, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.19342640361727081, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (3) does not match the gold answer (2), so it is factually incorrect relative to the provided gold label. However, there is no indication that the model ignored or contradicted the user’s query; it answered the question directly with a single numeric value, so it is fully faithful to the user’s request.", "main_total_tokens": 3367, "main_prompt_tokens": 3362, "main_completion_tokens": 5, "judge_total_tokens": 485, "judge_prompt_tokens": 386, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 278.65052223205566, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show, while the generated answer says there were none, which is factually incorrect. However, the model does not hallucinate a specific incorrect number or title; it acknowledges inability to find matches, so it is faithful to its own (simulated) uncertainty, even though the result is wrong relative to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 501, "judge_prompt_tokens": 398, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 316.4205551147461, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that there were 0 TV shows fitting the criteria. The generated answer says it cannot find any movies or shows matching those criteria, which effectively conveys that there are none. This matches the gold answer in substance and does not introduce unsupported details, so it is both correct and faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 400, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 278.1329154968262, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Naruto'. The generated answer says 'I don't know', which is factually incorrect, so correctness is 0. However, the model is honest about its lack of knowledge and does not fabricate information, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 470, "judge_prompt_tokens": 386, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 517.3687934875488, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that the movie was not released. The generated answer says 'I don't know,' which does not assert anything false but fails to provide the correct information. Therefore, it is fully faithful to the model’s uncertainty but incorrect relative to the gold answer.", "main_total_tokens": 565, "main_prompt_tokens": 564, "main_completion_tokens": 1, "judge_total_tokens": 473, "judge_prompt_tokens": 389, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 501.66964530944824, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that such a movie does not exist or was not released. The generated answer, 'I don't know,' is factual about the model's uncertainty but does not provide the correct information, so it is not correct. It is, however, faithful to the model's apparent knowledge state and does not fabricate an answer.", "main_total_tokens": 564, "main_prompt_tokens": 563, "main_completion_tokens": 1, "judge_total_tokens": 483, "judge_prompt_tokens": 386, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "I don't know.", "latency_ms": 1056.1213493347168, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is \"Star Wars: Droids - The Great Heep\". The generated answer says \"I don't know,\" which is incorrect. However, the model does not invent any incorrect facts; it simply expresses a lack of knowledge, so it is fully faithful to its own statement.", "main_total_tokens": 621, "main_prompt_tokens": 620, "main_completion_tokens": 1, "judge_total_tokens": 483, "judge_prompt_tokens": 393, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "I don't know.", "latency_ms": 941.6379928588867, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model truthfully states it does not know, so it is faithful to its own uncertainty, but the answer is incorrect because the correct year is 1986.", "main_total_tokens": 683, "main_prompt_tokens": 669, "main_completion_tokens": 14, "judge_total_tokens": 445, "judge_prompt_tokens": 381, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 247.11179733276367, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct rating is 8.466. The generated answer says 'I don't know,' which is factually incorrect, so correctness is 0. However, the model does not fabricate an answer and instead admits uncertainty, so it is fully faithful to its own stated uncertainty; faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 490, "judge_prompt_tokens": 398, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med", "latency_ms": 968.3501720428467, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Mighty Med' and correctly identifies the TV show described.", "main_total_tokens": 2516, "main_prompt_tokens": 2512, "main_completion_tokens": 4, "judge_total_tokens": 446, "judge_prompt_tokens": 394, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina [Doc 53582].", "latency_ms": 1636.5888118743896, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which country the prison was in; both the gold answer and the generated answer state Argentina. The model response matches the gold exactly and does not introduce unsupported information. Therefore it is fully correct and faithful.", "main_total_tokens": 3168, "main_prompt_tokens": 3160, "main_completion_tokens": 8, "judge_total_tokens": 472, "judge_prompt_tokens": 398, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "Jiang Jie [Doc 135231].", "latency_ms": 1510.876178741455, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the apprentice falls into Jiang Jie's trap, matching the gold answer exactly. The reference tag '[Doc 135231]' does not contradict or modify the answer, so the response is both correct and faithful to the given information.", "main_total_tokens": 2700, "main_prompt_tokens": 2690, "main_completion_tokens": 10, "judge_total_tokens": 474, "judge_prompt_tokens": 392, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "3", "latency_ms": 871.3722229003906, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.7095272044910244, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.7095272044910244, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer per the gold standard is 4, but the generated answer was 3, so the response is incorrect. The evaluation correctly notes the mismatch between gold (4) and generated (3), so it is faithful to the provided information.", "main_total_tokens": 3443, "main_prompt_tokens": 3438, "main_completion_tokens": 5, "judge_total_tokens": 466, "judge_prompt_tokens": 385, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 719.818115234375, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that no movies or shows match the criteria, which aligns with the gold answer that there are no movies with both Bruce Lee and Steven Seagal. It is fully faithful to the question and does not introduce unsupported claims, only generalizes slightly to \"movies or shows\" without changing the meaning.", "main_total_tokens": 2218, "main_prompt_tokens": 2217, "main_completion_tokens": 1, "judge_total_tokens": 500, "judge_prompt_tokens": 406, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3 [Doc 42750]", "latency_ms": 802.971363067627, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (6.3), so it is both correct and faithful to the provided reference.", "main_total_tokens": 2163, "main_prompt_tokens": 2154, "main_completion_tokens": 9, "judge_total_tokens": 449, "judge_prompt_tokens": 393, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "Drift-style racing [Doc 42750].", "latency_ms": 2330.9099674224854, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the style of racing as drift, matching the gold label. However, it adds an unnecessary reference marker '[Doc 42750]' that is not present in the gold answer, making it slightly less faithful to the expected concise output format.", "main_total_tokens": 2879, "main_prompt_tokens": 2856, "main_completion_tokens": 23, "judge_total_tokens": 472, "judge_prompt_tokens": 387, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer", "latency_ms": 1286.6790294647217, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct title of the movie. It is fully correct and faithful to the reference.", "main_total_tokens": 2723, "main_prompt_tokens": 2716, "main_completion_tokens": 7, "judge_total_tokens": 471, "judge_prompt_tokens": 412, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838 [Doc 20603]", "latency_ms": 670.7701683044434, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score value of 7.838 and does not introduce any extraneous or conflicting information, so it is both correct and faithful.", "main_total_tokens": 2328, "main_prompt_tokens": 2319, "main_completion_tokens": 9, "judge_total_tokens": 460, "judge_prompt_tokens": 395, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense", "latency_ms": 902.7848243713379, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie title, staying fully faithful to the provided information.", "main_total_tokens": 2643, "main_prompt_tokens": 2639, "main_completion_tokens": 4, "judge_total_tokens": 448, "judge_prompt_tokens": 394, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002 [Doc 27318].", "latency_ms": 1334.9864482879639, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold year 2002 exactly and does not contradict the provided information.", "main_total_tokens": 2935, "main_prompt_tokens": 2926, "main_completion_tokens": 9, "judge_total_tokens": 439, "judge_prompt_tokens": 389, "judge_completion_tokens": 50}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled", "latency_ms": 1215.9950733184814, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the title of the 2015 reboot of 'The Transporter' as 'The Transporter Refueled', demonstrating both correctness and faithfulness.", "main_total_tokens": 2978, "main_prompt_tokens": 2971, "main_completion_tokens": 7, "judge_total_tokens": 469, "judge_prompt_tokens": 397, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015", "latency_ms": 1873.0838298797607, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2015) exactly, so it is both correct and faithful to the reference.", "main_total_tokens": 2828, "main_prompt_tokens": 2825, "main_completion_tokens": 3, "judge_total_tokens": 442, "judge_prompt_tokens": 387, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "I don't know.", "latency_ms": 600.1927852630615, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold says the answer is that no such movie description can be found. The generated answer 'I don't know.' does not assert a wrong movie and is effectively consistent with the gold (lack of knowledge / no known example). Therefore it is correct and faithful to the gold reference.", "main_total_tokens": 573, "main_prompt_tokens": 572, "main_completion_tokens": 1, "judge_total_tokens": 480, "judge_prompt_tokens": 394, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Garden of Eden", "latency_ms": 1820.0411796569824, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer correctly identifies 'Havana Darkness,' a film explicitly inspired by an unpublished Ernest Hemingway manuscript. The generated answer 'Garden of Eden' is the title of a Hemingway novel that has been adapted into films, but those films do not specifically allude to a manuscript written by Hemingway in the way referenced by the question. Therefore, the generated answer is incorrect relative to the gold and not faithful to the required reference.", "main_total_tokens": 2579, "main_prompt_tokens": 2575, "main_completion_tokens": 4, "judge_total_tokens": 506, "judge_prompt_tokens": 391, "judge_completion_tokens": 115}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 1805.495262145996, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Doll & Em.'", "main_total_tokens": 2663, "main_prompt_tokens": 2648, "main_completion_tokens": 15, "judge_total_tokens": 444, "judge_prompt_tokens": 392, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam Foe", "latency_ms": 944.9613094329834, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2007 movie as 'Hallam Foe.'", "main_total_tokens": 2856, "main_prompt_tokens": 2851, "main_completion_tokens": 5, "judge_total_tokens": 458, "judge_prompt_tokens": 403, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Foe", "latency_ms": 1620.8274364471436, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer, 'Foe', matches the gold answer 'A foe' and correctly answers the question. It is fully faithful to the given gold standard.", "main_total_tokens": 3209, "main_prompt_tokens": 3186, "main_completion_tokens": 23, "judge_total_tokens": 451, "judge_prompt_tokens": 388, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18", "latency_ms": 933.9442253112793, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (18) and correctly reflects the information in the question.", "main_total_tokens": 2574, "main_prompt_tokens": 2572, "main_completion_tokens": 2, "judge_total_tokens": 439, "judge_prompt_tokens": 389, "judge_completion_tokens": 50}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Autism spectrum [Doc 124583].", "latency_ms": 1027.5065898895264, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the 2017 TV show Atypical, Sam is on the autism spectrum, which exactly matches the gold answer without adding contradictory information.", "main_total_tokens": 2396, "main_prompt_tokens": 2386, "main_completion_tokens": 10, "judge_total_tokens": 461, "judge_prompt_tokens": 395, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix [Doc 59659].", "latency_ms": 1133.8236331939697, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly states that Sandra was born in Chamonix, matching the gold answer, so correctness is 1. However, the reference to a specific document ID ([Doc 59659]) is an invented citation not grounded in the provided context, so the faithfulness score is 0.", "main_total_tokens": 3079, "main_prompt_tokens": 3069, "main_completion_tokens": 10, "judge_total_tokens": 497, "judge_prompt_tokens": 406, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481].", "latency_ms": 1535.3844165802002, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and does not introduce any unsupported information. The bracketed document reference does not alter the core answer, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2978, "main_prompt_tokens": 2969, "main_completion_tokens": 9, "judge_total_tokens": 471, "judge_prompt_tokens": 398, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland", "latency_ms": 917.6008701324463, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Greenland'), so it is both correct with respect to the question and fully faithful to the provided reference.", "main_total_tokens": 2888, "main_prompt_tokens": 2885, "main_completion_tokens": 3, "judge_total_tokens": 456, "judge_prompt_tokens": 395, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon", "latency_ms": 1218.6691761016846, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('One Afternoon'), so it is correct and faithful.", "main_total_tokens": 2477, "main_prompt_tokens": 2474, "main_completion_tokens": 3, "judge_total_tokens": 456, "judge_prompt_tokens": 406, "judge_completion_tokens": 50}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The title of the low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" [Doc 93260].", "latency_ms": 1022.2373008728027, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Life Ahead,\" which matches the gold answer exactly. It is also faithful to the provided information and does not introduce hallucinations or contradictions.", "main_total_tokens": 2698, "main_prompt_tokens": 2657, "main_completion_tokens": 41, "judge_total_tokens": 506, "judge_prompt_tokens": 438, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "Bruce McLaren [Doc 73859].", "latency_ms": 1220.0672626495361, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and matches the gold answer without adding unsupported information; the bracketed document reference does not affect factual accuracy or faithfulness.", "main_total_tokens": 2877, "main_prompt_tokens": 2867, "main_completion_tokens": 10, "judge_total_tokens": 465, "judge_prompt_tokens": 393, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren Motor Racing team [Doc 73859].", "latency_ms": 1542.593240737915, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer in substance; the only difference is the omission of the leading article 'The' and the addition of a citation marker, which does not change correctness. It remains fully faithful to the provided information.", "main_total_tokens": 2911, "main_prompt_tokens": 2887, "main_completion_tokens": 24, "judge_total_tokens": 471, "judge_prompt_tokens": 394, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "New Zealand [Doc 73859].", "latency_ms": 1695.2478885650635, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the country as New Zealand, matching the gold answer exactly. The bracketed citation does not change the meaning, so the response is both correct and faithful.", "main_total_tokens": 2889, "main_prompt_tokens": 2869, "main_completion_tokens": 20, "judge_total_tokens": 451, "judge_prompt_tokens": 384, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "None of the provided documents mention a movie where John McClane blows up the Great Wall of China. The 2013 movie \"A Good Day to Die Hard\" involves John McClane traveling to Moscow, not China [Doc 58791].", "latency_ms": 1218.461275100708, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no movie where John McClane blows up the Great Wall of China and accurately identifies that the 2013 film 'A Good Day to Die Hard' is set in Moscow, not China. This matches the information implied by the gold answer (no such movie in the database) and stays grounded in the referenced document, so both correctness and faithfulness are high.", "main_total_tokens": 3085, "main_prompt_tokens": 3033, "main_completion_tokens": 52, "judge_total_tokens": 561, "judge_prompt_tokens": 449, "judge_completion_tokens": 112}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit", "latency_ms": 1202.6808261871338, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2002 Japanese horror film as 'Tomie: Forbidden Fruit'. The response is fully correct and faithful to the provided information.", "main_total_tokens": 2956, "main_prompt_tokens": 2950, "main_completion_tokens": 6, "judge_total_tokens": 491, "judge_prompt_tokens": 422, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 [Doc 98566].", "latency_ms": 1002.7773380279541, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years) and does not introduce any unsupported information beyond a document reference, so it is both correct and faithful.", "main_total_tokens": 3230, "main_prompt_tokens": 3222, "main_completion_tokens": 8, "judge_total_tokens": 465, "judge_prompt_tokens": 402, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King and Tala search for a legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 1340.4414653778076, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that they search for The Book of Souls, matching the gold answer, and does not introduce any contradictions. The added description as a legendary relic is consistent with the film and with the cited document reference.", "main_total_tokens": 3166, "main_prompt_tokens": 3142, "main_completion_tokens": 24, "judge_total_tokens": 493, "judge_prompt_tokens": 417, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 585.3006839752197, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers express a lack of knowledge / data about the movie and do not fabricate a year. This matches the gold answer’s stance and is factually safe, so the response is both correct and faithful.", "main_total_tokens": 527, "main_prompt_tokens": 526, "main_completion_tokens": 1, "judge_total_tokens": 471, "judge_prompt_tokens": 395, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 601.0324954986572, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer convey lack of knowledge about the movie. The generated answer does not hallucinate a release year and stays consistent with the available information, so it is both correct and faithful.", "main_total_tokens": 589, "main_prompt_tokens": 588, "main_completion_tokens": 1, "judge_total_tokens": 466, "judge_prompt_tokens": 393, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 530.4160118103027, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is not in the database, which is equivalent to not knowing the release year. The generated answer, 'I don't know,' correctly reflects this and does not fabricate any information, so it is both correct and faithful.", "main_total_tokens": 550, "main_prompt_tokens": 549, "main_completion_tokens": 1, "judge_total_tokens": 475, "judge_prompt_tokens": 395, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 398.8451957702637, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly expresses a lack of knowledge, which matches the gold response indicating the movie is not in the database. It does not introduce any incorrect facts and is faithful to the information (or lack thereof) provided by the question.", "main_total_tokens": 536, "main_prompt_tokens": 535, "main_completion_tokens": 1, "judge_total_tokens": 473, "judge_prompt_tokens": 395, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year of release for \"Python's Revenge 2\" is not provided in the given documents.", "latency_ms": 1219.2418575286865, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the release year is not available; the generated answer is faithful to the (implied) context that the documents do not contain this information.", "main_total_tokens": 2777, "main_prompt_tokens": 2756, "main_completion_tokens": 21, "judge_total_tokens": 479, "judge_prompt_tokens": 411, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 378.8774013519287, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer and the generated answer are semantically equivalent: both correctly state a lack of knowledge about the movie's release year. This is correct given the database limitation and is faithful to the information provided.", "main_total_tokens": 526, "main_prompt_tokens": 525, "main_completion_tokens": 1, "judge_total_tokens": 465, "judge_prompt_tokens": 393, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 416.1643981933594, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a release year of a movie not in the database. The gold answer explicitly states the movie is not in the database, and the generated answer says 'I don't know.' Both effectively convey lack of knowledge and do not fabricate an answer, so the generated answer is correct and faithful to the gold intent.", "main_total_tokens": 580, "main_prompt_tokens": 579, "main_completion_tokens": 1, "judge_total_tokens": 491, "judge_prompt_tokens": 396, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Days of Lehman Brothers\" is not specified in the provided documents.", "latency_ms": 956.2716484069824, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not in the database. The generated answer states that the director is not specified in the provided documents. Both convey the same limitation: the information is not available in the given sources. This is factually correct relative to the described database context and fully faithful to the constraint that we only use the provided documents.", "main_total_tokens": 3178, "main_prompt_tokens": 3155, "main_completion_tokens": 23, "judge_total_tokens": 520, "judge_prompt_tokens": 419, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 817.3325061798096, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the database does not provide the director information, so not knowing is correct. The generated answer accurately reflects this lack of knowledge without inventing details, and is therefore both correct and faithful to the source limitation.", "main_total_tokens": 553, "main_prompt_tokens": 552, "main_completion_tokens": 1, "judge_total_tokens": 475, "judge_prompt_tokens": 399, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 586.4107608795166, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not available, so the correct response is to admit not knowing. The generated answer explicitly states 'I don't know,' which is both correct and faithful to the provided gold answer.", "main_total_tokens": 564, "main_prompt_tokens": 563, "main_completion_tokens": 1, "judge_total_tokens": 472, "judge_prompt_tokens": 397, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 570.1823234558105, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer admits lack of information due to database limits. The generated answer also expresses not knowing, without fabricating a director. This is correct (no unsupported claim) and faithful to the gold answer’s stance of uncertainty.", "main_total_tokens": 566, "main_prompt_tokens": 565, "main_completion_tokens": 1, "judge_total_tokens": 477, "judge_prompt_tokens": 401, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 1218.6973094940186, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Rise of the Mummy'), so it is both correct and faithful.", "main_total_tokens": 2650, "main_prompt_tokens": 2644, "main_completion_tokens": 6, "judge_total_tokens": 450, "judge_prompt_tokens": 396, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 2042.7837371826172, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Stranger in My Bed' and faithfully reflecting the provided reference.", "main_total_tokens": 2781, "main_prompt_tokens": 2764, "main_completion_tokens": 17, "judge_total_tokens": 459, "judge_prompt_tokens": 400, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 949.6498107910156, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer, 'Game of Silence', matches the gold answer exactly and correctly identifies the work described. The response is both correct and fully faithful to the given information.", "main_total_tokens": 2363, "main_prompt_tokens": 2359, "main_completion_tokens": 4, "judge_total_tokens": 465, "judge_prompt_tokens": 400, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 2189.849615097046, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'The Craigslist Killer.'", "main_total_tokens": 2655, "main_prompt_tokens": 2626, "main_completion_tokens": 29, "judge_total_tokens": 445, "judge_prompt_tokens": 395, "judge_completion_tokens": 50}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 933.9945316314697, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Wood Job!') and correctly identifies the movie based on the described plot, so it is both correct and faithful.", "main_total_tokens": 3020, "main_prompt_tokens": 3016, "main_completion_tokens": 4, "judge_total_tokens": 466, "judge_prompt_tokens": 403, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 1951.9529342651367, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold reference title 'Sesame Street: The Magical Wand Chase' and correctly identifies the movie described in the question. It is both correct and faithful.", "main_total_tokens": 3118, "main_prompt_tokens": 3095, "main_completion_tokens": 23, "judge_total_tokens": 484, "judge_prompt_tokens": 417, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond", "latency_ms": 1208.306074142456, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the movie as 'Beyond'. It is both correct and faithful to the given reference.", "main_total_tokens": 2683, "main_prompt_tokens": 2681, "main_completion_tokens": 2, "judge_total_tokens": 468, "judge_prompt_tokens": 409, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9 [Doc 22478].", "latency_ms": 855.1795482635498, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', exactly matching the gold answer. The added document reference '[Doc 22478]' does not change the substantive answer, so it remains fully correct and faithful to the gold.", "main_total_tokens": 2603, "main_prompt_tokens": 2590, "main_completion_tokens": 13, "judge_total_tokens": 496, "judge_prompt_tokens": 415, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread [Doc 32410].", "latency_ms": 1134.0456008911133, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'Club Dread', so correctness is 1. However, it adds an unsupported artifact '[Doc 32410]' that is not present in the gold answer or the question and appears to reference a non-existent or extraneous document, which reduces faithfulness to 0 under strict evaluation.", "main_total_tokens": 2833, "main_prompt_tokens": 2823, "main_completion_tokens": 10, "judge_total_tokens": 530, "judge_prompt_tokens": 433, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet", "latency_ms": 1574.5463371276855, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Toilet'), which is the correct movie title for the described plot. Therefore it is both correct and faithful to the reference.", "main_total_tokens": 3354, "main_prompt_tokens": 3351, "main_completion_tokens": 3, "judge_total_tokens": 480, "judge_prompt_tokens": 415, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel", "latency_ms": 1064.5668506622314, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Electric Angel'). The question is simply asking for the movie title and both gold and generated responses are identical, so the response is correct and faithful.", "main_total_tokens": 2924, "main_prompt_tokens": 2921, "main_completion_tokens": 3, "judge_total_tokens": 487, "judge_prompt_tokens": 418, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "Doomsday Stories [Doc 108366]", "latency_ms": 851.7704010009766, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer does not match the gold answer. The correct lowest-rated 2023 movie per the gold is 'Mega Ape', but the model answered 'Doomsday Stories [Doc 108366]'. This is factually incorrect in this evaluation context and not faithful to the provided gold label.", "main_total_tokens": 2685, "main_prompt_tokens": 2673, "main_completion_tokens": 12, "judge_total_tokens": 483, "judge_prompt_tokens": 391, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 292.5257682800293, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer according to the gold is 'Tea House'. The generated answer says 'I don't know', which does not match the gold, so it is factually incorrect for the task. However, the model does not hallucinate an incorrect show and instead admits uncertainty, so it is fully faithful to its own stated knowledge limits.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 386, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 302.83284187316895, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct movie title ('The Uncut Story of Mewtwo's Origin'), so it is incorrect. However, it does not introduce any false information; it simply expresses a lack of knowledge, so it is faithful to the (lack of) reasoning rather than hallucinating an answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 496, "judge_prompt_tokens": 395, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "es", "latency_ms": 1888.6306285858154, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer ('es') does not match the gold answer ('English'), so it is factually incorrect. There is no indication it followed any supporting context or reasoning, so it is also unfaithful.", "main_total_tokens": 2644, "main_prompt_tokens": 2642, "main_completion_tokens": 2, "judge_total_tokens": 457, "judge_prompt_tokens": 384, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was \"The Sopranos\" with a popularity score of 59.4284 [Doc 115146].", "latency_ms": 1208.7366580963135, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the TV show is 'Law & Order: Special Victims Unit', but the generated answer claims 'The Sopranos' was the most popular. This directly contradicts the gold answer, so it is factually incorrect relative to the task. It also references a specific popularity score and document ID that do not align with the gold answer, indicating that the reasoning is not faithful to the required answer or context.", "main_total_tokens": 3044, "main_prompt_tokens": 3009, "main_completion_tokens": 35, "judge_total_tokens": 542, "judge_prompt_tokens": 424, "judge_completion_tokens": 118}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2 [Doc 79355].", "latency_ms": 650.1579284667969, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 and does not introduce extraneous or conflicting information. It is correct and faithful to the provided gold answer.", "main_total_tokens": 2355, "main_prompt_tokens": 2346, "main_completion_tokens": 9, "judge_total_tokens": 457, "judge_prompt_tokens": 392, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "1999 [Doc 22481].", "latency_ms": 1572.0932483673096, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered 1999, but according to the provided gold answer the correct year is 2019, so the response is factually incorrect with respect to the task. However, the answer is faithful to the cited document [Doc 22481], so faithfulness is high even though the document itself appears to conflict with the gold.", "main_total_tokens": 2722, "main_prompt_tokens": 2713, "main_completion_tokens": 9, "judge_total_tokens": 492, "judge_prompt_tokens": 393, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 407.17315673828125, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer specifies that 'My Dog Skip' received 279 votes. The generated answer claims it cannot find any matching movies or shows, which is factually incorrect relative to the gold and does not attempt to answer the numeric question. Thus, it is both incorrect and unfaithful to the reference answer.", "main_total_tokens": 2189, "main_prompt_tokens": 2188, "main_completion_tokens": 1, "judge_total_tokens": 485, "judge_prompt_tokens": 392, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 893.3138847351074, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer clearly states that 'Oh My God!' received 12 votes. The generated response instead claims it cannot find any movies or shows matching the criteria, which directly contradicts the provided gold information. Therefore, it is both factually incorrect relative to the gold and unfaithful to the given context.", "main_total_tokens": 2229, "main_prompt_tokens": 2228, "main_completion_tokens": 1, "judge_total_tokens": 485, "judge_prompt_tokens": 392, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "2009", "latency_ms": 1794.6369647979736, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2009) exactly, so it is both correct and faithful.", "main_total_tokens": 2851, "main_prompt_tokens": 2848, "main_completion_tokens": 3, "judge_total_tokens": 442, "judge_prompt_tokens": 390, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en", "latency_ms": 1017.5788402557373, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The original language of the movie \"The King's Speech\" is English, which matches both the gold answer and the generated answer.", "main_total_tokens": 3123, "main_prompt_tokens": 3121, "main_completion_tokens": 2, "judge_total_tokens": 440, "judge_prompt_tokens": 384, "judge_completion_tokens": 56}
