{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The document does not contain information about a movie titled \"Wall Street\". However, there are several documents that mention \"Wall\" in their title, such as \"Wallay\", \"Walls\", and \"Wall to Wall\". Additionally, there is a TV series titled \"Gaming Wall St\" released in 2022.", "latency_ms": 16111.701250076294, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1987, but the generated answer does not provide this and instead claims the document lacks the information, so it is incorrect. However, it remains faithful to the (assumed) provided context by not hallucinating the year and only referencing items it says are in the document.", "main_total_tokens": 3044, "main_prompt_tokens": 2978, "main_completion_tokens": 66, "judge_total_tokens": 532, "judge_prompt_tokens": 441, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "According to the provided context, several movies feature classmates playing a survival game using their cell phones. Some examples include:\n\n* \"Naraka 19\" (2007) [Doc 41254]: A group of students are trapped in a terrifying mobile phone game from hell.\n* \"Signal 100\" (2020) [Doc 93745]: A group of high school students are forced into a game where they are hypnotized to commit suicide on an unknown command, using their cell phones.\n* \"6 Plots\" (2012) [Doc 56109]: A group of tech-savvy teens stream their wild party online, and when one of them wakes up, she's all alone, with only her phone to communicate with her friends who are trapped in separate boxes.\n\nThese movies feature classmates playing a survival game using their cell phones, but the specific details and plot vary between each movie.", "latency_ms": 22515.49530029297, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a single specific movie: 'Brain Fluid Explosion Girl.' The generated answer instead lists several different movies and does not mention the correct one, so it is factually incorrect. It is also unfaithful to the gold answer because it neither identifies nor aligns with the required specific title.", "main_total_tokens": 2850, "main_prompt_tokens": 2665, "main_completion_tokens": 185, "judge_total_tokens": 660, "judge_prompt_tokens": 569, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "According to [Doc 112849], the answer is \"Dungeons & Dragons\".", "latency_ms": 6243.663549423218, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the TV show in which six friends rely on a Dungeon Master to get home. The correct answer is indeed the animated TV series 'Dungeons & Dragons'. The generated answer matches the gold answer exactly and does not introduce any unsupported information, so it is both correct and faithful.", "main_total_tokens": 2477, "main_prompt_tokens": 2459, "main_completion_tokens": 18, "judge_total_tokens": 497, "judge_prompt_tokens": 407, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Not found.", "latency_ms": 5557.2710037231445, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer identifies the movie as 'Nacht der Wölfe', while the generated answer incorrectly says it is not found. This is factually wrong. However, the model does not invent an alternative answer; it simply admits not finding it, so it remains faithful to its own (claimed) uncertainty.", "main_total_tokens": 2994, "main_prompt_tokens": 2990, "main_completion_tokens": 4, "judge_total_tokens": 494, "judge_prompt_tokens": 401, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "According to [Doc 126191], the events of \"Picnic at Hanging Rock\" take place in the year 1900.", "latency_ms": 7446.152210235596, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of 'Picnic at Hanging Rock' take place in 1900, matching the gold answer. It explicitly attributes this information to a document, aligning with the provided context, so it is both correct and faithful.", "main_total_tokens": 2741, "main_prompt_tokens": 2713, "main_completion_tokens": 28, "judge_total_tokens": 494, "judge_prompt_tokens": 412, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "None of the provided documents mention a 2004 movie with this plot.", "latency_ms": 6704.0886878967285, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question clearly points to the 2004 movie \"June\" (the protagonist is a violinist named June who works in a coffee shop), so the correct answer is \"June.\" The generated answer incorrectly claims none of the documents mention such a movie instead of providing the answer, making it factually incorrect. However, it remains faithful to the (implied) retrieval results it describes, so faithfulness is high while correctness is low.", "main_total_tokens": 3006, "main_prompt_tokens": 2990, "main_completion_tokens": 16, "judge_total_tokens": 566, "judge_prompt_tokens": 447, "judge_completion_tokens": 119}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "According to the provided context, the highest rated Russian-language movie released in 1980 is \"Moscow Does Not Believe in Tears\" with a rating of 7.6 [Doc 61].", "latency_ms": 8621.067523956299, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the highest rated Russian-language movie released in 1980 is \"The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger,\" while the generated answer claims it is \"Moscow Does Not Believe in Tears\". This directly contradicts the gold, so the correctness is 0. The generated answer explicitly relies on a cited document [Doc 61] and says \"according to the provided context,\" but that context is inconsistent with the gold answer, so the response is not faithful to the required ground truth in this evaluation setting, yielding a faithfulness score of 0.", "main_total_tokens": 2734, "main_prompt_tokens": 2693, "main_completion_tokens": 41, "judge_total_tokens": 587, "judge_prompt_tokens": 435, "judge_completion_tokens": 152}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "According to the provided context, the highest Russian-language movie rating in 1999 is 7.117 for \"The Rifleman of the Voroshilov Regiment\" ([Doc 22123]).", "latency_ms": 8832.80897140503, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the highest Russian-language movie rating in 1999 is 7.6, but the generated answer claims it is 7.117 for a specific film, which is numerically incorrect. Additionally, the generated answer references a document and context that are not present in the prompt, so it is not faithful to the given information.", "main_total_tokens": 2822, "main_prompt_tokens": 2781, "main_completion_tokens": 41, "judge_total_tokens": 526, "judge_prompt_tokens": 425, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "According to the provided context, there are two Chinese movies released in 1987 with a rating over 7:\n\n1. A Chinese Ghost Story (Rating: 7.3)\n2. Last Eunuch In China (Rating: 7.8)", "latency_ms": 10174.69573020935, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there are 4 such movies, but the generated answer claims there are only 2, which is numerically incorrect. It also asserts that the answer is based on the provided context, but that context is not shown and contradicts the gold label, so the reasoning is not faithful to the ground truth.", "main_total_tokens": 2685, "main_prompt_tokens": 2631, "main_completion_tokens": 54, "judge_total_tokens": 540, "judge_prompt_tokens": 444, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "Based on the provided context, there is no information about any Chinese TV shows from 1987 with a rating over 7. However, there is one Chinese movie titled \"Last Eunuch In China\" (Doc 7754) that was released in 1987 and has a rating of 7.8.\n\nAdditionally, there are two Chinese movies: \"A Chinese Ghost Story\" (Doc 7064) with a rating of 7.3 and no TV shows mentioned in the context.", "latency_ms": 14859.90285873413, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer is 2, but the generated answer claims there are no such TV shows and instead discusses movies, so the numerical result is incorrect. It is partially faithful to the (implied) context by correctly referencing specific movies and their attributes, but it fails to address the actual query about TV shows and incorrectly infers that none exist based on incomplete information.", "main_total_tokens": 3135, "main_prompt_tokens": 3032, "main_completion_tokens": 103, "judge_total_tokens": 600, "judge_prompt_tokens": 494, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "According to the provided context, there are multiple French language TV shows released in 1980, not just one. However, I can provide you with a list of French language TV shows mentioned:\n\n* Dom (Doc 112605)\n* Pé de Vento is not exclusively French but has some French influence and was also released in 1980.\n* Królowa Bona is not exclusively French either.\n\nHowever, the only exclusively French language TV show I could find from the provided context is \"Dom\" (Doc 112605).", "latency_ms": 15033.024072647095, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the only French language TV show released in 1980, with the gold answer being 'Fantômas'. The generated answer instead claims there are multiple shows and then concludes that 'Dom' is the only exclusively French language TV show from the context, which contradicts the gold answer. This is factually incorrect given the gold. It is also unfaithful to the labeled correct answer, as it does not mention 'Fantômas' at all and introduces other shows instead.", "main_total_tokens": 2640, "main_prompt_tokens": 2530, "main_completion_tokens": 110, "judge_total_tokens": 623, "judge_prompt_tokens": 492, "judge_completion_tokens": 131}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "According to [Doc 11205], the movie \"Kill Cruise\" (1990) is about two British beauties going to Barbados with a yacht captain.", "latency_ms": 7933.63618850708, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' matching the gold answer. It does not introduce contradictions or unsupported claims beyond citing a document, so it is both correct and faithful.", "main_total_tokens": 2691, "main_prompt_tokens": 2657, "main_completion_tokens": 34, "judge_total_tokens": 485, "judge_prompt_tokens": 416, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 5805.976867675781, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct title of the movie about Fred Rogers and his TV series. It is both correct and fully faithful to the provided reference.", "main_total_tokens": 2965, "main_prompt_tokens": 2957, "main_completion_tokens": 8, "judge_total_tokens": 476, "judge_prompt_tokens": 408, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "1989", "latency_ms": 5197.309732437134, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer ('1989') does not match the gold answer ('2018'), so it is factually incorrect. It is also unfaithful to the reference, which clearly states the correct year as 2018.", "main_total_tokens": 2743, "main_prompt_tokens": 2740, "main_completion_tokens": 3, "judge_total_tokens": 456, "judge_prompt_tokens": 380, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Not found.", "latency_ms": 4952.109813690186, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'Astro Boy', but the generated answer was 'Not found', which is incorrect. It also does not reflect or utilize the information given in the question or the gold answer, so it is unfaithful.", "main_total_tokens": 2543, "main_prompt_tokens": 2539, "main_completion_tokens": 4, "judge_total_tokens": 468, "judge_prompt_tokens": 390, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "According to [Doc 67513], the woman and her young son were held captive for 7 years.", "latency_ms": 6493.65496635437, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, matching the gold answer. It accurately reflects the provided information without adding or contradicting any details.", "main_total_tokens": 2344, "main_prompt_tokens": 2321, "main_completion_tokens": 23, "judge_total_tokens": 485, "judge_prompt_tokens": 416, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m", "latency_ms": 5597.155570983887, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer (£26m) matches the provided gold answer (£26m), so it is both correct and faithful to the reference.", "main_total_tokens": 3060, "main_prompt_tokens": 3056, "main_completion_tokens": 4, "judge_total_tokens": 456, "judge_prompt_tokens": 398, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "According to the provided context, the most popular English-speaking TV show released in 1997 is \"The View\" with a popularity score of 14.1768 [Doc 114715].", "latency_ms": 8400.036334991455, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer gives the popularity score as 80.5772, while the generated answer claims the most popular English-speaking TV show in 1997 is \"The View\" with a popularity score of 14.1768. This directly contradicts the gold value, so the correctness is 0. However, the generated answer clearly cites and follows the provided context ([Doc 114715]) without inventing extra details, so it is faithful to the given evidence, yielding a faithfulness score of 1.", "main_total_tokens": 2599, "main_prompt_tokens": 2559, "main_completion_tokens": 40, "judge_total_tokens": 565, "judge_prompt_tokens": 431, "judge_completion_tokens": 134}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "None, according to the provided context. There are no Russian language TV shows mentioned in the given documents.", "latency_ms": 7077.536821365356, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the number of Russian language TV shows released in 1997 and the gold answer is 1. The generated answer claims that none are mentioned and effectively answers that the number is unknown/none, which contradicts the gold answer, so it is incorrect. However, the generated answer is fully consistent with the provided context (stating that the documents do not mention any), so it is faithful to the context.", "main_total_tokens": 2648, "main_prompt_tokens": 2626, "main_completion_tokens": 22, "judge_total_tokens": 521, "judge_prompt_tokens": 404, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes [Doc 121928]", "latency_ms": 6554.152250289917, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer, 'Angel Eyes', correctly matches the gold answer and directly addresses the question about the TV show. It is fully faithful to the provided information with no additions or contradictions.", "main_total_tokens": 3493, "main_prompt_tokens": 3484, "main_completion_tokens": 9, "judge_total_tokens": 471, "judge_prompt_tokens": 403, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "Movie [Doc 67258] has Spanish (es) as its language.", "latency_ms": 6171.1976528167725, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold label says the language is Korean, but the generated answer says Spanish, which is factually incorrect, so correctness is 0. The answer is internally consistent and directly addresses the question without contradiction or hallucinated context, so faithfulness to its own reasoning is 1.", "main_total_tokens": 2551, "main_prompt_tokens": 2534, "main_completion_tokens": 17, "judge_total_tokens": 479, "judge_prompt_tokens": 392, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "Based on the provided context, there are two English-speaking Horror movies released in 2009 with a popularity score greater than 0.7:\n\n1. Friday the 13th (Popularity: 4.9425)\n2. Triangle (Popularity: 4.9214)\n\nNote that the popularity scores are not necessarily greater than 7, but rather greater than 0.7.", "latency_ms": 12670.904397964478, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question asks for the number of English-speaking horror movies from 2009 with popularity greater than 7, with a gold answer of 2. The generated answer incorrectly interprets the threshold as 0.7 instead of 7 and lists movies whose popularity is below 7, so the numerical result and reasoning are incorrect. However, the answer does state there are two movies, matching the gold count by coincidence, but the justification conflicts with the stated condition. It also claims to use the provided context but changes the threshold, making it not fully faithful to the question. Thus correctness is 0 and faithfulness is partial.", "main_total_tokens": 2859, "main_prompt_tokens": 2777, "main_completion_tokens": 82, "judge_total_tokens": 630, "judge_prompt_tokens": 471, "judge_completion_tokens": 159}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "According to the provided context, [Doc 882] Board and Care is mentioned as being released in 1979 and winning an Oscar at the 52nd Academy Awards in 1980 for Best Short Subject. However, it does not explicitly state that it was the best-rated movie of 1979.\n\nThat being said, among the provided documents, [Doc 882] Board and Care has a rating of 6.333, but another document, [Doc 710], Confidence, is mentioned as having been released in 1980 with a rating of 7.6. However, it does not provide information about movies released specifically in 1979.\n\nIf we consider only the provided context, there isn't enough information to determine the best-rated movie released in 1979.", "latency_ms": 20457.68165588379, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the database has no movies from 1979. The generated answer instead analyzes specific movies and ratings, contradicting the gold and introducing unsupported details from outside the described database. It should have mirrored the gold response and said there are no 1979 movies in the database, so it is both incorrect and unfaithful to the gold.", "main_total_tokens": 3033, "main_prompt_tokens": 2873, "main_completion_tokens": 160, "judge_total_tokens": 655, "judge_prompt_tokens": 553, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "There is no information about a movie titled \"Back to the Future Part IV\" in the provided context. The last installment mentioned is \"Back to the Future Part III\", and there's no mention of a fourth installment.", "latency_ms": 9847.511768341064, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.8, "faithfulness_score": 1, "justification": "The generated answer correctly indicates that there is no information about 'Back to the Future Part IV' and implies that a fourth installment is not present, which aligns with reality and the gold answer. However, it does not directly say that it does not exist in its database or that no such movie exists, only that there is no information in the provided context. This slight indirectness makes it somewhat less precise for the question as posed, so correctness is slightly reduced. Faithfulness is full since it stays within the given context and does not hallucinate a release year.", "main_total_tokens": 3374, "main_prompt_tokens": 3329, "main_completion_tokens": 45, "judge_total_tokens": 588, "judge_prompt_tokens": 443, "judge_completion_tokens": 145}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "There is no mention of a movie about the Chernobyl disaster in the provided context. The Chernobyl disaster occurred in 1986, not in 1980.", "latency_ms": 8457.34429359436, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly notes that Chernobyl occurred in 1986, making a 1980 movie about it impossible, which aligns with the gold answer stating no such movies were found. It stays faithful to the question and the implied context about the disaster’s date, without adding unsupported details.", "main_total_tokens": 2970, "main_prompt_tokens": 2934, "main_completion_tokens": 36, "judge_total_tokens": 531, "judge_prompt_tokens": 441, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "According to the provided context, there are three instances with \"Astro Boy\" in the title:\n\n1. [Doc 116265] Title: Astro Boy (TV, 2003)\n2. [Doc 112591] Title: Astro Boy (TV, 1980)\n3. [Doc 44947] Title: Astro Boy (Movie, 2009)\n\nSo, there are three TV shows or movies with \"Astro Boy\" in the title.", "latency_ms": 14176.394701004028, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.8318724637288826, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.8318724637288826, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 4 titles, but the generated answer concludes there are 3, so it is factually incorrect with respect to the gold. However, it is faithful to the provided context it cites, correctly listing three instances from the docs and drawing a consistent (though incomplete) conclusion from them.", "main_total_tokens": 2885, "main_prompt_tokens": 2788, "main_completion_tokens": 97, "judge_total_tokens": 576, "judge_prompt_tokens": 482, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "Based on the provided context, I found one movie that alludes to the Holocaust:\n\n* [Doc 111684] Title: Dead to Rights\nType: Movie\nYear: 2025\nRating: 8.5\nPopularity: 8.6535\nLanguage: zh\nPlot: Title: Dead to Rights\nYear: 2025\nType: Movie\nGenres: ['Drama', 'War', 'History']\nRating: 8.5\nDescription: The story is based on real documented evidence of the atrocities committed by the Japanese army during the Nanjing Massacre.\n\nHowever, I also found another movie that might be related to the Holocaust:\n\n* [Doc 111702] Title: Day of Reckoning\nType: Movie\nYear: 2025\nRating: 6.409\nPopularity: 7.4402\nLanguage: en\nPlot: Title: Day of Reckoning\nYear: 2025\nType: Movie\nGenres: ['Western', 'Action']\nRating: 6.409\nDescription: Put-upon lawman John Dorsey is on the verge of losing his wife and his job as sheriff, so he posses up with bullish U.S. Marshall Butch Hayden to", "latency_ms": 29315.714359283447, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a numeric count of movies in 2025 that allude to the Holocaust, with the gold answer being 2. The generated answer identifies one definite film about the Nanjing Massacre and a second movie that \"might be related\" but does not clearly allude to the Holocaust, and never provides a numeric count. Thus it is incorrect relative to the gold label. It is also unfaithful to the question because it does not directly answer with a number and introduces speculative reasoning about the second film without clear evidence of a Holocaust allusion.", "main_total_tokens": 2944, "main_prompt_tokens": 2688, "main_completion_tokens": 256, "judge_total_tokens": 782, "judge_prompt_tokens": 637, "judge_completion_tokens": 145}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "According to the provided context, there are three TV shows related to Super Mario Bros:\n\n1. The Super Mario Bros. Super Show! (1989)\n2. The Adventures of Super Mario Bros. 3 (1990)\n3. Super Mario World (1991)\n\nNote that while The Super Mario Bros. Super Show! was released in 1989, it is often associated with the 1990s due to its airing during this time period.\n\nAdditionally, there is a movie titled \"Super Mario Bros.\" released in 1993, but it's not a TV show.", "latency_ms": 17199.62978363037, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question asks specifically for how many TV shows were released in the 1990s; the gold answer is 1. The generated answer lists three shows and never states the correct count, and it includes a 1989 show as if it were 1990s-related, so the numerical answer is incorrect. It is partially faithful to the (hypothetical) context by correctly naming Super Mario-related shows and their years, but it does not align with the gold label or the exact query requirement about 1990s releases.", "main_total_tokens": 3408, "main_prompt_tokens": 3289, "main_completion_tokens": 119, "judge_total_tokens": 644, "judge_prompt_tokens": 504, "judge_completion_tokens": 140}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "None, according to the provided context. However, there is a mention of \"Marsupilami\" which is an animated series based on the original Marsupilami comics created by André Franquin, but it's not related to Super Mario Bros.\n\nIf you're looking for information about TV shows or other media related to Super Mario Bros., I'd be happy to try and help you with that!", "latency_ms": 12858.654499053955, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is the numeric value 0, but the generated answer says 'None' and then digresses about 'Marsupilami', which is irrelevant. It does not clearly state the answer as 0, so it is incorrect with respect to the gold. It is also unfaithful because it introduces extra content about a different animated series not grounded in the question or needed for the answer.", "main_total_tokens": 3047, "main_prompt_tokens": 2966, "main_completion_tokens": 81, "judge_total_tokens": 576, "judge_prompt_tokens": 465, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "According to the provided context, the most popular TV show or movie about ninjas is \"NARUTO Spin-Off: Rock Lee & His Ninja Pals\" with a popularity rating of 10.7994 [Doc 120321].", "latency_ms": 9924.172163009644, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'Naruto' as the most popular ninja TV show. The generated answer instead claims that 'NARUTO Spin-Off: Rock Lee & His Ninja Pals' is the most popular, which contradicts the gold reference and is therefore incorrect. It also incorrectly attributes this conclusion to the provided context, which, in the evaluation setting, is not what determines correctness here. Thus the answer is neither correct relative to the gold nor faithful to the intended answer.", "main_total_tokens": 3031, "main_prompt_tokens": 2981, "main_completion_tokens": 50, "judge_total_tokens": 556, "judge_prompt_tokens": 430, "judge_completion_tokens": 126}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "There is no mention of a Jurassic Park/Power Rangers crossover movie in the provided context. The documents only list separate movies for Jurassic Park and Power Rangers, with no indication of a crossover film.", "latency_ms": 8761.378526687622, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie was not released. The generated answer correctly indicates that there is no mention or indication of such a crossover film, which aligns with the fact that it was not released. It does not introduce contradictory information and stays consistent with the provided context, so it is both correct and faithful.", "main_total_tokens": 2859, "main_prompt_tokens": 2819, "main_completion_tokens": 40, "judge_total_tokens": 517, "judge_prompt_tokens": 424, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "There is no information in the provided context about a movie where Barbie fights the Predator. However, there is a mention of \"Killer Barbys vs. Dracula\" [Doc 28703], but it does not involve the Predator.", "latency_ms": 9888.317346572876, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The gold answer states that the movie was not released, implying such a movie exists conceptually but was unreleased. The generated answer instead claims there is no information in the context about such a movie, and then references an unrelated film. It does correctly convey that there is no Barbie vs. Predator movie described in the context, but it fails to align with the gold explanation that the movie (presumably planned) was not released. Thus, correctness is high regarding factual outcome (no such released movie), but faithfulness to the gold answer is only partial.", "main_total_tokens": 3126, "main_prompt_tokens": 3077, "main_completion_tokens": 49, "judge_total_tokens": 573, "judge_prompt_tokens": 429, "judge_completion_tokens": 144}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "According to the provided context, here are the Star Wars movies in chronological order:\n\n1. [Doc 15245] Title: Stargate (1994) - Not a Star Wars movie\n2. [Doc 3986] Title: The Last Starfighter (1984)\n3. [Doc 3935] Title: Starman (1984)\n4. [Doc 5908] Title: Star Trek IV: The Voyage Home (1986)\n\nHowever, if we consider only the movies with \"Star Wars\" or \"Star\" in their title:\n\n1. [Doc 3986] Title: The Last Starfighter (1984)\n2. [Doc 3935] Title: Starman (1984)\n3. [Doc 5908] Title: Star Trek IV: The Voyage Home (1986) - Not a Star Wars movie\n4. [Doc 76818] Title: Star Wars: The Last Jedi (2017)\n\nSo, the 4th oldest Star Wars movie is not present in the provided context. However, if we consider all movies with \"Star\" in their title, the answer would be:\n\n[Doc 5908] Title: Star Trek IV: The Voyage Home (1986)", "latency_ms": 30175.501823425293, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the 4th oldest Star Wars movie, with the gold answer being 'Star Wars: Droids - The Great Heep'. The generated answer instead discusses several non–Star Wars films and ends up naming 'Star Trek IV: The Voyage Home', which is incorrect. It also misinterprets the task by including any movie with 'Star' in the title and explicitly concludes that the 4th oldest Star Wars movie is not present in the context, contradicting the gold. Therefore, the response is neither correct nor faithful to the required answer.", "main_total_tokens": 3385, "main_prompt_tokens": 3129, "main_completion_tokens": 256, "judge_total_tokens": 788, "judge_prompt_tokens": 643, "judge_completion_tokens": 145}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "1989", "latency_ms": 5190.9339427948, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer ('1989') does not match the gold answer ('1986'), so it is factually incorrect, resulting in a correctness score of 0. However, the answer directly addresses the question asked, with no additional fabricated details, so it is fully faithful to the prompt, giving a faithfulness score of 1.", "main_total_tokens": 2743, "main_prompt_tokens": 2740, "main_completion_tokens": 3, "judge_total_tokens": 478, "judge_prompt_tokens": 380, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "Not found in the provided context.", "latency_ms": 5643.548488616943, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the numeric rating (8.466), but the generated answer says it is not found instead of giving the rating, so it is incorrect. However, given the assistant only referenced the provided context and did not invent information, the answer is faithful to the context even though it is incomplete.", "main_total_tokens": 2887, "main_prompt_tokens": 2879, "main_completion_tokens": 8, "judge_total_tokens": 493, "judge_prompt_tokens": 401, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med [Doc 121125]", "latency_ms": 5201.49040222168, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Mighty Med') and directly responds to the question without adding unsupported information. The bracketed doc reference does not change the correctness or faithfulness of the core answer.", "main_total_tokens": 2225, "main_prompt_tokens": 2214, "main_completion_tokens": 11, "judge_total_tokens": 473, "judge_prompt_tokens": 400, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina", "latency_ms": 5181.568622589111, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (Argentina) and correctly identifies the country of the prison in the film 'Túnel de los huesos (2011)'.", "main_total_tokens": 2807, "main_prompt_tokens": 2805, "main_completion_tokens": 2, "judge_total_tokens": 458, "judge_prompt_tokens": 392, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "There is no information about a movie called \"Sweet Trap\" in the provided context, so I couldn't find any information about it.", "latency_ms": 7455.2648067474365, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly states the apprentice falls into Jiang Jie's trap. The generated answer claims no information is available instead of providing Jiang Jie, so it is factually incorrect. However, it does not invent specific false details about the plot; it only reports inability to answer, so it remains faithful to the (lack of) provided context.", "main_total_tokens": 2761, "main_prompt_tokens": 2733, "main_completion_tokens": 28, "judge_total_tokens": 509, "judge_prompt_tokens": 410, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "Based on the provided context, there are two Korean-language movies that mention assassins:\n\n1. [Doc 106219] Title: The Assassin (2023)\n2. [Doc 105198] Title: Kill Boksoon (2023)\n\nBoth of these movies were released in 2023 or later, and they both mention assassins.", "latency_ms": 11366.804122924805, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says 4, but the generated answer concludes there are 2. This is numerically incorrect relative to the gold, so correctness is 0. However, the assistant clearly explains its reasoning based only on the provided context (two specific documents), without inventing unseen information, so faithfulness to the given context is high.", "main_total_tokens": 2626, "main_prompt_tokens": 2554, "main_completion_tokens": 72, "judge_total_tokens": 552, "judge_prompt_tokens": 453, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "According to the provided context, there is no mention of Steven Seagal's filmography. However, I can tell you that according to outside knowledge, Steven Seagal has appeared in over 80 films.\n\nAs for Bruce Lee, within the provided context, there are multiple documentaries and biographical films about him, but if we're counting films where he was an actor, here are a few mentioned:\n\n* [Doc 46033] mentions no acting credits.\n* [Doc 56669], [Doc 423], and [Doc 49705] mention Bruce Lee as the main subject or actor.\n\nOutside of this context, according to outside knowledge, Bruce Lee acted in around 10-15 films during his lifetime.", "latency_ms": 19593.085289001465, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The user question asks how many movies feature both Steven Seagal and Bruce Lee together; the gold answer correctly says there are none. The generated answer instead interprets the question as asking for the number of movies each actor has appeared in individually, gives approximate counts using outside knowledge, and does not address the actual joint-film question. This makes the answer incorrect relative to the task. It is also unfaithful to the provided context: the gold/context states there are no movies with both actors, but the generated response ignores that and introduces external statistics about their separate filmographies, which are not supported by the given context.", "main_total_tokens": 3455, "main_prompt_tokens": 3309, "main_completion_tokens": 146, "judge_total_tokens": 691, "judge_prompt_tokens": 537, "judge_completion_tokens": 154}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3 [Doc 42750]", "latency_ms": 5803.652048110962, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (6.3) and stays within the information provided, so it is both correct and faithful.", "main_total_tokens": 2835, "main_prompt_tokens": 2825, "main_completion_tokens": 10, "judge_total_tokens": 452, "judge_prompt_tokens": 393, "judge_completion_tokens": 59}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The context does not explicitly state what style of racing is featured in the movie \"Fast Track: No Limits\". However, based on the plot description, it appears to be street racing.", "latency_ms": 8744.840860366821, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the style of racing as 'Drift'. The generated answer claims the movie appears to feature street racing and that the context does not specify the style, which directly contradicts the gold. Thus it is factually incorrect relative to the gold. However, it is faithful to the (stated) context it references, as it admits the context does not explicitly state the style and infers street racing from plot description, without fabricating details beyond that.", "main_total_tokens": 3042, "main_prompt_tokens": 3004, "main_completion_tokens": 38, "judge_total_tokens": 540, "judge_prompt_tokens": 415, "judge_completion_tokens": 125}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer", "latency_ms": 5396.1920738220215, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct title of the movie about Jillian Lauren and Samuel Little. It is both correct and faithful to the provided gold reference.", "main_total_tokens": 2675, "main_prompt_tokens": 2668, "main_completion_tokens": 7, "judge_total_tokens": 480, "judge_prompt_tokens": 412, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838", "latency_ms": 5615.205764770508, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838), indicating it is both correct and faithful to the reference.", "main_total_tokens": 3064, "main_prompt_tokens": 3060, "main_completion_tokens": 4, "judge_total_tokens": 445, "judge_prompt_tokens": 389, "judge_completion_tokens": 56}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense", "latency_ms": 5241.492509841919, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 1999 psychological thriller as 'The Sixth Sense' and is fully supported by the given information.", "main_total_tokens": 2784, "main_prompt_tokens": 2780, "main_completion_tokens": 4, "judge_total_tokens": 457, "judge_prompt_tokens": 394, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002", "latency_ms": 5399.1858959198, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer '2002' exactly matches the gold answer '2002', so it is both correct and faithful to the reference.", "main_total_tokens": 2981, "main_prompt_tokens": 2978, "main_completion_tokens": 3, "judge_total_tokens": 441, "judge_prompt_tokens": 383, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "According to [Doc 67500], the title of the 2015 movie is \"The Transporter Refueled\".", "latency_ms": 7631.32381439209, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the 2015 reboot is titled 'The Transporter Refueled', which matches the gold answer. It also attributes the information to a document without contradicting any provided context, so it is faithful.", "main_total_tokens": 3070, "main_prompt_tokens": 3044, "main_completion_tokens": 26, "judge_total_tokens": 489, "judge_prompt_tokens": 416, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "[Doc 67500] states that the movie \"The Transporter Refueled\" was released in 2015.", "latency_ms": 7485.171794891357, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, which matches the gold answer. It also clearly attributes this information to a specific document ([Doc 67500]), showing faithfulness to the cited source.", "main_total_tokens": 3052, "main_prompt_tokens": 3026, "main_completion_tokens": 26, "judge_total_tokens": 491, "judge_prompt_tokens": 409, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "Not found [Doc 101219], [Doc 98660], [Doc 98901], [Doc 98854], [Doc 103795], [Doc 105650], [Doc 102834], [Doc 96765], [Doc 103368], [Doc 105172], [Doc 100602], [Doc 96693], [Doc 103453], [Doc 105172], [Doc 100602], [Doc 96693], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc 103453], [Doc", "latency_ms": 30038.118362426758, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.7, "justification": "The gold answer says it cannot find such a movie; the generated answer similarly states 'Not found', which is effectively the same conclusion, so it's correct. However, the generated answer adds a long, somewhat noisy list of document IDs and repeats some of them, which is not grounded in the gold answer and appears like extraneous or possibly hallucinated retrieval metadata, slightly reducing faithfulness.", "main_total_tokens": 3191, "main_prompt_tokens": 2935, "main_completion_tokens": 256, "judge_total_tokens": 756, "judge_prompt_tokens": 646, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "[Doc 96517] Reminiscence", "latency_ms": 6494.701623916626, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'Reminiscence' is incorrect; the correct movie title is 'Havana Darkness'. It is not faithful to the gold answer and appears unrelated to the Hemingway-alluding manuscript described.", "main_total_tokens": 2729, "main_prompt_tokens": 2718, "main_completion_tokens": 11, "judge_total_tokens": 470, "judge_prompt_tokens": 398, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "According to [Doc 122259], the answer is \"Doll & Em\".", "latency_ms": 6417.878150939941, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Doll & Em') and correctly responds to the question. The reference to the document is consistent with the provided information, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2710, "main_prompt_tokens": 2692, "main_completion_tokens": 18, "judge_total_tokens": 480, "judge_prompt_tokens": 406, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "According to [Doc 38934], Hallam is not mentioned. However, according to [Doc 39440], it seems that there might be some confusion with the name \"Hallam\". In this document, a character named \"Halam\" is mentioned but no information about spying on people or attempting to find the cause of his mother's death is provided.\n\nHowever, I found another match in [Doc 41101]. The movie \"Cover\" (2007) features a character named \"Jack Halam\", who seems to be investigating something related to his family.", "latency_ms": 15870.772123336792, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct answer is the 2007 film 'Hallam Foe'. The generated answer does not provide this title and instead speculates about other movies and characters, so it is factually incorrect. It is partially faithful in that it describes its own document-based reasoning and notes lack of evidence, but it introduces an incorrect guess ('Cover' with Jack Halam) not supported by the question context or known facts.", "main_total_tokens": 2808, "main_prompt_tokens": 2692, "main_completion_tokens": 116, "judge_total_tokens": 629, "judge_prompt_tokens": 513, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "According to [Doc 22549], the title of the movie is \"He is My Enemy, Partner and Father-In-Law\". The main character's last name is Lee, which sounds more like a friendly name. However, the title suggests that he might be an enemy, so it's a play on words.", "latency_ms": 10537.457942962646, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "Gold answer: 'A foe'. The generated answer claims the last name 'Lee' sounds more friendly and discusses a movie title, contradicting the gold label and introducing unsupported details from an unspecified document. It is both incorrect with respect to the gold answer and unfaithful to the prompt context.", "main_total_tokens": 2439, "main_prompt_tokens": 2374, "main_completion_tokens": 65, "judge_total_tokens": 539, "judge_prompt_tokens": 449, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "According to [Doc 124583], Sam is an 18-year-old on the autism spectrum in the 2017 TV show Atypical.", "latency_ms": 7796.640396118164, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly states that Sam is 18 years old, matching the gold answer. However, it fabricates a citation to 'Doc 124583', which is not provided in the prompt, so it is not fully faithful to the given context or sources.", "main_total_tokens": 2929, "main_prompt_tokens": 2899, "main_completion_tokens": 30, "judge_total_tokens": 501, "judge_prompt_tokens": 417, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "According to [Doc 124583], in the 2017 TV show \"Atypical\", Sam is on the autism spectrum.", "latency_ms": 7401.599884033203, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the TV show 'Atypical', Sam is on the autism spectrum, which matches the gold answer exactly. It does not introduce any contradictions or extraneous, unverifiable details beyond citing a document, so it is both correct and faithful.", "main_total_tokens": 2825, "main_prompt_tokens": 2798, "main_completion_tokens": 27, "judge_total_tokens": 500, "judge_prompt_tokens": 414, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix", "latency_ms": 5888.037443161011, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Chamonix'), so it is both correct and faithful to the provided reference.", "main_total_tokens": 3275, "main_prompt_tokens": 3271, "main_completion_tokens": 4, "judge_total_tokens": 456, "judge_prompt_tokens": 400, "judge_completion_tokens": 56}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains", "latency_ms": 6522.570610046387, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains'), so it is both correct and fully faithful to the reference.", "main_total_tokens": 3852, "main_prompt_tokens": 3849, "main_completion_tokens": 3, "judge_total_tokens": 447, "judge_prompt_tokens": 392, "judge_completion_tokens": 55}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland", "latency_ms": 5518.887519836426, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Greenland' exactly matches the gold answer and correctly identifies the location where Tabaluga is crown-prince. It is both correct and faithful to the reference.", "main_total_tokens": 3105, "main_prompt_tokens": 3102, "main_completion_tokens": 3, "judge_total_tokens": 462, "judge_prompt_tokens": 395, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "According to [Doc 96838], the movie \"One Afternoon\" (2021) matches this description.", "latency_ms": 6695.530414581299, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as \"One Afternoon,\" matching the gold answer, so correctness is 1. However, it cites a specific document ([Doc 96838]) whose contents are not provided in the prompt, so the claimed source is unverifiable and likely fabricated, making the faithfulness score 0.", "main_total_tokens": 2478, "main_prompt_tokens": 2454, "main_completion_tokens": 24, "judge_total_tokens": 521, "judge_prompt_tokens": 426, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "Not found in provided context.", "latency_ms": 7963.571786880493, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "recall_at_10": 0.0, "precision_at_5": 0.0, "precision_at_10": 0.0, "ndcg_at_5": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_5": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The correct answer is 'The Life Ahead', so the generated answer is factually incorrect, yielding a correctness score of 0. However, given the constraint 'Not found in provided context', the model accurately reflected that it could not locate the answer within the supplied information, so its faithfulness to the stated context is high, though not perfect because it could have attempted a guess or indicated uncertainty more explicitly.", "main_total_tokens": 2777, "main_prompt_tokens": 2770, "main_completion_tokens": 7, "judge_total_tokens": 518, "judge_prompt_tokens": 404, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "[Doc 73859] The story of New Zealander Bruce McLaren, who founded the McLaren Motor Racing team...", "latency_ms": 7374.84073638916, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and is faithful to the gold answer, with no contradictions or unsupported additions.", "main_total_tokens": 2950, "main_prompt_tokens": 2925, "main_completion_tokens": 25, "judge_total_tokens": 473, "judge_prompt_tokens": 410, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "Newman-Haas (Andretti)", "latency_ms": 6028.120994567871, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'Newman-Haas (Andretti)' does not match the gold answer 'The McLaren Motor Racing team' and is therefore incorrect. It is also unfaithful to the provided gold reference, giving a completely different team name.", "main_total_tokens": 2639, "main_prompt_tokens": 2628, "main_completion_tokens": 11, "judge_total_tokens": 471, "judge_prompt_tokens": 390, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context does not provide information about the country of origin for \"he\".", "latency_ms": 6144.941091537476, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that he is from New Zealand. The generated answer claims the context does not provide the information and thus does not give the correct country, so it is factually incorrect relative to the gold. However, given the model only sees the context (which is described as not specifying the country), the response is faithful to the provided context and does not hallucinate, so faithfulness is high.", "main_total_tokens": 2570, "main_prompt_tokens": 2554, "main_completion_tokens": 16, "judge_total_tokens": 503, "judge_prompt_tokens": 391, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "There is no information in the provided context about a 2013 movie where John McClane blows up the Great Wall of China. However, there is a movie titled \"The Great Wall\" (Doc 72056) released in 2016, not 2013, and it does not feature John McClane.\n\nIt's possible that you're thinking of the movie \"Die Hard\" (not mentioned in the provided context), where the main character is John McClane, but it was released in 1988, not 2013.", "latency_ms": 15572.746753692627, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The question asks about a 2013 movie where John McClane blows up the Great Wall of China. The gold answer correctly states that no such movie exists in the database. The generated answer correctly concludes there is no such 2013 movie and notes that John McClane is from Die Hard, so in terms of the underlying real-world question it is effectively correct. However, the evaluation is with respect to the provided context/database: the system should answer based only on that context. The generated answer introduces external knowledge about 'Die Hard' that is not in the provided context and speculates about what the user might be thinking of, which violates faithfulness. Therefore correctness is 1, but faithfulness is 0.", "main_total_tokens": 2741, "main_prompt_tokens": 2631, "main_completion_tokens": 110, "judge_total_tokens": 685, "judge_prompt_tokens": 508, "judge_completion_tokens": 177}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "According to [Doc 28589], the answer is \"Tomie: Forbidden Fruit\".", "latency_ms": 6649.8260498046875, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2002 Japanese horror film as 'Tomie: Forbidden Fruit.' There is no contradiction or hallucination relative to the provided information, so both correctness and faithfulness are full.", "main_total_tokens": 2816, "main_prompt_tokens": 2797, "main_completion_tokens": 19, "judge_total_tokens": 514, "judge_prompt_tokens": 435, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 years", "latency_ms": 5744.256973266602, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer numerically (65) and only adds the word 'years', which is consistent with the question. It is both correct and faithful to the gold.", "main_total_tokens": 3242, "main_prompt_tokens": 3239, "main_completion_tokens": 3, "judge_total_tokens": 465, "judge_prompt_tokens": 397, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "Not found in provided context.", "latency_ms": 5195.674419403076, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks what the Scorpion King and Tala search for in 'Scorpion King: Book of Souls (2018)'. The correct answer is 'The Book of Souls', which is provided in the gold. The generated answer says 'Not found in provided context', which is incorrect with respect to the question, so correctness is 0. However, given that the model apparently had no context and honestly stated that it did not find the answer in the provided context, it is faithful to its own (lack of) evidence, so faithfulness is 1.", "main_total_tokens": 2546, "main_prompt_tokens": 2539, "main_completion_tokens": 7, "judge_total_tokens": 544, "judge_prompt_tokens": 400, "judge_completion_tokens": 144}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no movie titled \"Zombies on the Moon\". However, there are movies titled \"Zombie Honeymoon\", \"Zombies\", and \"Girl on the Moon\" in the provided context.", "latency_ms": 9243.110179901123, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer correctly states that the movie is not in the database. The generated answer incorrectly asserts that there is no movie titled \"Zombies on the Moon\", which goes beyond what the database information supports and thus is not faithful to the given gold response. It is also not correct in context, since absence from the database does not prove nonexistence.", "main_total_tokens": 3153, "main_prompt_tokens": 3111, "main_completion_tokens": 42, "judge_total_tokens": 532, "judge_prompt_tokens": 430, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Not found", "latency_ms": 5871.232748031616, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer explicitly states that the movie is not in the database, matching the style of the QA setting. The generated answer \"Not found\" is too vague and does not clearly convey that the movie is not in the database; it could mean many things. Therefore it is not considered correct relative to the gold, and it is not faithful to the required response format or content.", "main_total_tokens": 3396, "main_prompt_tokens": 3393, "main_completion_tokens": 3, "judge_total_tokens": 498, "judge_prompt_tokens": 391, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no information about a movie titled \"Return of the Null Pointer\".", "latency_ms": 6235.632658004761, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information about a movie titled 'Return of the Null Pointer', which is equivalent in meaning to the gold answer that it is not in the database. It is faithful to the prompt and does not introduce unsupported claims.", "main_total_tokens": 2657, "main_prompt_tokens": 2641, "main_completion_tokens": 16, "judge_total_tokens": 488, "judge_prompt_tokens": 406, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Not found [Doc 99061], [Doc 99194], [Doc 66433], [Doc 99132], [Doc 93758], [Doc 108363], [Doc 111075], [Doc 107179], [Doc 105633], [Doc 81337], [Doc 99830], [Doc 105128], [Doc 111878], [Doc 27335], and [Doc 100189]", "latency_ms": 13794.905662536621, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.5, "justification": "The question cannot be answered with the available information, and both the gold and generated answers correctly avoid fabricating a release year. So the generated answer is effectively correct in substance. However, faithfulness is reduced because the generated answer references specific document IDs and a retrieval process that are not mentioned in the gold answer; it adds non-essential implementation details and does not mirror the concise, database-based inability-to-answer given in the gold response.", "main_total_tokens": 2850, "main_prompt_tokens": 2756, "main_completion_tokens": 94, "judge_total_tokens": 603, "judge_prompt_tokens": 484, "judge_completion_tokens": 119}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no mention of \"Python's Revenge 2\" in the provided context. However, there is a movie titled \"Python 2\" which was released in 2002 [Doc 28408].", "latency_ms": 9146.521806716919, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks about the release year of 'Python's Revenge 2'. The gold answer correctly states that the movie is not in the database. The generated answer instead introduces information about a different movie ('Python 2' from 2002) that is not requested and not supported by the gold response. It therefore does not correctly answer the question and is not faithful to the given gold answer.", "main_total_tokens": 2952, "main_prompt_tokens": 2909, "main_completion_tokens": 43, "judge_total_tokens": 543, "judge_prompt_tokens": 433, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Not found [Doc 7765], [Doc 97355], [Doc 81817], [Doc 99015], [Doc 103729], [Doc 7092], [Doc 2111], [Doc 101519], [Doc 103549], [Doc 96931], [Doc 111259], [Doc 99713], [Doc 65718], [Doc 97252], [Doc 64811], [Doc 107422], [Doc 99519], [Doc 98294], [Doc 98685], and [Doc 8077]", "latency_ms": 16222.461938858032, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the movie is not in the database and does not provide a release year. The generated answer instead lists a set of document IDs as if they were sources, which neither answers the question about the release year nor matches the gold response. It is therefore not correct relative to the task and not faithful to the gold answer.", "main_total_tokens": 2504, "main_prompt_tokens": 2380, "main_completion_tokens": 124, "judge_total_tokens": 611, "judge_prompt_tokens": 512, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no movie titled \"The Man Who Ate The Sun\". However, there are movies with similar titles such as \"He Who Eats Children\" (2016) and \"The Man-Eater\" (1999).", "latency_ms": 9353.527784347534, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates uncertainty/absence of data, while the generated answer makes a definitive and likely incorrect claim that no such movie exists and then fabricates related examples. This is neither correct nor faithful to the gold response.", "main_total_tokens": 2838, "main_prompt_tokens": 2791, "main_completion_tokens": 47, "judge_total_tokens": 511, "judge_prompt_tokens": 436, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Not found.", "latency_ms": 5188.707590103149, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer convey that the director information is not available, so the generated answer is correct and faithful to the gold.", "main_total_tokens": 2714, "main_prompt_tokens": 2710, "main_completion_tokens": 4, "judge_total_tokens": 460, "judge_prompt_tokens": 400, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Not found.", "latency_ms": 5486.566066741943, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available, which is correct relative to the database. The generated answer is fully consistent with the gold answer and does not introduce unsupported information.", "main_total_tokens": 2994, "main_prompt_tokens": 2990, "main_completion_tokens": 4, "judge_total_tokens": 469, "judge_prompt_tokens": 398, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context does not provide information about who directed the movie \"Journey into Gaza\".", "latency_ms": 7450.096130371094, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not provide the director information, which matches the gold answer indicating the database lacks that information. It is both factually correct given the context and fully faithful to it.", "main_total_tokens": 3446, "main_prompt_tokens": 3428, "main_completion_tokens": 18, "judge_total_tokens": 482, "judge_prompt_tokens": 409, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Not found.", "latency_ms": 4742.098331451416, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The gold answer indicates that the database does not provide the director information, so the correct behavior is to state that it cannot be answered from the database. The generated answer 'Not found.' is effectively equivalent in meaning, correctly conveying that the information is unavailable, so it is correct. However, it is less explicit about the database limitation than the gold answer, so the faithfulness to the gold answer’s reasoning is slightly reduced.", "main_total_tokens": 2426, "main_prompt_tokens": 2422, "main_completion_tokens": 4, "judge_total_tokens": 518, "judge_prompt_tokens": 400, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "According to [Doc 98375], this happens in \"Rise of the Mummy\" (2021).", "latency_ms": 6874.966859817505, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.6, "justification": "The generated answer correctly identifies the movie as \"Rise of the Mummy,\" matching the gold answer, so correctness is 1. However, it fabricates a citation to an unspecified 'Doc 98375' and adds a specific year (2021) that is not supported by the provided information, so faithfulness is reduced.", "main_total_tokens": 2615, "main_prompt_tokens": 2591, "main_completion_tokens": 24, "judge_total_tokens": 512, "judge_prompt_tokens": 413, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "According to [Doc 35237], this happens in the movie \"Stranger in My Bed\".", "latency_ms": 6973.916530609131, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, correctly identifying 'Stranger in My Bed' as the movie. It is fully consistent with the provided information and does not introduce any unsupported details.", "main_total_tokens": 2647, "main_prompt_tokens": 2626, "main_completion_tokens": 21, "judge_total_tokens": 485, "judge_prompt_tokens": 416, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "According to [Doc 123938], this happens in \"Game of Silence\" (2016), a TV series.", "latency_ms": 6544.734001159668, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The gold answer is the TV series 'Game of Silence.' The generated answer also identifies 'Game of Silence' (2016), a TV series, which is correct. It slightly diverges by saying 'According to [Doc 123938],' referencing an external document that is not in the prompt, so faithfulness is slightly reduced, but the core claim matches the gold answer.", "main_total_tokens": 2262, "main_prompt_tokens": 2237, "main_completion_tokens": 25, "judge_total_tokens": 530, "judge_prompt_tokens": 421, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "According to [Doc 53039], this happens in \"The Craigslist Killer\".", "latency_ms": 6541.412830352783, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer', matching the gold answer. It is faithful to the given information and does not introduce unsupported details.", "main_total_tokens": 2639, "main_prompt_tokens": 2622, "main_completion_tokens": 17, "judge_total_tokens": 471, "judge_prompt_tokens": 408, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "According to [Doc 66094], this happens in the movie \"Wood Job!\".", "latency_ms": 7856.003046035767, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Wood Job!' and stays faithful to the information in the gold answer without adding incorrect details.", "main_total_tokens": 3780, "main_prompt_tokens": 3762, "main_completion_tokens": 18, "judge_total_tokens": 475, "judge_prompt_tokens": 417, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase [Doc 78373]", "latency_ms": 7194.92244720459, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase'. It is fully correct and faithful to the provided gold reference.", "main_total_tokens": 3463, "main_prompt_tokens": 3447, "main_completion_tokens": 16, "judge_total_tokens": 490, "judge_prompt_tokens": 423, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "According to [Doc 74597], this happens in the movie \"Beyond\" (2016).", "latency_ms": 6414.7889614105225, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question plot matches the gold answer 'Beyond'. The generated answer explicitly says this happens in the movie \"Beyond\" (2016), which is correct. It does not add any contradictory details beyond the year, and remains grounded in the given doc reference, so it is both correct and faithful.", "main_total_tokens": 2498, "main_prompt_tokens": 2477, "main_completion_tokens": 21, "judge_total_tokens": 518, "judge_prompt_tokens": 428, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "According to [Doc 22478], this happens in the movie \"6ixtynin9\".", "latency_ms": 6505.369424819946, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer identifies the movie as '6ixtynin9', which matches the gold answer exactly. It is correct, and it is directly based on the provided document reference without introducing unsupported information, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2480, "main_prompt_tokens": 2458, "main_completion_tokens": 22, "judge_total_tokens": 505, "judge_prompt_tokens": 423, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "According to [Doc 32410], this happens in the movie \"Club Dread\" (2004).", "latency_ms": 7036.7066860198975, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'Club Dread', which matches the gold answer. The added reference to a specific document ID is unverifiable from the prompt context and may be fabricated, so faithfulness is slightly reduced, but the core claim is accurate and aligned with the given information.", "main_total_tokens": 2867, "main_prompt_tokens": 2845, "main_completion_tokens": 22, "judge_total_tokens": 538, "judge_prompt_tokens": 446, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet (2010) [Doc 48873]", "latency_ms": 6490.164995193481, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Toilet') and does not introduce any contradictions or extraneous claims. The extra year and document reference do not affect the correctness regarding the movie title, and the response remains faithful to the question and given context.", "main_total_tokens": 3092, "main_prompt_tokens": 3079, "main_completion_tokens": 13, "judge_total_tokens": 507, "judge_prompt_tokens": 425, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "According to [Doc 1024], this happens in the movie \"Electric Angel\".", "latency_ms": 6792.7186489105225, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Electric Angel', which matches the gold answer. The explanation refers to a supporting document but does not contradict the given information, so it is faithful to the source context as presented.", "main_total_tokens": 3061, "main_prompt_tokens": 3043, "main_completion_tokens": 18, "judge_total_tokens": 508, "judge_prompt_tokens": 433, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "According to the provided context, the lowest rated movie from 2023 is \"Fall\" with a rating of 4.0 [Doc 106810].", "latency_ms": 6162.957668304443, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'Mega Ape' as the lowest rated movie from 2023, while the generated answer claims it is 'Fall' with a 4.0 rating. This is factually incorrect relative to the gold. Additionally, the generated answer references 'the provided context' and a specific document ID that are not mentioned in the question, so it is not faithful to the given information.", "main_total_tokens": 2908, "main_prompt_tokens": 2875, "main_completion_tokens": 33, "judge_total_tokens": 524, "judge_prompt_tokens": 413, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "According to the provided context, the highest rated TV show from 2010 is \"Tea House\" with a rating of 9.2 [Doc 119448].", "latency_ms": 6629.702568054199, "retrieval_recall": 1.0, "retrieval_mrr": 0.08333333333333333, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies \"Tea House\" as the highest rated TV show from 2010, matching the gold answer. It also aligns with the cited document reference, so it is both correct and faithful to the provided context.", "main_total_tokens": 3224, "main_prompt_tokens": 3188, "main_completion_tokens": 36, "judge_total_tokens": 493, "judge_prompt_tokens": 416, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "According to the provided context, the highest rated Japanese language movie from 1999 is \"Yu☆Gi☆Oh!\" with a rating of 7.0 [Doc 22216].", "latency_ms": 8787.388324737549, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'The Uncut Story of Mewtwo's Origin' as the correct movie, while the generated answer claims 'Yu☆Gi☆Oh!' is the highest rated Japanese language movie from 1999. This is factually incorrect relative to the gold. The generated answer also references 'the provided context' and a specific document rating that are not present in the user prompt, indicating it is not faithful to the given QA pair; it invents supporting context. Therefore both correctness and faithfulness are 0.", "main_total_tokens": 2846, "main_prompt_tokens": 2807, "main_completion_tokens": 39, "judge_total_tokens": 561, "judge_prompt_tokens": 429, "judge_completion_tokens": 132}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "According to the provided context, the lowest-rated movie from 1980 is \"Cataclysm\" with a rating of 3.889. The language of this movie is English (en).", "latency_ms": 9398.283004760742, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980, whose gold answer is English. The generated answer identifies the movie (Cataclysm, rating 3.889) and states its language is English (en), matching the gold. There is no contradiction with the given information, so it is both correct and faithful.", "main_total_tokens": 3350, "main_prompt_tokens": 3309, "main_completion_tokens": 41, "judge_total_tokens": 524, "judge_prompt_tokens": 423, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "According to the provided context, the TV show with the highest popularity in 1999 is \"The Amanda Show\" with a popularity rating of 3.4197 [Doc 115219].", "latency_ms": 9438.382863998413, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the gold label indicates that 'Law & Order: Special Victims Unit' was the TV show with the highest popularity in 1999, whereas the model responded with 'The Amanda Show'. However, the answer is faithful to the provided context, explicitly citing that this conclusion comes from Doc 115219, so it did not hallucinate beyond the given information.", "main_total_tokens": 3437, "main_prompt_tokens": 3397, "main_completion_tokens": 40, "judge_total_tokens": 538, "judge_prompt_tokens": 429, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2 [Doc 79355]", "latency_ms": 5912.291526794434, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (7.2) and does not add unsupported information. It is both correct and faithful.", "main_total_tokens": 2874, "main_prompt_tokens": 2864, "main_completion_tokens": 10, "judge_total_tokens": 450, "judge_prompt_tokens": 392, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "There are two movies with the title \"The Story of Us\". One was released in 1999 and the other in 2019.", "latency_ms": 7742.202281951904, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold label states the answer should be 2019, but the generated response lists both 1999 and 2019 without identifying which matches the question, so it does not correctly answer the specific question. However, the assistant accurately mentions that there is a 2019 movie with that title, and there is indeed also a 1999 film titled \"The Story of Us,\" so the information it provides is faithful to known facts rather than hallucinated.", "main_total_tokens": 2902, "main_prompt_tokens": 2873, "main_completion_tokens": 29, "judge_total_tokens": 535, "judge_prompt_tokens": 412, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The number of votes for the movie \"My Dog Skip\" is not provided in the given context. However, it has a rating of 6.7 and a popularity score of 1.7749.", "latency_ms": 9116.536140441895, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that 'My Dog Skip' received 279 votes. The generated answer incorrectly claims that the number of votes is not provided, so it is factually wrong with respect to the gold (correctness 0). However, the generated answer is consistent with its own stated context—saying the votes are not provided and instead giving rating and popularity—so it does not hallucinate specific vote counts; thus it is faithful to its (limited) context (faithfulness 1).", "main_total_tokens": 2922, "main_prompt_tokens": 2879, "main_completion_tokens": 43, "judge_total_tokens": 553, "judge_prompt_tokens": 423, "judge_completion_tokens": 130}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The number of votes for the movie \"Oh My God!\" is not explicitly mentioned in the provided context. However, it has a rating of 3.8 and a popularity score of 0.8047, but these do not directly translate to a specific number of votes.", "latency_ms": 10436.69319152832, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that 'Oh My God!' received 12 votes, but the generated answer claims the number of votes is not mentioned and provides no number. This directly contradicts the gold label, so the answer is factually incorrect. However, it is faithful to the (incorrect) reasoning it presents, as it does not invent a vote count and instead explains that the data is not available from the provided context, maintaining internal consistency with its own assumptions.", "main_total_tokens": 2986, "main_prompt_tokens": 2929, "main_completion_tokens": 57, "judge_total_tokens": 555, "judge_prompt_tokens": 437, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "Not found.", "latency_ms": 5423.415422439575, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2009, but the generated answer said 'Not found,' which is incorrect. However, the model did not hallucinate an incorrect year; it simply failed to provide the answer, so it remains faithful to its own (stated) lack of knowledge.", "main_total_tokens": 2636, "main_prompt_tokens": 2632, "main_completion_tokens": 4, "judge_total_tokens": 477, "judge_prompt_tokens": 390, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en", "latency_ms": 5335.90841293335, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The original language of 'The King's Speech' is English. The generated answer matches the gold label exactly and is fully correct and faithful to the question.", "main_total_tokens": 2936, "main_prompt_tokens": 2934, "main_completion_tokens": 2, "judge_total_tokens": 445, "judge_prompt_tokens": 384, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The year Wall Street was released is not found in the provided context.", "latency_ms": 4495.832204818726, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1987. The generated answer incorrectly states that the release year is not found, so it is factually wrong. However, it is faithful to the (incorrect) reasoning that it could not find the year in the provided context and does not hallucinate a date.", "main_total_tokens": 2140, "main_prompt_tokens": 2125, "main_completion_tokens": 15, "judge_total_tokens": 481, "judge_prompt_tokens": 392, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie where classmates have to play a survival game using their cell phones is \"Tokyo Gore School\" [Doc 45096].", "latency_ms": 6446.911334991455, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Brain Fluid Explosion Girl', but the generated answer claims it is 'Tokyo Gore School', which is incorrect. However, the answer is faithful to the cited document and does not contradict its own source, so faithfulness is high even though the final answer is wrong.", "main_total_tokens": 1979, "main_prompt_tokens": 1951, "main_completion_tokens": 28, "judge_total_tokens": 502, "judge_prompt_tokens": 414, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show where six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" (1983). [Doc 112849]", "latency_ms": 6674.73840713501, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Dungeons & Dragons,\" which matches the gold answer. It stays faithful to the provided information and does not introduce contradictory or extraneous claims; specifying the 1983 series is accurate context.", "main_total_tokens": 1730, "main_prompt_tokens": 1696, "main_completion_tokens": 34, "judge_total_tokens": 502, "judge_prompt_tokens": 423, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones is not found in the provided context.", "latency_ms": 6918.618440628052, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the movie is 'Nacht der Wölfe'. The generated answer claims the movie is not found, which is incorrect with respect to the gold, so correctness is 0. However, it does not contradict any given context and simply states lack of information, so it is faithful to its (limited) context, yielding a faithfulness score of 1.", "main_total_tokens": 2089, "main_prompt_tokens": 2057, "main_completion_tokens": 32, "judge_total_tokens": 536, "judge_prompt_tokens": 429, "judge_completion_tokens": 107}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of Picnic at Hanging Rock take place in 1900 [Doc 126191].", "latency_ms": 5829.900026321411, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of Picnic at Hanging Rock take place in 1900, which matches the gold answer. It is fully consistent and grounded in the provided information.", "main_total_tokens": 1984, "main_prompt_tokens": 1963, "main_completion_tokens": 21, "judge_total_tokens": 472, "judge_prompt_tokens": 404, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The movie is not found in the given context.", "latency_ms": 5320.51157951355, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'June', but the generated answer claims the movie is not found instead of giving 'June'. This is incorrect with respect to the gold answer. However, the response is faithful to the (nonexistent) provided context it references, as it does not hallucinate a specific movie title from that context.", "main_total_tokens": 2330, "main_prompt_tokens": 2319, "main_completion_tokens": 11, "judge_total_tokens": 537, "judge_prompt_tokens": 442, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1353.7828922271729, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct movie title, so it is incorrect. However, it does not hallucinate a wrong answer and accurately reflects uncertainty, so it is faithful to the model's knowledge limits.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 479, "judge_prompt_tokens": 399, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 231.77313804626465, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric rating, and the gold answer is 7.6. The generated answer says 'I don't know,' which is not the correct rating, so correctness is 0. However, the model honestly states its lack of knowledge instead of inventing a value, so it is fully faithful to its own (claimed) uncertainty, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 495, "judge_prompt_tokens": 389, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1015.8617496490479, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3535124404757273, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is incorrect. However, it is a direct numeric response to the user query without adding fabricated reasoning or extra claims, so it is faithful to the (incorrect) prediction itself.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 477, "judge_prompt_tokens": 394, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 184.5400333404541, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. There is no indication the model ignored given information or fabricated reasoning beyond the incorrect count, so it remains faithful to its (incorrect) reasoning.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 474, "judge_prompt_tokens": 395, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1540.3516292572021, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not identify the correct show 'Fantômas,' so it is incorrect. However, it is faithful to the model's uncertainty and does not hallucinate an incorrect show.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 389, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" (1990). [Doc 11205]", "latency_ms": 5945.117473602295, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise' and matches the gold answer exactly. There are no hallucinations or contradictions; the citation tag does not conflict with the content.", "main_total_tokens": 1952, "main_prompt_tokens": 1921, "main_completion_tokens": 31, "judge_total_tokens": 481, "judge_prompt_tokens": 413, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life's most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 7849.8992919921875, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Won't You Be My Neighbor?'. It remains faithful to the question and does not introduce any unsupported information beyond a neutral citation marker.", "main_total_tokens": 2188, "main_prompt_tokens": 2148, "main_completion_tokens": 40, "judge_total_tokens": 513, "judge_prompt_tokens": 439, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "It was released in 1989 [Doc 9460].", "latency_ms": 5174.840211868286, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states 1989, while the correct year is 2018, so it is factually incorrect. It also conflicts directly with the gold answer, so it is not faithful to the reference.", "main_total_tokens": 2002, "main_prompt_tokens": 1988, "main_completion_tokens": 14, "judge_total_tokens": 463, "judge_prompt_tokens": 390, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is not found in the provided context.", "latency_ms": 6303.693056106567, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'Astro Boy', but the generated answer claims the series is not found, which is factually incorrect. It also contradicts the given gold answer, so it is not faithful to the reference.", "main_total_tokens": 2004, "main_prompt_tokens": 1978, "main_completion_tokens": 26, "judge_total_tokens": 487, "judge_prompt_tokens": 412, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "According to [Doc 67513], the woman and her young son were held captive for 7 years in an enclosed space before gaining their freedom.", "latency_ms": 6387.446641921997, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states they were held captive for 7 years, and the generated answer also states 7 years. This matches exactly, so it is both correct and faithful to the gold.", "main_total_tokens": 1714, "main_prompt_tokens": 1683, "main_completion_tokens": 31, "judge_total_tokens": 493, "judge_prompt_tokens": 424, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "According to [Doc 133469], Brink's-Mat security depot was holding gold bullion worth £26m before it was broken into on 26 November 1983.", "latency_ms": 7779.374837875366, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the provided gold answer exactly (£26m) and does not introduce contradictory or extraneous claims. It is both correct and fully grounded in the given information.", "main_total_tokens": 2143, "main_prompt_tokens": 2105, "main_completion_tokens": 38, "judge_total_tokens": 496, "judge_prompt_tokens": 430, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6395.396709442139, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score (80.5772). The generated answer says, 'I don't know,' which is incorrect. However, it does not invent or hallucinate a value; it honestly indicates a lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 396, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 169.57688331604004, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates the correct number is 1, while the generated answer is 0, so the generated answer is factually incorrect, yielding a correctness score of 0. The generated answer directly responds to the question without adding unsupported claims or fabricated details, so it is fully faithful to the (albeit incorrect) single fact it asserts, giving a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 494, "judge_prompt_tokens": 385, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" (2014). [Doc 121928]", "latency_ms": 6906.980752944946, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Angel Eyes,' matching the gold answer exactly. The additional detail of the year (2014) does not contradict the gold and is plausible contextual information, so the response is both correct and faithful to the source.", "main_total_tokens": 2567, "main_prompt_tokens": 2534, "main_completion_tokens": 33, "judge_total_tokens": 509, "judge_prompt_tokens": 426, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of this movie is not specified in the provided context. However, based on the title \"What Is This Film Called Love?\" [Doc 58692], it can be inferred that the language might be English, as the title is written in English and there is no indication of any other language being used.\n\nTo confirm this, we need to look at the \"Language\" field in the provided context. According to [Doc 58692], the language is indeed \"en\", which stands for English.\n\nTherefore, based on the provided context, the answer is: The language of this movie is English.", "latency_ms": 15295.440196990967, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states the language is Korean, but the generated answer concludes it is English, citing the context. Even if the system context may have indicated English, correctness is evaluated against the gold label, so the answer is incorrect. The reasoning is consistent with the provided context, so it is faithful to that context, but it does not match the gold.", "main_total_tokens": 1986, "main_prompt_tokens": 1863, "main_completion_tokens": 123, "judge_total_tokens": 600, "judge_prompt_tokens": 498, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 662.1029376983643, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold standard (2), so it is factually incorrect. However, there is no indication that the model contradicted its own reasoning or used unsupported information; it simply produced the wrong numeric result, so faithfulness to its (unstated) reasoning is assumed.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 394, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The best-rated movie released in 1979 is Board and Care with a rating of 6.333 [Doc 882].", "latency_ms": 5587.104320526123, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that there are no movies from 1979 in the database, so the system cannot identify a best-rated movie for that year. The generated answer asserts a specific film and rating allegedly from a document, directly contradicting the database limitation. Thus it is not correct with respect to the task and not faithful to the provided gold behavior or constraints.", "main_total_tokens": 2225, "main_prompt_tokens": 2198, "main_completion_tokens": 27, "judge_total_tokens": 524, "judge_prompt_tokens": 421, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "There is no mention of Back to the Future Part IV in any of the provided documents.", "latency_ms": 6480.275392532349, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly indicate that there is no information/mention about 'Back to the Future Part IV' in the respective sources, implicitly conveying that such a movie is not present in the data. The generated answer is faithful to the given documents (no hallucinated year) and thus is both correct and faithful.", "main_total_tokens": 2720, "main_prompt_tokens": 2701, "main_completion_tokens": 19, "judge_total_tokens": 514, "judge_prompt_tokens": 418, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot find any movies or shows matching the criteria, which aligns with the gold answer that there were no movies from 1980 related to the Chernobyl disaster. It does not invent a movie or a popularity score and remains faithful to the information implied by the gold response, only adding a reasonable clarification about the event timing.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 537, "judge_prompt_tokens": 434, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 237.17546463012695, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) is incorrect because the gold answer is 4. However, the response does not contradict any provided context or fabricate details; it simply gives an incorrect number, so it is unfaithful to the gold label in terms of content but not hallucinated from nonexistent context. Given the rubric, correctness is 0, and faithfulness is 1 as it does not introduce extraneous or made-up information beyond the incorrect numeric answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 510, "judge_prompt_tokens": 388, "judge_completion_tokens": 122}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 162.6725196838379, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer is 0, which is numerically incorrect. However, the model did not fabricate reasoning or external facts beyond giving the (incorrect) number, so it is still faithful to its own (albeit wrong) prediction.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 472, "judge_prompt_tokens": 386, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 116.27197265625, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show in the 1990s about Super Mario Bros, while the generated answer is 0. This is factually incorrect compared to the gold, so correctness is 0. The generated answer directly answers the question and does not add unsupported information, so it is faithful to the (incorrect) claim it makes, giving faithfulness a 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 499, "judge_prompt_tokens": 389, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 131.6523551940918, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there were 0 TV shows about Super Mario Bros released between 2000 and 2010, which matches the gold answer and is faithful to it.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 459, "judge_prompt_tokens": 391, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 147.52793312072754, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto', a specific and correct response. The generated answer says 'I don't know', which is incorrect relative to the gold. However, it is faithful because the model accurately states its (claimed) lack of knowledge rather than hallucinating an answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 472, "judge_prompt_tokens": 386, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "There is no mention of a Jurassic Park/Power Ranger crossover movie in the provided context.", "latency_ms": 4942.101001739502, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie was not released. The generated answer effectively conveys the same idea by saying there is no mention of such a crossover movie, implying it does not exist in the context. This is correct and aligned with the gold, so both correctness and faithfulness are high.", "main_total_tokens": 2096, "main_prompt_tokens": 2077, "main_completion_tokens": 19, "judge_total_tokens": 492, "judge_prompt_tokens": 403, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Barbie fights the Predator in [Doc 28703] Title: Killer Barbys vs. Dracula, where Barbie is part of a band called \"Killer Barbies\" that performs at an amusement park, and they are stalked by Count Dracula who has been awakened from his slumber.", "latency_ms": 10063.895225524902, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer invents a crossover movie and incorrectly claims Barbie fights the Predator in an existing film. The correct answer is that no such movie was released. This is factually wrong and also contradicts the given gold answer, so it is neither correct nor faithful.", "main_total_tokens": 2304, "main_prompt_tokens": 2242, "main_completion_tokens": 62, "judge_total_tokens": 524, "judge_prompt_tokens": 440, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The question is asking for the 4th oldest Star Wars movie.\n\nFrom the provided context, we can see that there are several movies mentioned, but only a few of them are related to Star Wars. The relevant documents are:\n\n* [Doc 3986] Title: The Last Starfighter (1984)\n* [Doc 3935] Title: Starman (1984)\n* [Doc 5908] Title: Star Trek IV: The Voyage Home (1986)\n\nHowever, these movies are not part of the Star Wars franchise. They seem to be science fiction movies with similar themes.\n\nThere is no clear answer to the question based on the provided context.", "latency_ms": 17349.1427898407, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The correct answer is 'Star Wars: Droids - The Great Heep', but the generated answer says there is no clear answer, so it is incorrect. It is somewhat faithful to the provided context in that it notes the documents are not Star Wars movies and avoids fabricating from them, but it fails to answer despite the gold answer existing, so faithfulness is not perfect.", "main_total_tokens": 2520, "main_prompt_tokens": 2383, "main_completion_tokens": 137, "judge_total_tokens": 634, "judge_prompt_tokens": 525, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "It was released in 1989 [Doc 9460].", "latency_ms": 5234.0381145477295, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 1989, but the correct year is 1986, so it is factually incorrect. However, it appears to be faithful to its cited source ([Doc 9460]) and does not hallucinate beyond that, so faithfulness is high even though the source itself conflicts with the gold answer.", "main_total_tokens": 2002, "main_prompt_tokens": 1988, "main_completion_tokens": 14, "judge_total_tokens": 486, "judge_prompt_tokens": 390, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 243.61801147460938, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the specific numerical rating of the movie Forrest Gump, and the gold answer is 8.466. The generated answer says 'I don't know,' which does not provide the requested value, so it is incorrect. However, it does not invent or contradict any facts; it simply admits lack of knowledge, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 500, "judge_prompt_tokens": 398, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is Mighty Med [Doc 121125].", "latency_ms": 5144.238233566284, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med,' which matches the gold answer exactly. It remains faithful to the user question, restating it accurately and providing the correct title without adding unsupported or irrelevant information.", "main_total_tokens": 1736, "main_prompt_tokens": 1707, "main_completion_tokens": 29, "judge_total_tokens": 496, "judge_prompt_tokens": 420, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "According to [Doc 53582], seven prisoners escaped from a prison in Argentina.", "latency_ms": 6533.36763381958, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that the seven prisoners escaped from a prison in Argentina, which exactly matches the gold answer. It does not introduce extraneous or contradictory information and is fully grounded in the provided gold fact.", "main_total_tokens": 2118, "main_prompt_tokens": 2100, "main_completion_tokens": 18, "judge_total_tokens": 480, "judge_prompt_tokens": 408, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The apprentice falls into the trap of the sexual sorceress in Sweet Trap (2024), as described in [Doc 25258] Title: Sex Files: Sexually Bewitched.", "latency_ms": 7732.694387435913, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the apprentice falls into Jiang Jie's trap. The generated answer instead says the apprentice falls into the trap of a 'sexual sorceress' and even references a seemingly unrelated document and title, indicating it is not grounded in the provided gold answer or context. Thus, it is incorrect with respect to the question and unfaithful to the source.", "main_total_tokens": 2190, "main_prompt_tokens": 2151, "main_completion_tokens": 39, "judge_total_tokens": 527, "judge_prompt_tokens": 422, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 571.904182434082, "retrieval_recall": 0.0, "retrieval_mrr": 0.25, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.16812753627111746, "hit_rate_at_5": 1.0, "recall_at_10": 0.25, "precision_at_10": 0.1, "ndcg_at_10": 0.16812753627111746, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is numerically incorrect compared to the gold answer (0 vs 4), so correctness is 0. The assistant did provide a concrete numeric answer to the question asked, so with respect to faithfulness to the user query and format, it is faithful, even though the content is wrong; thus faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 484, "judge_prompt_tokens": 385, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "35", "latency_ms": 31.23307228088379, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '35' is factually incorrect given the gold explanation that there are no movies featuring both Bruce Lee and Steven Seagal together. It is also unfaithful to the gold response, which explicitly states that no such movies can be found, whereas the generated answer confidently asserts a specific (and incorrect) number.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 395, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 4972.4109172821045, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3 and correctly attributes it to the document, so it is both correct and faithful.", "main_total_tokens": 2077, "main_prompt_tokens": 2056, "main_completion_tokens": 21, "judge_total_tokens": 463, "judge_prompt_tokens": 403, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in this movie is street racing.", "latency_ms": 5329.8938274383545, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer specifies 'Drift' as the racing style, while the generated answer claims 'street racing.' Drift and generic street racing are not equivalent, so the generated answer is incorrect relative to the gold. The evaluation is based entirely on the given Q, gold, and generated responses, so the incorrect content is also unfaithful to the gold.", "main_total_tokens": 2299, "main_prompt_tokens": 2287, "main_completion_tokens": 12, "judge_total_tokens": 491, "judge_prompt_tokens": 389, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little is \"Confronting a Serial Killer\". [Doc 131298]", "latency_ms": 8272.663593292236, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Confronting a Serial Killer' and does not introduce unsupported information beyond what is stated.", "main_total_tokens": 2091, "main_prompt_tokens": 2045, "main_completion_tokens": 46, "judge_total_tokens": 516, "judge_prompt_tokens": 451, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838. [Doc 20603]", "latency_ms": 6635.136365890503, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold value (7.838) and does not introduce unsupported information. It is both correct and faithful.", "main_total_tokens": 2379, "main_prompt_tokens": 2355, "main_completion_tokens": 24, "judge_total_tokens": 466, "judge_prompt_tokens": 408, "judge_completion_tokens": 58}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\". [Doc 22098]", "latency_ms": 7174.325466156006, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the film as 'The Sixth Sense' and remains faithful to the provided information.", "main_total_tokens": 2150, "main_prompt_tokens": 2120, "main_completion_tokens": 30, "judge_total_tokens": 478, "judge_prompt_tokens": 420, "judge_completion_tokens": 58}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in 2002. [Doc 27318]", "latency_ms": 5823.26078414917, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer. It is fully supported by the provided gold fact, so it is both correct and faithful.", "main_total_tokens": 2262, "main_prompt_tokens": 2245, "main_completion_tokens": 17, "judge_total_tokens": 466, "judge_prompt_tokens": 396, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\". [Doc 67500]", "latency_ms": 7106.078147888184, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer in both content and wording aside from minor formatting, correctly identifying the title as 'The Transporter Refueled'. It does not introduce any contradictions or hallucinations and is fully supported by the provided gold reference.", "main_total_tokens": 2337, "main_prompt_tokens": 2307, "main_completion_tokens": 30, "judge_total_tokens": 501, "judge_prompt_tokens": 420, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The Transporter Refueled was released in 2015 [Doc 67500].", "latency_ms": 5995.771408081055, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It is also faithful to the cited document reference, with no hallucinated details beyond the source.", "main_total_tokens": 2256, "main_prompt_tokens": 2237, "main_completion_tokens": 19, "judge_total_tokens": 478, "judge_prompt_tokens": 402, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "The romantic comedy where Abraham Lincoln comes back from the dead is not found in the provided context.", "latency_ms": 5944.417715072632, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a specific romantic comedy, but both the gold and generated answers correctly state that such a movie is not found / cannot be identified. This is factually correct given the context and matches the information provided, so the response is both correct and faithful to the gold answer.", "main_total_tokens": 2195, "main_prompt_tokens": 2175, "main_completion_tokens": 20, "judge_total_tokens": 497, "judge_prompt_tokens": 409, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by Ernest Hemingway is not found in the provided context.", "latency_ms": 6728.184938430786, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Havana Darkness'. The generated answer claims the title is not found, which is factually incorrect relative to the gold. However, it does not hallucinate a different title and stays faithful to the (limited) context it assumes, so it is faithful but incorrect.", "main_total_tokens": 2140, "main_prompt_tokens": 2114, "main_completion_tokens": 26, "judge_total_tokens": 500, "judge_prompt_tokens": 411, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show \"Doll & Em\". [Doc 122259]", "latency_ms": 6708.312034606934, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Doll & Em,\" which matches the gold answer exactly. It is fully supported by the provided information and does not introduce contradictions or unsupported claims.", "main_total_tokens": 2005, "main_prompt_tokens": 1975, "main_completion_tokens": 30, "judge_total_tokens": 487, "judge_prompt_tokens": 417, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in the movie \"Sleuth\" (2007).", "latency_ms": 7038.114786148071, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer incorrectly identifies the movie as 'Sleuth' (2007), while the correct answer is 'Hallam Foe'. It is fully faithful to the question but factually wrong.", "main_total_tokens": 2078, "main_prompt_tokens": 2045, "main_completion_tokens": 33, "judge_total_tokens": 502, "judge_prompt_tokens": 431, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Based on the context, it seems that his last name could sound like either a friend or a foe depending on the situation.\n\nIn [Doc 46423], \"20th Century Boys 3: Redemption\", there is a character named Friend who is not necessarily what he seems to be, suggesting that having a certain name does not determine one's nature.\n\nIn [Doc 66883], \"Vanmam\", two friends become bitter foes after a fall out, indicating that relationships can change over time.\n\nIn [Doc 91229], \"Forky Asks a ", "latency_ms": 14596.940994262695, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a categorical answer (friend or foe). The gold label is 'A foe', but the generated answer equivocates and says it could sound like either, so it is incorrect. It also fails to address the specific instance the question refers to; instead it brings in unrelated documents and never links them to the actual question, so it is not faithful to the intended query or gold answer.", "main_total_tokens": 1937, "main_prompt_tokens": 1821, "main_completion_tokens": 116, "judge_total_tokens": 610, "judge_prompt_tokens": 499, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "The age of Sam in the 2017 TV show Atypical is not found in the provided context. However, according to [Doc 124583], Sam is an 18-year-old on the autism spectrum.\n\nSo, the answer is: Sam is 18 years old.", "latency_ms": 9519.895076751709, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly states that Sam is 18 years old, which matches the gold answer, so correctness is 1. However, it incorrectly claims support from a specific document ID ([Doc 124583]) that is not in the provided context, making the explanation unfaithful to the given information, so faithfulness is 0.", "main_total_tokens": 2279, "main_prompt_tokens": 2222, "main_completion_tokens": 57, "judge_total_tokens": 543, "judge_prompt_tokens": 444, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam is on the autism spectrum. [Doc 124583]", "latency_ms": 5408.809185028076, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, matching the gold answer exactly and staying faithful to the provided information.", "main_total_tokens": 2167, "main_prompt_tokens": 2153, "main_completion_tokens": 14, "judge_total_tokens": 457, "judge_prompt_tokens": 400, "judge_completion_tokens": 57}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "According to [Doc 59659], Sandra was born in Chamonix.", "latency_ms": 6051.298379898071, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, matching the gold answer exactly. It does not introduce any unsupported information beyond referencing a document number, so it is faithful to the provided gold fact.", "main_total_tokens": 2483, "main_prompt_tokens": 2466, "main_completion_tokens": 17, "judge_total_tokens": 487, "judge_prompt_tokens": 413, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "it rains", "latency_ms": 5306.786775588989, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'it rains' matches the gold answer 'rains' semantically and grammatically within the sentence. It is fully correct and faithful to the reference.", "main_total_tokens": 2879, "main_prompt_tokens": 2876, "main_completion_tokens": 3, "judge_total_tokens": 458, "judge_prompt_tokens": 393, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Tabaluga is the last of the dragons and the crown-prince of Greenland. [Doc 114810]", "latency_ms": 6674.0312576293945, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying Greenland as the place where Tabaluga is crown-prince. It is both correct and faithful to the provided information.", "main_total_tokens": 2429, "main_prompt_tokens": 2405, "main_completion_tokens": 24, "judge_total_tokens": 483, "judge_prompt_tokens": 417, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers\" is not found among the provided documents.\n\nHowever, there are some movies that have similar descriptions:\n\n* [Doc 56229] Title: Axed - A strict father loosens up enough to let his children take a day off school for a trip to the countryside.\n* [Doc 64467] Title: The Bush - A father and son, isolated in a dilapidated country house, try to survive a hellish summer.\n\nBut none of these movies exactly match the given description.", "latency_ms": 16255.674362182617, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is the film 'One Afternoon'. The generated answer wrongly claims the movie is not found and does not identify 'One Afternoon', so it is factually incorrect about the question. However, its statements about the absence of an exact match among the cited documents appear consistent with the information it references, so it is faithful to its own cited evidence, even though it fails to answer the question correctly.", "main_total_tokens": 1929, "main_prompt_tokens": 1795, "main_completion_tokens": 134, "judge_total_tokens": 648, "judge_prompt_tokens": 536, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The movie you're referring to is likely \"See You Tomorrow\" (2013), which has a rating of 5.8 and is in Italian language.\n\nHowever, I couldn't find any information about a holocaust survivor taking care of a 12-year-old street kid who recently robbed her in the provided context.", "latency_ms": 6938.825607299805, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "recall_at_10": 0.0, "precision_at_5": 0.0, "precision_at_10": 0.0, "ndcg_at_5": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_5": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'The Life Ahead', while the generated answer claims it is 'See You Tomorrow (2013)', which is incorrect. It also incorrectly states that the information about a Holocaust survivor caring for a 12-year-old street kid who robbed her cannot be found in the context, contradicting the gold answer. Thus, both correctness and faithfulness are zero.", "main_total_tokens": 2283, "main_prompt_tokens": 2218, "main_completion_tokens": 65, "judge_total_tokens": 565, "judge_prompt_tokens": 460, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The documentary \"McLaren\" is about Bruce McLaren, a New Zealander who founded the McLaren Motor Racing team. [Doc 73859]", "latency_ms": 7208.787202835083, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and adds accurate contextual information about him founding the McLaren racing team. This is fully consistent with the gold answer and does not introduce unsupported claims.", "main_total_tokens": 2297, "main_prompt_tokens": 2265, "main_completion_tokens": 32, "judge_total_tokens": 493, "judge_prompt_tokens": 416, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The name of his racing team is not found in the provided context.", "latency_ms": 5252.59804725647, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is faithful to the (unstated) context by not hallucinating a team name, but it is factually incorrect relative to the gold answer, which specifies 'The McLaren Motor Racing team.'", "main_total_tokens": 1964, "main_prompt_tokens": 1949, "main_completion_tokens": 15, "judge_total_tokens": 470, "judge_prompt_tokens": 397, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The country of origin for \"He\" is not specified in any of the provided documents.", "latency_ms": 5431.329250335693, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'New Zealand', but the generated answer claims the country is not specified. This is factually incorrect relative to the gold, so correctness is 0. However, given the prompt to the model only includes 'provided documents' (which we do not see here), the answer is internally consistent with that premise and does not hallucinate a country, so faithfulness to the stated context is 1.", "main_total_tokens": 1854, "main_prompt_tokens": 1835, "main_completion_tokens": 19, "judge_total_tokens": 509, "judge_prompt_tokens": 394, "judge_completion_tokens": 115}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "There is no mention of John McClane blowing up the great wall of China in any of the provided documents.", "latency_ms": 5884.839296340942, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that there is no such movie in the database, i.e., effectively that no such movie is known. The generated answer says there is no mention of such an event in the provided documents. Both align in substance that the scenario is not documented, and the generated answer does not introduce incorrect facts. It is therefore both correct and faithful to the implied evidence context.", "main_total_tokens": 1937, "main_prompt_tokens": 1914, "main_completion_tokens": 23, "judge_total_tokens": 528, "judge_prompt_tokens": 421, "judge_completion_tokens": 107}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with the description \"When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental\" is Tomie: Forbidden Fruit [Doc 28589].", "latency_ms": 9503.705739974976, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Tomie: Forbidden Fruit' and correctly restates the description. It does not introduce contradictions or unsupported claims and remains faithful to the provided information.", "main_total_tokens": 2251, "main_prompt_tokens": 2194, "main_completion_tokens": 57, "judge_total_tokens": 542, "judge_prompt_tokens": 473, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years. [Doc 98566]", "latency_ms": 6377.610683441162, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, which matches the gold answer. It is fully accurate and consistent with the provided reference.", "main_total_tokens": 2485, "main_prompt_tokens": 2465, "main_completion_tokens": 20, "judge_total_tokens": 480, "judge_prompt_tokens": 413, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King and Tala search for the Book of Souls in Scorpion King: Book of Souls (2018).", "latency_ms": 6284.98101234436, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Scorpion King and Tala search for the Book of Souls, matching the gold answer exactly and remaining faithful to the question context.", "main_total_tokens": 1967, "main_prompt_tokens": 1941, "main_completion_tokens": 26, "judge_total_tokens": 481, "judge_prompt_tokens": 418, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 5254.897832870483, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates that the movie is not in the database, effectively giving no release year. The generated answer correctly states that the answer is not found in the provided context, which aligns with the gold response. There is no incorrect added information and it faithfully reflects the lack of available data.", "main_total_tokens": 2298, "main_prompt_tokens": 2287, "main_completion_tokens": 11, "judge_total_tokens": 490, "judge_prompt_tokens": 401, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year of \"The Galactic baker\" is not found in the provided context.", "latency_ms": 6217.573642730713, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.5, "justification": "The gold answer says the movie is not in the database, while the generated answer says the release year is not found in the provided context. Both effectively state that the release year is unknown, so factually it is correct. However, the generated answer refers to a 'provided context' that is not mentioned in the gold, adding an ungrounded assumption about the source of information, so faithfulness is partial.", "main_total_tokens": 2523, "main_prompt_tokens": 2505, "main_completion_tokens": 18, "judge_total_tokens": 522, "judge_prompt_tokens": 406, "judge_completion_tokens": 116}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"Return of the Null Pointer\" was released is not found in the provided context.", "latency_ms": 5780.875682830811, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available / not found, which is correct given the context and does not hallucinate a year. The generated response is faithful to the (unstated) requirement to rely only on provided context.", "main_total_tokens": 2109, "main_prompt_tokens": 2089, "main_completion_tokens": 20, "judge_total_tokens": 492, "judge_prompt_tokens": 410, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no mention of \"Data Science: The Movie\" in any of the provided documents.", "latency_ms": 5779.086589813232, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the movie is not present in the available information, and neither fabricates a release year. The generated answer is faithful to the (implied) context of relying on provided documents, so both correctness and faithfulness are high.", "main_total_tokens": 2058, "main_prompt_tokens": 2038, "main_completion_tokens": 20, "judge_total_tokens": 494, "judge_prompt_tokens": 410, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "\"Python's Revenge 2\" was not found in the provided context.", "latency_ms": 5455.373287200928, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie was not found and does not invent a release year. It is consistent with the gold response that the movie is not in the database, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2101, "main_prompt_tokens": 2085, "main_completion_tokens": 16, "judge_total_tokens": 482, "judge_prompt_tokens": 406, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "\"The Last Pixel\" is not found in the provided context.", "latency_ms": 4872.998476028442, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a release year of a movie that is not in the database/context. The gold answer appropriately states lack of knowledge. The generated answer similarly states that the movie is not found in the provided context. Both avoid hallucinating a year and correctly reflect the absence of information, so the response is both correct and faithful to the given context.", "main_total_tokens": 1814, "main_prompt_tokens": 1801, "main_completion_tokens": 13, "judge_total_tokens": 503, "judge_prompt_tokens": 402, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The title \"The Man Who Ate The Sun\" is not found in any of the provided documents.", "latency_ms": 5918.289422988892, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both Gold and Generated correctly state that the title is not found/known in the provided context and do not hallucinate a release year. The generated answer is fully consistent with the information given (faithful) and answers the question as far as the available data allows (correct).", "main_total_tokens": 2023, "main_prompt_tokens": 2001, "main_completion_tokens": 22, "judge_total_tokens": 498, "judge_prompt_tokens": 412, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Days of Lehman Brothers\" is not found in the provided context.", "latency_ms": 6274.551630020142, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the given data/context. This matches the constraint and does not introduce unsupported claims, so it is both correct and faithful to the provided information.", "main_total_tokens": 2197, "main_prompt_tokens": 2174, "main_completion_tokens": 23, "judge_total_tokens": 493, "judge_prompt_tokens": 419, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"A Love to Die For\" is not found in the provided context.", "latency_ms": 6172.257423400879, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not found in the provided context, which aligns with the gold response that the database does not provide the director information. It is both correct and faithful to the given information.", "main_total_tokens": 2313, "main_prompt_tokens": 2292, "main_completion_tokens": 21, "judge_total_tokens": 489, "judge_prompt_tokens": 415, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not found in the provided context.", "latency_ms": 6329.702615737915, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not found in the provided context, which matches the gold answer that the database does not provide the director information. It does not invent a director and remains grounded in the context, so it is both correct and faithful.", "main_total_tokens": 2459, "main_prompt_tokens": 2439, "main_completion_tokens": 20, "judge_total_tokens": 494, "judge_prompt_tokens": 411, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of \"The Last Night at Tremore Beach\" is not found in the provided context.", "latency_ms": 5612.354516983032, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer state that the director information is not available in the provided data/context. The generated response does not hallucinate a director and matches the limitation described by the gold answer, so it is both correct and faithful to the context.", "main_total_tokens": 1854, "main_prompt_tokens": 1833, "main_completion_tokens": 21, "judge_total_tokens": 500, "judge_prompt_tokens": 417, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which the following happens: A group of archaeology students awaken an ancient mummy is [Doc 98375] Title: Rise of the Mummy, Year: 2021.", "latency_ms": 7618.414640426636, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Rise of the Mummy,' which matches the gold answer exactly. The mention of it being a 2021 film is plausible and does not contradict the provided information. The model remains grounded in the question and aligns with the gold label, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 1989, "main_prompt_tokens": 1948, "main_completion_tokens": 41, "judge_total_tokens": 526, "judge_prompt_tokens": 429, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death and flees to escape her abusive spouse is \"Stranger in My Bed\" (2005) [Doc 35237].", "latency_ms": 7243.676424026489, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Stranger in My Bed', which matches the gold answer. It is also faithful to the provided information and does not introduce unsupported details beyond confirming the title and year.", "main_total_tokens": 2006, "main_prompt_tokens": 1969, "main_completion_tokens": 37, "judge_total_tokens": 505, "judge_prompt_tokens": 432, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie that matches the given description is not found in the provided context. However, based on the plot descriptions, some movies seem to have similar themes of friends reuniting after a long time or attorneys dealing with past issues.\n\nIf we consider the specific detail \"A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years,\" none of the provided movie plots exactly match this description.", "latency_ms": 11340.169668197632, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly identifies the work as 'Game of Silence'. The generated answer incorrectly claims that none of the plots match the description and fails to name 'Game of Silence', so it is factually wrong relative to the gold. However, the generated answer is faithful to the (limited) context it assumes it has, explicitly stating that it cannot find a match rather than hallucinating a title.", "main_total_tokens": 1775, "main_prompt_tokens": 1692, "main_completion_tokens": 83, "judge_total_tokens": 589, "judge_prompt_tokens": 479, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie is \"The Craigslist Killer\" [Doc 53039].", "latency_ms": 5238.350391387939, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question clearly refers to the film 'The Craigslist Killer,' and the generated answer correctly identifies that movie. It also aligns fully with the given Gold answer, so it is both correct and faithful.", "main_total_tokens": 1988, "main_prompt_tokens": 1973, "main_completion_tokens": 15, "judge_total_tokens": 476, "judge_prompt_tokens": 406, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie is \"Wood Job!\" [Doc 66094].", "latency_ms": 6210.776567459106, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Wood Job!' and matches the gold answer without adding unsupported details, so it is both correct and faithful to the provided information.", "main_total_tokens": 2870, "main_prompt_tokens": 2856, "main_completion_tokens": 14, "judge_total_tokens": 473, "judge_prompt_tokens": 413, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie where a playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure is \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 8863.349437713623, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase'. It is fully consistent with the provided information and does not introduce any unsupported claims, so both correctness and faithfulness are maximal.", "main_total_tokens": 2416, "main_prompt_tokens": 2368, "main_completion_tokens": 48, "judge_total_tokens": 536, "judge_prompt_tokens": 456, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a rock fall traps his parents inside a burning car, and a 7 year-old boy must find the hero within him in order to save their lives is \"Beyond\" (2016). [Doc 74597]", "latency_ms": 8307.745695114136, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Beyond,\" matching the gold answer. Adding the year (2016) is a harmless, plausible detail and does not contradict the provided information, so the response is both correct and faithful to the question and gold label.", "main_total_tokens": 1923, "main_prompt_tokens": 1874, "main_completion_tokens": 49, "judge_total_tokens": 539, "judge_prompt_tokens": 456, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie that matches the given context is [Doc 22478] Title: 6ixtynin9.\n\nIn this movie, a woman returns home with no money after being fired from a financial corporation during the Asia crisis.", "latency_ms": 8222.479581832886, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"6ixtynin9\" and accurately restates the key plot detail from the question. It does not introduce contradictions or unsupported information and remains faithful to the provided gold answer.", "main_total_tokens": 1942, "main_prompt_tokens": 1894, "main_completion_tokens": 48, "judge_total_tokens": 525, "judge_prompt_tokens": 449, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie is Club Dread (2004). The plot matches the description provided: \"When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!\" [Doc 32410]", "latency_ms": 10736.599683761597, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread', which matches the gold answer exactly. It also stays faithful to the given description without adding unsupported information, only providing the year and a confirming note, which are reasonable and not contradictory.", "main_total_tokens": 2274, "main_prompt_tokens": 2204, "main_completion_tokens": 70, "judge_total_tokens": 573, "judge_prompt_tokens": 493, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie that matches the given context is: Toilet (2010) [Doc 48873].", "latency_ms": 6374.026775360107, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Toilet' and matches the gold answer exactly. It does not introduce contradictions or unsupported claims, only restating the correct title with minimal additional metadata (year), which is reasonable. Therefore, both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2428, "main_prompt_tokens": 2407, "main_completion_tokens": 21, "judge_total_tokens": 516, "judge_prompt_tokens": 434, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie that matches the given description is \"Electric Angel\" (1981). The description states: \"A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.\" This text can be found in [Doc 1024].", "latency_ms": 11053.336143493652, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies the movie as \"Electric Angel,\" matching the gold answer, so correctness is 1. However, it fabricates a citation to \"[Doc 1024]\" which is not grounded in the given prompt, slightly reducing faithfulness.", "main_total_tokens": 2310, "main_prompt_tokens": 2237, "main_completion_tokens": 73, "judge_total_tokens": 573, "judge_prompt_tokens": 487, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is \"Fall\" with a rating of 4.0 [Doc 106810].", "latency_ms": 6440.40322303772, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the lowest rated 2023 movie is 'Mega Ape', while the generated answer claims it is 'Fall' with a 4.0 rating. This directly contradicts the gold, so the answer is factually incorrect relative to the given reference. It also appears to hallucinate a specific movie and rating not supported by the gold, so it is unfaithful to the provided information.", "main_total_tokens": 2169, "main_prompt_tokens": 2142, "main_completion_tokens": 27, "judge_total_tokens": 520, "judge_prompt_tokens": 407, "judge_completion_tokens": 113}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 1313.7423992156982, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is a specific TV show title, while the generated answer says 'I don't know,' which is incorrect. However, it does not invent any incorrect information, so it is fully faithful to the model's stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 463, "judge_prompt_tokens": 386, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 524.0564346313477, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific movie, and the correct answer is given in the gold. The generated answer says 'I don't know,' which is incorrect, so correctness is 0. However, the model does not hallucinate or provide fabricated information and honestly states lack of knowledge, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 489, "judge_prompt_tokens": 395, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is Cataclysm with a rating of 3.889 [Doc 744]. The language of this movie is en (English).", "latency_ms": 6742.899179458618, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980. The gold answer is 'English'. The generated answer identifies the lowest-rated movie as Cataclysm and states its language is 'en (English)'. This matches the gold answer, so it is correct. The claim about the language is explicitly tied to the document reference, so it is faithful to the provided evidence.", "main_total_tokens": 2395, "main_prompt_tokens": 2359, "main_completion_tokens": 36, "judge_total_tokens": 530, "judge_prompt_tokens": 419, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity is Zoom [Doc 115355], however its rating is not provided. The next highest rated TV show is Amazon [Doc 115326] with a rating of 7.9, followed by People Like Us [Doc 115326] also with a rating of 7.9.", "latency_ms": 10973.480463027954, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer does not match the gold answer; it lists Zoom, Amazon, and People Like Us instead of Law & Order: Special Victims Unit, so it is factually incorrect. It also fabricates a ranking of popularity and mentions specific documents and ratings that are not supported by the provided gold information, so it is unfaithful to the reference.", "main_total_tokens": 2398, "main_prompt_tokens": 2327, "main_completion_tokens": 71, "judge_total_tokens": 563, "judge_prompt_tokens": 460, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2. [Doc 79355]", "latency_ms": 6104.954719543457, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (7.2) and correctly cites the document. It is both correct and faithful to the provided information.", "main_total_tokens": 2258, "main_prompt_tokens": 2238, "main_completion_tokens": 20, "judge_total_tokens": 462, "judge_prompt_tokens": 401, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in 1999 [Doc 22481].", "latency_ms": 6063.607215881348, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says the movie was released in 1999, but the gold label is 2019, so the answer is factually incorrect relative to the provided gold. However, the answer is faithful to its cited document [Doc 22481], so faithfulness is high even though the citation content itself conflicts with the gold.", "main_total_tokens": 2200, "main_prompt_tokens": 2179, "main_completion_tokens": 21, "judge_total_tokens": 502, "judge_prompt_tokens": 404, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "1", "latency_ms": 206.5143585205078, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1) does not match the gold answer (279), so it is incorrect. However, there is no sign of hallucinated reasoning or extraneous claims: it simply provides an incorrect numeric value, so faithfulness to its own (albeit wrong) output is maintained.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 472, "judge_prompt_tokens": 383, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "7", "latency_ms": 21.84319496154785, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (7) does not match the gold answer (12), so it is incorrect. However, there is no indication that the model contradicted the provided context beyond the numeric error, so it remains faithful to its (albeit incorrect) output.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 466, "judge_prompt_tokens": 383, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was not found in the provided context.", "latency_ms": 4911.559343338013, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the film's release year (2009), but the generated answer claims the movie was not found in the context and does not provide any year, so it is factually incorrect. However, relative to the (unstated) context, the answer does not invent a date and simply says it cannot find the movie, so it remains faithful to the given context and does not hallucinate details.", "main_total_tokens": 2083, "main_prompt_tokens": 2063, "main_completion_tokens": 20, "judge_total_tokens": 519, "judge_prompt_tokens": 406, "judge_completion_tokens": 113}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English (en). [Doc 48035]", "latency_ms": 6412.657737731934, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the original language of 'The King's Speech' is English, matching the gold label 'en'. It is fully consistent with the provided information and does not introduce unsupported claims.", "main_total_tokens": 2247, "main_prompt_tokens": 2223, "main_completion_tokens": 24, "judge_total_tokens": 476, "judge_prompt_tokens": 405, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "I don't know.", "latency_ms": 1431.0588836669922, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1987, but the model said it did not know, so the answer is incorrect. However, the answer is faithful to the model's uncertainty and does not hallucinate, so faithfulness is high.", "main_total_tokens": 562, "main_prompt_tokens": 560, "main_completion_tokens": 2, "judge_total_tokens": 459, "judge_prompt_tokens": 382, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Tokyo Gore School", "latency_ms": 5319.9217319488525, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer, 'Tokyo Gore School', is incorrect because the correct movie is 'Brain Fluid Explosion Girl'. However, the answer is faithful to the question and does not introduce unrelated or fabricated details; it simply names the wrong film.", "main_total_tokens": 2530, "main_prompt_tokens": 2523, "main_completion_tokens": 7, "judge_total_tokens": 469, "judge_prompt_tokens": 391, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 4986.0711097717285, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which correctly identifies the TV show as 'Dungeons & Dragons.'", "main_total_tokens": 2217, "main_prompt_tokens": 2210, "main_completion_tokens": 7, "judge_total_tokens": 447, "judge_prompt_tokens": 394, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "I don't know.", "latency_ms": 2306.875467300415, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Nacht der Wölfe'. The generated answer says 'I don't know,' which is incorrect. However, it does not introduce any fabricated information and is thus fully faithful to the model's uncertainty.", "main_total_tokens": 555, "main_prompt_tokens": 553, "main_completion_tokens": 2, "judge_total_tokens": 478, "judge_prompt_tokens": 402, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900", "latency_ms": 5129.432678222656, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('1900') and correctly identifies the year in which the events of Picnic at Hanging Rock take place.", "main_total_tokens": 2567, "main_prompt_tokens": 2562, "main_completion_tokens": 5, "judge_total_tokens": 448, "judge_prompt_tokens": 388, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "May", "latency_ms": 1798.3129024505615, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'June' but the generated answer was 'May', which is incorrect. The model’s response is still faithful to the question format; it attempted to answer the question directly without adding fabricated context, but the content is factually wrong.", "main_total_tokens": 167, "main_prompt_tokens": 165, "main_completion_tokens": 2, "judge_total_tokens": 515, "judge_prompt_tokens": 433, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 258.38279724121094, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a particular film, so saying 'I don't know' is factually incorrect relative to the task. However, the model does not hallucinate any incorrect film and instead accurately reports its lack of knowledge, so it is fully faithful to its own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 399, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 149.06620979309082, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a definite numeric answer (7.6). The generated response says 'I don't know,' which is incorrect. However, it does not invent any false information and accurately reflects a lack of knowledge, so it is fully faithful to the model's (stated) knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 477, "judge_prompt_tokens": 389, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 26668.802499771118, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3535124404757273, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is factually incorrect for this query. However, there is no indication that the model ignored or contradicted its own reasoning steps or instructions beyond giving the wrong count, so it is still faithful to its (incorrect) internal process.", "main_total_tokens": 1714, "main_prompt_tokens": 1458, "main_completion_tokens": 256, "judge_total_tokens": 488, "judge_prompt_tokens": 394, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 283.66565704345703, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer contradicts the gold answer: it says there are no matching shows, while the gold indicates there are 2. This is factually incorrect with respect to the task, so correctness is 0. However, the model does not hallucinate extra details or fabricate numbers; it simply states it cannot find any, which is internally consistent and faithful to its stated limitations, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 518, "judge_prompt_tokens": 404, "judge_completion_tokens": 114}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 147.17459678649902, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer admits not knowing and does not provide an incorrect alternative, so it is faithful but not correct with respect to the gold answer 'Fantômas'.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 453, "judge_prompt_tokens": 389, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise", "latency_ms": 4310.242652893066, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the movie as 'Kill Cruise.'", "main_total_tokens": 2509, "main_prompt_tokens": 2504, "main_completion_tokens": 5, "judge_total_tokens": 436, "judge_prompt_tokens": 387, "judge_completion_tokens": 49}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 6079.126596450806, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct title of the movie about Fred Rogers and his TV series. It is both correct and fully faithful to the reference.", "main_total_tokens": 2771, "main_prompt_tokens": 2761, "main_completion_tokens": 10, "judge_total_tokens": 475, "judge_prompt_tokens": 408, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018", "latency_ms": 6153.851747512817, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (2018), so it is both correct and faithful.", "main_total_tokens": 2700, "main_prompt_tokens": 2692, "main_completion_tokens": 8, "judge_total_tokens": 431, "judge_prompt_tokens": 380, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Kid Cosmic", "latency_ms": 5378.488540649414, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for an animated TV series about a young boy robot in 2030 AD, which is clearly 'Astro Boy'. The generated answer 'Kid Cosmic' is incorrect. The evaluation correctly recognizes the mismatch with the gold answer, so it is faithful to the given data but the generated answer itself is not correct.", "main_total_tokens": 2572, "main_prompt_tokens": 2567, "main_completion_tokens": 5, "judge_total_tokens": 485, "judge_prompt_tokens": 389, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 years", "latency_ms": 5104.107141494751, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, stating they were held captive for 7 years, which is correct for the 2015 movie 'Room.'", "main_total_tokens": 2236, "main_prompt_tokens": 2231, "main_completion_tokens": 5, "judge_total_tokens": 459, "judge_prompt_tokens": 397, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m", "latency_ms": 4571.713924407959, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The Brink's-Mat robbery involved about £26 million in gold (plus some cash and diamonds), so the generated answer of £26m for the gold holding is correct and matches the provided gold value.", "main_total_tokens": 2122, "main_prompt_tokens": 2118, "main_completion_tokens": 4, "judge_total_tokens": 470, "judge_prompt_tokens": 398, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 579.9410343170166, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered that it did not know the value, which does not match the gold answer of 80.5772, so correctness is 0. However, saying 'I don't know' is faithful to its uncertainty and does not introduce incorrect fabricated information, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 486, "judge_prompt_tokens": 396, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 115.62538146972656, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct answer is 1, but the generated answer claims there are none, so it is factually incorrect. It is partially faithful in that it admits inability rather than hallucinating a specific incorrect number, but it still contradicts the gold answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 472, "judge_prompt_tokens": 394, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes", "latency_ms": 5091.090679168701, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Angel Eyes' exactly matches the gold answer and correctly identifies the TV show. It is both correct and fully faithful to the provided information.", "main_total_tokens": 3164, "main_prompt_tokens": 3159, "main_completion_tokens": 5, "judge_total_tokens": 459, "judge_prompt_tokens": 397, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "ko", "latency_ms": 7117.706298828125, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'ko' is the correct ISO language code for Korean, which matches the gold answer 'Korean'. The evaluation directly matches the provided gold label, so it is both correct and faithful.", "main_total_tokens": 2921, "main_prompt_tokens": 2904, "main_completion_tokens": 17, "judge_total_tokens": 449, "judge_prompt_tokens": 377, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 631.9506168365479, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer claims there are no matching movies, so it is factually incorrect. However, it faithfully reflects the model's stated inability to find such movies and does not hallucinate a specific incorrect number.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 403, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 1276.2722969055176, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold answer says the system has no movies from 1979; the generated answer says 'I don't know,' which is equivalent in this context. It does not add any incorrect information and is fully consistent with the gold.", "main_total_tokens": 562, "main_prompt_tokens": 560, "main_completion_tokens": 2, "judge_total_tokens": 474, "judge_prompt_tokens": 399, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "There is no Back to the Future part IV movie mentioned in the provided context.", "latency_ms": 8392.892599105835, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly note that there is no movie titled 'Back to the Future Part IV', so no release year exists. The generated answer is consistent with the given information and does not introduce unsupported claims.", "main_total_tokens": 3328, "main_prompt_tokens": 3309, "main_completion_tokens": 19, "judge_total_tokens": 491, "judge_prompt_tokens": 416, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that no movies from 1980 about the Chernobyl disaster can be found. The generated answer is faithful to the question by explaining the lack of matching movies and hinting that the event timing may be incorrect, which is consistent with known history (Chernobyl occurred in 1986).", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 532, "judge_prompt_tokens": 434, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 240.767240524292, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 4 such titles, but the generated answer claims there are none, which is factually incorrect. However, the generated answer is internally consistent with its own stated inability to find matches, so it is faithful to its (incorrect) reasoning process.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 397, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "3", "latency_ms": 15410.563468933105, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 2 but the generated answer is 3, so the response is incorrect. However, the model directly answered the question without introducing fabricated reasoning or external content, so it is faithful to its own (incorrect) claim.", "main_total_tokens": 3430, "main_prompt_tokens": 3319, "main_completion_tokens": 111, "judge_total_tokens": 465, "judge_prompt_tokens": 386, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 270.08533477783203, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show, but the generated answer claims there were none. This is factually incorrect relative to the gold, so correctness is 0. However, the answer does not hallucinate details and simply expresses inability to find matches, so it remains faithful to its own (albeit limited) information, leading to a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 506, "judge_prompt_tokens": 398, "judge_completion_tokens": 108}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 83.01353454589844, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says there are 0 such TV shows. The generated answer states that it cannot find any movies or shows matching the criteria, which effectively conveys that there are none. This matches the gold answer in substance, so it is correct and faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 482, "judge_prompt_tokens": 400, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 85.08563041687012, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the most popular TV show or movie about ninjas, and the gold answer is 'Naruto.' The generated answer says 'I don't know,' which is incorrect because a correct answer exists and is provided by the gold. However, the generated answer is faithful in that it does not hallucinate or provide wrong information; it simply admits lack of knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 386, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 1753.600835800171, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that no such movie was released. The generated answer says 'I don't know,' which is faithful to its own uncertainty but factually incorrect with respect to the gold answer. Hence correctness is 0, but faithfulness (to its own stated knowledge) is 1.", "main_total_tokens": 544, "main_prompt_tokens": 542, "main_completion_tokens": 2, "judge_total_tokens": 478, "judge_prompt_tokens": 389, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 2207.1757316589355, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that no such movie was released. The generated answer says 'I don't know,' which is not correct, since the information is knowable and provided in the gold. However, it is faithful in that it does not introduce any incorrect fabricated movie; it simply expresses a lack of knowledge.", "main_total_tokens": 546, "main_prompt_tokens": 544, "main_completion_tokens": 2, "judge_total_tokens": 478, "judge_prompt_tokens": 386, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "4", "latency_ms": 5709.774494171143, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '4' is not a movie title and does not correspond to the correct gold answer 'Star Wars: Droids - The Great Heep'. It is therefore incorrect. It is also unfaithful to the question because it does not provide a movie title or attempt to answer which film is the 4th oldest; it simply outputs a number without explanation.", "main_total_tokens": 2960, "main_prompt_tokens": 2956, "main_completion_tokens": 4, "judge_total_tokens": 497, "judge_prompt_tokens": 391, "judge_completion_tokens": 106}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "2002", "latency_ms": 8117.276191711426, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer (2002) does not match the gold answer (1986), so it is factually incorrect. It is therefore not faithful to the reference answer either.", "main_total_tokens": 3253, "main_prompt_tokens": 3228, "main_completion_tokens": 25, "judge_total_tokens": 446, "judge_prompt_tokens": 380, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 231.80246353149414, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct rating is 8.466, but the generated answer says 'I don't know,' which is incorrect. However, the model does not hallucinate a wrong rating and instead admits ignorance, so it is fully faithful to its own knowledge state.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 479, "judge_prompt_tokens": 398, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med", "latency_ms": 4172.951221466064, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct name of the TV show. It is both correct and faithful.", "main_total_tokens": 2255, "main_prompt_tokens": 2248, "main_completion_tokens": 7, "judge_total_tokens": 451, "judge_prompt_tokens": 394, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina", "latency_ms": 5469.187259674072, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer ('Argentina') exactly and is fully consistent with the provided information.", "main_total_tokens": 2691, "main_prompt_tokens": 2687, "main_completion_tokens": 4, "judge_total_tokens": 443, "judge_prompt_tokens": 392, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "I don't know.", "latency_ms": 2365.997552871704, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is Jiang Jie, but the generated answer says 'I don't know,' which is incorrect. However, it does not fabricate any false specific information and honestly states ignorance, so it is fully faithful to the model's stated uncertainty.", "main_total_tokens": 579, "main_prompt_tokens": 577, "main_completion_tokens": 2, "judge_total_tokens": 467, "judge_prompt_tokens": 388, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "6", "latency_ms": 15579.77032661438, "retrieval_recall": 0.0, "retrieval_mrr": 0.25, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.16812753627111746, "hit_rate_at_5": 1.0, "recall_at_10": 0.25, "precision_at_10": 0.1, "ndcg_at_10": 0.16812753627111746, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model output '6' while the gold answer is '4'. The response is therefore incorrect. However, the assistant did attempt to directly answer the numerical question without adding fabricated context, so it is faithful to the (incorrect) reasoning implied by the short answer format.", "main_total_tokens": 3546, "main_prompt_tokens": 3444, "main_completion_tokens": 102, "judge_total_tokens": 470, "judge_prompt_tokens": 385, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 4132.160663604736, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says there are no movies with both Bruce Lee and Steven Seagal. The generated answer states it cannot find any movies or shows matching all those criteria, which implies the same. This is correct and faithful to the gold response, with only a minor wording difference that does not change the meaning.", "main_total_tokens": 2250, "main_prompt_tokens": 2248, "main_completion_tokens": 2, "judge_total_tokens": 498, "judge_prompt_tokens": 406, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3", "latency_ms": 4629.026651382446, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated rating 6.3 matches the gold rating 6.3 exactly, so it is both correct and faithful.", "main_total_tokens": 2073, "main_prompt_tokens": 2069, "main_completion_tokens": 4, "judge_total_tokens": 443, "judge_prompt_tokens": 387, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "Drift-style racing", "latency_ms": 7293.859004974365, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Drift-style racing' correctly identifies the style of racing as drifting, matching the gold label 'Drift'. It is faithful to the gold answer with only a minor wording variation that does not change the meaning.", "main_total_tokens": 2865, "main_prompt_tokens": 2844, "main_completion_tokens": 21, "judge_total_tokens": 458, "judge_prompt_tokens": 381, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer", "latency_ms": 6671.478748321533, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct title of the movie about Jillian Lauren and Samuel Little. It is fully correct and faithful.", "main_total_tokens": 2667, "main_prompt_tokens": 2658, "main_completion_tokens": 9, "judge_total_tokens": 475, "judge_prompt_tokens": 412, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838", "latency_ms": 4736.972332000732, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) with correct value and format.", "main_total_tokens": 2372, "main_prompt_tokens": 2368, "main_completion_tokens": 4, "judge_total_tokens": 440, "judge_prompt_tokens": 389, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense", "latency_ms": 5546.769618988037, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 1999 psychological thriller as 'The Sixth Sense' and is fully supported by the question.", "main_total_tokens": 2699, "main_prompt_tokens": 2693, "main_completion_tokens": 6, "judge_total_tokens": 456, "judge_prompt_tokens": 394, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002", "latency_ms": 5631.443500518799, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (2002), so it is both correct and faithful.", "main_total_tokens": 2818, "main_prompt_tokens": 2813, "main_completion_tokens": 5, "judge_total_tokens": 434, "judge_prompt_tokens": 383, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled", "latency_ms": 6142.856121063232, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the title of the 2015 reboot of 'The Transporter' as 'The Transporter Refueled.'", "main_total_tokens": 2905, "main_prompt_tokens": 2896, "main_completion_tokens": 9, "judge_total_tokens": 462, "judge_prompt_tokens": 397, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015", "latency_ms": 5691.691160202026, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (2015), so it is both correct and faithful.", "main_total_tokens": 2830, "main_prompt_tokens": 2825, "main_completion_tokens": 5, "judge_total_tokens": 438, "judge_prompt_tokens": 387, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "I don't know.", "latency_ms": 2343.3473110198975, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no such movie description can be found. The generated answer admits not knowing. Given the information, saying 'I don't know' is effectively correct and does not invent details, so it is both correct and faithful to the gold.", "main_total_tokens": 578, "main_prompt_tokens": 576, "main_completion_tokens": 2, "judge_total_tokens": 475, "judge_prompt_tokens": 394, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "I don't know.", "latency_ms": 2231.741189956665, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answer is incorrect because the correct title is 'Havana Darkness', but the model responded with 'I don't know.' However, it is faithful in that it does not hallucinate an incorrect movie title and instead honestly states a lack of knowledge.", "main_total_tokens": 584, "main_prompt_tokens": 582, "main_completion_tokens": 2, "judge_total_tokens": 473, "judge_prompt_tokens": 392, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 5368.499994277954, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Doll & Em'. It is both correct and faithful.", "main_total_tokens": 2565, "main_prompt_tokens": 2558, "main_completion_tokens": 7, "judge_total_tokens": 451, "judge_prompt_tokens": 392, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in the movie \"Sleuth\".", "latency_ms": 8703.047513961792, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the described 2007 movie is 'Hallam Foe', not 'Sleuth'. However, the response is fully faithful to the given generated text without adding unsupported information.", "main_total_tokens": 2672, "main_prompt_tokens": 2641, "main_completion_tokens": 31, "judge_total_tokens": 500, "judge_prompt_tokens": 427, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "At first, it sounds like he's a foe because \"Foe\" is a surname that means enemy or adversary.", "latency_ms": 8765.17128944397, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer matches the gold label ('A foe'), so correctness is 1. The explanation about the surname sounding like 'foe' and meaning enemy is plausible and consistent with the question, though it slightly extends beyond the provided context, so faithfulness is high but not perfect.", "main_total_tokens": 2858, "main_prompt_tokens": 2822, "main_completion_tokens": 36, "judge_total_tokens": 500, "judge_prompt_tokens": 410, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18", "latency_ms": 5609.301805496216, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (18), so it is both correct and faithful to the reference.", "main_total_tokens": 2811, "main_prompt_tokens": 2807, "main_completion_tokens": 4, "judge_total_tokens": 442, "judge_prompt_tokens": 389, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam is on the autism spectrum.", "latency_ms": 6024.00541305542, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which exactly matches the gold answer and is faithful to the question and reference.", "main_total_tokens": 2761, "main_prompt_tokens": 2751, "main_completion_tokens": 10, "judge_total_tokens": 453, "judge_prompt_tokens": 394, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix", "latency_ms": 6030.115127563477, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Chamonix'), so it is both correct and faithful to the provided reference.", "main_total_tokens": 3077, "main_prompt_tokens": 3071, "main_completion_tokens": 6, "judge_total_tokens": 456, "judge_prompt_tokens": 400, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "It rains", "latency_ms": 6493.061780929565, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'It rains' is semantically identical to the gold answer 'rains' and correctly fills the blank. It is fully consistent with the provided information.", "main_total_tokens": 3512, "main_prompt_tokens": 3507, "main_completion_tokens": 5, "judge_total_tokens": 458, "judge_prompt_tokens": 393, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland", "latency_ms": 5899.924039840698, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Greenland'), and it is correct for the question about Tabaluga being crown-prince of Greenland.", "main_total_tokens": 2989, "main_prompt_tokens": 2984, "main_completion_tokens": 5, "judge_total_tokens": 456, "judge_prompt_tokens": 395, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon", "latency_ms": 5212.834596633911, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'One Afternoon' exactly matches the gold answer and correctly identifies the movie from the description. There is no deviation or hallucination from the provided information.", "main_total_tokens": 2374, "main_prompt_tokens": 2368, "main_completion_tokens": 6, "judge_total_tokens": 470, "judge_prompt_tokens": 406, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The movie \"Low Life\" (Doc 56834) has a rating of 5.1, which is considered low-rated. The plot does not mention a holocaust survivor taking care of a 12-year-old street kid who recently robbed her.\n\nHowever, another movie \"See You Tomorrow\" (Doc 61957) has an Italian language and a rating of 5.8, but its plot is not provided in the context.\n\nA third movie \"Low Profile\" (Doc 58235) has a French language and a rating of 5.969, but its plot does not mention a holocaust survivor or a 12-year-old street kid.\n\nThe closest match to the question is \"See You Tomorrow\" (Doc 61957), which is an Italian movie with a low rating, but its plot is not provided in the context.", "latency_ms": 15241.50824546814, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "recall_at_10": 0.0, "precision_at_5": 0.0, "precision_at_10": 0.0, "ndcg_at_5": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_5": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.2, "justification": "The correct answer is 'The Life Ahead'. The generated answer instead guesses 'See You Tomorrow' as the closest match and never identifies the correct film, so the factual answer to the user query is wrong (correctness 0). In terms of faithfulness to the provided context, the response describes details (ratings, languages, plots) drawn from specific docs and does not invent obviously contradictory facts, but it incorrectly concludes a closest match and speculates about the answer without support for the Holocaust-survivor plot. This partial speculation and incorrect selection reduce faithfulness (0.2).", "main_total_tokens": 2999, "main_prompt_tokens": 2824, "main_completion_tokens": 175, "judge_total_tokens": 719, "judge_prompt_tokens": 570, "judge_completion_tokens": 149}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "Bruce McLaren", "latency_ms": 5797.928810119629, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying Bruce McLaren as the subject of the 2016 documentary 'McLaren.'", "main_total_tokens": 2859, "main_prompt_tokens": 2854, "main_completion_tokens": 5, "judge_total_tokens": 445, "judge_prompt_tokens": 387, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren", "latency_ms": 6476.487159729004, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0.0, "faithfulness_score": 1.0, "justification": "The gold answer specifies the full team name 'The McLaren Motor Racing team', while the generated answer only says 'McLaren', which is incomplete and not strictly correct as a name. However, it is consistent with the information given (a shortened form rather than a contradiction), so faithfulness is high but correctness is low.", "main_total_tokens": 2798, "main_prompt_tokens": 2788, "main_completion_tokens": 10, "judge_total_tokens": 485, "judge_prompt_tokens": 385, "judge_completion_tokens": 100}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "Sweden", "latency_ms": 6037.012815475464, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer 'Sweden' is incorrect because the gold label is 'New Zealand'. However, the answer is a direct attempt to answer the question without adding unsupported information, so it is faithful to the prompt itself.", "main_total_tokens": 2519, "main_prompt_tokens": 2507, "main_completion_tokens": 12, "judge_total_tokens": 452, "judge_prompt_tokens": 377, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "I don't know.", "latency_ms": 2308.067560195923, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that there is no such movie in the database, which is effectively equivalent to saying the system does not know of such a movie. The generated answer says \"I don't know,\" which matches that stance. There is in fact no 2013 Die Hard/John McClane movie involving blowing up the Great Wall of China, so declining knowledge is correct. The generated answer is fully consistent with the gold answer and does not introduce any incorrect information.", "main_total_tokens": 558, "main_prompt_tokens": 556, "main_completion_tokens": 2, "judge_total_tokens": 521, "judge_prompt_tokens": 402, "judge_completion_tokens": 119}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit", "latency_ms": 5893.422603607178, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2002 Japanese horror film as 'Tomie: Forbidden Fruit'. It is both correct and fully faithful to the provided gold reference.", "main_total_tokens": 2853, "main_prompt_tokens": 2845, "main_completion_tokens": 8, "judge_total_tokens": 492, "judge_prompt_tokens": 422, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65", "latency_ms": 5904.215097427368, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65), so it is both correct and faithful to the given reference.", "main_total_tokens": 3069, "main_prompt_tokens": 3065, "main_completion_tokens": 4, "judge_total_tokens": 450, "judge_prompt_tokens": 396, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "I don't know.", "latency_ms": 2372.2736835479736, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'The Book of Souls.' The generated answer, 'I don't know,' is incorrect. However, the model did not hallucinate an incorrect fact; it admitted ignorance, so it is fully faithful to its own uncertainty.", "main_total_tokens": 592, "main_prompt_tokens": 590, "main_completion_tokens": 2, "judge_total_tokens": 477, "judge_prompt_tokens": 398, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2209.0137004852295, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about a movie the system doesn’t know. The gold answer says it’s not in the database; the generated answer says “I don’t know.” This is effectively the same stance—no fabricated year is given. The answer is therefore correct (no incorrect facts) and faithful (aligned with the gold’s lack of knowledge).", "main_total_tokens": 570, "main_prompt_tokens": 568, "main_completion_tokens": 2, "judge_total_tokens": 489, "judge_prompt_tokens": 395, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2231.050968170166, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is not in the database, so admitting not knowing is correct. The generated response matches that uncertainty and does not invent information, so it is both correct and faithful to the gold.", "main_total_tokens": 581, "main_prompt_tokens": 579, "main_completion_tokens": 2, "judge_total_tokens": 466, "judge_prompt_tokens": 393, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2223.5851287841797, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database, which is equivalent to the generated answer of not knowing. Both convey lack of knowledge about the release year, so the generated answer is correct and faithful to the gold.", "main_total_tokens": 563, "main_prompt_tokens": 561, "main_completion_tokens": 2, "judge_total_tokens": 466, "judge_prompt_tokens": 395, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2244.267225265503, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer explains that the movie is not in the database, while the generated answer states simply 'I don't know.' These are not equivalent: lacking the movie in a database does not necessarily equal not knowing in the same sense, so the generated answer does not faithfully match the gold response. However, the generated answer does reflect uncertainty and does not hallucinate a year, so it is epistemically cautious and consistent with the idea of not having the needed information. It is still not correct relative to the specified gold.", "main_total_tokens": 565, "main_prompt_tokens": 563, "main_completion_tokens": 2, "judge_total_tokens": 529, "judge_prompt_tokens": 395, "judge_completion_tokens": 134}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "No information found about \"Python's Revenge 2\".", "latency_ms": 6128.817081451416, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that no information is found about the movie, matching the gold response that it is not in the database. It does not hallucinate a release year and is fully faithful to the implied knowledge limits.", "main_total_tokens": 2687, "main_prompt_tokens": 2673, "main_completion_tokens": 14, "judge_total_tokens": 477, "judge_prompt_tokens": 402, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2373.898983001709, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database; the generated answer says 'I don't know.' Both convey lack of knowledge and are effectively equivalent. Therefore the generated answer is correct and faithful to the gold.", "main_total_tokens": 529, "main_prompt_tokens": 527, "main_completion_tokens": 2, "judge_total_tokens": 467, "judge_prompt_tokens": 393, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2231.905460357666, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, which is equivalent in substance to the generated answer saying 'I don't know.' Both convey lack of knowledge about the release year, so the generated answer is correct and faithful to the gold.", "main_total_tokens": 572, "main_prompt_tokens": 570, "main_completion_tokens": 2, "judge_total_tokens": 477, "judge_prompt_tokens": 396, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The question is \"Who directed the movie 'The Last Days of Lehman Brothers'?\"\n\nUnfortunately, there is no information about the director in the provided context.", "latency_ms": 8398.592948913574, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director information is not available and aligns with the gold answer indicating the database does not provide the director. It does not introduce unsupported information and is faithful to the given constraints.", "main_total_tokens": 2783, "main_prompt_tokens": 2748, "main_completion_tokens": 35, "judge_total_tokens": 501, "judge_prompt_tokens": 429, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 2345.247507095337, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the system cannot answer because the information is not in the database. The generated answer says \"I don't know,\" which is effectively the same epistemic stance: it does not hallucinate a director and admits lack of knowledge. This is correct and faithful to the gold response.", "main_total_tokens": 575, "main_prompt_tokens": 573, "main_completion_tokens": 2, "judge_total_tokens": 489, "judge_prompt_tokens": 399, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of \"Journey into Gaza\" is not mentioned in the provided context.", "latency_ms": 7199.711084365845, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned and aligns with the gold response indicating the database does not provide the director information. It does not introduce any unsupported facts and remains faithful to the given context.", "main_total_tokens": 3021, "main_prompt_tokens": 3001, "main_completion_tokens": 20, "judge_total_tokens": 482, "judge_prompt_tokens": 409, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The question does not provide enough context to determine who directed \"The Last Night at Tremore Beach\".", "latency_ms": 6823.164224624634, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the system cannot answer because the director is not in the database. The generated answer similarly says there is not enough context to determine the director. Both refrain from inventing a director and align in substance, so it is correct relative to the gold and faithful to the limited information.", "main_total_tokens": 2420, "main_prompt_tokens": 2397, "main_completion_tokens": 23, "judge_total_tokens": 507, "judge_prompt_tokens": 417, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 5717.90885925293, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy' and faithfully using only the given information.", "main_total_tokens": 2570, "main_prompt_tokens": 2561, "main_completion_tokens": 9, "judge_total_tokens": 456, "judge_prompt_tokens": 396, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 5564.5432472229, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Stranger in My Bed') and correctly identifies the movie described. It is both correct and fully faithful to the reference.", "main_total_tokens": 2574, "main_prompt_tokens": 2566, "main_completion_tokens": 8, "judge_total_tokens": 464, "judge_prompt_tokens": 400, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 5399.189710617065, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question clearly refers to the TV show 'Game of Silence', but it asks for a movie. Both the gold and generated answers give 'Game of Silence', matching each other and the intended work, so the generated answer is correct relative to the gold and faithful to the prompt.", "main_total_tokens": 2257, "main_prompt_tokens": 2251, "main_completion_tokens": 6, "judge_total_tokens": 487, "judge_prompt_tokens": 400, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 5339.3237590789795, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'The Craigslist Killer' based on the given description.", "main_total_tokens": 2548, "main_prompt_tokens": 2542, "main_completion_tokens": 6, "judge_total_tokens": 451, "judge_prompt_tokens": 395, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 6525.021314620972, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which correctly identifies the movie as 'Wood Job!'. The response is both correct and fully faithful to the reference.", "main_total_tokens": 3481, "main_prompt_tokens": 3475, "main_completion_tokens": 6, "judge_total_tokens": 466, "judge_prompt_tokens": 403, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 7595.9978103637695, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'Sesame Street: The Magical Wand Chase.'", "main_total_tokens": 3073, "main_prompt_tokens": 3051, "main_completion_tokens": 22, "judge_total_tokens": 472, "judge_prompt_tokens": 417, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond", "latency_ms": 5153.7909507751465, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Beyond'). The plot description corresponds to the film 'Beyond' and there is no additional or conflicting information, so the response is both correct and faithful.", "main_total_tokens": 2472, "main_prompt_tokens": 2468, "main_completion_tokens": 4, "judge_total_tokens": 479, "judge_prompt_tokens": 409, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9", "latency_ms": 5560.7452392578125, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('6ixtynin9') and is consistent with the question. Therefore it is fully correct and faithful.", "main_total_tokens": 2497, "main_prompt_tokens": 2488, "main_completion_tokens": 9, "judge_total_tokens": 471, "judge_prompt_tokens": 409, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread", "latency_ms": 5681.034088134766, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Club Dread'), correctly identifying the movie and fully aligning with the given reference.", "main_total_tokens": 2825, "main_prompt_tokens": 2820, "main_completion_tokens": 5, "judge_total_tokens": 483, "judge_prompt_tokens": 427, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet", "latency_ms": 5896.223545074463, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the movie as 'Toilet'. It is fully consistent with the provided information.", "main_total_tokens": 3021, "main_prompt_tokens": 3016, "main_completion_tokens": 5, "judge_total_tokens": 473, "judge_prompt_tokens": 415, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel", "latency_ms": 5838.838815689087, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Electric Angel', so it is both correct and fully faithful to the reference.", "main_total_tokens": 2789, "main_prompt_tokens": 2784, "main_completion_tokens": 5, "judge_total_tokens": 473, "judge_prompt_tokens": 418, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "5.207", "latency_ms": 4379.515886306763, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the lowest rated movie from 2023, and the gold answer is 'Mega Ape'. The generated answer '5.207' is not a movie title and does not match the gold answer or the type of answer requested, so it is incorrect. It is also not a faithful transformation or paraphrase of the gold answer.", "main_total_tokens": 2693, "main_prompt_tokens": 2687, "main_completion_tokens": 6, "judge_total_tokens": 485, "judge_prompt_tokens": 385, "judge_completion_tokens": 100}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 156.2511920928955, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Tea House', but the generated answer says 'I don't know', which is incorrect with respect to the gold answer. However, the model does not fabricate an incorrect fact; it admits not knowing, so it is fully faithful to its own (lack of) knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 386, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 164.13068771362305, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies an actual movie title, while the generated answer says 'I don't know,' which does not provide the correct information. Therefore, the correctness is 0. However, the model is honest about its uncertainty and does not fabricate an answer, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 484, "judge_prompt_tokens": 395, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is Cataclysm with a rating of 3.889.\n\nIts language is en (English).", "latency_ms": 7486.145496368408, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980. The gold answer is simply 'English'. The generated answer identifies the movie as 'Cataclysm' and then states its language is 'en (English)'. Although it adds extra detail (the movie title and rating), it correctly provides the requested language and matches the gold label. There is no contradiction with the gold answer, so it is both correct and faithful to the gold.", "main_total_tokens": 2994, "main_prompt_tokens": 2963, "main_completion_tokens": 31, "judge_total_tokens": 536, "judge_prompt_tokens": 412, "judge_completion_tokens": 124}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity is Zoom [Doc 115355].", "latency_ms": 7389.469861984253, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold label indicates that the TV show is 'Law & Order: Special Victims Unit', while the model output says 'Zoom'. Additionally, the generated answer fabricates a citation '[Doc 115355]' that is not grounded in the provided information, making it unfaithful.", "main_total_tokens": 2937, "main_prompt_tokens": 2915, "main_completion_tokens": 22, "judge_total_tokens": 503, "judge_prompt_tokens": 409, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2.", "latency_ms": 5512.3021602630615, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 for 'Bluefin' and does not add any unsupported information.", "main_total_tokens": 2265, "main_prompt_tokens": 2251, "main_completion_tokens": 14, "judge_total_tokens": 452, "judge_prompt_tokens": 395, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "1999", "latency_ms": 5687.2193813323975, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1999) is incorrect because the gold label specifies 2019 as the release year. However, the model is consistently asserting 1999, so it is faithful to its (incorrect) assertion.", "main_total_tokens": 2770, "main_prompt_tokens": 2765, "main_completion_tokens": 5, "judge_total_tokens": 462, "judge_prompt_tokens": 387, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "1", "latency_ms": 4509.954214096069, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The answer '1' is incorrect because the gold label shows the correct number of votes is 279. However, the model did respond to the user’s question and did not fabricate any additional information beyond providing the (incorrect) number, so it is faithful to the prompt but not to the ground truth.", "main_total_tokens": 2246, "main_prompt_tokens": 2244, "main_completion_tokens": 2, "judge_total_tokens": 470, "judge_prompt_tokens": 383, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "6", "latency_ms": 12253.905296325684, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (6) does not match the gold answer (12), so it is incorrect. However, there is no indication that the model contradicted the provided context or hallucinated beyond the question; it simply gave the wrong number, so faithfulness is preserved.", "main_total_tokens": 2375, "main_prompt_tokens": 2287, "main_completion_tokens": 88, "judge_total_tokens": 468, "judge_prompt_tokens": 383, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "I don't know.", "latency_ms": 2551.0714054107666, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered 'I don't know,' which is incorrect because the correct answer is 2009. However, the response is faithful in that it does not fabricate an incorrect year or additional information.", "main_total_tokens": 539, "main_prompt_tokens": 537, "main_completion_tokens": 2, "judge_total_tokens": 461, "judge_prompt_tokens": 391, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_bge-m3_llama3.1-70b-iq2_xs", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en", "latency_ms": 5539.862632751465, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The movie \"The King’s Speech\" is originally in English, and both the gold answer and the generated answer correctly state \"en\" as the language. The response is fully correct and matches the provided gold label.", "main_total_tokens": 2823, "main_prompt_tokens": 2819, "main_completion_tokens": 4, "judge_total_tokens": 461, "judge_prompt_tokens": 384, "judge_completion_tokens": 77}
