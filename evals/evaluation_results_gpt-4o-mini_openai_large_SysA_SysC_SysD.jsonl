{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "Wall Street was released in 1987 [Doc 6993].", "latency_ms": 14900.502681732178, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, which matches the gold answer. It is also faithful to the provided information and does not introduce unsupported claims.", "main_total_tokens": 2912, "main_prompt_tokens": 2898, "main_completion_tokens": 14, "judge_total_tokens": 459, "judge_prompt_tokens": 392, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "In the movie \"Brain Fluid Explosion Girl,\" classmates are forced to take part in a survival game using their cellphones [Doc 69929].", "latency_ms": 2163.583278656006, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie features classmates playing a survival game using their cell phones. The gold answer is 'Brain Fluid Explosion Girl.' The generated answer correctly identifies the same movie and restates the premise. There are no contradictions or hallucinations beyond referencing a supporting document. Thus, it is both correct and faithful to the provided gold answer.", "main_total_tokens": 2860, "main_prompt_tokens": 2831, "main_completion_tokens": 29, "judge_total_tokens": 515, "judge_prompt_tokens": 417, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show in which six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons,\" which aired in 1983 [Doc 112849].", "latency_ms": 1165.3010845184326, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.4306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.719741384391281, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' matching the gold answer exactly, and does not introduce any contradictory or unsupported information. Reference to airing in 1983 is accurate and consistent with the known facts, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3042, "main_prompt_tokens": 3006, "main_completion_tokens": 36, "judge_total_tokens": 514, "judge_prompt_tokens": 426, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is titled \"Nacht der Wölfe,\" released in 1982 [Doc 2590].", "latency_ms": 2561.014413833618, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe,' which matches the gold answer exactly. It also remains faithful to the provided information and does not introduce contradictions; the added release year detail is plausible and does not conflict with the gold answer.", "main_total_tokens": 2840, "main_prompt_tokens": 2795, "main_completion_tokens": 45, "judge_total_tokens": 527, "judge_prompt_tokens": 443, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of \"Picnic at Hanging Rock\" take place in the year 1900 [Doc 126191].", "latency_ms": 1163.780927658081, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 1900 as the year in which the events of 'Picnic at Hanging Rock' take place, matching the gold answer. It is fully consistent with the provided information and does not introduce unsupported claims.", "main_total_tokens": 2641, "main_prompt_tokens": 2617, "main_completion_tokens": 24, "judge_total_tokens": 486, "judge_prompt_tokens": 409, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The movie you are referring to is \"June,\" which was released in 2004. It features a young violinist named June who works in a coffee shop in Venice and dreams about her ideal man while rehearsing for her music career [Doc 32483].", "latency_ms": 2299.3853092193604, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a 2004 movie about a young violinist named after a calendar month, working in a Venice coffee shop and pursuing a music career. The gold answer is 'June.' The generated answer correctly identifies the movie as 'June' and restates the premise without introducing contradictions. It remains faithful to the information in the prompt and gold answer, so both correctness and faithfulness are 1.", "main_total_tokens": 3114, "main_prompt_tokens": 3061, "main_completion_tokens": 53, "judge_total_tokens": 598, "judge_prompt_tokens": 485, "judge_completion_tokens": 113}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "The highest rated Russian-language movie released in 1980 is \"Moscow Does Not Believe in Tears,\" which has a rating of 7.6 [Doc 61].", "latency_ms": 1904.58345413208, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer names \"Moscow Does Not Believe in Tears\" as the highest rated Russian-language movie of 1980, but the gold answer indicates that the correct title is \"The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger.\" Therefore the answer is factually incorrect. It is also unfaithful to the gold/reference answer, since it contradicts it rather than paraphrasing or aligning with it.", "main_total_tokens": 2873, "main_prompt_tokens": 2838, "main_completion_tokens": 35, "judge_total_tokens": 546, "judge_prompt_tokens": 430, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest Russian-language movie rating in 1999 is 7.117, for the movie \"The Rifleman of the Voroshilov Regiment\" [Doc 22123].", "latency_ms": 1127.0816326141357, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the highest rating is 7.6, while the generated answer claims it is 7.117 for a specific movie. This directly contradicts the gold value, so the answer is incorrect. Additionally, the model asserts a specific rating and film based on a cited document, but we are evaluating against the gold data, and the claim does not match; thus it is unfaithful to the ground truth for this task.", "main_total_tokens": 2739, "main_prompt_tokens": 2702, "main_completion_tokens": 37, "judge_total_tokens": 542, "judge_prompt_tokens": 422, "judge_completion_tokens": 120}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "There are three movies released in 1987 that have a rating over 7 and are in Chinese:\n\n1. **A Chinese Ghost Story** - Rating: 7.3 [Doc 7064]\n2. **Last Eunuch In China** - Rating: 7.8 [Doc 7754]\n3. **The Last Emperor** - Rating: 7.599 [Doc 6989]", "latency_ms": 2594.766855239868, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the number of movies; the gold answer is 4, but the generated answer says there are three, which is numerically incorrect. The model then lists three specific movies, asserting that these are the complete set matching the criteria, which conflicts with the gold label indicating there are four such movies, so it is not faithful to the ground truth.", "main_total_tokens": 2895, "main_prompt_tokens": 2811, "main_completion_tokens": 84, "judge_total_tokens": 580, "judge_prompt_tokens": 476, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "There are two TV shows released in 1987, have a rating over 7, and are in Chinese:\n\n1. **Journey to the West** - Rating: 8.4 [Doc 113169]\n2. **Dream of the Red Chamber** - Rating: 9.4 [Doc 113285]", "latency_ms": 1822.8299617767334, "retrieval_recall": 0.0, "retrieval_mrr": 0.07142857142857142, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there are two TV shows matching the criteria, which matches the gold answer (2). It also stays faithful to the question, listing two Chinese TV shows from 1987 with ratings over 7 without adding contradictory or irrelevant information.", "main_total_tokens": 2753, "main_prompt_tokens": 2687, "main_completion_tokens": 66, "judge_total_tokens": 542, "judge_prompt_tokens": 459, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "The only French language TV show released in 1980 is \"Fantômas\" [Doc 112642].", "latency_ms": 1056.525468826294, "retrieval_recall": 1.0, "retrieval_mrr": 0.2, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.38685280723454163, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.38685280723454163, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 'Fantômas' as the only French language TV show released in 1980, matching the gold answer exactly and remaining faithful to the provided information.", "main_total_tokens": 2687, "main_prompt_tokens": 2664, "main_completion_tokens": 23, "judge_total_tokens": 475, "judge_prompt_tokens": 408, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is \"Kill Cruise,\" released in 1990 [Doc 11205].", "latency_ms": 1302.687644958496, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' matching the gold answer exactly. It is faithful to the provided information and does not introduce unsupported details beyond the title and release year, which is accurate.", "main_total_tokens": 2927, "main_prompt_tokens": 2898, "main_completion_tokens": 29, "judge_total_tokens": 487, "judge_prompt_tokens": 414, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 1560.7712268829346, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as \"Won't You Be My Neighbor?\" and remains faithful to the question without adding incorrect details.", "main_total_tokens": 3278, "main_prompt_tokens": 3239, "main_completion_tokens": 39, "judge_total_tokens": 506, "judge_prompt_tokens": 440, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The release years for the movies mentioned are as follows:\n\n- \"Year of the Horse\": 1997 [Doc 19452]\n- \"YEAR\": 2023 [Doc 107925]\n- \"What It Was\": 2014 [Doc 65131]\n- \"Year of the Devil\": 2002 [Doc 28274]\n- \"The Year After\": 2007 [Doc 40870]\n- \"Another Year\": 2014 [Doc 62952]\n- \"The Year of the Wolf\": 2007 [Doc 39815]\n- \"Year of the Jellyfish\": 1984 [Doc 4092]\n- \"The Year of the Dog\": 1994 [Doc 15249]\n- \"The Last Year\": 2002 [Doc 28876]\n- \"Is It Easy to Be Young?\": 1986 [Doc 6805]\n- \"The Year of the Deer\": 1995 [Doc 17784]\n- \"Year of the Dragon\": 1985 [Doc 5019]\n- \"Let It Be\": 2005 [Doc 35791]\n- \"Greatest Geek Year Ever! 1982\": 2023 [Doc 134707]\n- \"What Do You Think?\": 1986 [Doc 6849]\n- \"The Year of the Tick\": 2004 [Doc 32597]\n- \"The Year That Trembled\": 2002 [Doc 29080]\n- \"This Is the Year\": 2020 [Doc 94546]\n- \"They Were There\": 2011 [Doc 54825]", "latency_ms": 8274.114608764648, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a single year of release with a gold answer of 2018. The generated answer instead lists many different movies and years, none of which is 2018, and does not identify or answer the specific target item. Thus it is incorrect. It is also unfaithful to the gold answer because it does not mention 2018 or focus on the intended entity.", "main_total_tokens": 3389, "main_prompt_tokens": 3053, "main_completion_tokens": 336, "judge_total_tokens": 822, "judge_prompt_tokens": 713, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is \"Astro Boy,\" which was released in 1980. The plot involves a young boy robot who is rescued by an Android Advocate, Dr. Elefun, after being abandoned by his creator, and he is guided in the use of his special powers to aid and protect mankind and robot-kind [Doc 112591].", "latency_ms": 2431.4682483673096, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the series as \"Astro Boy,\" matching the gold answer. The added plot details and year reference are consistent with known information and align with the cited document, so the response remains faithful to the given context.", "main_total_tokens": 3285, "main_prompt_tokens": 3202, "main_completion_tokens": 83, "judge_total_tokens": 548, "judge_prompt_tokens": 470, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room,\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 1250.676155090332, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The answer correctly states that in the 2015 film \"Room,\" the woman and her son were held captive for 7 years. This matches the gold answer and does not introduce any hallucinated details, so it is both correct and faithful.", "main_total_tokens": 2486, "main_prompt_tokens": 2456, "main_completion_tokens": 30, "judge_total_tokens": 504, "judge_prompt_tokens": 424, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 2061.004877090454, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before the 26 November 1983 break-in, which matches the provided gold answer (£26m). It is fully faithful to the given information and does not introduce unsupported claims.", "main_total_tokens": 2772, "main_prompt_tokens": 2739, "main_completion_tokens": 33, "judge_total_tokens": 516, "judge_prompt_tokens": 428, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "The most popular English-speaking TV show released in 1997 is \"USA High,\" which has a popularity score of 3.535 [Doc 114769].", "latency_ms": 1423.7005710601807, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The answer is numerically incorrect; the question asks for a popularity score of 80.5772, but the generated answer gives 3.535. It is also unfaithful to the gold/reference information, as it invents a different show and score than the correct one.", "main_total_tokens": 2577, "main_prompt_tokens": 2544, "main_completion_tokens": 33, "judge_total_tokens": 513, "judge_prompt_tokens": 425, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "The context does not provide any information about Russian language TV shows released in 1997. Therefore, I cannot answer that question.", "latency_ms": 1220.168113708496, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 1, but the generated answer says it cannot answer the question, which is incorrect. However, given only the question and no supporting context, the model is faithful in acknowledging lack of information rather than hallucinating a number.", "main_total_tokens": 2606, "main_prompt_tokens": 2580, "main_completion_tokens": 26, "judge_total_tokens": 489, "judge_prompt_tokens": 409, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 1273.423671722412, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Angel Eyes', which matches the gold answer exactly and is fully supported by the cited document.", "main_total_tokens": 3139, "main_prompt_tokens": 3104, "main_completion_tokens": 35, "judge_total_tokens": 488, "judge_prompt_tokens": 429, "judge_completion_tokens": 59}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French (fr) [Doc 110681].", "latency_ms": 802.53005027771, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states the movie language is French, while the gold label says Korean, so the answer is factually incorrect. However, the answer is internally consistent with its own cited document and does not contradict its own evidence, so it is faithful to the provided context.", "main_total_tokens": 2997, "main_prompt_tokens": 2977, "main_completion_tokens": 20, "judge_total_tokens": 481, "judge_prompt_tokens": 396, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "There are no English-speaking horror movies released in 2009 with a popularity score greater than 7 based on the provided context. The highest popularity score for a 2009 horror movie listed is 1.9113 for \"Possessed\" [Doc 45493].", "latency_ms": 1568.3238506317139, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 2 such movies, while the generated answer claims there are none, so it is factually incorrect with respect to the gold. However, the generated answer is consistent with the context it cites (claiming the highest popularity is 1.9113), so it is internally faithful to the provided context.", "main_total_tokens": 2729, "main_prompt_tokens": 2674, "main_completion_tokens": 55, "judge_total_tokens": 545, "judge_prompt_tokens": 447, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The context provided does not include any movies released in 1979, so I cannot provide an answer.", "latency_ms": 1018.9592838287354, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there is no information about movies released in 1979 in the given context/database, so an answer cannot be provided. The generated answer is consistent with the gold response and does not introduce any unsupported claims.", "main_total_tokens": 2791, "main_prompt_tokens": 2770, "main_completion_tokens": 21, "judge_total_tokens": 497, "judge_prompt_tokens": 416, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "There is no movie titled \"Back to the Future Part IV\" mentioned in the provided context. The existing entries include \"Back to the Future\" (1985), \"Back to the Future Part II\" (1989), and \"Back to the Future Part III\" (1990) [Doc 9405], [Doc 10621], [Doc 4834].", "latency_ms": 2594.434976577759, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no movie titled 'Back to the Future Part IV' in the provided context, matching the gold answer’s assertion that such a movie is not in the database. It does not hallucinate a release year and is fully grounded in the given documents, so both correctness and faithfulness are maximized.", "main_total_tokens": 3382, "main_prompt_tokens": 3306, "main_completion_tokens": 76, "judge_total_tokens": 574, "judge_prompt_tokens": 476, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "There is no movie about the nuclear Chernobyl disaster from 1980 in the provided context. The only movie listed from 1980 is \"Cataclysm,\" which is not related to the Chernobyl disaster. Therefore, I cannot provide a popularity score for such a movie.", "latency_ms": 1860.640525817871, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the popularity score of the least popular 1980 movie about the Chernobyl disaster. Both the gold and generated answers correctly state that there is no such movie from 1980, so no popularity score can be given. The generated answer also correctly references the given context (mentioning the only 1980 movie listed, \"Cataclysm,\" and noting it is unrelated). This is fully consistent with the gold answer and the available information, so it is both correct and faithful.", "main_total_tokens": 3164, "main_prompt_tokens": 3105, "main_completion_tokens": 59, "judge_total_tokens": 598, "judge_prompt_tokens": 465, "judge_completion_tokens": 133}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are four entries with 'Astro Boy' in the title:\n\n1. Astro Boy (Movie, 2009) - [Doc 44947]\n2. Astro Boy (TV, 2003) - [Doc 116265]\n3. Astro Boy (TV, 1980) - [Doc 112591]\n4. Astro Boy vs The Junkyard Pirates (Movie, 2010) - [Doc 49121]", "latency_ms": 3328.6333084106445, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 1.1510196182278052, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.773714921545009, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states there are four entries with 'Astro Boy' in the title, matching the gold answer of 4. It also lists specific examples that are consistent with the count and does not introduce contradictions, so it is both correct and faithful.", "main_total_tokens": 2931, "main_prompt_tokens": 2840, "main_completion_tokens": 91, "judge_total_tokens": 560, "judge_prompt_tokens": 477, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "Based on the provided context, there are no movies in 2025 that specifically allude to the Holocaust. The movies listed for 2025 do not mention the Holocaust in their descriptions. Therefore, the answer is none.", "latency_ms": 1389.7664546966553, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 2 such movies, while the generated answer claims there are none. This directly contradicts the gold, so the correctness is 0. However, the generated answer is consistent with its own stated reasoning about the provided context (it infers from the listed descriptions that none allude to the Holocaust), so it is faithful to the context and reasoning it presents, yielding a faithfulness score of 1.", "main_total_tokens": 3160, "main_prompt_tokens": 3115, "main_completion_tokens": 45, "judge_total_tokens": 547, "judge_prompt_tokens": 429, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "There are three TV shows about Super Mario Bros. that were released in the 1990s:\n\n1. The Super Mario Bros. Super Show! (1989)\n2. The Adventures of Super Mario Bros. 3 (1990)\n3. Super Mario World (1991) \n\n[Doc 113517], [Doc 113661], [Doc 113814]", "latency_ms": 2086.0555171966553, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks how many TV shows were released in the 1990s; the gold answer is 1. The generated answer lists three shows, including one from 1989 (not in the 1990s) and asserts there are three 1990s shows, contradicting the gold. This is factually incorrect with respect to the query and gold label. It also misuses the cited documents by counting a 1989 show as a 1990s release, so it is not faithful to the intended interpretation of the evidence.", "main_total_tokens": 3108, "main_prompt_tokens": 3031, "main_completion_tokens": 77, "judge_total_tokens": 605, "judge_prompt_tokens": 464, "judge_completion_tokens": 141}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "Based on the provided context, there is one TV show related to Super Mario Bros released between 2000 and 2010:\n\n1. **Mario** - Released in 2013, but it is not within the specified range.\n2. **Dov'è Mario?** - Released in 2016, also outside the specified range.\n\nNo TV shows about Super Mario Bros were released between 2000 and 2010 according to the provided documents.", "latency_ms": 1907.9053401947021, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.3, "justification": "The gold answer states that the correct number is 0. The generated answer lists one show as being related to Super Mario Bros but released in 2013, explicitly outside the 2000-2010 window, and then concludes that no TV shows about Super Mario Bros were released between 2000 and 2010. This is contradictory: it both implies there is one related show and then states there are none in the range, and it never clearly answers the question with the correct numeric value 0. Therefore, the final answer is not correct. In terms of faithfulness, the model references titles and years that appear to come from the provided context and then draws its conclusion from that context, but it adds an unclear and misleading enumeration (\"there is one TV show\" followed by items that are outside the range). So it is only weakly faithful to the context and internally inconsistent.", "main_total_tokens": 2676, "main_prompt_tokens": 2584, "main_completion_tokens": 92, "judge_total_tokens": 693, "judge_prompt_tokens": 481, "judge_completion_tokens": 212}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular movie about ninjas is \"Ninja Assassin,\" which has a popularity rating of 5.8597 [Doc 44828]. For TV shows, \"Supah Ninjas\" is the most popular, with a popularity rating of 2.8397 [Doc 119858].", "latency_ms": 1923.7737655639648, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question expects 'Naruto' as the most popular ninja-related TV show or movie. The generated answer instead claims 'Ninja Assassin' and 'Supah Ninjas' are the most popular, which is incorrect given the gold answer. It also relies on specific document popularity scores that conflict with the provided gold label, so it is not faithful to the intended answer.", "main_total_tokens": 3074, "main_prompt_tokens": 3013, "main_completion_tokens": 61, "judge_total_tokens": 548, "judge_prompt_tokens": 443, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "There is no information about a Jurassic Park/Power Ranger crossover movie in the provided context.", "latency_ms": 966.1641120910645, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about the release date of a Jurassic Park/Power Ranger crossover movie. The gold answer clarifies that such a movie was not released. The generated answer states that there is no information about such a crossover movie in the provided context, which effectively aligns with the gold answer by not inventing a release date or claiming the movie exists. It is therefore correct relative to the task and faithful to the given context.", "main_total_tokens": 2857, "main_prompt_tokens": 2839, "main_completion_tokens": 18, "judge_total_tokens": 518, "judge_prompt_tokens": 403, "judge_completion_tokens": 115}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "The context provided does not mention any movie where Barbie fights the Predator. Therefore, I cannot provide an answer based on the given information.", "latency_ms": 1235.884189605713, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "Gold states explicitly that the movie was not released, implying such a movie does exist but unreleased. The generated answer instead claims the context does not mention any such movie and refuses to answer, which contradicts the gold’s information. Thus, it is incorrect. It is partially faithful to the (unstated) context by acknowledging lack of information, but not faithful to the gold answer that we evaluate against.", "main_total_tokens": 3243, "main_prompt_tokens": 3216, "main_completion_tokens": 27, "judge_total_tokens": 523, "judge_prompt_tokens": 409, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The context provided does not contain any information about Star Wars movies or their release dates. Therefore, I cannot answer the question based on the given context.", "latency_ms": 2224.102735519409, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question can be answered from general knowledge about Star Wars films and does not depend on the provided context. The generated answer incorrectly claims it cannot be answered from the context, so it fails correctness relative to the gold answer. However, it is faithful to the given (lack of) context, since the assistant indeed only relies on the context and notes its absence.", "main_total_tokens": 3229, "main_prompt_tokens": 3199, "main_completion_tokens": 30, "judge_total_tokens": 522, "judge_prompt_tokens": 419, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The release year varies depending on the movie. Here are the years for the movies mentioned in the context:\n\n- \"Year of the Horse\": 1997 [Doc 19452]\n- \"YEAR\": 2023 [Doc 107925]\n- \"What It Was\": 2014 [Doc 65131]\n- \"Year of the Devil\": 2002 [Doc 28274]\n- \"The Year After\": 2007 [Doc 40870]\n- \"Another Year\": 2014 [Doc 62952]\n- \"The Year of the Wolf\": 2007 [Doc 39815]\n- \"Year of the Jellyfish\": 1984 [Doc 4092]\n- \"The Year of the Dog\": 1994 [Doc 15249]\n- \"The Last Year\": 2002 [Doc 28876]\n- \"Is It Easy to Be Young?\": 1986 [Doc 6805]\n- \"The Year of the Deer\": 1995 [Doc 17784]\n- \"Year of the Dragon\": 1985 [Doc 5019]\n- \"Let It Be\": 2005 [Doc 35791]\n- \"Greatest Geek Year Ever! 1982\": 2023 [Doc 134707]\n- \"What Do You Think?\": 1986 [Doc 6849]\n- \"The Year of the Tick\": 2004 [Doc 32597]\n- \"The Year That Trembled\": 2002 [Doc 29080]\n- \"This Is the Year\": 2020 [Doc 94546]\n- \"They Were There\": 2011 [Doc 54825]", "latency_ms": 11100.512027740479, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question expects the specific answer '1986', but the generated response does not provide a single year and instead lists multiple films with different release years. It fails to answer the question and adds unsupported context about 'movies mentioned in the context' that is not tied to the gold answer, so it is neither correct nor faithful to the intended answer.", "main_total_tokens": 3399, "main_prompt_tokens": 3053, "main_completion_tokens": 346, "judge_total_tokens": 823, "judge_prompt_tokens": 723, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The rating of the movie Forrest Gump is 8.466 [Doc 15236].", "latency_ms": 1289.7906303405762, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating value of 8.466 for Forrest Gump and attributes it to the referenced document, so it is both correct and faithful to the provided evidence.", "main_total_tokens": 2953, "main_prompt_tokens": 2934, "main_completion_tokens": 19, "judge_total_tokens": 482, "judge_prompt_tokens": 413, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 1355.0662994384766, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the show as 'Mighty Med,' which exactly matches the gold answer and stays faithful to the provided information.", "main_total_tokens": 2604, "main_prompt_tokens": 2573, "main_completion_tokens": 31, "judge_total_tokens": 481, "judge_prompt_tokens": 423, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In \"Túnel de los huesos\" (2011), seven prisoners escaped from a prison in Argentina, specifically from the Villa Devoto prison in Buenos Aires [Doc 53582].", "latency_ms": 1602.5638580322266, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the country where the prison was located; the gold answer is Argentina. The generated answer states that the escape occurred in Argentina, which is correct, and the added detail about Villa Devoto prison in Buenos Aires is consistent and does not contradict the gold. Thus it is both correct and faithful to the gold answer.", "main_total_tokens": 3341, "main_prompt_tokens": 3302, "main_completion_tokens": 39, "judge_total_tokens": 527, "judge_prompt_tokens": 430, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice Li Nai falls into a trap designed by the \"Chef Assassin\" Jiang Jie [Doc 135231].", "latency_ms": 1172.652006149292, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates that the apprentice falls into Jiang Jie's trap. The generated answer adds extra, likely incorrect details, specifying the apprentice as Li Nai and describing Jiang Jie as the 'Chef Assassin,' and citing a document. These details are not supported by the gold and cannot be verified from the provided information, so the response is neither correct overall nor faithful to the source.", "main_total_tokens": 2915, "main_prompt_tokens": 2883, "main_completion_tokens": 32, "judge_total_tokens": 522, "judge_prompt_tokens": 416, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "There are two Korean-language movies that mention assassins and were released after 2023:\n\n1. \"The Assassin\" (2023) - Rating: 5.581, Genres: Action [Doc 106219].\n2. \"Kill Boksoon\" (2023) - Rating: 6.8, Genres: Action, Drama, Thriller, Crime [Doc 105198].", "latency_ms": 2757.0583820343018, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the number of Korean-language movies mentioning assassins after 2023; the gold answer is 4. The generated answer instead states there are two such movies and even includes 2023 releases, which contradicts the temporal condition and the gold count, so it is factually incorrect. It also fabricates specific movie details without reference to the provided gold label, so it is not faithful to the given information.", "main_total_tokens": 2877, "main_prompt_tokens": 2798, "main_completion_tokens": 79, "judge_total_tokens": 579, "judge_prompt_tokens": 462, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "The context provided does not contain any information about Steven Seagal or the number of movies he has. Therefore, I cannot answer your question based on the given context.", "latency_ms": 1220.376968383789, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.5, "faithfulness_score": 1, "justification": "Gold answer says there are no movies with both Bruce Lee and Steven Seagal. The generated answer instead claims the context has no information about Steven Seagal or movie counts, and therefore it cannot answer the question. This does not correctly address the user query or match the gold conclusion, so correctness is partial at best. However, the generated answer is faithful to the given (unspecified) context, as it honestly states the lack of information, so faithfulness is high.", "main_total_tokens": 3040, "main_prompt_tokens": 3007, "main_completion_tokens": 33, "judge_total_tokens": 553, "judge_prompt_tokens": 426, "judge_completion_tokens": 127}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 1205.608606338501, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (6.3) and does not add unsupported information. It is both correct and faithful to the provided reference.", "main_total_tokens": 2839, "main_prompt_tokens": 2819, "main_completion_tokens": 20, "judge_total_tokens": 465, "judge_prompt_tokens": 403, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The movie \"The Last Race\" focuses on grassroots auto racing, specifically highlighting a small town stock car track and the drivers involved in that community [Doc 86410].", "latency_ms": 1305.0858974456787, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the style of racing as drift, but the generated answer describes grassroots stock car racing at a small town track, which is incorrect for this question. The answer is internally consistent with its cited document, so it is faithful to its own source but not correct relative to the gold.", "main_total_tokens": 3271, "main_prompt_tokens": 3237, "main_completion_tokens": 34, "judge_total_tokens": 502, "judge_prompt_tokens": 412, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 1426.687479019165, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the title as 'Confronting a Serial Killer' and stays faithful to the provided information.", "main_total_tokens": 2890, "main_prompt_tokens": 2844, "main_completion_tokens": 46, "judge_total_tokens": 513, "judge_prompt_tokens": 452, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 1474.745750427246, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold value (7.838) and clearly attributes it, so it is both correct and faithful to the provided information.", "main_total_tokens": 3088, "main_prompt_tokens": 3066, "main_completion_tokens": 22, "judge_total_tokens": 468, "judge_prompt_tokens": 407, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 1336.4019393920898, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'The Sixth Sense' and stays fully grounded in the provided information.", "main_total_tokens": 2868, "main_prompt_tokens": 2839, "main_completion_tokens": 29, "judge_total_tokens": 480, "judge_prompt_tokens": 420, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 988.8453483581543, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold year 2002, and it is directly supported by the provided information without adding unsupported details.", "main_total_tokens": 3036, "main_prompt_tokens": 3019, "main_completion_tokens": 17, "judge_total_tokens": 454, "judge_prompt_tokens": 397, "judge_completion_tokens": 57}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is 'The Transporter Refueled' [Doc 67500].", "latency_ms": 1188.6606216430664, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2015 reboot title as 'The Transporter Refueled' and does not introduce any contradictions or extraneous fabricated details beyond a harmless document reference.", "main_total_tokens": 3183, "main_prompt_tokens": 3154, "main_completion_tokens": 29, "judge_total_tokens": 494, "judge_prompt_tokens": 420, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in 2015 [Doc 67500].", "latency_ms": 1519.1738605499268, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It is fully faithful to the provided information without adding unsupported claims.", "main_total_tokens": 3070, "main_prompt_tokens": 3050, "main_completion_tokens": 20, "judge_total_tokens": 474, "judge_prompt_tokens": 404, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "There is no romantic comedy in the provided context that features Abraham Lincoln coming back from the dead. The closest related title is \"Re-Elected,\" which involves former U.S. presidents coming back from the dead as zombies, but it is categorized as a horror comedy, not a romantic comedy [Doc 94414].", "latency_ms": 1791.2936210632324, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says it cannot find such a movie; the generated answer similarly states there is no romantic comedy with Abraham Lincoln coming back from the dead, and notes that the closest match is a horror comedy. This directly aligns with the gold conclusion and stays within the provided context, so it is both correct and faithful.", "main_total_tokens": 2774, "main_prompt_tokens": 2711, "main_completion_tokens": 63, "judge_total_tokens": 547, "judge_prompt_tokens": 453, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of the movie that alludes to a manuscript written by Ernest Hemingway is \"Genius\" [Doc 73141].", "latency_ms": 972.5155830383301, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.4, "justification": "The generated answer is incorrect: the movie that alludes to a manuscript written by Ernest Hemingway is \"Havana Darkness,\" not \"Genius.\" Thus correctness is 0. The answer partially references a document ID, suggesting some grounding, but it fabricates the specific title relative to the question, so faithfulness is low but not necessarily entirely ungrounded, hence 0.4.", "main_total_tokens": 2781, "main_prompt_tokens": 2755, "main_completion_tokens": 26, "judge_total_tokens": 525, "judge_prompt_tokens": 414, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show \"Doll & Em\" [Doc 122259].", "latency_ms": 1072.859287261963, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em', matching the gold answer exactly. It remains fully grounded in the provided information and does not introduce unsupported content, so both correctness and faithfulness are maximal.", "main_total_tokens": 2683, "main_prompt_tokens": 2654, "main_completion_tokens": 29, "judge_total_tokens": 493, "judge_prompt_tokens": 417, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The movie is \"Hallam Foe,\" released in 2007. Hallam's talent for spying on people reveals his darkest fears and peculiar desires as he tries to expose the true cause of his mother's death [Doc 39389].", "latency_ms": 1668.8878536224365, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Hallam Foe', matching the gold answer. The additional descriptive details about the plot are consistent with the film and do not contradict the question or given information, so the response is both correct and faithful.", "main_total_tokens": 2952, "main_prompt_tokens": 2904, "main_completion_tokens": 48, "judge_total_tokens": 528, "judge_prompt_tokens": 447, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The context does not provide information about a specific character's last name or whether it sounds like a friend or a foe. Therefore, I cannot answer the question based on the provided documents.", "latency_ms": 1320.51682472229, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is an explicit label 'A foe', indicating the question has a known correct response. The generated answer claims the question cannot be answered from context, which contradicts the gold label and is therefore incorrect. However, it is faithful to the assistant's stated limitation: it does not invent an answer and accurately reflects that it is not using unseen information, so faithfulness is high.", "main_total_tokens": 3000, "main_prompt_tokens": 2963, "main_completion_tokens": 37, "judge_total_tokens": 532, "judge_prompt_tokens": 423, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "In the 2017 TV show Atypical, Sam is 18 years old [Doc 124583].", "latency_ms": 1306.6649436950684, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: Sam is 18 years old in the 2017 TV show Atypical. It is correct and directly supported by the provided gold information, so both correctness and faithfulness are maximal.", "main_total_tokens": 2723, "main_prompt_tokens": 2700, "main_completion_tokens": 23, "judge_total_tokens": 488, "judge_prompt_tokens": 410, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 1034.3225002288818, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the TV show Atypical, Sam is on the autism spectrum, which matches the gold answer exactly and does not introduce any contradictions or unsupported content.", "main_total_tokens": 2461, "main_prompt_tokens": 2438, "main_completion_tokens": 23, "judge_total_tokens": 478, "judge_prompt_tokens": 410, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "In \"À corde tendue,\" Sandra was born in Chamonix [Doc 59659].", "latency_ms": 1046.656847000122, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, which matches the gold answer exactly. It is fully faithful to the provided information and does not introduce unsupported details.", "main_total_tokens": 3355, "main_prompt_tokens": 3335, "main_completion_tokens": 20, "judge_total_tokens": 484, "judge_prompt_tokens": 417, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In this Japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 956.5315246582031, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and correctly fills the blank with no added or conflicting information, so it is both correct and faithful.", "main_total_tokens": 3073, "main_prompt_tokens": 3052, "main_completion_tokens": 21, "judge_total_tokens": 475, "judge_prompt_tokens": 412, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the low-rated German language animation \"Tabaluga,\" the character Tabaluga is the last of the dragons and the crown-prince of Greenland, a magical place inhabited by talking animals of many different species [Doc 114810].", "latency_ms": 2211.214065551758, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, matching the gold answer. The extra descriptive detail about it being a magical place with talking animals is plausible but not clearly supported by the provided gold snippet, so there is a minor faithfulness deduction for adding unsupported specifics.", "main_total_tokens": 3224, "main_prompt_tokens": 3176, "main_completion_tokens": 48, "judge_total_tokens": 535, "judge_prompt_tokens": 442, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is titled \"One Afternoon,\" released in 2021 [Doc 96838].", "latency_ms": 2059.434413909912, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The core answer (that the movie is 'One Afternoon') is correct and matches the gold. However, the added details about the release year (2021) and a specific document reference [Doc 96838] are not grounded in the provided information, making those parts potentially unfaithful. Overall correctness is 1, but faithfulness is slightly reduced due to the extra unsupported specifics.", "main_total_tokens": 2600, "main_prompt_tokens": 2550, "main_completion_tokens": 50, "judge_total_tokens": 565, "judge_prompt_tokens": 454, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead,\" which has a rating of 6.746 [Doc 93260].", "latency_ms": 8291.230201721191, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'The Life Ahead,' matching the gold answer, so it is correct. However, the mention of a specific rating of 6.746 and a citation to [Doc 93260] are hallucinated given only the QA pair provided, so it is not fully faithful to the given information context.", "main_total_tokens": 2897, "main_prompt_tokens": 2851, "main_completion_tokens": 46, "judge_total_tokens": 543, "judge_prompt_tokens": 444, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team, showcasing how a man of humble beginnings could take on the elite of motor racing and win [Doc 73859].", "latency_ms": 1476.058006286621, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the subject of the 2016 documentary 'McLaren' as Bruce McLaren, matching the gold answer. It adds minor contextual information about him founding the McLaren racing team, which is accurate and does not contradict the source, so it is both correct and faithful.", "main_total_tokens": 3142, "main_prompt_tokens": 3091, "main_completion_tokens": 51, "judge_total_tokens": 525, "judge_prompt_tokens": 435, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The context does not provide the name of Sonny Hayes' racing team in the movie \"F1\" [Doc 111503].", "latency_ms": 2174.6811866760254, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly states the team name as 'The McLaren Motor Racing team'. The generated answer incorrectly claims the context does not provide the name, so it is factually incorrect with respect to the gold. However, if the referenced context truly omitted the team name, then the model’s statement about the context would be faithful to that context. Thus correctness is 0, faithfulness to the given context is 1.", "main_total_tokens": 3301, "main_prompt_tokens": 3275, "main_completion_tokens": 26, "judge_total_tokens": 525, "judge_prompt_tokens": 409, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context does not provide information about the nationality or country of origin of any character or individual mentioned. Therefore, I cannot answer your question.", "latency_ms": 1267.3418521881104, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states the person is from New Zealand. The generated answer says the context does not provide this information and declines to answer, which is incorrect relative to the gold. However, it is fully faithful to the (stated) lack of context rather than hallucinating, so faithfulness is high while correctness is zero.", "main_total_tokens": 3070, "main_prompt_tokens": 3042, "main_completion_tokens": 28, "judge_total_tokens": 500, "judge_prompt_tokens": 404, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "The context provided does not mention any movie in which John McClane blows up the Great Wall of China in 2013. Therefore, I cannot provide an answer based on the given information.", "latency_ms": 1459.0284824371338, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct response is that there is no such movie; John McClane never blows up the Great Wall of China in any film, including 2013 releases. The gold answer reflects this by explicitly stating the database lacks such a movie. The generated answer instead equivocates, suggesting only that the context does not mention it and that it cannot answer based on the given information, which is misleading in this QA setting. It neither clearly denies the premise nor aligns with the gold answer, so it is not fully correct or faithful to the intended ground truth.", "main_total_tokens": 3284, "main_prompt_tokens": 3245, "main_completion_tokens": 39, "judge_total_tokens": 578, "judge_prompt_tokens": 437, "judge_completion_tokens": 141}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The movie you are referring to is \"Tomie: Forbidden Fruit,\" which was released in 2002. The description matches perfectly: \"When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental\" [Doc 28589].", "latency_ms": 1991.6973114013672, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as \"Tomie: Forbidden Fruit,\" matching the gold answer and the description. However, it cites a specific document ID [Doc 28589] that we cannot verify from the given prompt, which slightly reduces faithfulness to the provided context.", "main_total_tokens": 3202, "main_prompt_tokens": 3136, "main_completion_tokens": 66, "judge_total_tokens": 572, "judge_prompt_tokens": 483, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 1374.314308166504, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, which matches the gold answer. It does not introduce unsupported details and is fully consistent with the provided information, so it is both correct and faithful.", "main_total_tokens": 3486, "main_prompt_tokens": 3457, "main_completion_tokens": 29, "judge_total_tokens": 502, "judge_prompt_tokens": 423, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In \"Scorpion King: Book of Souls\" (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls, which will allow them to put an end to an evil warlord [Doc 82582].", "latency_ms": 1437.2074604034424, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies that The Scorpion King and Tala are searching for the Book of Souls, matching the gold answer. The added detail about it being a legendary relic used to end an evil warlord is consistent with the movie’s plot, though it introduces extra information and a citation-like tag [Doc 82582] that is not grounded in the provided gold answer. Content-wise it remains accurate, but the reference tag is not supported by the prompt itself, slightly reducing faithfulness.", "main_total_tokens": 3272, "main_prompt_tokens": 3221, "main_completion_tokens": 51, "judge_total_tokens": 576, "judge_prompt_tokens": 445, "judge_completion_tokens": 131}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"Zombies on the Moon\" was released is not found in the provided context.", "latency_ms": 1737.6527786254883, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the release year of 'Zombies on the Moon' is not available / not found, and the generated answer stays faithful to the given context by not inventing a year.", "main_total_tokens": 2720, "main_prompt_tokens": 2701, "main_completion_tokens": 19, "judge_total_tokens": 485, "judge_prompt_tokens": 410, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year of release for \"The Galactic Baker\" is not found in the provided context.", "latency_ms": 1066.9517517089844, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available/known. The generated response is consistent with the information given and with the gold answer, so it is both correct and faithful.", "main_total_tokens": 3009, "main_prompt_tokens": 2991, "main_completion_tokens": 18, "judge_total_tokens": 478, "judge_prompt_tokens": 407, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year of release for \"Return of the Null Pointer\" is not found in the provided context.", "latency_ms": 972.6088047027588, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer convey that the release year is unknown/not available. The generated response is correct relative to the (likely non-existent) movie and does not invent a date. It is also faithful to the given context, explicitly stating that the year is not found in the provided context rather than making unsupported claims.", "main_total_tokens": 2996, "main_prompt_tokens": 2976, "main_completion_tokens": 20, "judge_total_tokens": 508, "judge_prompt_tokens": 411, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year of release for \"Data Science: The Movie\" is not found in the provided context.", "latency_ms": 925.2583980560303, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available/known from the given information. The generated answer is correct with respect to the question (no year is provided) and faithful to the idea that the context does not contain that detail.", "main_total_tokens": 2968, "main_prompt_tokens": 2948, "main_completion_tokens": 20, "judge_total_tokens": 493, "judge_prompt_tokens": 411, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The context does not provide any information about a movie titled \"Python's Revenge 2.\" Therefore, I cannot answer your question.", "latency_ms": 1389.545202255249, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot answer because there is no information about the movie, which aligns with the gold response indicating the movie is not in the database. It does not introduce unsupported facts and remains faithful to the provided context.", "main_total_tokens": 2944, "main_prompt_tokens": 2918, "main_completion_tokens": 26, "judge_total_tokens": 495, "judge_prompt_tokens": 417, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The information about \"The Last Pixel\" is not found in the provided context.", "latency_ms": 956.8033218383789, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the information about 'The Last Pixel' is not available, without fabricating a release year. This is correct given the lack of data, and the generated response stays faithful to the implied source/context limitations.", "main_total_tokens": 2664, "main_prompt_tokens": 2648, "main_completion_tokens": 16, "judge_total_tokens": 485, "judge_prompt_tokens": 405, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The context provided does not contain any information about a movie titled \"The Man Who Ate the Sun.\" Therefore, I cannot provide the year of its release.", "latency_ms": 1150.6857872009277, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer explicitly states that the movie is not in the database, while the generated answer implies it cannot answer because the context lacks that information. Under this evaluation scheme, the correct behavior is to say it is not in the database, so the generated response is not strictly correct. However, the generated answer is faithful to the given context because it accurately reflects that no information about the movie is present there.", "main_total_tokens": 3342, "main_prompt_tokens": 3311, "main_completion_tokens": 31, "judge_total_tokens": 535, "judge_prompt_tokens": 423, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Days of Lehman Brothers\" is not mentioned in the provided context. Therefore, I cannot provide that information.", "latency_ms": 1135.1633071899414, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the director is not provided in the database. The generated answer correctly states that the director is not mentioned in the provided context and thus cannot be given. This matches the limitation specified by the gold answer and does not introduce unsupported information, so it is both correct and faithful.", "main_total_tokens": 3416, "main_prompt_tokens": 3386, "main_completion_tokens": 30, "judge_total_tokens": 516, "judge_prompt_tokens": 427, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The information regarding the director of the movie \"A Love That Kills\" is not found in the provided context.", "latency_ms": 950.1917362213135, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question is about the movie 'A Love to Die For', but the generated answer refers to 'A Love That Kills', which is a different title. Although both the gold and generated answers express that the director information is not available, the generated response is incorrect with respect to the asked movie title, so correctness is 0. It is partially faithful because it maintains the key idea that the director information is not in the provided context, but it alters the movie title, so faithfulness is reduced.", "main_total_tokens": 2868, "main_prompt_tokens": 2845, "main_completion_tokens": 23, "judge_total_tokens": 552, "judge_prompt_tokens": 418, "judge_completion_tokens": 134}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not mentioned in the provided context.", "latency_ms": 1035.1405143737793, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned in the provided context, which aligns with the gold answer that the database does not provide the director information. It is both correct and faithful to the given context.", "main_total_tokens": 3111, "main_prompt_tokens": 3093, "main_completion_tokens": 18, "judge_total_tokens": 485, "judge_prompt_tokens": 411, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The information regarding the director of \"The Last Night at Tremore Beach\" is not provided in the context.", "latency_ms": 818.7038898468018, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director information is not provided, matching the gold response. It does not introduce any unsupported details and remains faithful to the given database/context limitation.", "main_total_tokens": 2702, "main_prompt_tokens": 2680, "main_completion_tokens": 22, "judge_total_tokens": 485, "judge_prompt_tokens": 419, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" (2021) [Doc 98375].", "latency_ms": 1621.9522953033447, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy'. It does not introduce contradictions or unsupported claims beyond a harmless extra detail (year), so it is both correct and faithful to the given information.", "main_total_tokens": 2748, "main_prompt_tokens": 2717, "main_completion_tokens": 31, "judge_total_tokens": 502, "judge_prompt_tokens": 422, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse is \"Stranger in My Bed\" (2005) [Doc 35237].", "latency_ms": 1753.5109519958496, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as \"Stranger in My Bed\" and does not introduce unsupported information beyond a harmless year and document reference. It is both correct and faithful to the provided information.", "main_total_tokens": 2724, "main_prompt_tokens": 2691, "main_completion_tokens": 33, "judge_total_tokens": 506, "judge_prompt_tokens": 429, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie you are referring to is \"Game of Silence\" from 2016. In this story, a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years, leading to the resurfacing of a dark secret from their past [Doc 123938].", "latency_ms": 1762.0506286621094, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question explicitly asks for a movie, but the answer identifies \"Game of Silence,\" which is a TV series, not a movie, so the response is incorrect. It is partially faithful to the provided gold label (same title and plot description), but it incorrectly calls it a movie and adds an ungrounded citation placeholder, so faithfulness is not perfect.", "main_total_tokens": 2436, "main_prompt_tokens": 2379, "main_completion_tokens": 57, "judge_total_tokens": 559, "judge_prompt_tokens": 454, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie you are referring to is \"The Craigslist Killer,\" which is based on the true story of Philip Markoff, a med student who leads a double life as a brutal sexual deviant abusing prostitutes he finds via Craigslist [Doc 53039].", "latency_ms": 1643.1903839111328, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer' and accurately describes that it is based on the true story of Philip Markoff, a med student who attacks women he meets via Craigslist. This matches the gold answer on the key point (movie title), so correctness is 1. The added details about him being a brutal sexual deviant abusing prostitutes are broadly in line with the known story, but the specific phrasing and some embellishment (e.g., emphasis on prostitutes vs. massage therapists/escorts, and the explicit tone) are not clearly sourced from the provided doc reference and may slightly overstate or distort details. Therefore, faithfulness is slightly less than perfect.", "main_total_tokens": 2841, "main_prompt_tokens": 2791, "main_completion_tokens": 50, "judge_total_tokens": 613, "judge_prompt_tokens": 442, "judge_completion_tokens": 171}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which Yuki Hirano sets his eyes on the role of a forestry trainee after failing his university entrance exam is \"Wood Job!\" [Doc 66094].", "latency_ms": 1157.2580337524414, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", which matches the gold answer exactly. It is faithful to the question and does not introduce any contradictory or extraneous information beyond a harmless document citation.", "main_total_tokens": 3274, "main_prompt_tokens": 3239, "main_completion_tokens": 35, "judge_total_tokens": 508, "judge_prompt_tokens": 435, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure, is titled \"Sesame Street: The Magical Wand Chase\" from the year 2017 [Doc 78373].", "latency_ms": 1644.6290016174316, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Sesame Street: The Magical Wand Chase,' which matches the gold answer exactly. The added detail about the year 2017 and a document reference does not contradict the question and is consistent with the source, so the response is both correct and faithful to the provided information.", "main_total_tokens": 3255, "main_prompt_tokens": 3198, "main_completion_tokens": 57, "judge_total_tokens": 561, "judge_prompt_tokens": 467, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a 7-year-old boy must find the hero within him to save his parents trapped inside a burning car due to a rock fall is titled \"Beyond,\" released in 2016 [Doc 74597].", "latency_ms": 1367.6717281341553, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question’s gold answer is \"Beyond,\" and the generated answer also identifies the film as \"Beyond\" with consistent details. This matches the gold exactly and does not introduce contradictions or fabricated information, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2820, "main_prompt_tokens": 2774, "main_completion_tokens": 46, "judge_total_tokens": 534, "judge_prompt_tokens": 454, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie you are referring to is \"6ixtynin9\" from the year 1999. In this film, a woman is fired from a financial corporation during the Asia crisis and returns home with no money [Doc 22478].", "latency_ms": 1553.28369140625, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"6ixtynin9\" and matches the gold answer exactly. It does not introduce contradictions or extraneous, unfounded claims, and aligns with the described plot premise. Therefore, both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2964, "main_prompt_tokens": 2914, "main_completion_tokens": 50, "judge_total_tokens": 538, "judge_prompt_tokens": 452, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie you are referring to is \"Club Dread\" from the year 2004 [Doc 32410].", "latency_ms": 1157.6218605041504, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer exactly. It stays faithful to the given information and does not introduce contradictions or unsupported claims, aside from a benign citation-style note that does not alter the core answer.", "main_total_tokens": 2887, "main_prompt_tokens": 2863, "main_completion_tokens": 24, "judge_total_tokens": 530, "judge_prompt_tokens": 448, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie you are referring to is \"Toilet,\" which was released in 2010. The plot involves a 30-something Gundam fan and engineer who is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral [Doc 48873].", "latency_ms": 2085.503578186035, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title given a plot description. The gold answer is \"Toilet.\" The generated answer correctly identifies the movie as \"Toilet\" and provides consistent plot details. There is no hallucinated contradiction with the gold; the citation style reference does not conflict with any provided source. Thus, the response is both correct and faithful.", "main_total_tokens": 3595, "main_prompt_tokens": 3530, "main_completion_tokens": 65, "judge_total_tokens": 580, "judge_prompt_tokens": 479, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie you are referring to is \"Electric Angel,\" which is a typical experimental film that uses a variety of audiovisual techniques to create a sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century [Doc 1024].", "latency_ms": 1612.4439239501953, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'Electric Angel,' matching the gold answer, so correctness is 1. However, the mention of a citation '[Doc 1024]' is fabricated and not grounded in the provided content, reducing faithfulness; thus faithfulness is 0.", "main_total_tokens": 3166, "main_prompt_tokens": 3113, "main_completion_tokens": 53, "judge_total_tokens": 557, "judge_prompt_tokens": 469, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is \"Doomsday Stories,\" which has a rating of 2.167 [Doc 108366].", "latency_ms": 1053.675651550293, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold label states that the lowest rated movie from 2023 is 'Mega Ape,' while the model claims it is 'Doomsday Stories.' This directly contradicts the provided gold answer, so correctness is 0. Faithfulness is also 0 because the model explicitly references a document and rating that conflict with the gold standard, indicating it is not faithfully aligned with the given ground truth.", "main_total_tokens": 2776, "main_prompt_tokens": 2745, "main_completion_tokens": 31, "judge_total_tokens": 524, "judge_prompt_tokens": 412, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "The highest rated TV show from 2010 is \"Downton Abbey,\" which has a rating of 8.1 [Doc 119146].", "latency_ms": 1059.7047805786133, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says the correct response is 'Tea House,' while the generated answer claims it is 'Downton Abbey' with a rating of 8.1. This directly contradicts the gold label, so the generated answer is incorrect. Additionally, it fabricates a specific rating and document reference without support from the provided gold answer, so it is not faithful to the given information.", "main_total_tokens": 3310, "main_prompt_tokens": 3280, "main_completion_tokens": 30, "judge_total_tokens": 520, "judge_prompt_tokens": 412, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest rated Japanese language movie from 1999 is \"Kikujiro,\" which has a rating of 7.5 [Doc 22213].", "latency_ms": 1220.2463150024414, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the highest rated Japanese language movie from 1999 per the gold reference is 'The Uncut Story of Mewtwo's Origin', not 'Kikujiro'. It is also unfaithful to the provided gold answer, directly contradicting it.", "main_total_tokens": 2792, "main_prompt_tokens": 2760, "main_completion_tokens": 32, "judge_total_tokens": 511, "judge_prompt_tokens": 423, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is \"The Worst Horror Movie Ever Made,\" which has a rating of 2.615 and is in English [Doc 35944].", "latency_ms": 1336.3735675811768, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks only for the language of the lowest-rated 1980 movie, which is English. The generated answer adds specific movie title and rating details that are not in the gold answer and likely hallucinated. Although it does state the language as English, the extra fabricated details violate faithfulness to the provided information, so both correctness and faithfulness are scored 0 in this evaluation setup.", "main_total_tokens": 2845, "main_prompt_tokens": 2809, "main_completion_tokens": 36, "judge_total_tokens": 528, "judge_prompt_tokens": 419, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was \"The Sopranos,\" which had a popularity rating of 59.4284 [Doc 115146].", "latency_ms": 1807.1575164794922, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer incorrectly identifies 'The Sopranos' as the most popular TV show in 1999, while the gold answer is 'Law & Order: Special Victims Unit'. Thus, it is factually incorrect relative to the gold. It also cites a specific document rating without any indication this aligns with the gold reference, so with respect to the given QA pair it is not faithful to the expected answer.", "main_total_tokens": 3137, "main_prompt_tokens": 3102, "main_completion_tokens": 35, "judge_total_tokens": 539, "judge_prompt_tokens": 425, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 850.635290145874, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and attributes it consistently to the same movie. No contradictions or hallucinations beyond the provided document reference.", "main_total_tokens": 2998, "main_prompt_tokens": 2980, "main_completion_tokens": 18, "judge_total_tokens": 464, "judge_prompt_tokens": 400, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "There are multiple movies titled \"The Story of Us.\" One was released in 1999 [Doc 22481], another in 2025 [Doc 112077], and a third in 2019 [Doc 88811].", "latency_ms": 1329.810619354248, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a single year (2019), but the generated answer lists three different years (1999, 2025, and 2019) without identifying which corresponds to the asked-about movie. This does not correctly answer the specific question, so correctness is 0. However, the answer is internally consistent and grounded in the provided documents, so faithfulness is 1.", "main_total_tokens": 2927, "main_prompt_tokens": 2879, "main_completion_tokens": 48, "judge_total_tokens": 541, "judge_prompt_tokens": 432, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The context does not provide information about the number of votes \"My Dog Skip\" received. Therefore, I cannot answer that question.", "latency_ms": 1305.624008178711, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 279 votes, but the model said it could not answer. This is incorrect with respect to the gold. However, given the described context, the model accurately stated its inability to find the information, so it is faithful to the provided context.", "main_total_tokens": 3058, "main_prompt_tokens": 3032, "main_completion_tokens": 26, "judge_total_tokens": 486, "judge_prompt_tokens": 407, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The context does not provide information about the number of votes received by \"Oh My God!\" or any of the movies listed. Therefore, I cannot answer that question.", "latency_ms": 1302.1690845489502, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 12, but the generated answer claims the information is not provided and does not give 12, so it is incorrect. However, given only the context described in the prompt, the model faithfully states that the context does not provide the number of votes, so it is consistent with the provided context and thus faithful.", "main_total_tokens": 2793, "main_prompt_tokens": 2760, "main_completion_tokens": 33, "judge_total_tokens": 507, "judge_prompt_tokens": 414, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 1151.1788368225098, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie was released in 2009, which matches the gold answer. It is also faithful to the provided information, simply restating the correct year without adding unsupported details.", "main_total_tokens": 2871, "main_prompt_tokens": 2848, "main_completion_tokens": 23, "judge_total_tokens": 482, "judge_prompt_tokens": 410, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_openai-large_gpt-4o-mini", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 1806.7247867584229, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the original language of 'The King’s Speech' is English, matching the gold answer ('en'), and it remains faithful to the provided information without adding unsupported claims.", "main_total_tokens": 3309, "main_prompt_tokens": 3290, "main_completion_tokens": 19, "judge_total_tokens": 475, "judge_prompt_tokens": 402, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "Wall Street was released in the year 1987 [Doc 6993].", "latency_ms": 1210.5259895324707, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer. It does not introduce contradictory or extraneous information and is faithful to the provided gold data.", "main_total_tokens": 2218, "main_prompt_tokens": 2202, "main_completion_tokens": 16, "judge_total_tokens": 463, "judge_prompt_tokens": 394, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie in which classmates have to play a survival game using their cell phones is \"Brain Fluid Explosion Girl\" from 2015. In this film, Hana Ichii and her classmates find themselves locked in a cage and are forced to participate in a survival game that utilizes their cellphones [Doc 69929].", "latency_ms": 2455.6007385253906, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'Brain Fluid Explosion Girl' and describes the premise consistent with the gold answer, so correctness is 1. It adds a specific year (2015) and a character name plus a citation-like reference [Doc 69929] that are not in the provided gold answer, so there's a small amount of unverifiable embellishment, slightly reducing faithfulness.", "main_total_tokens": 2197, "main_prompt_tokens": 2134, "main_completion_tokens": 63, "judge_total_tokens": 563, "judge_prompt_tokens": 451, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show in which six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons,\" which aired in 1983. In this show, the friends are transported into the Dungeons & Dragons realm and must try to find a way home with the help of their guide, the Dungeon Master [Doc 112849].", "latency_ms": 2098.472833633423, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.4306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.719741384391281, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' matching the gold answer exactly. The additional detail about the premise and year is accurate and consistent with the question and does not contradict the gold answer, so the response is both correct and faithful.", "main_total_tokens": 2264, "main_prompt_tokens": 2194, "main_completion_tokens": 70, "judge_total_tokens": 544, "judge_prompt_tokens": 460, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is titled **Nacht der Wölfe**. It is described as the German version of \"West Side Story\" and was released in 1982 [Doc 2590].", "latency_ms": 1721.6455936431885, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe,' matching the gold answer exactly. The added details (German version of West Side Story, 1982, doc reference) are plausible and appear drawn from the provided source, with no contradictions or unsupported hallucinations given the context, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2136, "main_prompt_tokens": 2075, "main_completion_tokens": 61, "judge_total_tokens": 561, "judge_prompt_tokens": 459, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of \"Picnic at Hanging Rock\" take place on Valentine's Day, 1900 [Doc 126191].", "latency_ms": 987.687349319458, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the year as 1900, matching the gold answer, and it is consistent with the cited document reference, so it is both correct and faithful.", "main_total_tokens": 1950, "main_prompt_tokens": 1925, "main_completion_tokens": 25, "judge_total_tokens": 476, "judge_prompt_tokens": 410, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie you are referring to is \"June.\" It features a young, innocent, and quirky violinist named June, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man destined for her. The plot also includes her discovering that the man she meets is engaged to someone else, leading to romantic conflict [Doc 32483].", "latency_ms": 2108.938694000244, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as \"June,\" matching the gold answer. It accurately repeats the core premise from the question and adds a plausible plot detail about the man being engaged, which is not in the question but is likely drawn from the cited document. There is a small risk that these extra specifics could be slightly embellished or not fully supported by the provided context, so faithfulness is slightly less than perfect, but overall it stays aligned with the described film.", "main_total_tokens": 2423, "main_prompt_tokens": 2334, "main_completion_tokens": 89, "judge_total_tokens": 647, "judge_prompt_tokens": 521, "judge_completion_tokens": 126}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1388.5130882263184, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered 'I don't know,' which is incorrect because the gold answer is known and specific. However, the response is faithful to the model's uncertainty and does not introduce fabricated information, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 474, "judge_prompt_tokens": 399, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 432.09123611450195, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the highest Russian-language movie rating in 1999 is 7.6, but the generated answer says 'I don't know,' which is incorrect relative to the gold. However, the model does not invent any incorrect facts or contradict provided information; it simply expresses a lack of knowledge, so it is fully faithful to its own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 493, "judge_prompt_tokens": 389, "judge_completion_tokens": 104}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1460.6578350067139, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 0.9828920819566879, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3364045224324153, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is incorrect. However, the model makes a clear numeric claim without contradicting its own reasoning, so it is internally consistent/faithful to its (albeit wrong) computation or retrieval.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 394, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 263.03744316101074, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. The question is simple counting based on given conditions, so answering 0 contradicts the provided gold label. However, the answer is at least faithful in that it does not invent additional reasoning or contradictory information beyond the incorrect count itself.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 395, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1619.5721626281738, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which is incorrect because the correct answer is 'Fantômas.' However, the model is honest about its lack of knowledge and does not hallucinate an incorrect show, so it is fully faithful to its own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 473, "judge_prompt_tokens": 389, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is titled **Kill Cruise**. It was released in 1990 and is described as a thriller where the yacht captain does not know what he's in for when he takes the two British beauties on their trip to Barbados [Doc 11205].", "latency_ms": 3212.3758792877197, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' which matches the gold answer. It also adds accurate contextual details (year, genre, brief plot) that are consistent with the described scenario, and these details are attributed to the referenced document. There is no hallucinated contradiction with the given gold answer, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2153, "main_prompt_tokens": 2091, "main_completion_tokens": 62, "judge_total_tokens": 550, "judge_prompt_tokens": 447, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 1537.550926208496, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title \"Won't You Be My Neighbor?\" and correctly identifies it as the movie about Fred Rogers and his TV series. It does not add contradictory or unsupported information; the citation reference is plausible and does not conflict with the query.", "main_total_tokens": 2285, "main_prompt_tokens": 2246, "main_completion_tokens": 39, "judge_total_tokens": 525, "judge_prompt_tokens": 440, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The movie \"Year of the Horse\" was released in 1997 [Doc 19452].", "latency_ms": 990.6072616577148, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer gives a different year (1997) than the gold answer (2018), so it is factually incorrect for this QA pair. However, it appears to be consistent with its cited document (Doc 19452), so it is faithful to the provided source even though it does not match the gold label.", "main_total_tokens": 2428, "main_prompt_tokens": 2408, "main_completion_tokens": 20, "judge_total_tokens": 493, "judge_prompt_tokens": 397, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is \"Astro Boy.\" In this series, after being abandoned by his creator, the young boy robot is rescued by an Android Advocate, Dr. Elefun, who adopts him and guides him in using his special powers to aid and protect mankind and robot-kind [Doc 112591].", "latency_ms": 2254.4050216674805, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy,' matching the gold answer exactly. The additional descriptive details about the plot are plausible and consistent with known information about Astro Boy, and there is no contradiction with the provided gold answer. Thus, the response is both correct and faithful to the question and gold label.", "main_total_tokens": 2376, "main_prompt_tokens": 2301, "main_completion_tokens": 75, "judge_total_tokens": 558, "judge_prompt_tokens": 462, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room,\" released in 2015, the woman and her son were held captive for 7 years in an enclosed space [Doc 67513].", "latency_ms": 1343.397855758667, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, matching the gold answer exactly and staying faithful to the provided information.", "main_total_tokens": 1867, "main_prompt_tokens": 1833, "main_completion_tokens": 34, "judge_total_tokens": 491, "judge_prompt_tokens": 428, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "Before it was broken into on 26 November 1983, the Brink's-Mat security depot was holding gold bullion worth £26 million [Doc 133469].", "latency_ms": 1267.3008441925049, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the provided gold value exactly (£26 million) and correctly restates it in a full sentence without adding unsupported information. It is both correct and faithful to the source.", "main_total_tokens": 2109, "main_prompt_tokens": 2075, "main_completion_tokens": 34, "judge_total_tokens": 497, "judge_prompt_tokens": 429, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6415.385484695435, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric value (the popularity score), which is provided in the gold answer. The generated answer says only 'I don't know' and does not attempt to give a score, so it is incorrect. However, it does not contradict the given information or invent any facts; it simply admits not knowing, so it is fully faithful to its own (lack of) knowledge and the prompt.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 509, "judge_prompt_tokens": 396, "judge_completion_tokens": 113}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 385.41126251220703, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (1), so it is incorrect. The faithfulness is high because the evaluation directly compares the provided generated answer to the given gold answer without inventing extra information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 461, "judge_prompt_tokens": 385, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 1287.4553203582764, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Angel Eyes' and is fully supported by the cited document.", "main_total_tokens": 2386, "main_prompt_tokens": 2351, "main_completion_tokens": 35, "judge_total_tokens": 488, "judge_prompt_tokens": 429, "judge_completion_tokens": 59}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French (fr) [Doc 110681].", "latency_ms": 834.1591358184814, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer incorrectly states the language is French, while the gold answer says Korean. It also invents a movie title and document reference not grounded in the given information, so it is neither correct nor faithful to the gold.", "main_total_tokens": 2192, "main_prompt_tokens": 2172, "main_completion_tokens": 20, "judge_total_tokens": 472, "judge_prompt_tokens": 396, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 757.8539848327637, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.5, "precision_at_5": 0.2, "ndcg_at_5": 0.6131471927654584, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.6131471927654584, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the count of English-speaking Horror movies released in 2009 with popularity > 7. The gold answer is 2, but the generated answer is 1, which is numerically incorrect. However, the model did attempt to answer the specific question asked (same filters and metric), so it is faithful to the query but factually wrong compared to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 502, "judge_prompt_tokens": 394, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The context provided does not contain information about the best-rated movie released in 1979. Therefore, I cannot answer your question based on the available documents.", "latency_ms": 1507.030725479126, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Given the gold answer that the database has no movies from 1979, the generated answer correctly and faithfully states that it cannot answer the question from the provided context. It does not hallucinate a movie and is consistent with the limitation described in the gold response.", "main_total_tokens": 2217, "main_prompt_tokens": 2186, "main_completion_tokens": 31, "judge_total_tokens": 509, "judge_prompt_tokens": 426, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The movie \"Back to the Future Part IV\" was not mentioned in the provided context, and therefore, I cannot provide a release year for it.", "latency_ms": 1204.3163776397705, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is that 'Back to the Future Part IV' does not exist, but the generated answer instead attributes the inability to answer to missing context, implying the movie might exist but is just not in the context. This is incorrect. It is also unfaithful to the gold answer, which explicitly states that the movie does not exist in the database.", "main_total_tokens": 2526, "main_prompt_tokens": 2496, "main_completion_tokens": 30, "judge_total_tokens": 533, "judge_prompt_tokens": 430, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly indicates that there are no movies matching the criteria, which aligns with the gold answer stating that no such movies from 1980 related to Chernobyl could be found. It faithfully reflects the limitation of available information and even hints at the temporal mismatch (Chernobyl occurred in 1986), without inventing a movie or a popularity score.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 538, "judge_prompt_tokens": 434, "judge_completion_tokens": 104}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 324.3706226348877, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer '0' is incorrect because the gold answer is 4. The model clearly and directly contradicted the known correct answer, so correctness is 0. However, there is no sign of internal inconsistency or deviation from its own reasoning (it simply gave a wrong fact), so faithfulness to its own (albeit faulty) reasoning is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 493, "judge_prompt_tokens": 388, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 358.35862159729004, "retrieval_recall": 0.0, "retrieval_mrr": 0.125, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.19342640361727081, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states 2, but the generated answer is 1, so the generated answer is factually incorrect relative to the provided gold. The assistant clearly and directly answers the question (\"1\") without adding unsupported details, so it is faithful to its own reasoning, but not correct with respect to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 386, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 324.709415435791, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1 TV show, but the generated answer is 0, so it is incorrect. The evaluation (that it is incorrect) is consistent with the provided gold answer, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 464, "judge_prompt_tokens": 389, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 325.0741958618164, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (0). The question is factual and the response is numerically correct and consistent with the provided gold label, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 461, "judge_prompt_tokens": 391, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 232.02109336853027, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer identifies 'Naruto' as the most popular ninja-related TV show. The generated answer says 'I don't know,' which does not provide the correct information, so correctness is 0. However, it does not fabricate any facts and accurately reflects uncertainty, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 477, "judge_prompt_tokens": 386, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The context provided does not mention a Jurassic Park/Power Ranger crossover movie, nor does it provide a release date for such a film. Therefore, I cannot answer your question based on the available information.", "latency_ms": 1683.2003593444824, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.5, "faithfulness_score": 1, "justification": "The gold answer states that the movie was not released. The generated answer instead says it cannot answer based on the available information, implying uncertainty rather than correctly stating that no such movie exists or was released. This makes it only partially correct with respect to the ground truth. However, it is fully faithful to the (unstated) context it references, since it accurately reports that the context does not mention such a movie or release date and does not invent details.", "main_total_tokens": 2230, "main_prompt_tokens": 2190, "main_completion_tokens": 40, "judge_total_tokens": 549, "judge_prompt_tokens": 425, "judge_completion_tokens": 124}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "There is no mention of a movie where Barbie fights the Predator in the provided context.", "latency_ms": 998.5940456390381, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.9, "faithfulness_score": 1, "justification": "Gold answer: explicitly states that the movie was not released. Generated answer: states there is no mention of such a movie in the provided context. This is faithful to the given context but slightly weaker than the gold, since it does not clearly answer the question about existence/release of the movie, only about the context. Hence nearly correct but not fully aligned with the gold.", "main_total_tokens": 2560, "main_prompt_tokens": 2543, "main_completion_tokens": 17, "judge_total_tokens": 507, "judge_prompt_tokens": 399, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The context provided does not contain information about the Star Wars movies, including which one is the 4th oldest. Therefore, I cannot answer the question based on the given context.", "latency_ms": 1363.1212711334229, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is faithful to the (unstated) context by honestly stating that it cannot answer based on the provided information, so faithfulness is high. However, it is factually incorrect relative to the gold answer, which expects a specific movie title; the model should have answered 'Star Wars: Droids - The Great Heep' rather than refusing, so correctness is 0.", "main_total_tokens": 2587, "main_prompt_tokens": 2551, "main_completion_tokens": 36, "judge_total_tokens": 534, "judge_prompt_tokens": 425, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The movie \"Year of the Horse\" was released in 1997 [Doc 19452].", "latency_ms": 913.0229949951172, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the year of release corresponding to the gold answer 1986, but the generated answer gives 1997 and references a different movie (\"Year of the Horse\"), so it is factually incorrect relative to the gold. It also does not remain faithful to the provided gold answer or context, instead introducing a different film and year.", "main_total_tokens": 2428, "main_prompt_tokens": 2408, "main_completion_tokens": 20, "judge_total_tokens": 499, "judge_prompt_tokens": 397, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 261.5468502044678, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific numeric rating (8.466), but the generated answer says 'I don't know,' which does not match the correct rating. Therefore, it is incorrect. However, it is faithful in that it honestly reflects not knowing the answer instead of hallucinating a value.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 398, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is **Mighty Med**. It premiered in 2013 and features the characters discovering a secret hospital for superheroes and being offered a job to \"save the people who save people\" [Doc 121125].", "latency_ms": 1907.1574211120605, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med,' which matches the gold answer and accurately describes the premise. However, it adds an extra detail referencing a specific document ID ([Doc 121125]) that is not present in the provided gold answer. This minor hallucinated citation slightly reduces faithfulness.", "main_total_tokens": 2000, "main_prompt_tokens": 1938, "main_completion_tokens": 62, "judge_total_tokens": 550, "judge_prompt_tokens": 454, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In \"Túnel de los huesos\" (2011), seven prisoners escaped from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 1088.6876583099365, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the prison was in Argentina. The generated answer specifies Villa Devoto prison in Buenos Aires, Argentina. This is consistent and correctly specifies the country as Argentina, with no contradictions or hallucinations relative to the gold.", "main_total_tokens": 2577, "main_prompt_tokens": 2543, "main_completion_tokens": 34, "judge_total_tokens": 502, "judge_prompt_tokens": 425, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice Li Nai falls into a trap designed by the \"Chef Assassin\" Jiang Jie [Doc 135231].", "latency_ms": 1690.5949115753174, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The question asks whose trap the apprentice falls into; the gold answer is simply 'Jiang Jie.' The generated answer correctly identifies Jiang Jie as the person whose trap it is, so it is correct. However, it adds extra details (the apprentice's name Li Nai and describing Jiang Jie as a 'Chef Assassin') and cites a specific document ID that are not present in the gold answer and may not be grounded, slightly reducing faithfulness.", "main_total_tokens": 2166, "main_prompt_tokens": 2134, "main_completion_tokens": 32, "judge_total_tokens": 537, "judge_prompt_tokens": 416, "judge_completion_tokens": 121}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 595.221757888794, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.7095272044910244, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.7095272044910244, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 4, but the generated answer is 0, so the response is factually incorrect. The evaluation correctly compares the generated answer to the given gold answer without adding unsupported information, so it is faithful to the provided data.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 385, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "47", "latency_ms": 269.76490020751953, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '47' is factually incorrect because there are no movies featuring both Bruce Lee and Steven Seagal together, matching the gold answer. The model fabricated a precise number without evidence from the question or any provided context, so it is neither correct nor faithful to the source information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 484, "judge_prompt_tokens": 395, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of \"Tokyo Mater\" (2008) is 6.3 [Doc 42750].", "latency_ms": 919.123649597168, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3 and does not introduce any contradictions or unsupported information beyond a generic citation marker.", "main_total_tokens": 2163, "main_prompt_tokens": 2141, "main_completion_tokens": 22, "judge_total_tokens": 464, "judge_prompt_tokens": 405, "judge_completion_tokens": 59}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in the movie \"The Last Race\" is grassroots auto racing, specifically focusing on a small town stock car track and the community of drivers involved in that tradition [Doc 86410].", "latency_ms": 3042.794942855835, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the style of racing in the movie, and the gold answer is 'Drift.' The generated answer instead describes 'grassroots auto racing' at a small town stock car track, which does not match the gold label and is therefore incorrect. However, the generated answer appears internally consistent and faithful to its own cited document, so there is no evidence of hallucination relative to its source, yielding a high faithfulness score despite being factually wrong per the task.", "main_total_tokens": 2353, "main_prompt_tokens": 2312, "main_completion_tokens": 41, "judge_total_tokens": 546, "judge_prompt_tokens": 419, "judge_completion_tokens": 127}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 1568.2053565979004, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Confronting a Serial Killer.' It does not introduce any contradictions or unsupported claims and remains faithful to the information given.", "main_total_tokens": 2161, "main_prompt_tokens": 2115, "main_completion_tokens": 46, "judge_total_tokens": 523, "judge_prompt_tokens": 452, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of \"Armageddon\" (1998) is 7.838 [Doc 20603].", "latency_ms": 1088.893175125122, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and clearly attributes it, so it is both correct and faithful to the provided gold information.", "main_total_tokens": 2330, "main_prompt_tokens": 2306, "main_completion_tokens": 24, "judge_total_tokens": 472, "judge_prompt_tokens": 409, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 1041.2683486938477, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly giving the movie title as 'The Sixth Sense' with no contradictions or unsupported additions.", "main_total_tokens": 2078, "main_prompt_tokens": 2049, "main_completion_tokens": 29, "judge_total_tokens": 478, "judge_prompt_tokens": 420, "judge_completion_tokens": 58}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 834.7339630126953, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, which matches the gold answer. The citation style aligns with referencing a supporting document, so the answer is both correct and faithful.", "main_total_tokens": 2369, "main_prompt_tokens": 2352, "main_completion_tokens": 17, "judge_total_tokens": 469, "judge_prompt_tokens": 397, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is 'The Transporter Refueled' [Doc 67500].", "latency_ms": 1042.027235031128, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2015 reboot as 'The Transporter Refueled'. It does not add unsupported information and stays faithful to the given content.", "main_total_tokens": 2402, "main_prompt_tokens": 2373, "main_completion_tokens": 29, "judge_total_tokens": 490, "judge_prompt_tokens": 420, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in the year 2015 [Doc 67500].", "latency_ms": 1051.0780811309814, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It does not introduce contradictory or extraneous information beyond a generic citation placeholder, so it is both correct and faithful.", "main_total_tokens": 2260, "main_prompt_tokens": 2238, "main_completion_tokens": 22, "judge_total_tokens": 486, "judge_prompt_tokens": 406, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "The romantic comedy in which Abraham Lincoln comes back from the dead is not explicitly mentioned in the provided context. However, the movie \"Re-Elected\" (2020) features former U.S. presidents, including Abraham Lincoln, coming back from the dead as zombies, but it is categorized as a horror comedy rather than a romantic comedy [Doc 94414]. \n\nIf you are looking for a specific romantic comedy that includes this premise, it does not appear to be present in the provided documents.", "latency_ms": 2054.678201675415, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.1, "faithfulness_score": 0.9, "justification": "The gold answer correctly states that there is no such romantic comedy found, while the generated answer speculates about the film 'Re-Elected' (a horror comedy) and partially contradicts the gold by implying a possible candidate. This makes the core factual answer to the user query incorrect, since the correct response is that no such romantic comedy is identified. However, the generated answer remains mostly faithful to the described document context by noting that 'Re-Elected' is horror comedy and that no specific romantic comedy with that premise appears in the documents.", "main_total_tokens": 2119, "main_prompt_tokens": 2020, "main_completion_tokens": 99, "judge_total_tokens": 632, "judge_prompt_tokens": 489, "judge_completion_tokens": 143}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by Ernest Hemingway is \"Genius\" (2016). The plot involves Max Perkins, a literary editor, who discovers a sprawling, chaotic manuscript by an unknown writer, which he believes to be a literary genius, and he is known for signing literary greats like Ernest Hemingway [Doc 73141].", "latency_ms": 2108.2541942596436, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the title of a movie that alludes to a manuscript written by Ernest Hemingway, and the gold answer is 'Havana Darkness'. The generated answer gives 'Genius (2016)', which is incorrect. However, the answer remains faithful to the provided document context about 'Genius' and does not hallucinate beyond that context, so faithfulness is high even though the answer is wrong for the task.", "main_total_tokens": 2093, "main_prompt_tokens": 2020, "main_completion_tokens": 73, "judge_total_tokens": 577, "judge_prompt_tokens": 461, "judge_completion_tokens": 116}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "The TV show in which Doll heads to Hollywood to be with her childhood best friend Em is titled \"Doll & Em\" [Doc 122259].", "latency_ms": 1034.9903106689453, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em,' which matches the gold answer exactly. It is faithful to the provided information and does not introduce unsupported content.", "main_total_tokens": 2047, "main_prompt_tokens": 2016, "main_completion_tokens": 31, "judge_total_tokens": 487, "judge_prompt_tokens": 420, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The 2007 movie in which Hallam has a talent for spying on people and attempts to find the cause of his mother’s death is titled \"Hallam Foe\" [Doc 39389].", "latency_ms": 2191.4150714874268, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Hallam Foe') and correctly restates the movie title without adding unsupported or contradictory information. It is both correct and faithful to the provided reference.", "main_total_tokens": 2300, "main_prompt_tokens": 2259, "main_completion_tokens": 41, "judge_total_tokens": 510, "judge_prompt_tokens": 440, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The context does not provide information about a specific last name or whether it sounds like a friend or a foe. Therefore, I cannot answer the question based on the provided documents.", "latency_ms": 1607.332468032837, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'A foe', meaning the model should identify the last name as sounding like an enemy. The generated answer instead claims it cannot answer from context, which contradicts the gold. This is incorrect. However, it is faithful to the (assumed) lack of explicit information in the context, so faithfulness is high, but correctness with respect to the gold label is zero.", "main_total_tokens": 2234, "main_prompt_tokens": 2199, "main_completion_tokens": 35, "judge_total_tokens": 531, "judge_prompt_tokens": 421, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "In the 2017 TV show *Atypical*, Sam is described as an 18-year-old on the autism spectrum [Doc 124583].", "latency_ms": 1077.4178504943848, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that Sam is an 18-year-old, which matches the gold answer of 18, so it is correct. It is also faithful to the provided gold information with no contradictions or hallucinated details beyond the age description.", "main_total_tokens": 2024, "main_prompt_tokens": 1993, "main_completion_tokens": 31, "judge_total_tokens": 497, "judge_prompt_tokens": 418, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show *Atypical*, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 1056.9632053375244, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which matches the gold answer exactly. It does not introduce contradictions or unsupported claims; the bracketed document reference does not affect factuality, so the response is faithful to the provided gold answer.", "main_total_tokens": 1860, "main_prompt_tokens": 1836, "main_completion_tokens": 24, "judge_total_tokens": 493, "judge_prompt_tokens": 411, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "In the movie À corde tendue, Sandra was born in Chamonix [Doc 59659].", "latency_ms": 973.2341766357422, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, which matches the gold answer. It is fully consistent with the provided information and does not introduce contradictions or unsupported claims.", "main_total_tokens": 2494, "main_prompt_tokens": 2473, "main_completion_tokens": 21, "judge_total_tokens": 486, "judge_prompt_tokens": 418, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In the movie \"When the Rain Falls,\" Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 1358.701467514038, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly fills in the blank with 'rains,' matching the gold answer, so correctness is 1. However, it invents a specific movie title ('When the Rain Falls') and a document reference that are not supported by the given information, so faithfulness is slightly reduced.", "main_total_tokens": 2394, "main_prompt_tokens": 2370, "main_completion_tokens": 24, "judge_total_tokens": 507, "judge_prompt_tokens": 415, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the low-rated German language animation \"Tabaluga,\" the character Tabaluga is the last of the dragons and the crown-prince of Greenland, a magical place inhabited by talking animals of many different species [Doc 114810].", "latency_ms": 1644.122838973999, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies that in the animation Tabaluga is the crown-prince of Greenland, matching the gold answer. However, it adds extra descriptive information about Greenland being a magical place inhabited by talking animals and cites a specific document ID, neither of which is supported or requested in the original question and may not be grounded in the provided data, slightly reducing faithfulness.", "main_total_tokens": 2360, "main_prompt_tokens": 2312, "main_completion_tokens": 48, "judge_total_tokens": 550, "judge_prompt_tokens": 442, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie that matches the description of \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers\" is **One Afternoon** from the year 2021 [Doc 96838].", "latency_ms": 1336.7893695831299, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'One Afternoon', matching the gold answer exactly. It also remains faithful to the provided description and does not introduce contradictions; mentioning the year and a document reference does not conflict with the gold answer.", "main_total_tokens": 1957, "main_prompt_tokens": 1905, "main_completion_tokens": 52, "judge_total_tokens": 534, "judge_prompt_tokens": 456, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is titled \"The Life Ahead,\" released in 2020. It has a rating of 6.746 [Doc 93260].", "latency_ms": 1374.1483688354492, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The answer correctly identifies the movie as \"The Life Ahead\" and accurately describes the plot premise given in the question. However, it fabricates an exact numeric rating (6.746) and a document reference [Doc 93260] that are not grounded in the user-provided content, reducing faithfulness slightly.", "main_total_tokens": 2127, "main_prompt_tokens": 2074, "main_completion_tokens": 53, "judge_total_tokens": 547, "judge_prompt_tokens": 451, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team. The film illustrates how he, coming from humble beginnings, was able to compete against and win against the elite of motor racing [Doc 73859].", "latency_ms": 1621.3841438293457, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and adds accurate contextual information about him founding the McLaren racing team. It is fully consistent with the gold answer and does not introduce contradictions, so it is both correct and faithful.", "main_total_tokens": 2342, "main_prompt_tokens": 2284, "main_completion_tokens": 58, "judge_total_tokens": 526, "judge_prompt_tokens": 442, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The context provided does not specify the name of Sonny Hayes' racing team in the movie \"F1\" [Doc 111503]. Therefore, I cannot provide that information.", "latency_ms": 1823.0676651000977, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the team is the McLaren Motor Racing team. The generated answer says the context does not specify the name and withholds an answer. Given the gold, this is factually incorrect relative to the labeled truth, so correctness is 0. However, the model accurately reflects that it did not see the answer in the provided context and does not hallucinate, so it is faithful to the given context, yielding faithfulness 1.", "main_total_tokens": 2441, "main_prompt_tokens": 2406, "main_completion_tokens": 35, "judge_total_tokens": 540, "judge_prompt_tokens": 418, "judge_completion_tokens": 122}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context does not provide information about a specific individual or their country of origin. Therefore, I cannot answer the question based on the provided documents.", "latency_ms": 1097.5241661071777, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates the person is from New Zealand, but the generated answer claims the country cannot be determined. This is incorrect relative to the gold, so correctness is 0. However, the generated answer is faithful to its stated reasoning that the context did not specify a country, so faithfulness is 1.", "main_total_tokens": 2382, "main_prompt_tokens": 2353, "main_completion_tokens": 29, "judge_total_tokens": 499, "judge_prompt_tokens": 405, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "In the 2013 movie \"A Good Day to Die Hard,\" John McClane does not blow up the Great Wall of China. The plot involves him traveling to Moscow to help his son, who is a CIA operative, but there is no mention of the Great Wall of China being blown up in the context provided [Doc 58791].", "latency_ms": 2271.209478378296, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.2, "faithfulness_score": 0.9, "justification": "The gold answer correctly says there is no such movie in the database. The generated answer instead confidently names a specific 2013 film (\"A Good Day to Die Hard\") and gives plot details, which goes beyond the gold and effectively asserts an answer rather than acknowledging the information is not present. While it is true that in that film McClane does not blow up the Great Wall, the question expects recognition that no such film is found. Thus, relative to the task and gold, the generated answer is mostly incorrect. It is, however, largely faithful to its cited document and does not hallucinate events within that film, so faithfulness is high but not perfect due to the overreach beyond the database scope.", "main_total_tokens": 2491, "main_prompt_tokens": 2421, "main_completion_tokens": 70, "judge_total_tokens": 647, "judge_prompt_tokens": 468, "judge_completion_tokens": 179}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie that matches the description is **\"Tomie: Forbidden Fruit.\"** The plot involves a lonely high school girl who befriends a strange, mysterious, and beautiful girl, leading to the realization that their meeting was not purely accidental [Doc 28589].", "latency_ms": 1706.6590785980225, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'Tomie: Forbidden Fruit,' matching the gold answer and the description, so correctness is 1. However, it cites a specific source [Doc 28589] that is not present in the provided context, making that reference potentially fabricated; thus faithfulness is 0.", "main_total_tokens": 2402, "main_prompt_tokens": 2343, "main_completion_tokens": 59, "judge_total_tokens": 571, "judge_prompt_tokens": 476, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie \"Lei mi parla ancora,\" Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 1325.6111145019531, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, matching the gold answer. It is directly supported by the cited document, so it is both correct and faithful.", "main_total_tokens": 2685, "main_prompt_tokens": 2655, "main_completion_tokens": 30, "judge_total_tokens": 496, "judge_prompt_tokens": 424, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In \"Scorpion King: Book of Souls\" (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls, which will allow them to put an end to an evil warlord [Doc 82582].", "latency_ms": 1274.1742134094238, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly states that in 'Scorpion King: Book of Souls' (2018), the Scorpion King and Tala search for the Book of Souls, matching the gold answer. However, it adds extra detail about the relic allowing them to end an evil warlord and even cites a document reference '[Doc 82582]', which is speculative in this context and not grounded in the provided gold answer. Thus, while the core answer is correct, faithfulness is slightly reduced due to unnecessary, unverified embellishment.", "main_total_tokens": 2573, "main_prompt_tokens": 2522, "main_completion_tokens": 51, "judge_total_tokens": 583, "judge_prompt_tokens": 445, "judge_completion_tokens": 138}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The context provided does not mention a movie titled \"Zombies on the Moon.\" Therefore, I cannot provide the year of its release.", "latency_ms": 1057.5566291809082, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot provide the release year because the movie is not mentioned in the context, which matches the gold response in substance. It does not invent a date or contradict the given information, so it is both correct and faithful.", "main_total_tokens": 2102, "main_prompt_tokens": 2075, "main_completion_tokens": 27, "judge_total_tokens": 499, "judge_prompt_tokens": 418, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"The Galactic Baker\" was released is not found in the provided context.", "latency_ms": 927.3056983947754, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available / not found. Given the context is limited and does not include the film, the generated answer is correct and aligned with the provided information, and it faithfully reflects the lack of data without inventing a year.", "main_total_tokens": 2370, "main_prompt_tokens": 2353, "main_completion_tokens": 17, "judge_total_tokens": 493, "judge_prompt_tokens": 406, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"Return of the Null Pointer\" was released is not found in the provided context.", "latency_ms": 803.565263748169, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available/known, which is correct given the context. The generated answer does not hallucinate a year and stays faithful to the lack of information, matching the intent of the gold response.", "main_total_tokens": 2342, "main_prompt_tokens": 2323, "main_completion_tokens": 19, "judge_total_tokens": 491, "judge_prompt_tokens": 410, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year of \"Data Science: The Movie\" is not found in the provided context.", "latency_ms": 918.9870357513428, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available / not found. This is correct given no information about such a movie, and the generated answer faithfully reflects the lack of information without adding fabricated details.", "main_total_tokens": 2302, "main_prompt_tokens": 2283, "main_completion_tokens": 19, "judge_total_tokens": 485, "judge_prompt_tokens": 410, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Python's Revenge 2\" is not found in the provided context.", "latency_ms": 1235.6081008911133, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database. The generated answer states that the release year is not found in the provided context, which is effectively the same outcome: it does not fabricate a year and acknowledges the absence of information. This is correct given no known data, and it is faithful to the (lack of) context.", "main_total_tokens": 2172, "main_prompt_tokens": 2153, "main_completion_tokens": 19, "judge_total_tokens": 510, "judge_prompt_tokens": 410, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year of \"The Last Pixel\" is not found in the provided context.", "latency_ms": 568.6888694763184, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer and the generated answer both correctly indicate that the release year is not available/known from the given information. The generated answer is fully consistent with the gold answer and does not hallucinate a year.", "main_total_tokens": 1987, "main_prompt_tokens": 1970, "main_completion_tokens": 17, "judge_total_tokens": 479, "judge_prompt_tokens": 406, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"The Man Who Ate the Sun\" was released is not found in the provided context.", "latency_ms": 911.8781089782715, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database, and the generated answer correctly reflects that the release year is not found in the provided context. Both semantically convey that the information is unavailable, so the response is correct relative to the task and faithful to the given context.", "main_total_tokens": 2532, "main_prompt_tokens": 2512, "main_completion_tokens": 20, "judge_total_tokens": 499, "judge_prompt_tokens": 412, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was directed by Adrian Sitaru [Doc 46069].", "latency_ms": 965.9795761108398, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer incorrectly states that Adrian Sitaru directed 'The Last Days of Lehman Brothers,' while the gold answer says the database does not provide the director information. This contradicts the given database constraint and fabricates a specific director attribution, so it is both factually incorrect relative to the task setup and unfaithful to the gold response and the database limitations.", "main_total_tokens": 2596, "main_prompt_tokens": 2572, "main_completion_tokens": 24, "judge_total_tokens": 526, "judge_prompt_tokens": 421, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not mention a movie titled \"A Love to Die For,\" nor does it provide information about its director. Therefore, I cannot answer your question based on the given context.", "latency_ms": 1405.069351196289, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "Both the gold and generated answers correctly state that the director cannot be identified from the provided information, so the generated answer is effectively correct. However, the gold specifically notes that the director information is not in the database, while the generated answer frames it as not in the context, which is a slightly different justification, so faithfulness is very high but not perfect.", "main_total_tokens": 2197, "main_prompt_tokens": 2159, "main_completion_tokens": 38, "judge_total_tokens": 538, "judge_prompt_tokens": 433, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not specified in the provided context. Therefore, I cannot provide that information.", "latency_ms": 1135.7150077819824, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not provided in the database and thus cannot be answered. The generated answer similarly states that the director is not specified in the provided context and therefore cannot be given. This matches the gold both in content and reasoning. The answer is correct and faithful to the given gold standard.", "main_total_tokens": 2323, "main_prompt_tokens": 2297, "main_completion_tokens": 26, "judge_total_tokens": 513, "judge_prompt_tokens": 419, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Night at Tremore Beach\" is not mentioned in the provided context. Therefore, I cannot provide that information.", "latency_ms": 1053.431749343872, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the director of the movie, and the gold answer states that this information is not available in the database. The generated answer correctly states that the director is not mentioned in the provided context and therefore cannot be given. This matches the gold response and stays within the given information, so it is both correct and faithful.", "main_total_tokens": 1924, "main_prompt_tokens": 1894, "main_completion_tokens": 30, "judge_total_tokens": 524, "judge_prompt_tokens": 427, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is titled **\"Rise of the Mummy,\"** released in 2021. In this film, the students become trapped in a time loop and must defeat the mummy to escape, as it seeks to collect their souls to walk the earth permanently [Doc 98375].", "latency_ms": 1784.8484516143799, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer only requires identifying the movie title, 'Rise of the Mummy.' The generated answer not only gives the correct title but also fabricates specific plot details—being trapped in a time loop, the mummy collecting souls to walk the earth permanently, and a citation [Doc 98375]—which are hallucinated and not grounded in the provided information. Therefore, despite naming the correct movie, the presence of invented supporting details makes the response unfaithful to the source and overall incorrect under the criteria for this evaluation.", "main_total_tokens": 2131, "main_prompt_tokens": 2063, "main_completion_tokens": 68, "judge_total_tokens": 596, "judge_prompt_tokens": 459, "judge_completion_tokens": 137}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse is titled **\"Stranger in My Bed,\"** released in 2005 [Doc 35237].", "latency_ms": 1282.6619148254395, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Stranger in My Bed.' It does not introduce contradictions or unsupported claims, and citing a document ID does not detract from faithfulness to the provided gold answer.", "main_total_tokens": 2165, "main_prompt_tokens": 2128, "main_completion_tokens": 37, "judge_total_tokens": 512, "judge_prompt_tokens": 433, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The scenario described occurs in the TV series \"Game of Silence,\" where a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years. This leads to the resurfacing of a dark secret from their past, prompting the group to band together to address the wrongs they experienced [Doc 123938].", "latency_ms": 3412.7516746520996, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie (actually a TV show) in which a successful Atlanta attorney's long-lost childhood friends reappear after 25 years. The gold answer is 'Game of Silence.' The generated answer correctly identifies it as the TV series 'Game of Silence' and accurately describes the scenario, matching the gold. It remains faithful to the given information and does not introduce contradictions, so both correctness and faithfulness are full.", "main_total_tokens": 1863, "main_prompt_tokens": 1798, "main_completion_tokens": 65, "judge_total_tokens": 580, "judge_prompt_tokens": 462, "judge_completion_tokens": 118}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie you are referring to is \"The Craigslist Killer,\" which was released in 2011. It is based on the true story of Philip Markoff, a med student who led a double life as a brutal sexual deviant, abusing prostitutes he found via Craigslist [Doc 53039].", "latency_ms": 1467.5467014312744, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the description, and the gold answer is 'The Craigslist Killer.' The generated answer correctly identifies the same movie and accurately adds details about its release year and the real-life inspiration, remaining consistent with the query and gold answer. No contradictions or hallucinations relative to the given information are present.", "main_total_tokens": 2041, "main_prompt_tokens": 1982, "main_completion_tokens": 59, "judge_total_tokens": 545, "judge_prompt_tokens": 451, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which Yuki Hirano sets his eyes on the role of a forestry trainee after failing his university entrance exam is titled **Wood Job!**. It was released in 2014 and is categorized as a drama and comedy [Doc 66094].", "latency_ms": 1945.3520774841309, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'Wood Job!' and stays faithful to the question. The extra details about release year and genre are not in the gold answer but are accurate; however, the reference to '[Doc 66094]' suggests reliance on a specific source not provided in the prompt, slightly reducing faithfulness under strict criteria.", "main_total_tokens": 2473, "main_prompt_tokens": 2420, "main_completion_tokens": 53, "judge_total_tokens": 554, "judge_prompt_tokens": 453, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure, is **\"Sesame Street: The Magical Wand Chase\"** released in 2017 [Doc 78373].", "latency_ms": 2069.9615478515625, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'Sesame Street: The Magical Wand Chase,' matching the gold answer, so correctness is 1. However, it adds a release year and a citation-like reference ([Doc 78373]) that are not present in the provided context, indicating invented sourcing details, so faithfulness is 0.", "main_total_tokens": 2480, "main_prompt_tokens": 2423, "main_completion_tokens": 57, "judge_total_tokens": 566, "judge_prompt_tokens": 467, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a 7-year-old boy must find the hero within him to save his parents trapped inside a burning car due to a rock fall is titled **Beyond**, released in **2016** [Doc 74597].", "latency_ms": 2069.9193477630615, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Beyond,\" which matches the gold label. The added detail about the 2016 release is plausible and does not contradict the given information. The answer is fully consistent with the prompt, so both correctness and faithfulness are high.", "main_total_tokens": 2140, "main_prompt_tokens": 2093, "main_completion_tokens": 47, "judge_total_tokens": 540, "judge_prompt_tokens": 455, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman is fired from a financial corporation during the Asia crisis and returns home with no money is \"6ixtynin9\" from 1999 [Doc 22478].", "latency_ms": 1621.4649677276611, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', which matches the gold answer. It is directly grounded in the provided information and does not introduce unsupported claims.", "main_total_tokens": 2046, "main_prompt_tokens": 2005, "main_completion_tokens": 41, "judge_total_tokens": 511, "judge_prompt_tokens": 443, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie you are referring to is **Club Dread** (2004). In this film, a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort, which is described as a hedonistic island paradise for swingers. The club's staff must deal with the violence or at least try to hide it [Doc 32410].", "latency_ms": 3058.53271484375, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies the movie as 'Club Dread', matching the gold answer, so correctness is 1. The explanation restates the plot given in the question and adds a citation [Doc 32410] that we cannot verify from the prompt, so there is a small risk of hallucination regarding the source, hence faithfulness is slightly reduced.", "main_total_tokens": 2276, "main_prompt_tokens": 2204, "main_completion_tokens": 72, "judge_total_tokens": 601, "judge_prompt_tokens": 496, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie you are referring to is titled \"Toilet,\" which was released in 2010. The plot involves a 30-something Gundam fan and engineer who is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral [Doc 48873].", "latency_ms": 2308.42661857605, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" matching the gold answer exactly. It remains faithful to the given information and does not introduce contradictions or unsupported claims, aside from a neutral reference tag, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2780, "main_prompt_tokens": 2714, "main_completion_tokens": 66, "judge_total_tokens": 562, "judge_prompt_tokens": 480, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie you are referring to is \"Electric Angel,\" which was released in 1981. It is described as a typical experimental film that employs a variety of audiovisual techniques to evoke a sense of polymorphic eroticism, drawing from European and Mediterranean cinematography of the 20th century. The film combines animation and live action, featuring a loose structure that enhances its open-ended character, with rhythm being a key element [Doc 1024].", "latency_ms": 2255.8538913726807, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0.9, "faithfulness_score": 1, "justification": "The question asks for the movie title; the generated answer correctly identifies it as \"Electric Angel,\" matching the gold label. The added release year and descriptive details are plausible but not verifiable from the prompt alone; however, they are consistent with the given description rather than contradicting it. Thus, correctness is high but not perfect due to unverifiable specifics, while faithfulness to the provided information is complete.", "main_total_tokens": 2452, "main_prompt_tokens": 2363, "main_completion_tokens": 89, "judge_total_tokens": 619, "judge_prompt_tokens": 505, "judge_completion_tokens": 114}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is \"Doomsday Stories,\" which has a rating of 2.167 [Doc 108366].", "latency_ms": 915.1451587677002, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the gold label indicates that the lowest rated movie from 2023 is 'Mega Ape', while the generated response claims it is 'Doomsday Stories'. However, the answer is faithful to its cited document since it directly reflects the rating and title mentioned there, so faithfulness is high even though it conflicts with the external gold standard.", "main_total_tokens": 2141, "main_prompt_tokens": 2110, "main_completion_tokens": 31, "judge_total_tokens": 512, "judge_prompt_tokens": 412, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 1482.361078262329, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific highest rated TV show from 2010, with the gold answer being 'Tea House'. The generated answer says 'I don't know,' which does not match the gold answer and is therefore incorrect. However, the model is honest about its lack of knowledge and does not fabricate an answer, so it is fully faithful to its own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 386, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 701.7688751220703, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because the gold standard specifies a concrete movie title, while the model responded with not knowing. However, the model did not hallucinate an incorrect alternative; it simply expressed uncertainty, which is faithful to its own stated knowledge limits.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 395, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The language of the lowest-rated movie from 1980, which is \"Bloodeaters\" with a rating of 4.0, is English (en) [Doc 820].", "latency_ms": 957.0446014404297, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980. The gold answer is 'English'. The generated answer states that the language is English and even notes the code (en) and references a document. It matches the gold answer, and there is no indication of hallucination or contradiction, so it is both correct and faithful.", "main_total_tokens": 2123, "main_prompt_tokens": 2085, "main_completion_tokens": 38, "judge_total_tokens": 522, "judge_prompt_tokens": 421, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was \"The Sopranos,\" which had a popularity rating of 59.4284 and a rating of 8.6 [Doc 115146].", "latency_ms": 1621.9499111175537, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold standard identifies 'Law & Order: Special Victims Unit' as the TV show with the highest popularity in 1999, while the generated response states 'The Sopranos.' It is also unfaithful, as it cites a specific popularity rating from a document (Doc 115146) that conflicts with the gold answer, indicating it is not grounded in the expected reference.", "main_total_tokens": 2466, "main_prompt_tokens": 2423, "main_completion_tokens": 43, "judge_total_tokens": 543, "judge_prompt_tokens": 433, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 1019.719123840332, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 and attributes it to a document, indicating it is grounded in the provided source. Both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2351, "main_prompt_tokens": 2333, "main_completion_tokens": 18, "judge_total_tokens": 469, "judge_prompt_tokens": 400, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in the year 1999 [Doc 22481].", "latency_ms": 1041.5587425231934, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states the movie was released in 1999, but the gold label says 2019, so the answer is factually incorrect with respect to the gold. However, the model explicitly attributes the 1999 date to a cited document ([Doc 22481]) and does not contradict its own source, so it is faithful to the provided evidence, yielding low correctness but high faithfulness.", "main_total_tokens": 2173, "main_prompt_tokens": 2151, "main_completion_tokens": 22, "judge_total_tokens": 518, "judge_prompt_tokens": 406, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 185.01591682434082, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer (3) does not match the gold answer (279), so it is incorrect. There is no indication that the model is following or grounded in the provided gold data, so faithfulness is also low.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 458, "judge_prompt_tokens": 383, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "12", "latency_ms": 247.33257293701172, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (12 votes), so it is both correct and faithful to the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 437, "judge_prompt_tokens": 383, "judge_completion_tokens": 54}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in the year 2009 [Doc 46069].", "latency_ms": 1722.6557731628418, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'The Last Days of Lehman Brothers' was released in 2009, matching the gold answer. It is also faithful to the provided information without adding unsupported details.", "main_total_tokens": 2295, "main_prompt_tokens": 2270, "main_completion_tokens": 25, "judge_total_tokens": 485, "judge_prompt_tokens": 412, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 819.0979957580566, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the language is English, and the generated answer correctly states that the original language is English. The citation to a document does not contradict or add unsupported information, so the response is both correct and faithful.", "main_total_tokens": 2561, "main_prompt_tokens": 2542, "main_completion_tokens": 19, "judge_total_tokens": 477, "judge_prompt_tokens": 402, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "1987", "latency_ms": 1389.7628784179688, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly. Wall Street was indeed released in 1987. The response is correct and fully faithful to the reference.", "main_total_tokens": 2761, "main_prompt_tokens": 2758, "main_completion_tokens": 3, "judge_total_tokens": 442, "judge_prompt_tokens": 381, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Brain Fluid Explosion Girl", "latency_ms": 950.308084487915, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie where classmates play a survival game using their cell phones as 'Brain Fluid Explosion Girl.'", "main_total_tokens": 2703, "main_prompt_tokens": 2698, "main_completion_tokens": 5, "judge_total_tokens": 453, "judge_prompt_tokens": 392, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 2347.00083732605, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.4306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.719741384391281, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Dungeons & Dragons.'", "main_total_tokens": 2792, "main_prompt_tokens": 2787, "main_completion_tokens": 5, "judge_total_tokens": 446, "judge_prompt_tokens": 394, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 1120.5615997314453, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct movie title for the described plot. It is both correct and faithful.", "main_total_tokens": 2650, "main_prompt_tokens": 2643, "main_completion_tokens": 7, "judge_total_tokens": 461, "judge_prompt_tokens": 403, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900", "latency_ms": 1035.4092121124268, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (1900) and is fully supported by the reference, so it is both correct and faithful.", "main_total_tokens": 2507, "main_prompt_tokens": 2504, "main_completion_tokens": 3, "judge_total_tokens": 447, "judge_prompt_tokens": 388, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June", "latency_ms": 524.1260528564453, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the name of the movie character, which matches the gold answer \"June\"; the generated answer is also \"June,\" so it is both correct and faithful to the gold.", "main_total_tokens": 126, "main_prompt_tokens": 125, "main_completion_tokens": 1, "judge_total_tokens": 503, "judge_prompt_tokens": 433, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 231.9328784942627, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer admits not knowing instead of giving an incorrect movie title. While it does not supply the correct film, it does not present any false information relative to the question or the gold answer, so it is fully faithful and not factually wrong in what it states.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 484, "judge_prompt_tokens": 399, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 325.2720832824707, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 7.6, but the generated response said 'I don't know,' which is incorrect. However, it does not fabricate any information and honestly admits not knowing, so it is fully faithful to the model's stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 389, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "8", "latency_ms": 1355.6690216064453, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 0.9828920819566879, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3364045224324153, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (8) does not match the gold answer (4), so it is incorrect. However, the model directly answered the question asked without adding extraneous information, so it is faithful to the user query.", "main_total_tokens": 1457, "main_prompt_tokens": 1442, "main_completion_tokens": 15, "judge_total_tokens": 469, "judge_prompt_tokens": 394, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 285.68434715270996, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The gold answer says there are 2 such TV shows, while the generated answer claims there are none, so it is factually incorrect (score 0 for correctness). The model does, however, stay within its own stated limitations and does not hallucinate specific titles or numbers; it simply says it cannot find any matching items, which is internally consistent and cautious, so faithfulness is relatively high though not perfect because it should ideally acknowledge uncertainty or data limits more explicitly.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 531, "judge_prompt_tokens": 404, "judge_completion_tokens": 127}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 387.1591091156006, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific French-language TV show from 1980 (Fantômas). The generated answer says 'I don't know,' which does not provide the correct show, so correctness is 0. However, the answer does not invent any incorrect information and accurately reflects a lack of knowledge, so it is fully faithful to the model's uncertainty, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 500, "judge_prompt_tokens": 389, "judge_completion_tokens": 111}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise", "latency_ms": 1389.888048171997, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Kill Cruise') and correctly identifies the movie described. It is fully faithful to the provided information with no hallucinations or deviations.", "main_total_tokens": 2653, "main_prompt_tokens": 2650, "main_completion_tokens": 3, "judge_total_tokens": 452, "judge_prompt_tokens": 387, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 3366.7702674865723, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the title of the movie about Fred Rogers and his TV series as 'Won't You Be My Neighbor?'. It is both correct and fully faithful to the gold reference.", "main_total_tokens": 2851, "main_prompt_tokens": 2843, "main_completion_tokens": 8, "judge_total_tokens": 484, "judge_prompt_tokens": 408, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018", "latency_ms": 1807.1393966674805, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (2018), so it is both correct and faithful.", "main_total_tokens": 2896, "main_prompt_tokens": 2880, "main_completion_tokens": 16, "judge_total_tokens": 431, "judge_prompt_tokens": 380, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy", "latency_ms": 953.0465602874756, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV series. It is both correct and faithful to the reference.", "main_total_tokens": 2904, "main_prompt_tokens": 2900, "main_completion_tokens": 4, "judge_total_tokens": 445, "judge_prompt_tokens": 389, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 years", "latency_ms": 972.1629619598389, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer that they were held captive for 7 years. It is both factually correct and fully faithful to the reference.", "main_total_tokens": 2411, "main_prompt_tokens": 2408, "main_completion_tokens": 3, "judge_total_tokens": 459, "judge_prompt_tokens": 397, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m", "latency_ms": 734.2562675476074, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The Brink's-Mat robbery involved about £26 million in gold bullion (plus diamonds and cash) at the time. The generated answer matches the provided gold value and the known historical figure, so it is both correct and faithful to the given information.", "main_total_tokens": 2091, "main_prompt_tokens": 2088, "main_completion_tokens": 3, "judge_total_tokens": 479, "judge_prompt_tokens": 398, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 548.7234592437744, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score (80.5772) of the most popular English-speaking TV show released in 1997. The generated answer says only 'I don't know,' which does not provide the correct value, so correctness is 0. However, it does not invent any incorrect facts and honestly admits uncertainty, so it is fully faithful to the model’s knowledge and limitations, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 516, "judge_prompt_tokens": 396, "judge_completion_tokens": 120}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 370.0432777404785, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 Russian language TV show released in 1997. The generated answer claims it cannot find any matching movies or shows, which contradicts the gold and is therefore factually incorrect. However, the generated answer is internally consistent and does not introduce hallucinated details; it simply states a lack of information, so it is faithful to its own (albeit limited) reasoning.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 506, "judge_prompt_tokens": 394, "judge_completion_tokens": 112}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes", "latency_ms": 903.679370880127, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Angel Eyes') and correctly identifies the TV show in which Soo-wan regains her sight after an eye transplant. It is both correct and faithful to the reference.", "main_total_tokens": 2964, "main_prompt_tokens": 2961, "main_completion_tokens": 3, "judge_total_tokens": 470, "judge_prompt_tokens": 397, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "en", "latency_ms": 1458.796739578247, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'en' is incorrect because the gold label indicates the movie language is Korean. It is also unfaithful to the given gold answer, which explicitly states 'Korean' as the correct language.", "main_total_tokens": 2878, "main_prompt_tokens": 2868, "main_completion_tokens": 10, "judge_total_tokens": 451, "judge_prompt_tokens": 377, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 656.9099426269531, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.5, "precision_at_5": 0.2, "ndcg_at_5": 0.6131471927654584, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.6131471927654584, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a numeric count and the gold answer is 2, but the generated answer is 1, which is incorrect. The model is clearly addressing the correct question, so it is faithful to the task, but the final value is wrong compared to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 394, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 524.3914127349854, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The gold answer states there are no movies from 1979 in its database, so it cannot provide one. The generated answer simply says 'I don't know,' which is consistent with not having the information, so it is correct relative to the gold. However, it does not reproduce the key constraint from the gold answer (that the limitation is due to the database containing no 1979 movies), so it is not fully faithful to the gold explanation.", "main_total_tokens": 579, "main_prompt_tokens": 578, "main_completion_tokens": 1, "judge_total_tokens": 520, "judge_prompt_tokens": 399, "judge_completion_tokens": 121}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The movie Back to the Future Part IV was not released, as there is no mention of it in the provided context. The last installment in the original trilogy is Back to the Future Part III, released in 1990 [Doc 10621].", "latency_ms": 2007.7965259552002, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about the release year of 'Back to the Future Part IV.' Both the gold and generated answers correctly state that such a movie does not exist / was not released. The generated answer adds extra context about Part III being the last installment, which is true and consistent with known facts and does not contradict the gold. It is therefore both correct and faithful to the gold answer.", "main_total_tokens": 3142, "main_prompt_tokens": 3091, "main_completion_tokens": 51, "judge_total_tokens": 558, "judge_prompt_tokens": 450, "judge_completion_tokens": 108}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot find any movies or shows matching the criteria, which aligns with the gold answer that there were no movies from 1980 related to the Chernobyl disaster. It is faithful to the information implied by the gold and does not introduce incorrect specific titles or fabricated data, only adding a reasonable note about the event timing.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 536, "judge_prompt_tokens": 434, "judge_completion_tokens": 102}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 339.1385078430176, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because there are 4 TV shows or movies with 'Astro Boy' in the title, while it claims there are none. However, the model does not introduce any fabricated specific titles or numbers; it simply states it cannot find any, so despite being wrong, it remains faithful to its own (albeit limited) retrieval and does not hallucinate concrete details.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 508, "judge_prompt_tokens": 397, "judge_completion_tokens": 111}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 988.018274307251, "retrieval_recall": 0.0, "retrieval_mrr": 0.125, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.19342640361727081, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer says there are no such movies, so it is factually incorrect relative to the gold, giving correctness 0. The model does not invent specific movies or numbers and explicitly states it cannot find any, which is consistent with its apparent knowledge state, so it is faithful to its own reasoning and limitations, giving faithfulness 1.", "main_total_tokens": 3363, "main_prompt_tokens": 3362, "main_completion_tokens": 1, "judge_total_tokens": 503, "judge_prompt_tokens": 395, "judge_completion_tokens": 108}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 215.99245071411133, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there was 1 TV show, while the generated answer claims there were none. This is factually incorrect relative to the gold. It also contradicts the provided gold answer, so it is unfaithful to the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 398, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 247.4215030670166, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the number of TV shows about Super Mario Bros released between 2000 and 2010. The gold answer is 0. The generated answer states that it cannot find any movies or shows matching those criteria, which is consistent with there being zero such TV shows. Therefore it is correct and faithful to the gold answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 499, "judge_prompt_tokens": 400, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 253.85546684265137, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto', a specific popular ninja-themed TV show. The generated answer says 'I don't know', which is incorrect because it fails to provide the correct show. However, the answer is faithful in that it does not hallucinate or contradict the given information; it simply admits lack of knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 386, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 981.9521903991699, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that the movie was not released; saying 'I don't know' is not correct with respect to the gold answer. However, the model does not fabricate an incorrect date or details and admits uncertainty, so it is fully faithful to its own stated uncertainty.", "main_total_tokens": 565, "main_prompt_tokens": 564, "main_completion_tokens": 1, "judge_total_tokens": 474, "judge_prompt_tokens": 389, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 655.677080154419, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that this movie was not released, while the generated answer says 'I don't know,' which is incorrect. However, the model does not invent any false specific movie or details; it simply expresses ignorance, so it remains faithful to its own uncertainty and does not hallucinate facts.", "main_total_tokens": 564, "main_prompt_tokens": 563, "main_completion_tokens": 1, "judge_total_tokens": 476, "judge_prompt_tokens": 386, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The 4th oldest Star Wars movie is not specified in the provided context.", "latency_ms": 1311.2330436706543, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the 4th oldest Star Wars movie, according to the gold reference, is 'Star Wars: Droids - The Great Heep', but the model instead said it was not specified. This also shows a lack of faithfulness to the given gold answer, as it ignores the provided correct title and claims insufficient information.", "main_total_tokens": 3201, "main_prompt_tokens": 3184, "main_completion_tokens": 17, "judge_total_tokens": 506, "judge_prompt_tokens": 405, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "2004", "latency_ms": 1659.5947742462158, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states the correct year is 1986, but the generated answer gives 2004, which is incorrect. The faithfulness is 1 because the evaluation is based directly on the provided gold and generated answers without introducing external information.", "main_total_tokens": 2866, "main_prompt_tokens": 2850, "main_completion_tokens": 16, "judge_total_tokens": 460, "judge_prompt_tokens": 380, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 300.717830657959, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific numerical rating (8.466), but the generated answer says 'I don't know,' which does not match the correct value. This makes the answer incorrect. However, it is faithful because the model admits lack of knowledge instead of fabricating a rating, so it does not contradict the given information or hallucinate details.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 498, "judge_prompt_tokens": 398, "judge_completion_tokens": 100}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med", "latency_ms": 1104.2184829711914, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct name of the TV show. It is fully correct and faithful.", "main_total_tokens": 2516, "main_prompt_tokens": 2512, "main_completion_tokens": 4, "judge_total_tokens": 451, "judge_prompt_tokens": 394, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina", "latency_ms": 903.3288955688477, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (Argentina) exactly, so it is both correct and faithful to the reference.", "main_total_tokens": 3162, "main_prompt_tokens": 3160, "main_completion_tokens": 2, "judge_total_tokens": 446, "judge_prompt_tokens": 392, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The apprentice falls into a trap designed by Jiang Jie [Doc 135231].", "latency_ms": 1342.6861763000488, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the apprentice falls into a trap designed by Jiang Jie, which matches the gold answer exactly. It is fully accurate and directly supported by the provided information.", "main_total_tokens": 2707, "main_prompt_tokens": 2690, "main_completion_tokens": 17, "judge_total_tokens": 467, "judge_prompt_tokens": 400, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "3", "latency_ms": 718.0783748626709, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.7095272044910244, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.7095272044910244, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (3) does not match the gold answer (4), so it is incorrect. There is no indication the model contradicted its own reasoning, only that its final numeric answer is wrong, so faithfulness remains high.", "main_total_tokens": 3443, "main_prompt_tokens": 3438, "main_completion_tokens": 5, "judge_total_tokens": 463, "judge_prompt_tokens": 385, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 918.7276363372803, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.75, "justification": "The generated answer is effectively correct: there are no known movies featuring both Steven Seagal and Bruce Lee, so saying 'I cannot find any movies or shows matching all of those criteria' conveys the same substantive information as the gold answer. However, it is less specific than the gold, adding 'or shows' and 'all of those criteria' instead of clearly restating the exact entities asked about, so it is slightly less faithful to the wording and focus of the gold response.", "main_total_tokens": 2218, "main_prompt_tokens": 2217, "main_completion_tokens": 1, "judge_total_tokens": 535, "judge_prompt_tokens": 406, "judge_completion_tokens": 129}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3", "latency_ms": 656.2795639038086, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated rating 6.3 exactly matches the gold rating 6.3, so the answer is both correct and fully faithful to the provided reference.", "main_total_tokens": 2157, "main_prompt_tokens": 2154, "main_completion_tokens": 3, "judge_total_tokens": 449, "judge_prompt_tokens": 387, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "Drift-style racing [Doc 42750].", "latency_ms": 2077.1849155426025, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The gold answer is 'Drift' and the generated answer is 'Drift-style racing', which is semantically correct. However, the generated answer includes an unexplained reference '[Doc 42750]', which indicates reliance on or citation of a source not present in the user prompt, violating faithfulness under the given evaluation criteria.", "main_total_tokens": 2873, "main_prompt_tokens": 2850, "main_completion_tokens": 23, "judge_total_tokens": 484, "judge_prompt_tokens": 387, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer", "latency_ms": 1274.3873596191406, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct title of the movie about Jillian Lauren and Samuel Little. It is both factually correct and fully consistent with the provided information.", "main_total_tokens": 2723, "main_prompt_tokens": 2716, "main_completion_tokens": 7, "judge_total_tokens": 482, "judge_prompt_tokens": 412, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838", "latency_ms": 640.0058269500732, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838), so it is both correct and faithful.", "main_total_tokens": 2322, "main_prompt_tokens": 2319, "main_completion_tokens": 3, "judge_total_tokens": 442, "judge_prompt_tokens": 389, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense", "latency_ms": 1219.8946475982666, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 1999 psychological thriller as 'The Sixth Sense' and remains fully faithful to the question.", "main_total_tokens": 2643, "main_prompt_tokens": 2639, "main_completion_tokens": 4, "judge_total_tokens": 456, "judge_prompt_tokens": 394, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002", "latency_ms": 988.2051944732666, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (2002), which is the correct release year of 'The Transporter' film. The response is both correct and fully consistent with the provided reference.", "main_total_tokens": 2929, "main_prompt_tokens": 2926, "main_completion_tokens": 3, "judge_total_tokens": 453, "judge_prompt_tokens": 383, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled", "latency_ms": 1119.8053359985352, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the title of the 2015 reboot of 'The Transporter' as 'The Transporter Refueled'. It is both correct and faithful to the reference.", "main_total_tokens": 2978, "main_prompt_tokens": 2971, "main_completion_tokens": 7, "judge_total_tokens": 472, "judge_prompt_tokens": 397, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015", "latency_ms": 981.6319942474365, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2015) exactly and is directly supported by the provided reference.", "main_total_tokens": 2828, "main_prompt_tokens": 2825, "main_completion_tokens": 3, "judge_total_tokens": 439, "judge_prompt_tokens": 387, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "I don't know.", "latency_ms": 949.6803283691406, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no such movie description can be found, effectively expressing lack of knowledge of such a film. The generated answer says 'I don't know,' which is consistent with the gold. Both are factually acceptable given no known matching film and the evaluation frame, and the generated response accurately reflects the gold answer's stance.", "main_total_tokens": 573, "main_prompt_tokens": 572, "main_completion_tokens": 1, "judge_total_tokens": 491, "judge_prompt_tokens": 394, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Garden of Eden", "latency_ms": 1103.865385055542, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the title of a movie that alludes to a manuscript written by Ernest Hemingway. The correct answer is 'Havana Darkness', which matches the gold reference. The generated answer 'Garden of Eden' is incorrect for this question and does not align with the gold answer, so both correctness and faithfulness are 0.", "main_total_tokens": 2579, "main_prompt_tokens": 2575, "main_completion_tokens": 4, "judge_total_tokens": 489, "judge_prompt_tokens": 391, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 2054.9960136413574, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Doll & Em' and correctly identifies the TV show, so it is both correct and faithful.", "main_total_tokens": 2701, "main_prompt_tokens": 2667, "main_completion_tokens": 34, "judge_total_tokens": 452, "judge_prompt_tokens": 392, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in the movie \"Hallam Foe\" [Doc 39389].", "latency_ms": 1334.5906734466553, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Hallam Foe,' which matches the gold answer. It is fully supported by the given information and does not introduce any contradictions or extraneous claims.", "main_total_tokens": 2887, "main_prompt_tokens": 2851, "main_completion_tokens": 36, "judge_total_tokens": 504, "judge_prompt_tokens": 434, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "His last name sounds at first like he’s a friend. [Doc 134352]", "latency_ms": 2085.8325958251953, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks whether his last name sounds at first like he’s a friend or a foe. The gold answer is 'A foe', but the generated answer says it sounds like he’s a friend, which is the opposite. This is incorrect. The generated answer is, however, directly based on and consistent with the given question, so it is faithful to the prompt even though it is wrong relative to the gold.", "main_total_tokens": 2874, "main_prompt_tokens": 2837, "main_completion_tokens": 37, "judge_total_tokens": 513, "judge_prompt_tokens": 404, "judge_completion_tokens": 109}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18", "latency_ms": 941.4160251617432, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and is faithful to the provided information.", "main_total_tokens": 2574, "main_prompt_tokens": 2572, "main_completion_tokens": 2, "judge_total_tokens": 436, "judge_prompt_tokens": 389, "judge_completion_tokens": 47}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam is on the autism spectrum [Doc 124583].", "latency_ms": 1237.8389835357666, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which exactly matches the gold answer. It is both factually correct and faithful to the provided information.", "main_total_tokens": 2399, "main_prompt_tokens": 2386, "main_completion_tokens": 13, "judge_total_tokens": 463, "judge_prompt_tokens": 399, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix", "latency_ms": 1679.70609664917, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Chamonix'), so it is correct and faithful to the provided reference.", "main_total_tokens": 3073, "main_prompt_tokens": 3069, "main_completion_tokens": 4, "judge_total_tokens": 455, "judge_prompt_tokens": 400, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "in this japanese language movie, Hazuki clips her nails every time it rains. [Doc 100481]", "latency_ms": 1236.3612651824951, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly fills in the blank with 'rains,' matching the gold answer exactly and preserving the original context.", "main_total_tokens": 2992, "main_prompt_tokens": 2969, "main_completion_tokens": 23, "judge_total_tokens": 468, "judge_prompt_tokens": 413, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 2239.2468452453613, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the place of which Tabaluga is crown-prince. The gold answer is 'Greenland'. The generated answer states 'Tabaluga is the last of the dragons and the crown-prince of Greenland', which exactly matches the gold. Therefore it is fully correct and faithful to the given information.", "main_total_tokens": 2908, "main_prompt_tokens": 2885, "main_completion_tokens": 23, "judge_total_tokens": 511, "judge_prompt_tokens": 416, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon", "latency_ms": 1252.0005702972412, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('One Afternoon') and is fully supported by the given information.", "main_total_tokens": 2477, "main_prompt_tokens": 2474, "main_completion_tokens": 3, "judge_total_tokens": 458, "judge_prompt_tokens": 406, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" [Doc 93260].", "latency_ms": 1248.8648891448975, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title of the low-rated Italian language movie where a Holocaust survivor cares for a 12-year-old street kid who robbed her. Both the gold answer and the generated answer identify the film as 'The Life Ahead.' This is correct, and the generated answer matches the gold exactly in content, with only minor rephrasing and an added citation marker. There are no contradictions or hallucinations relative to the provided gold answer.", "main_total_tokens": 2695, "main_prompt_tokens": 2657, "main_completion_tokens": 38, "judge_total_tokens": 554, "judge_prompt_tokens": 435, "judge_completion_tokens": 119}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary “McLaren” is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team, showing how a man of humble beginnings could take on the elite of motor racing and win [Doc 73859].", "latency_ms": 2710.7574939727783, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary, matching the gold answer, so it is fully correct. However, it adds specific explanatory details and a citation ([Doc 73859]) that are not present in the provided gold answer or question context, so those details are not strictly grounded in the given information, reducing faithfulness under the defined criteria.", "main_total_tokens": 2919, "main_prompt_tokens": 2867, "main_completion_tokens": 52, "judge_total_tokens": 544, "judge_prompt_tokens": 435, "judge_completion_tokens": 109}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren", "latency_ms": 1838.615894317627, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 0.5, "faithfulness_score": 1, "justification": "The generated answer captures the core entity (McLaren) but omits the full team name, which in the gold answer is 'The McLaren Motor Racing team'. It is partially correct but not fully matching the expected name. The answer is faithful to the prompt and does not introduce any unsupported information.", "main_total_tokens": 2839, "main_prompt_tokens": 2824, "main_completion_tokens": 15, "judge_total_tokens": 478, "judge_prompt_tokens": 385, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "United Kingdom", "latency_ms": 1351.4463901519775, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'United Kingdom' while the correct (gold) answer is 'New Zealand', so it is factually incorrect. However, it directly answers the question with a specific country and does not add extraneous or hallucinated context, so it is fully faithful to the prompt structure.", "main_total_tokens": 2546, "main_prompt_tokens": 2535, "main_completion_tokens": 11, "judge_total_tokens": 468, "judge_prompt_tokens": 378, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "A Good Day to Die Hard", "latency_ms": 1119.574785232544, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect. In 'A Good Day to Die Hard' (2013), John McClane goes to Russia; he does not blow up the Great Wall of China. The gold answer correctly indicates that no such movie is in the database, so the model output is not accurate relative to known facts, but it is faithful to the user query's topic.", "main_total_tokens": 3040, "main_prompt_tokens": 3033, "main_completion_tokens": 7, "judge_total_tokens": 510, "judge_prompt_tokens": 404, "judge_completion_tokens": 106}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit", "latency_ms": 1977.3643016815186, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2002 Japanese horror movie as 'Tomie: Forbidden Fruit'. The reasoning is fully aligned with the provided information.", "main_total_tokens": 2956, "main_prompt_tokens": 2950, "main_completion_tokens": 6, "judge_total_tokens": 489, "judge_prompt_tokens": 422, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 1797.2619533538818, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, which matches the gold answer. It is fully consistent with the provided information and does not introduce unsupported claims.", "main_total_tokens": 3284, "main_prompt_tokens": 3265, "main_completion_tokens": 19, "judge_total_tokens": 483, "judge_prompt_tokens": 412, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King and Tala search for a legendary relic known as The Book of Souls, which will allow them to put an end to an evil warlord [Doc 82582].", "latency_ms": 1945.2733993530273, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies that they search for The Book of Souls, matching the gold answer. The added detail about it being a legendary relic that can end an evil warlord is broadly accurate to the film, but the bracketed citation-like tag [Doc 82582] is fabricated and not grounded in the provided context, slightly reducing faithfulness.", "main_total_tokens": 3180, "main_prompt_tokens": 3142, "main_completion_tokens": 38, "judge_total_tokens": 534, "judge_prompt_tokens": 431, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 502.0098686218262, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, which is equivalent in content to the generated answer saying 'I don't know.' Both convey lack of knowledge about the release year, so the generated response is correct and faithful to the gold.", "main_total_tokens": 527, "main_prompt_tokens": 526, "main_completion_tokens": 1, "judge_total_tokens": 476, "judge_prompt_tokens": 395, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 990.5712604522705, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the database contains no information about the movie. The generated answer explicitly acknowledges not knowing, which matches the gold answer and does not introduce unsupported facts. Therefore it is both correct and faithful.", "main_total_tokens": 589, "main_prompt_tokens": 588, "main_completion_tokens": 1, "judge_total_tokens": 465, "judge_prompt_tokens": 393, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 617.8855895996094, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database, which is equivalent in substance to not knowing the answer. The generated answer, 'I don't know,' is therefore correct relative to the gold and faithful to it.", "main_total_tokens": 550, "main_prompt_tokens": 549, "main_completion_tokens": 1, "judge_total_tokens": 470, "judge_prompt_tokens": 395, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1167.5338745117188, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states lack of knowledge, which matches the gold response indicating the movie is not in the database. It does not introduce any incorrect information and is fully consistent with the gold.", "main_total_tokens": 536, "main_prompt_tokens": 535, "main_completion_tokens": 1, "judge_total_tokens": 464, "judge_prompt_tokens": 395, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"Python's Revenge 2\" was released is not specified in the provided context.", "latency_ms": 1605.051040649414, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer correctly states the movie is not in the database. The generated answer incorrectly references a 'provided context' that does not exist and implies the answer might be knowable from context, which is unfaithful to the original task setup and not the expected behavior. It should instead acknowledge that the movie is not in the database.", "main_total_tokens": 2776, "main_prompt_tokens": 2756, "main_completion_tokens": 20, "judge_total_tokens": 508, "judge_prompt_tokens": 410, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 630.3849220275879, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is not in the database, which is effectively equivalent to not knowing the answer. The generated answer 'I don't know.' conveys the same lack of knowledge, so it is correct relative to the gold and fully faithful.", "main_total_tokens": 526, "main_prompt_tokens": 525, "main_completion_tokens": 1, "judge_total_tokens": 473, "judge_prompt_tokens": 393, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 417.050838470459, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers convey lack of knowledge/coverage, which is appropriate given the question and unspecified movie. The generated answer is correct and faithful to the gold response in meaning.", "main_total_tokens": 580, "main_prompt_tokens": 579, "main_completion_tokens": 1, "judge_total_tokens": 464, "judge_prompt_tokens": 396, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was directed by Adrian Sitaru [Doc 46069].", "latency_ms": 1490.7898902893066, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the director information is not provided in the database, so the system should reply that it cannot answer. The generated answer instead confidently names a director (Adrian Sitaru), which directly contradicts the gold and is not supported by the given database. Therefore it is both incorrect relative to the task and unfaithful to the provided information.", "main_total_tokens": 3180, "main_prompt_tokens": 3155, "main_completion_tokens": 25, "judge_total_tokens": 520, "judge_prompt_tokens": 421, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 549.2143630981445, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not available in the database, and the generated answer says, 'I don't know.' Both convey that the director cannot be provided. This is correct given the gold and fully faithful to it.", "main_total_tokens": 553, "main_prompt_tokens": 552, "main_completion_tokens": 1, "judge_total_tokens": 477, "judge_prompt_tokens": 399, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The movie \"Journey into Gaza\" was directed by an unspecified individual in the provided context.", "latency_ms": 1542.4118041992188, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer correctly states that the director information is not available in the database. The generated answer incorrectly asserts that it was directed by an unspecified individual, which is not supported by the database and implies knowledge that is not present. This is both factually incorrect relative to the task and unfaithful to the provided context.", "main_total_tokens": 2892, "main_prompt_tokens": 2873, "main_completion_tokens": 19, "judge_total_tokens": 506, "judge_prompt_tokens": 411, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The movie \"The Last Night at Tremore Beach\" is directed by an unspecified individual in the provided context.", "latency_ms": 1336.1546993255615, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer correctly states that the director information is not available, while the generated answer invents that it is directed by an unspecified individual, implying some knowledge where there is none. This is not faithful to the provided database context and is factually incorrect relative to the task, which requires acknowledging the lack of information.", "main_total_tokens": 2495, "main_prompt_tokens": 2472, "main_completion_tokens": 23, "judge_total_tokens": 513, "judge_prompt_tokens": 419, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 1342.1189785003662, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Rise of the Mummy') and is faithful to the given information.", "main_total_tokens": 2650, "main_prompt_tokens": 2644, "main_completion_tokens": 6, "judge_total_tokens": 450, "judge_prompt_tokens": 396, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 1056.821346282959, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the correct movie 'Stranger in My Bed'. It is both correct and fully faithful to the provided information.", "main_total_tokens": 2733, "main_prompt_tokens": 2727, "main_completion_tokens": 6, "judge_total_tokens": 463, "judge_prompt_tokens": 400, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 1251.3117790222168, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is actually describing the premise of a TV series, not a movie, but both the gold and generated answers match each other and correctly identify the work as 'Game of Silence.' The generated answer is therefore both correct with respect to the task and faithful to the gold response.", "main_total_tokens": 2363, "main_prompt_tokens": 2359, "main_completion_tokens": 4, "judge_total_tokens": 487, "judge_prompt_tokens": 400, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 2061.2759590148926, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'The Craigslist Killer'. It is both correct and fully faithful to the provided information.", "main_total_tokens": 2635, "main_prompt_tokens": 2616, "main_completion_tokens": 19, "judge_total_tokens": 457, "judge_prompt_tokens": 395, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 1708.0531120300293, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer, 'Wood Job!', exactly matches the gold answer and correctly identifies the movie from the description. It is fully correct and faithful to the provided gold output.", "main_total_tokens": 3020, "main_prompt_tokens": 3016, "main_completion_tokens": 4, "judge_total_tokens": 468, "judge_prompt_tokens": 403, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 2440.633773803711, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase' and is fully consistent with the provided information.", "main_total_tokens": 3106, "main_prompt_tokens": 3089, "main_completion_tokens": 17, "judge_total_tokens": 481, "judge_prompt_tokens": 417, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond", "latency_ms": 994.117259979248, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title; both the gold and generated answer give 'Beyond'. This matches exactly and is faithful to the prompt.", "main_total_tokens": 2683, "main_prompt_tokens": 2681, "main_completion_tokens": 2, "judge_total_tokens": 468, "judge_prompt_tokens": 409, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9", "latency_ms": 1187.6208782196045, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('6ixtynin9'), correctly identifying the movie described. It is fully correct and faithful to the reference.", "main_total_tokens": 2597, "main_prompt_tokens": 2590, "main_completion_tokens": 7, "judge_total_tokens": 472, "judge_prompt_tokens": 409, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread", "latency_ms": 976.7372608184814, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Club Dread' exactly matches the gold answer and correctly identifies the movie based on the description. It is fully correct and faithful to the provided information.", "main_total_tokens": 2827, "main_prompt_tokens": 2823, "main_completion_tokens": 4, "judge_total_tokens": 492, "judge_prompt_tokens": 427, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet", "latency_ms": 797.3992824554443, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the movie as \"Toilet.\" The response is fully faithful to the provided information.", "main_total_tokens": 3354, "main_prompt_tokens": 3351, "main_completion_tokens": 3, "judge_total_tokens": 474, "judge_prompt_tokens": 415, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel", "latency_ms": 835.4475498199463, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Electric Angel'), so it is correct. It is also fully faithful to the provided information, with no additional or contradictory content.", "main_total_tokens": 2924, "main_prompt_tokens": 2921, "main_completion_tokens": 3, "judge_total_tokens": 483, "judge_prompt_tokens": 418, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "2.167", "latency_ms": 1040.9071445465088, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is a number (2.167) instead of the movie title 'Mega Ape'. It does not correctly answer the question and is not faithful to the gold answer.", "main_total_tokens": 2677, "main_prompt_tokens": 2673, "main_completion_tokens": 4, "judge_total_tokens": 452, "judge_prompt_tokens": 385, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 266.232967376709, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific factual answer (the highest rated TV show from 2010, which the gold specifies as 'Tea House'). The generated answer says 'I don't know,' which does not provide the required correct result, so correctness is 0. However, the model does not hallucinate any incorrect facts and accurately reflects its lack of knowledge, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 495, "judge_prompt_tokens": 386, "judge_completion_tokens": 109}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 412.9304885864258, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a particular movie, while the generated answer says 'I don't know.' This is incorrect because it fails to provide the correct movie. However, it is faithful, as it does not hallucinate or contradict the lack of knowledge; it simply admits not knowing.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 395, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "es", "latency_ms": 850.059986114502, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980. The gold answer is 'English', but the generated answer is 'es', which corresponds to Spanish. This is factually incorrect relative to the gold. However, the answer is at least attempting to provide a language code, which is consistent with the type of information requested, so it is faithful to the query but incorrect in content.", "main_total_tokens": 2644, "main_prompt_tokens": 2642, "main_completion_tokens": 2, "judge_total_tokens": 497, "judge_prompt_tokens": 384, "judge_completion_tokens": 113}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was The Sopranos, with a popularity score of 59.4284 [Doc 115146].", "latency_ms": 1490.2114868164062, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold label indicates that 'Law & Order: Special Victims Unit' was the TV show with the highest popularity in 1999, whereas the model responded with 'The Sopranos.' Additionally, the answer cites a specific popularity score and document reference that are not supported by the provided gold information, so it is not faithful to the given data.", "main_total_tokens": 3043, "main_prompt_tokens": 3009, "main_completion_tokens": 34, "judge_total_tokens": 531, "judge_prompt_tokens": 423, "judge_completion_tokens": 108}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2", "latency_ms": 941.3497447967529, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2, so it is both correct and faithful.", "main_total_tokens": 2349, "main_prompt_tokens": 2346, "main_completion_tokens": 3, "judge_total_tokens": 439, "judge_prompt_tokens": 386, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "1999", "latency_ms": 1182.8596591949463, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1999) does not match the gold answer (2019), so it is factually incorrect. However, it directly addresses the question without introducing unrelated or fabricated details, so it is fully faithful to the prompt.", "main_total_tokens": 2716, "main_prompt_tokens": 2713, "main_completion_tokens": 3, "judge_total_tokens": 465, "judge_prompt_tokens": 387, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 872.4241256713867, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 279 votes, but the generated answer was 3, which is incorrect. The question is factual and the model clearly did not match the gold value, so correctness is 0. The model did at least attempt to answer the question directly rather than changing it, so faithfulness is 1.", "main_total_tokens": 2193, "main_prompt_tokens": 2188, "main_completion_tokens": 5, "judge_total_tokens": 478, "judge_prompt_tokens": 383, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 718.0302143096924, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 12 votes, but the generated response states it cannot find any matching movies or shows instead of providing the number. This is factually incorrect and does not reflect the provided gold answer, so both correctness and faithfulness are 0.", "main_total_tokens": 2229, "main_prompt_tokens": 2228, "main_completion_tokens": 1, "judge_total_tokens": 474, "judge_prompt_tokens": 392, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "2009", "latency_ms": 1776.3025760650635, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2009) exactly and is fully supported by the given information.", "main_total_tokens": 2851, "main_prompt_tokens": 2848, "main_completion_tokens": 3, "judge_total_tokens": 442, "judge_prompt_tokens": 390, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_openai-large_gpt-4o-mini", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en", "latency_ms": 1289.22438621521, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer and the generated answer both state that the original language of 'The King's Speech' is English ('en'), which is correct and fully aligned with the reference.", "main_total_tokens": 3123, "main_prompt_tokens": 3121, "main_completion_tokens": 2, "judge_total_tokens": 449, "judge_prompt_tokens": 384, "judge_completion_tokens": 65}
