{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "*Wall Street* was released in 1987 [Doc 6993].", "latency_ms": 4056.2307834625244, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, which matches the gold answer and does not introduce unsupported information beyond a harmless document reference.", "main_total_tokens": 3056, "main_prompt_tokens": 3036, "main_completion_tokens": 20, "judge_total_tokens": 457, "judge_prompt_tokens": 394, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie where classmates are forced to take part in a survival game using their cellphones is *Brain Fluid Explosion Girl* [Doc 69929].", "latency_ms": 772.6011276245117, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Brain Fluid Explosion Girl,' matching the gold answer exactly. It is fully faithful to the provided information and does not introduce any unsupported details.", "main_total_tokens": 2775, "main_prompt_tokens": 2742, "main_completion_tokens": 33, "judge_total_tokens": 485, "judge_prompt_tokens": 418, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show in which six friends are transported into the Dungeons & Dragons realm and must try to find a way home with the help of their guide 'Dungeon Master' is *Dungeons & Dragons* [Doc 112849].", "latency_ms": 890.8724784851074, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the show as 'Dungeons & Dragons' and accurately reflects the information implied by the question and gold answer without adding unsupported claims.", "main_total_tokens": 2831, "main_prompt_tokens": 2779, "main_completion_tokens": 52, "judge_total_tokens": 499, "judge_prompt_tokens": 437, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is titled *Nacht der Wölfe* [Doc 2590].", "latency_ms": 855.5374145507812, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe,' which matches the gold answer. It directly and faithfully restates the information without adding unsupported details.", "main_total_tokens": 2822, "main_prompt_tokens": 2780, "main_completion_tokens": 42, "judge_total_tokens": 504, "judge_prompt_tokens": 438, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of *Picnic at Hanging Rock* (TV, 2018) take place on Valentine's Day, 1900 [Doc 126191].", "latency_ms": 776.1564254760742, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly states that the events take place in 1900, matching the gold answer. However, it specifically attributes this to the 2018 TV adaptation and cites a document, which is external to the provided gold statement. Since the explanation relies on that external document rather than strictly on the given gold information, it is not fully faithful under the constraints.", "main_total_tokens": 3087, "main_prompt_tokens": 3045, "main_completion_tokens": 42, "judge_total_tokens": 521, "judge_prompt_tokens": 417, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie about a young violinist named June, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man destined for her, is titled *June* [Doc 32483].", "latency_ms": 935.8890056610107, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title of the 2004 movie; the correct answer is 'June'. The generated answer correctly identifies the movie as 'June' and restates the description without adding contradictions or fabrications. It is both correct and faithful to the gold answer.", "main_total_tokens": 3161, "main_prompt_tokens": 3099, "main_completion_tokens": 62, "judge_total_tokens": 576, "judge_prompt_tokens": 491, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "The highest rated Russian-language movie released in 1980 is *Moscow Does Not Believe in Tears*, with a rating of 7.6 [Doc 61].", "latency_ms": 2373.663902282715, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer names 'Moscow Does Not Believe in Tears' as the highest rated Russian-language movie of 1980, but the gold answer indicates that the correct film is 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger'. Thus the content is factually incorrect. It is also unfaithful to the provided gold answer, directly contradicting it.", "main_total_tokens": 3606, "main_prompt_tokens": 3200, "main_completion_tokens": 406, "judge_total_tokens": 535, "judge_prompt_tokens": 429, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest Russian-language movie rating in 1999 is 7.117, for the film *The Rifleman of the Voroshilov Regiment* [Doc 22123].", "latency_ms": 3727.1037101745605, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the correct value is 7.6, while the generated answer claims 7.117 and specifies a particular film. This is numerically incorrect. Additionally, there is no support from the provided context here for the specific value 7.117 or the identification of that film as having the highest rating, so the answer is not faithful to the given ground truth.", "main_total_tokens": 3773, "main_prompt_tokens": 3123, "main_completion_tokens": 650, "judge_total_tokens": 530, "judge_prompt_tokens": 422, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "Two movies released in 1987 have a rating over 7 and are in Chinese (cn):\n\n1.  *A Chinese Ghost Story*, with a rating of 7.3 [", "latency_ms": 4555.41467666626, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there are 4 movies, but the generated answer claims there are 2, which is factually incorrect. It also only partially answers the question without clear grounding in the provided gold data, so it is not faithful to the expected answer.", "main_total_tokens": 4005, "main_prompt_tokens": 2983, "main_completion_tokens": 1022, "judge_total_tokens": 513, "judge_prompt_tokens": 430, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "Based strictly on the provided context, zero TV shows were released in 1987, have a rating over 7, and are in Chinese.", "latency_ms": 4023.470878601074, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 2 such TV shows, but the generated answer claims there are zero, so it is incorrect with respect to the gold. However, the generated answer explicitly conditions on the provided context and does not appear to invent specific shows or numbers beyond stating that none are mentioned, so it is faithful to the (unstated) context assumption given in the prompt.", "main_total_tokens": 4038, "main_prompt_tokens": 3266, "main_completion_tokens": 772, "judge_total_tokens": 529, "judge_prompt_tokens": 422, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "Based on the context provided, there are no French language TV shows listed as released in 1980.\n\nThe TV shows released in 1980 are:\n*   *Therese Raquin* (Language: en) [Doc 112644]\n*   *Luzie, der Schrecken der Straße* (Language: de) [Doc 112657]\n*   *Pé de Vento* (Language: pt) [Doc 112590]", "latency_ms": 1158.6041450500488, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates that the only French language TV show released in 1980 is 'Fantômas'. The generated answer instead claims there are no French language TV shows released in 1980 and lists only non-French shows. This directly contradicts the gold answer, making it factually incorrect. It also contradicts the implied underlying data (which includes 'Fantômas'), so it is unfaithful to the source context.", "main_total_tokens": 3275, "main_prompt_tokens": 3162, "main_completion_tokens": 113, "judge_total_tokens": 598, "judge_prompt_tokens": 479, "judge_completion_tokens": 119}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is *Kill Cruise* [Doc 11205].", "latency_ms": 751.3735294342041, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' which matches the gold answer exactly. It does not introduce any contradictory or extraneous information beyond a harmless citation marker, so it is both correct and faithful to the reference.", "main_total_tokens": 2988, "main_prompt_tokens": 2961, "main_completion_tokens": 27, "judge_total_tokens": 486, "judge_prompt_tokens": 409, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The movie about the TV series in which Fred Rogers speaks directly to young children about some of life's most important issues is titled *Won't You Be My Neighbor?* [Doc 84838].", "latency_ms": 796.3411808013916, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'Won't You Be My Neighbor?' and matches the gold answer exactly. It does not introduce contradictions or unsupported claims and remains faithful to the provided information.", "main_total_tokens": 3238, "main_prompt_tokens": 3193, "main_completion_tokens": 45, "judge_total_tokens": 511, "judge_prompt_tokens": 440, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "", "latency_ms": 4702.0158767700195, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is missing/empty, so it does not provide the correct year (2018) and cannot be considered faithful to the gold answer.", "main_total_tokens": 4377, "main_prompt_tokens": 3354, "main_completion_tokens": 1023, "judge_total_tokens": 439, "judge_prompt_tokens": 378, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is *Astro Boy* [Doc 112591].", "latency_ms": 825.3560066223145, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy,' which matches the gold answer exactly. It is fully supported by the cited document, so both correctness and faithfulness are complete.", "main_total_tokens": 3085, "main_prompt_tokens": 3050, "main_completion_tokens": 35, "judge_total_tokens": 486, "judge_prompt_tokens": 416, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room,\" released in 2015, the woman and her young son were held captive for 7 years [Doc 67513].", "latency_ms": 745.715856552124, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer. It is fully supported by the provided information and does not introduce contradictions or unsupported details.", "main_total_tokens": 2775, "main_prompt_tokens": 2739, "main_completion_tokens": 36, "judge_total_tokens": 498, "judge_prompt_tokens": 425, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The context states that six armed men broke into the Brink's-Mat security depot on November 26, 1983, and stumbled across gold bullion worth £26 million [Doc 133469].\n\nThe context does not specify the total amount of gold the depot was holding before the break-in, only the value of the gold bullion that was discovered [Doc 133469].", "latency_ms": 959.5038890838623, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the amount of gold Brink's-Mat was holding before the break-in. The provided gold value is £26m, which corresponds to the gold bullion worth £26 million mentioned in the context. The generated answer correctly notes that the context only specifies the value of the bullion discovered (worth £26 million), not the total amount of gold the depot was holding before the break-in. This is both factually correct and faithful to the given context.", "main_total_tokens": 3208, "main_prompt_tokens": 3118, "main_completion_tokens": 90, "judge_total_tokens": 594, "judge_prompt_tokens": 470, "judge_completion_tokens": 124}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "The popularity score of the most popular English-speaking TV show released in 1997 is 10.3614, which belongs to the show *Recess* [Doc 114720].", "latency_ms": 3190.171480178833, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer gives a specific popularity score (10.3614) and TV show (Recess) that do not match the gold answer (80.5772), so it is factually incorrect. It also introduces details (the specific show and score) that contradict the reference gold value, so it is not faithful to the provided information.", "main_total_tokens": 3891, "main_prompt_tokens": 3260, "main_completion_tokens": 631, "judge_total_tokens": 525, "judge_prompt_tokens": 430, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "Based on the context provided, there were no Russian language TV shows released in 1997. The only Russian language entry is a movie from 2016 [Doc 72036].", "latency_ms": 894.1047191619873, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 Russian language TV show released in 1997, but the generated answer claims there were none. This is factually incorrect relative to the gold, so correctness is 0. The generated answer is consistent with the context it references (stating only a 2016 movie exists in Russian in that context), so it is faithful to the provided context, giving faithfulness a score of 1.", "main_total_tokens": 3390, "main_prompt_tokens": 3346, "main_completion_tokens": 44, "judge_total_tokens": 538, "judge_prompt_tokens": 420, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "The TV show in which Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight is *Angel Eyes* [Doc 121928].", "latency_ms": 796.0324287414551, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Angel Eyes') and correctly identifies the TV show. It remains faithful to the provided information without adding contradictions, only appending a citation-style note that does not change the core answer.", "main_total_tokens": 3520, "main_prompt_tokens": 3481, "main_completion_tokens": 39, "judge_total_tokens": 507, "judge_prompt_tokens": 431, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "Please specify which movie you are asking about.", "latency_ms": 755.9070587158203, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer directly states the movie language is Korean. The generated answer instead asks the user to specify which movie, failing to answer the question and thus is incorrect. It is, however, faithful to the user question in that it does not hallucinate a language or movie; it simply requests clarification.", "main_total_tokens": 3197, "main_prompt_tokens": 3188, "main_completion_tokens": 9, "judge_total_tokens": 476, "judge_prompt_tokens": 385, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "", "latency_ms": 4517.30489730835, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The model output is empty, but the implied answer compared to the gold is 2, which matches the gold. No reasoning is present to contradict the question, so it is correct and faithful to the gold answer.", "main_total_tokens": 4265, "main_prompt_tokens": 3242, "main_completion_tokens": 1023, "judge_total_tokens": 467, "judge_prompt_tokens": 393, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Based strictly on the context provided, the best-rated movie released in 1979 cannot be determined, as no movies from that year are listed.", "latency_ms": 2806.1485290527344, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Given the provided gold answer that the database contains no movies from 1979, the generated response correctly states that the best-rated movie from 1979 cannot be determined from the context. It is fully consistent with and faithful to the given context and does not introduce unsupported information.", "main_total_tokens": 3762, "main_prompt_tokens": 3170, "main_completion_tokens": 592, "judge_total_tokens": 510, "judge_prompt_tokens": 424, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "Based strictly on the context provided, the movie *Back to the Future Part IV* is not mentioned, and its release year is not available [Doc 10621, Doc 4834, Doc 113788, Doc 9405]. The context refers repeatedly to the \"Back to the Future trilogy\" [Doc 113788, Doc 28792, Doc 68688, Doc 103759].", "latency_ms": 3026.0205268859863, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The gold answer correctly states that Back to the Future Part IV does not exist in the database, effectively conveying that there is no such film/release year. The generated answer, however, talks about the movie not being mentioned in the provided context and that its release year is not available from that context. This is not an appropriate direct answer to the user’s question; the correct response should say that such a movie does not exist (or is not in the database). Therefore, in terms of answering the question correctly, the generated answer is incorrect. In terms of faithfulness to the given context and citations, it appears to accurately describe that only the trilogy is mentioned and that Part IV is not found in those documents, so it is mostly faithful, though it partially sidesteps the core issue that the film itself does not exist rather than merely not appearing in the context.", "main_total_tokens": 4586, "main_prompt_tokens": 4041, "main_completion_tokens": 545, "judge_total_tokens": 689, "judge_prompt_tokens": 483, "judge_completion_tokens": 206}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "Answer based strictly on the context:\n\nNo movies about the nuclear Chernobyl disaster released in 1980 were found in the provided context.", "latency_ms": 3466.8543338775635, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that no movies about the Chernobyl disaster released in 1980 were found, which aligns with the gold answer. It remains faithful to the given context and does not introduce unsupported information.", "main_total_tokens": 3712, "main_prompt_tokens": 3098, "main_completion_tokens": 614, "judge_total_tokens": 508, "judge_prompt_tokens": 434, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are three titles that contain \"Astro Boy\":\n1. *Astro Boy* (TV, 1980) [Doc 112591]\n2. *Astro Boy* (Movie, 2009) [Doc 44947]\n3. *Astro Boy* (TV, 2003) [Doc 116265]", "latency_ms": 1060.9443187713623, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.8318724637288826, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.8318724637288826, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 4 titles, but the generated answer lists only 3, so the count is incorrect. However, the generated response is internally consistent and does not contradict itself, so it is faithful to its own content and reasoning.", "main_total_tokens": 3208, "main_prompt_tokens": 3117, "main_completion_tokens": 91, "judge_total_tokens": 533, "judge_prompt_tokens": 457, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "Based strictly on the context provided, there are no movies listed with a release year of 2025. Therefore, the number of movies in 2025 that allude to the Holocaust cannot be determined.", "latency_ms": 4016.411304473877, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states the correct number is 2, but the generated answer claims it cannot be determined, which is factually incorrect relative to the gold. However, given only the user question and no additional context, the assistant consistently reasons from the (stated) context that no 2025 movies are listed, so its explanation is faithful to its own (limited) context, not hallucinating extra data.", "main_total_tokens": 3940, "main_prompt_tokens": 3109, "main_completion_tokens": 831, "judge_total_tokens": 538, "judge_prompt_tokens": 425, "judge_completion_tokens": 113}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "There were two TV shows released in the 1990s about Super Mario Bros.:\n\n1.  *The Adventures of Super Mario Bros. 3* (1990) [Doc 113661]\n2.  *Super Mario World* (1991) [Doc 113814]", "latency_ms": 3806.671142578125, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says 1 show, but the generated answer lists 2 shows from the 1990s, making it factually incorrect relative to the gold. However, the generated answer is internally consistent and does not contradict itself or its own stated evidence, so it is faithful to its own reasoning.", "main_total_tokens": 3932, "main_prompt_tokens": 3117, "main_completion_tokens": 815, "judge_total_tokens": 538, "judge_prompt_tokens": 446, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "Based strictly on the provided context, there were zero TV shows released between 2000 and 2010 about Super Mario Bros. The Super Mario Bros. related TV shows listed were released in 1989, 1990, and 1991 [Doc 113517, Doc 113661, Doc 113814].", "latency_ms": 4595.8051681518555, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states there were 0 TV shows about Super Mario Bros released between 2000 and 2010. The generated answer also states that there were zero such TV shows, and correctly justifies this by noting the listed shows were from 1989, 1990, and 1991, which aligns with the provided context. Thus it is both correct and faithful to the context.", "main_total_tokens": 3996, "main_prompt_tokens": 3099, "main_completion_tokens": 897, "judge_total_tokens": 566, "judge_prompt_tokens": 455, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular TV show or movie about ninjas, based on the provided popularity scores, is *UNDER NINJA* [Doc 133621], which has a popularity rating of 3.1982 [Doc 133621].", "latency_ms": 954.9973011016846, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates that 'Naruto' is the correct response. The generated answer instead selects 'UNDER NINJA' based on some popularity score reasoning, which contradicts the gold answer and therefore is factually incorrect in this evaluation context. It is also unfaithful to the intended answer, as it ignores 'Naruto' and introduces a different title and justification from other sources.", "main_total_tokens": 3411, "main_prompt_tokens": 3354, "main_completion_tokens": 57, "judge_total_tokens": 539, "judge_prompt_tokens": 430, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "Answer based strictly on the context. Do not use outside knowledge. Cite sources for every fact using the format [Doc X]. If answer is not found, say so.\n\nThe context provided does not contain information about a Jurassic Park/Power Ranger crossover movie.", "latency_ms": 3319.9405670166016, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies that the context does not contain information about a Jurassic Park/Power Ranger crossover movie and states that accordingly, which aligns with the gold answer that the movie was not released. It follows the instruction to base the answer strictly on the context and to say the answer is not found when not in the context. Therefore, it is both correct and faithful to the given instructions and context.", "main_total_tokens": 3930, "main_prompt_tokens": 3233, "main_completion_tokens": 697, "judge_total_tokens": 547, "judge_prompt_tokens": 436, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Based strictly on the context provided, the movie in which Barbie fights the Predator is not mentioned.", "latency_ms": 4103.527784347534, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the movie was not released, implying that such a movie exists conceptually but was unreleased. The generated answer instead claims the movie is not mentioned in the context, which contradicts the gold answer’s implication and does not address the question correctly. It is therefore neither correct nor faithful to the provided gold answer.", "main_total_tokens": 4272, "main_prompt_tokens": 3420, "main_completion_tokens": 852, "judge_total_tokens": 494, "judge_prompt_tokens": 401, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "", "latency_ms": 4833.096027374268, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "No generated answer was provided to evaluate for correctness or faithfulness against the gold answer.", "main_total_tokens": 4509, "main_prompt_tokens": 3486, "main_completion_tokens": 1023, "judge_total_tokens": 438, "judge_prompt_tokens": 390, "judge_completion_tokens": 48}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "", "latency_ms": 4813.805341720581, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is empty and does not state the correct year 1986, so it is incorrect and cannot be considered faithful to the gold answer.", "main_total_tokens": 4377, "main_prompt_tokens": 3354, "main_completion_tokens": 1023, "judge_total_tokens": 439, "judge_prompt_tokens": 378, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The rating of the movie *Forrest Gump* is 8.466 [Doc 15236].", "latency_ms": 745.4771995544434, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states the rating as 8.466, which matches the gold answer, and does not introduce any contradictions or unsupported claims. It is both correct and faithful.", "main_total_tokens": 3107, "main_prompt_tokens": 3080, "main_completion_tokens": 27, "judge_total_tokens": 483, "judge_prompt_tokens": 416, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The TV show about two comic book fanboys who discover a secret hospital for superheroes and are offered a job there is called *Mighty Med* [Doc 121125].", "latency_ms": 796.550989151001, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the show as 'Mighty Med' and accurately describes the premise consistent with the gold answer, without adding unsupported or contradictory information.", "main_total_tokens": 2681, "main_prompt_tokens": 2642, "main_completion_tokens": 39, "judge_total_tokens": 489, "judge_prompt_tokens": 427, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "The seven prisoners in *Túnel de los huesos* (2011) escaped from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 713.3030891418457, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that the prisoners escaped from a prison in Buenos Aires, Argentina, which matches the gold answer of Argentina. The additional detail about Villa Devoto prison is plausible and does not contradict the source. The answer is both correct and faithful to the gold label.", "main_total_tokens": 3004, "main_prompt_tokens": 2965, "main_completion_tokens": 39, "judge_total_tokens": 511, "judge_prompt_tokens": 426, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In *Sweet Trap* (2024), the apprentice Li Nai falls into a trap designed by the \"Chef Assassin\" Jiang Jie [Doc 135231].", "latency_ms": 856.297492980957, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the apprentice falls into Jiang Jie's trap. The generated answer also states that the apprentice falls into a trap designed by Jiang Jie, just adding extra descriptive detail. This matches the gold answer, so it is both correct and faithful.", "main_total_tokens": 3182, "main_prompt_tokens": 3144, "main_completion_tokens": 38, "judge_total_tokens": 498, "judge_prompt_tokens": 416, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "Based strictly on the provided context, there are no Korean-language movies released after 2023 that mention assassins.", "latency_ms": 4755.934000015259, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 4, but the generated answer claims there are none, which is factually incorrect, so correctness is 0. The answer is, however, consistent with and explicitly grounded in the (limited) provided context, so faithfulness is 1.", "main_total_tokens": 4037, "main_prompt_tokens": 3024, "main_completion_tokens": 1013, "judge_total_tokens": 491, "judge_prompt_tokens": 406, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "Based strictly on the context provided, there are no movies listed that feature both Steven Seagal and Bruce Lee [Doc 119040, Doc 79200, Doc 85241, Doc 70236, Doc 56669, Doc 107081, Doc 46659, Doc 118291, Doc 49705, Doc 16203, Doc 4204, Doc 423, Doc 16319, Doc 14235, Doc 24534, Doc 22549, Doc 46033, Doc 540].", "latency_ms": 1243.6468601226807, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states simply that no movies can be found with both Bruce Lee and Steven Seagal. The generated answer instead asserts that there are no movies featuring both actors and cites multiple document IDs as support, which are hallucinated given the context. It also goes beyond the question, which only asked how many movies they have, by inventing specific document references. Thus it is not faithful to the gold response and introduces unsupported details, making it incorrect and unfaithful.", "main_total_tokens": 3583, "main_prompt_tokens": 3419, "main_completion_tokens": 164, "judge_total_tokens": 628, "judge_prompt_tokens": 503, "judge_completion_tokens": 125}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of *Tokyo Mater* (2008) is 6.3 [Doc 42750].", "latency_ms": 816.4150714874268, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (6.3) and explicitly attributes it to the same movie and year, so it is both correct and faithful to the provided information.", "main_total_tokens": 3237, "main_prompt_tokens": 3210, "main_completion_tokens": 27, "judge_total_tokens": 472, "judge_prompt_tokens": 405, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "", "latency_ms": 5204.037189483643, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is empty, so it does not state the correct style of racing (drift) and does not address the question at all. Therefore, it is both incorrect and unfaithful to the gold answer.", "main_total_tokens": 4444, "main_prompt_tokens": 3421, "main_completion_tokens": 1023, "judge_total_tokens": 454, "judge_prompt_tokens": 379, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 2374.1815090179443, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Confronting a Serial Killer', and does not introduce any contradictions or unsupported claims relative to the provided information.", "main_total_tokens": 3441, "main_prompt_tokens": 3129, "main_completion_tokens": 312, "judge_total_tokens": 492, "judge_prompt_tokens": 423, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of *Armageddon* (1998) is 7.838 [Doc 20603].", "latency_ms": 638.0734443664551, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) for Armageddon (1998) and explicitly attributes it to the cited document, so it is both correct and faithful to the provided evidence.", "main_total_tokens": 3270, "main_prompt_tokens": 3240, "main_completion_tokens": 30, "judge_total_tokens": 478, "judge_prompt_tokens": 409, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 movie is **Victim** [Doc 22488].\n\nThe description notes that the main character's girlfriend \"senses he's no longer the same person, and becomes convinced that something supernatural is corrupting his soul\" [Doc 22488].", "latency_ms": 3970.9837436676025, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'The Sixth Sense', but the generated answer claims the title is 'Victim', which is incorrect. It also fabricates a specific document reference and plot description that do not align with the known 1999 psychological thriller alluding to an extra sense, so it is not faithful to the source task.", "main_total_tokens": 3669, "main_prompt_tokens": 2987, "main_completion_tokens": 682, "judge_total_tokens": 543, "judge_prompt_tokens": 447, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The movie *The Transporter* was released in 2002 [Doc 27318].", "latency_ms": 712.3310565948486, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer. It directly uses the provided document reference, so it is faithful to the given source.", "main_total_tokens": 3359, "main_prompt_tokens": 3335, "main_completion_tokens": 24, "judge_total_tokens": 468, "judge_prompt_tokens": 399, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 movie installment featuring Frank Martin, known as The Transporter, is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 2693.472385406494, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer provides the correct title, 'The Transporter Refueled', matching the gold answer exactly. It does not contradict the given information and is fully supported by it, so both correctness and faithfulness are high.", "main_total_tokens": 3642, "main_prompt_tokens": 3118, "main_completion_tokens": 524, "judge_total_tokens": 501, "judge_prompt_tokens": 425, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in 2015 [Doc 67500].", "latency_ms": 696.9871520996094, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It does not introduce any contradictory or unsupported information and remains faithful to the provided data, with the citation '[Doc 67500]' not affecting factual accuracy.", "main_total_tokens": 3458, "main_prompt_tokens": 3433, "main_completion_tokens": 25, "judge_total_tokens": 492, "judge_prompt_tokens": 404, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "Based on the context provided, there is no romantic comedy listed where Abraham Lincoln comes back from the dead.\n\nThe context mentions a movie titled \"False Lady\" (1992) which is a romantic comedy where a man is killed and returns to earth in the body of a beautiful woman [Doc 13075]. However, this movie is not about Abraham Lincoln.\n\nThe movies featuring Abraham Lincoln involve genres such as Documentary, Drama, History, War & Politics, Action, Fantasy, Horror, and Thriller [Doc 60175, Doc 13472, Doc 55023, Doc 113428, Doc 71540, Doc 54943, Doc 59279, Doc 57213, Doc 132705, Doc 21729, Doc 17466, Doc 132037, Doc 64067].", "latency_ms": 1665.052890777588, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a romantic comedy in which Abraham Lincoln comes back from the dead. The gold answer says no such movie can be found. The generated answer clearly states that there is no romantic comedy listed where Abraham Lincoln comes back from the dead, and explains that the only related romantic comedy involves a different premise and is not about Lincoln. This matches the gold answer and stays within the provided documents, so it is both correct and faithful to the context.", "main_total_tokens": 3095, "main_prompt_tokens": 2880, "main_completion_tokens": 215, "judge_total_tokens": 676, "judge_prompt_tokens": 555, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript supposedly written by Ernest Hemingway is *Havana Darkness* [Doc 90923].", "latency_ms": 635.1995468139648, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 'Havana Darkness' as the movie title that alludes to a manuscript written by Ernest Hemingway, matching the gold answer exactly and remaining faithful to the provided content.", "main_total_tokens": 2893, "main_prompt_tokens": 2862, "main_completion_tokens": 31, "judge_total_tokens": 485, "judge_prompt_tokens": 416, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "The TV show is *Doll & Em* [Doc 122259].", "latency_ms": 716.9907093048096, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, identifying 'Doll & Em' as the TV show. It is correct and directly grounded in the provided information.", "main_total_tokens": 3251, "main_prompt_tokens": 3232, "main_completion_tokens": 19, "judge_total_tokens": 468, "judge_prompt_tokens": 405, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The 2007 movie is *Hallam Foe* [Doc 39389]. In the plot description, Hallam's talent for spying on people is mentioned, and he is \"Driven to expose the true cause of his mother's death\" [Doc 39389].", "latency_ms": 951.9753456115723, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the 2007 movie where Hallam, who has a talent for spying on people, attempts to find the cause of his mother's death. The gold answer is 'Hallam Foe'. The generated answer correctly identifies the film as 'Hallam Foe' and supports it with plot details. It matches the gold answer exactly and does not introduce contradictions, so it is both correct and faithful to the referenced document.", "main_total_tokens": 3378, "main_prompt_tokens": 3312, "main_completion_tokens": 66, "judge_total_tokens": 572, "judge_prompt_tokens": 454, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Based strictly on the provided context, there is no character mentioned whose last name sounds like \"friend\" or \"foe.\"", "latency_ms": 4952.736854553223, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold label indicates the correct answer is 'A foe', meaning the last name does sound like a foe. The generated answer incorrectly claims there is no such character mentioned, so it fails to provide the correct label. However, it is faithful to the (limited) context it references, as it explicitly bases its response on the provided context and does not introduce new information.", "main_total_tokens": 4240, "main_prompt_tokens": 3290, "main_completion_tokens": 950, "judge_total_tokens": 511, "judge_prompt_tokens": 411, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "Sam is 18 years old in the 2017 TV show *Atypical* [Doc 124583].", "latency_ms": 696.8348026275635, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that Sam is 18 years old in the 2017 TV show Atypical, which matches the gold answer of 18. There are no contradictions or hallucinations beyond a generic document reference, so it is fully correct and faithful.", "main_total_tokens": 2959, "main_prompt_tokens": 2929, "main_completion_tokens": 30, "judge_total_tokens": 494, "judge_prompt_tokens": 411, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show *Atypical*, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 666.3570404052734, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the 2017 TV show 'Atypical', Sam is on the autism spectrum, which matches the gold answer exactly. It is faithful to the source information given and does not introduce contradictory or extraneous claims.", "main_total_tokens": 2879, "main_prompt_tokens": 2850, "main_completion_tokens": 29, "judge_total_tokens": 492, "judge_prompt_tokens": 411, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "In the movie *À corde tendue*, released in 2013, Sandra was born in Chamonix [Doc 59659].", "latency_ms": 756.55198097229, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, which matches the gold answer exactly. It does not introduce contradictions or unsupported information and remains faithful to the given content.", "main_total_tokens": 3346, "main_prompt_tokens": 3313, "main_completion_tokens": 33, "judge_total_tokens": 493, "judge_prompt_tokens": 425, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In this Japanese language movie, Hazuki clips her nails every time it **rains** [Doc 100481].", "latency_ms": 834.9854946136475, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and correctly fills in the blank. It is fully consistent with the provided information, so both correctness and faithfulness are maximal.", "main_total_tokens": 3608, "main_prompt_tokens": 3581, "main_completion_tokens": 27, "judge_total_tokens": 483, "judge_prompt_tokens": 415, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Tabaluga is the crown-prince of Greenland [Doc 114810].", "latency_ms": 2139.5158767700195, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which matches the gold answer exactly. It is both correct and faithful to the reference.", "main_total_tokens": 3749, "main_prompt_tokens": 3421, "main_completion_tokens": 328, "judge_total_tokens": 475, "judge_prompt_tokens": 410, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with that description is *One Afternoon* [Doc 96838].", "latency_ms": 695.8589553833008, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'One Afternoon,' which matches the gold answer exactly. It does not introduce contradictions or unsupported claims beyond a benign document reference, so it is fully faithful to the provided information.", "main_total_tokens": 2732, "main_prompt_tokens": 2713, "main_completion_tokens": 19, "judge_total_tokens": 494, "judge_prompt_tokens": 420, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The Italian language movie where a Holocaust survivor with a daycare business takes in a 12-year-old street kid who recently robbed her is *The Life Ahead* [Doc 93260]. It has a rating of 6.746 [Doc 93260].", "latency_ms": 5638.349533081055, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'The Life Ahead', matching the gold answer, so correctness is full. However, it introduces extra details (daycare business, specific rating 6.746, document reference [Doc 93260]) that are not present in the original question or gold answer, so faithfulness to the provided content is not maintained.", "main_total_tokens": 3911, "main_prompt_tokens": 3173, "main_completion_tokens": 738, "judge_total_tokens": 553, "judge_prompt_tokens": 450, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary *McLaren* is about the story of New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 2250.3061294555664, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the 2016 documentary 'McLaren' is about Bruce McLaren, matching the gold answer. It remains faithful to the given information, only adding accurate contextual detail about him founding the McLaren Motor Racing team.", "main_total_tokens": 3287, "main_prompt_tokens": 3002, "main_completion_tokens": 285, "judge_total_tokens": 500, "judge_prompt_tokens": 420, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The context does not specify whose racing team is being referred to. However, the following racing teams or related entities are mentioned:\n\n*   McLaren Motor Racing team [Doc 73859]\n*   Newman-Haas (Andretti) racing team [Doc 20456]\n*   Bobby Spencer Racing team (fictional NASCAR team) [Doc 130571]\n*   Speed Angels racing team [Doc 51728]\n*   Team Sonic, Team Rose, and Team Dark (competing on the Wisp Circuit course) [Doc 128290]", "latency_ms": 1327.5501728057861, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the name of \"his\" racing team, and the gold answer is specifically \"The McLaren Motor Racing team.\" The generated answer instead claims the context does not specify whose team is being referred to and lists multiple possible teams, without selecting McLaren or directly answering the question. This is incorrect with respect to the gold answer. It is also unfaithful to the gold reference because it ignores the clear, specific answer and introduces unnecessary ambiguity and extra entities.", "main_total_tokens": 3320, "main_prompt_tokens": 3184, "main_completion_tokens": 136, "judge_total_tokens": 621, "judge_prompt_tokens": 495, "judge_completion_tokens": 126}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "Based on the context provided, here are the details about the countries of origin mentioned for various individuals:\n\n*   **Karol Wojtyla (Pope John Paul II)** is described as a Polish Roman Catholic [Doc 1545].\n*   **Heinz Stücke** left Germany in 1962 [Doc 100044].\n*   **Martin Rauch** is an East Germany native [Doc 122771].\n*   **Paul Krüger** is a Berlin pensioner who gets to know his place of birth in the Czech Republic [Doc 92915].\n*   A man in the movie *The Grass is Greener Everywhere Else* migrates from Poland over Germany into the US [Doc 10348].\n*   A family in the movie *In America* are Irish immigrants [Doc 30504].\n*   **Guy Bennett** (in *Another Country*) recalls his last year at public school, fifty years before, and how it contributed to him becoming a spy [Doc 4064].\n*   A man in the movie *Travelling Warrior* travels through Switzerland selling cosmetics [Doc 1816].", "latency_ms": 1795.9764003753662, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a single country of origin (New Zealand), but the generated answer lists multiple people and their countries, never mentioning New Zealand or identifying who \"he\" is. It does not answer the question correctly and is not faithful to the gold answer.", "main_total_tokens": 3019, "main_prompt_tokens": 2761, "main_completion_tokens": 258, "judge_total_tokens": 688, "judge_prompt_tokens": 605, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based strictly on the provided context, the information that John McClane blows up the Great Wall of China in a 2013 movie is not found. The 2013 movie featuring John McClane, *A Good Day to Die Hard*, is described as taking place in Moscow [Doc 58791].", "latency_ms": 3284.8305702209473, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct response is simply that there is no such movie in the database, matching the gold answer. The generated answer adds unsupported details about a specific 2013 movie and its setting that are not in the provided gold context. Therefore it is not correct with respect to the gold answer and not faithful to the given context.", "main_total_tokens": 3958, "main_prompt_tokens": 3356, "main_completion_tokens": 602, "judge_total_tokens": 556, "judge_prompt_tokens": 461, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with that description is *Tomie: Forbidden Fruit* [Doc 28589].", "latency_ms": 1053.1013011932373, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the film as 'Tomie: Forbidden Fruit,' which matches the gold answer. It is also faithful to the provided information, simply restating the title with no hallucinated details.", "main_total_tokens": 3313, "main_prompt_tokens": 3283, "main_completion_tokens": 30, "judge_total_tokens": 514, "judge_prompt_tokens": 442, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie *Lei mi parla ancora*, Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 666.6719913482666, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, which matches the gold answer. It is also faithful to the referenced document snippet [Doc 98566] that explicitly provides this information.", "main_total_tokens": 3675, "main_prompt_tokens": 3640, "main_completion_tokens": 35, "judge_total_tokens": 501, "judge_prompt_tokens": 424, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In *Scorpion King: Book of Souls* (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 869.2760467529297, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in 'Scorpion King: Book of Souls' (2018), The Scorpion King and Tala search for the Book of Souls, matching the gold answer. It is faithful to the provided information without adding unsupported claims beyond calling it a legendary relic, which is consistent with typical descriptions.", "main_total_tokens": 3664, "main_prompt_tokens": 3623, "main_completion_tokens": 41, "judge_total_tokens": 525, "judge_prompt_tokens": 431, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Zombies on the Moon\" is not available in the provided context.", "latency_ms": 3153.8519859313965, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, effectively indicating the system does not know the release year. The generated answer claims that the release year is not available in the provided context, which is a different claim: it attributes the lack of information to the context rather than the database/knowledge. This is not the correct gold response, so correctness is 0. However, the generated answer does not contradict any given context snippet and accurately reflects that it cannot supply a release year, so it is faithful to the (limited) context; thus faithfulness is 1.", "main_total_tokens": 4056, "main_prompt_tokens": 3407, "main_completion_tokens": 649, "judge_total_tokens": 558, "judge_prompt_tokens": 410, "judge_completion_tokens": 148}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Answer based strictly on the context. Do not use outside knowledge. Cite sources for every fact using the format [Doc X]. If answer is not found, say so.\n\nThe release year for \"The Galactic baker\" is not found in the provided context.", "latency_ms": 2577.7862071990967, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The gold answer says the movie is not in the database, which aligns in substance with the generated answer stating the release year is not found in the provided context, so it is substantively correct. However, the instructions for the generated answer explicitly say to base the answer strictly on the context and to cite sources like [Doc X], and also to say the answer is not found if not present; the generated answer does not mention or cite any [Doc X] sources, so it does not fully follow the given instructions, making it unfaithful to them.", "main_total_tokens": 3674, "main_prompt_tokens": 3202, "main_completion_tokens": 472, "judge_total_tokens": 583, "judge_prompt_tokens": 440, "judge_completion_tokens": 143}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Return of the Null Pointer\" is not available in the provided context.", "latency_ms": 2383.943557739258, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers convey that the release year is unknown/not available. The generated answer is correct and faithful to the gold response, just phrased differently.", "main_total_tokens": 3127, "main_prompt_tokens": 2682, "main_completion_tokens": 445, "judge_total_tokens": 474, "judge_prompt_tokens": 410, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Data Science: The Movie\" is not available in the provided context.", "latency_ms": 3102.442741394043, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, implying we should not assert any specific release information. The generated answer claims that the release year is not available in the provided context, which subtly changes the reason (context vs. database) and could suggest that the information might exist elsewhere. It does not explicitly align with the database-based limitation, so it is not fully faithful to the gold response, and it does not correctly mirror the intended answer behavior. The generated answer is thus incorrect but largely faithful in that it also withholds a specific year.", "main_total_tokens": 3989, "main_prompt_tokens": 3369, "main_completion_tokens": 620, "judge_total_tokens": 553, "judge_prompt_tokens": 410, "judge_completion_tokens": 143}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for the movie \"Python's Revenge 2\" is not available. The context includes information for \"Python 2\" (2002) [Doc 28408] and several movies titled \"Revenge,\" but not the specific combination requested.", "latency_ms": 3250.6392002105713, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database. The generated answer correctly says that the release year is not available and explains that the provided context does not include that specific movie. This matches the gold response in substance (no year provided) and is fully grounded in the given context.", "main_total_tokens": 3834, "main_prompt_tokens": 3196, "main_completion_tokens": 638, "judge_total_tokens": 541, "judge_prompt_tokens": 451, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Last Pixel\" is not available in the provided context.", "latency_ms": 2370.506525039673, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database. The generated answer says the release year is not available in the provided context. Both convey that the release year is unknown/unavailable, so the response is correct relative to the gold and does not introduce unsupported facts.", "main_total_tokens": 3283, "main_prompt_tokens": 2871, "main_completion_tokens": 412, "judge_total_tokens": 491, "judge_prompt_tokens": 406, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for \"The man who ate the sun\" cannot be determined because this title is not listed in the documents.", "latency_ms": 3036.712408065796, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year, but both the gold answer and the generated answer state that the movie is not in the available data, so the year cannot be provided. This is correct given the context, and the generated answer does not introduce any unsupported information, staying faithful to the provided documents.", "main_total_tokens": 4283, "main_prompt_tokens": 3666, "main_completion_tokens": 617, "judge_total_tokens": 515, "judge_prompt_tokens": 424, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Based strictly on the context provided, the director of the movie \"The Last Days of Lehman Brothers\" is not mentioned [Doc 46069].", "latency_ms": 1512.2394561767578, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned in the provided context and does not introduce external information, matching the gold answer’s limitation to the database. It is both correct and faithful to the given context.", "main_total_tokens": 3752, "main_prompt_tokens": 3567, "main_completion_tokens": 185, "judge_total_tokens": 501, "judge_prompt_tokens": 427, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context does not contain information about a movie titled \"A Love to Die For,\" nor does it list the directors for any of the films mentioned.", "latency_ms": 3475.832939147949, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not contain information about the movie or its director, which aligns with the gold answer indicating the database does not provide the director information. It does not introduce any unsupported facts and remains faithful to the given context.", "main_total_tokens": 3851, "main_prompt_tokens": 3215, "main_completion_tokens": 636, "judge_total_tokens": 506, "judge_prompt_tokens": 425, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Based strictly on the context provided, the director of the movie \"Journey into Gaza\" is not mentioned [Doc 111360].", "latency_ms": 1633.2767009735107, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director of \"Journey into Gaza\" is not mentioned, which aligns with the gold response that the director information is not provided in the database. It does not introduce unsupported claims and stays faithful to the given context.", "main_total_tokens": 3429, "main_prompt_tokens": 3194, "main_completion_tokens": 235, "judge_total_tokens": 499, "judge_prompt_tokens": 419, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Answer not found in the context.", "latency_ms": 741.7676448822021, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the director information is not available in the given context/database, so the model does not hallucinate and remains faithful to the provided information.", "main_total_tokens": 3121, "main_prompt_tokens": 3114, "main_completion_tokens": 7, "judge_total_tokens": 471, "judge_prompt_tokens": 404, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The following movies involve a group of archaeology students awakening an ancient mummy:\n\n*   *Rise of the Mummy* (2021), where a group of archaeology students awaken an ancient mummy [Doc 98375].\n*   *Ancient Evil: Scream of the Mummy* (2000), where six young archaeology students discover the remains of an ancient Aztec mummy and accidentally unleash the fury of an evil god [Doc 24655].", "latency_ms": 1042.5333976745605, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0.5, "faithfulness_score": 1, "justification": "The question appears to ask for a single movie title; the gold answer is \"Rise of the Mummy.\" The generated answer lists two films, only one of which matches the gold. Thus, it is only partially correct. However, the generated answer is faithful to its own cited documents and does not contradict them, so faithfulness is high.", "main_total_tokens": 3067, "main_prompt_tokens": 2970, "main_completion_tokens": 97, "judge_total_tokens": 583, "judge_prompt_tokens": 481, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie where Sarah fakes her own death to escape her abusive spouse is *Stranger in My Bed* [Doc 35237].", "latency_ms": 1035.7997417449951, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Stranger in My Bed,' which matches the gold answer exactly. It remains faithful to the given information without adding contradictory or unsupported details; the bracketed citation is neutral and does not affect faithfulness.", "main_total_tokens": 3038, "main_prompt_tokens": 3008, "main_completion_tokens": 30, "judge_total_tokens": 504, "judge_prompt_tokens": 424, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The event described happens in the TV show *Game of Silence* [Doc 123938].", "latency_ms": 690.5031204223633, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer identifies 'Game of Silence' as a movie, but it is actually a TV show. The generated answer correctly identifies the work as a TV show, so it is faithful to the gold only in naming the same title but correct in format, making the gold partially inaccurate. Since the task is to match the gold, the generated answer is considered incorrect relative to the specified gold, but it does not hallucinate beyond the given information.", "main_total_tokens": 2500, "main_prompt_tokens": 2477, "main_completion_tokens": 23, "judge_total_tokens": 536, "judge_prompt_tokens": 416, "judge_completion_tokens": 120}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie based on a true story about a killer who abuses prostitutes he finds via Craigslist is *The Craigslist Killer* [Doc 53039].", "latency_ms": 737.2281551361084, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer,' matching the gold answer exactly. It remains faithful to the given information without adding unsupported claims, only rephrasing the description of the plot.", "main_total_tokens": 2919, "main_prompt_tokens": 2887, "main_completion_tokens": 32, "judge_total_tokens": 488, "judge_prompt_tokens": 421, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie is *Wood Job!* [Doc 66094].", "latency_ms": 650.91872215271, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Wood Job!' and directly matches the gold answer without adding incorrect or extraneous information.", "main_total_tokens": 3344, "main_prompt_tokens": 3327, "main_completion_tokens": 17, "judge_total_tokens": 470, "judge_prompt_tokens": 413, "judge_completion_tokens": 57}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "This happens in the movie *Sesame Street: The Magical Wand Chase* [Doc 78373].", "latency_ms": 738.7967109680176, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Sesame Street: The Magical Wand Chase,' which exactly matches the gold answer. There are no contradictions or hallucinations beyond a harmless document reference, and the content is fully faithful to the given information.", "main_total_tokens": 3044, "main_prompt_tokens": 3020, "main_completion_tokens": 24, "judge_total_tokens": 511, "judge_prompt_tokens": 431, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "This happens in the movie *Beyond* [Doc 74597].", "latency_ms": 775.1386165618896, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Beyond' and does not introduce any unsupported details or contradictions. It is both correct and faithful.", "main_total_tokens": 2581, "main_prompt_tokens": 2564, "main_completion_tokens": 17, "judge_total_tokens": 487, "judge_prompt_tokens": 422, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie is *6ixtynin9* [Doc 22478].", "latency_ms": 692.6920413970947, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as '6ixtynin9'. It is fully correct and faithful to the provided gold label.", "main_total_tokens": 2631, "main_prompt_tokens": 2611, "main_completion_tokens": 20, "judge_total_tokens": 483, "judge_prompt_tokens": 419, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie described is *Club Dread* [Doc 32410].", "latency_ms": 740.4570579528809, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer exactly. It does not introduce any unsupported or extraneous claims beyond a minor citation tag, so it is fully faithful to the given information.", "main_total_tokens": 3207, "main_prompt_tokens": 3190, "main_completion_tokens": 17, "judge_total_tokens": 516, "judge_prompt_tokens": 439, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is titled *Toilet* [Doc 48873].", "latency_ms": 948.617696762085, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Toilet' and restates the question scenario without adding unsupported details or contradictions, so it is both correct and faithful to the gold answer.", "main_total_tokens": 3476, "main_prompt_tokens": 3419, "main_completion_tokens": 57, "judge_total_tokens": 534, "judge_prompt_tokens": 466, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie described is *Electric Angel* [Doc 1024].", "latency_ms": 539.7748947143555, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the given description. The gold answer is 'Electric Angel'. The generated answer explicitly states that the movie is 'Electric Angel', which is correct. It does not add any contradictory or unsupported information, so it is fully faithful to the provided data and correctly answers the question.", "main_total_tokens": 2994, "main_prompt_tokens": 2978, "main_completion_tokens": 16, "judge_total_tokens": 516, "judge_prompt_tokens": 430, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 in the context is *Fall*, with a rating of 4.0 [Doc 106810].", "latency_ms": 3495.316743850708, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'Mega Ape' as the lowest rated movie from 2023, while the generated answer incorrectly states 'Fall' with a rating of 4.0. This contradicts the gold label and misidentifies the movie, so it is factually incorrect relative to the task. It is also unfaithful to the provided gold answer and context as defined by the task, since it introduces a different movie and rating not aligned with the gold.", "main_total_tokens": 3984, "main_prompt_tokens": 3083, "main_completion_tokens": 901, "judge_total_tokens": 534, "judge_prompt_tokens": 410, "judge_completion_tokens": 124}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "The highest rated TV show from 2010 is *Giant*, which has a rating of 8.1 [Doc 119282].", "latency_ms": 3858.5550785064697, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the highest rated TV show from 2010, with the gold answer being 'Tea House'. The generated answer instead claims the highest rated show is 'Giant' with a rating of 8.1, which contradicts the gold answer and is therefore incorrect. It also fabricates a specific rating and citation ([Doc 119282]) that does not match the provided gold answer, so it is not faithful to the reference.", "main_total_tokens": 4523, "main_prompt_tokens": 3531, "main_completion_tokens": 992, "judge_total_tokens": 532, "judge_prompt_tokens": 411, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest rated Japanese language movie from 1999 is *Kikujiro*, with a rating of 7.5 [Doc 22213].", "latency_ms": 1063.546895980835, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold label indicates that 'The Uncut Story of Mewtwo's Origin' is the highest rated Japanese language movie from 1999, not 'Kikujiro'. Additionally, the reference to a specific document [Doc 22213] asserting this claim is inconsistent with the provided gold answer, so the response is not faithful to the given evidence setup.", "main_total_tokens": 3370, "main_prompt_tokens": 3334, "main_completion_tokens": 36, "judge_total_tokens": 532, "judge_prompt_tokens": 422, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "", "latency_ms": 4422.966241836548, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "There is no generated answer to evaluate, so correctness and faithfulness cannot be confirmed.", "main_total_tokens": 4216, "main_prompt_tokens": 3193, "main_completion_tokens": 1023, "judge_total_tokens": 432, "judge_prompt_tokens": 384, "judge_completion_tokens": 48}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was *Freaks and Geeks*, with a popularity rating of 5.8043 [Doc 115187].", "latency_ms": 809.9157810211182, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold answer states that the show is 'Law & Order: Special Victims Unit', but the model answered 'Freaks and Geeks'. It is also unfaithful to the provided gold reference, as it contradicts it and instead cites a different show and a specific rating that are not supported by the gold answer.", "main_total_tokens": 3755, "main_prompt_tokens": 3712, "main_completion_tokens": 43, "judge_total_tokens": 529, "judge_prompt_tokens": 426, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 856.9481372833252, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 and directly uses it without contradiction or hallucination. It is both correct and faithful.", "main_total_tokens": 3213, "main_prompt_tokens": 3192, "main_completion_tokens": 21, "judge_total_tokens": 461, "judge_prompt_tokens": 400, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in 1999 [Doc 22481], 2019 [Doc 88811], and 2025 [Doc 112077].", "latency_ms": 954.3254375457764, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 2019, but the generated response lists three different years (1999, 2019, 2025) without choosing one, so it is not a correct answer. It is also unfaithful because it fabricates multiple release years and document references rather than giving the single correct year indicated by the gold answer.", "main_total_tokens": 3212, "main_prompt_tokens": 3157, "main_completion_tokens": 55, "judge_total_tokens": 523, "judge_prompt_tokens": 423, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The context provides the rating (6.7) and popularity (1.7749) for \"My Dog Skip,\" but it does not state how many votes the movie received [Doc 24114].", "latency_ms": 774.8985290527344, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the number of votes (279). The generated answer claims the context does not state how many votes the movie received, which contradicts the gold answer, so it is factually incorrect. However, the answer is consistent with its own description of the context (it does not invent a vote count), so it is faithful to the provided context.", "main_total_tokens": 2968, "main_prompt_tokens": 2922, "main_completion_tokens": 46, "judge_total_tokens": 520, "judge_prompt_tokens": 422, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "Based strictly on the context provided, the number of votes received for \"Oh My God!\" is not available. The context only provides the rating (3.8) and popularity (0.8047) [Doc 85434].", "latency_ms": 3273.7936973571777, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that \"Oh My God!\" received 12 votes, but the generated answer claims the information is not available. This contradicts the gold, so it is factually incorrect with respect to the task. However, the model explicitly grounds its claim in the provided context and does not invent a number of votes, so it remains faithful to the (stated) context, yielding high faithfulness but incorrectness relative to the gold.", "main_total_tokens": 3854, "main_prompt_tokens": 3243, "main_completion_tokens": 611, "judge_total_tokens": 549, "judge_prompt_tokens": 428, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 763.4334564208984, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (2009) and does not introduce any contradictory or extraneous information. It is both correct and faithful to the reference.", "main_total_tokens": 3561, "main_prompt_tokens": 3534, "main_completion_tokens": 27, "judge_total_tokens": 474, "judge_prompt_tokens": 410, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English (en) [Doc 48035].", "latency_ms": 812.2398853302002, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the original language of 'The King\\'s Speech' is English, matching the gold answer 'en'. It is fully consistent with the provided information and does not introduce contradictions.", "main_total_tokens": 3357, "main_prompt_tokens": 3330, "main_completion_tokens": 27, "judge_total_tokens": 478, "judge_prompt_tokens": 405, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The movie *Wall Street* was released in 1987 [Doc 6993].", "latency_ms": 702.2337913513184, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, which matches the gold answer, and it remains faithful to the provided information without introducing unsupported details.", "main_total_tokens": 2410, "main_prompt_tokens": 2388, "main_completion_tokens": 22, "judge_total_tokens": 461, "judge_prompt_tokens": 396, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie in which classmates are forced to take part in a survival game using their cell phones is *Brain Fluid Explosion Girl* [Doc 69929].", "latency_ms": 737.3414039611816, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 'Brain Fluid Explosion Girl' as the movie where classmates must play a survival game using their cell phones. This matches the gold answer exactly, indicating both factual correctness and full faithfulness to the provided reference.", "main_total_tokens": 2078, "main_prompt_tokens": 2044, "main_completion_tokens": 34, "judge_total_tokens": 496, "judge_prompt_tokens": 419, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show is *Dungeons & Dragons* [Doc 112849]. The description states that \"Six friends are transported into the Dungeons & Dragons realm and must try to find a way home with the help of their guide 'Dungeon Master'\" [Doc 112849].", "latency_ms": 3584.752082824707, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' which matches the gold answer. The explanation is directly supported by the cited document and stays consistent with the provided information, so it is both correct and faithful.", "main_total_tokens": 2760, "main_prompt_tokens": 2040, "main_completion_tokens": 720, "judge_total_tokens": 523, "judge_prompt_tokens": 446, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is *Nacht der Wölfe* [Doc 2590].", "latency_ms": 828.056812286377, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying 'Nacht der Wölfe' as the movie about rival German and Turkish teen gangs in Munich. It is correct and fully supported by the cited document reference.", "main_total_tokens": 2097, "main_prompt_tokens": 2056, "main_completion_tokens": 41, "judge_total_tokens": 511, "judge_prompt_tokens": 437, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of *Picnic at Hanging Rock* take place in 1900 [Doc 126191].", "latency_ms": 1919.7897911071777, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of Picnic at Hanging Rock take place in 1900, which matches the gold answer. It is fully faithful to the provided information and does not introduce contradictions or extraneous claims.", "main_total_tokens": 2630, "main_prompt_tokens": 2350, "main_completion_tokens": 280, "judge_total_tokens": 482, "judge_prompt_tokens": 407, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie described is *June* [Doc 32483].", "latency_ms": 3428.3411502838135, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title of a 2004 movie matching a specific plot, and the gold answer is 'June'. The generated answer correctly identifies the movie as 'June' and does not introduce any contradictions or extraneous claims beyond restating the correct title, so it is both correct and faithful.", "main_total_tokens": 2962, "main_prompt_tokens": 2316, "main_completion_tokens": 646, "judge_total_tokens": 540, "judge_prompt_tokens": 448, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1405.0633907318115, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which is factually incorrect because the highest rated Russian-language movie released in 1980 is 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger.' However, it does not introduce any fabricated information; it simply expresses a lack of knowledge. Thus it is unfaithful to the gold answer but faithful in the sense of not hallucinating content.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 511, "judge_prompt_tokens": 399, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 471.13728523254395, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a concrete rating (7.6), but the generated answer says 'I don't know,' which is factually incorrect with respect to the gold. However, the model does not fabricate any incorrect details; it simply expresses a lack of knowledge, so it is fully faithful to its own reasoning but incorrect relative to the task.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 488, "judge_prompt_tokens": 389, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1184.7264766693115, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.366123019050001, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer (18) does not match the gold answer (4), so it is incorrect. There is no indication it followed or was grounded in the provided gold data, so it is unfaithful to the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 394, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 326.0955810546875, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the count of TV shows matching specific criteria; the gold answer is 2 but the generated answer is 0. This is factually incorrect relative to the gold, so correctness is 0. The model did output a numeric answer to the question rather than something unrelated, so it is faithful to the query, giving faithfulness 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 498, "judge_prompt_tokens": 395, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1658.2908630371094, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer admits not knowing the response, while the gold answer specifies 'Fantômas'. Since the model did not provide the correct show, correctness is 0. However, the response is internally consistent and does not invent facts, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 474, "judge_prompt_tokens": 389, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" [Doc 11205].", "latency_ms": 857.5847148895264, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' which matches the gold answer and stays within the provided information without adding unsupported details.", "main_total_tokens": 2264, "main_prompt_tokens": 2237, "main_completion_tokens": 27, "judge_total_tokens": 468, "judge_prompt_tokens": 409, "judge_completion_tokens": 59}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie is *Won't You Be My Neighbor?* [Doc 84838].", "latency_ms": 2813.8909339904785, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'Won't You Be My Neighbor?' and stays faithful to the question and the provided gold answer without adding incorrect information.", "main_total_tokens": 3012, "main_prompt_tokens": 2552, "main_completion_tokens": 460, "judge_total_tokens": 486, "judge_prompt_tokens": 422, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The specific title of the work is not mentioned in the question, so the release year cannot be determined.\n\nThe context provides release years for several different titles, including:\n*   *...And the Third Year, He Resuscitated* (1980) [Doc 675]\n*   *1991: The Year Punk Broke* (1992) [Doc 13886]\n*   *1999* (1997) [Doc 19562]\n*   *2002* (2001) [Doc 25738]\n*   *1981* (2009) [Doc 45703]\n*   *1982* (2013) [Doc 62279]\n*   *1992* (201", "latency_ms": 5050.352334976196, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2018, but the generated answer says the release year cannot be determined, which is incorrect. However, given the truncated context shown, the model accurately reflects that the specific title is not mentioned and thus remains faithful to the provided information, even though it does not match the gold label.", "main_total_tokens": 3604, "main_prompt_tokens": 2582, "main_completion_tokens": 1022, "judge_total_tokens": 620, "judge_prompt_tokens": 527, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is **Astro Boy** [Doc 112591].\n\nThe description states: \"In 2030 AD, after being abandoned by his creator, a young boy robot is rescued by an Android Advocate, Dr Elefun, who adopts him as his own and guides him in the use of his special powers to aid and protect mankind and robot-kind\" [Doc 112591]. The type is TV and the genres include 'Animation' [Doc 112591].", "latency_ms": 1187.1821880340576, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy' and supports it with a quotation that matches the described plot and metadata from the cited document. It aligns with the gold answer and stays faithful to the provided information.", "main_total_tokens": 2394, "main_prompt_tokens": 2266, "main_completion_tokens": 128, "judge_total_tokens": 574, "judge_prompt_tokens": 497, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "The woman and her son in the movie \"Room\" (2015) were held captive for 7 years [Doc 67513].", "latency_ms": 944.8277950286865, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer. It does not introduce any contradictions or extraneous, ungrounded details, so it is fully faithful to the provided information.", "main_total_tokens": 1973, "main_prompt_tokens": 1940, "main_completion_tokens": 33, "judge_total_tokens": 503, "judge_prompt_tokens": 422, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The gold bullion stumbled across by the robbers at the Brink's-Mat security depot on 26 November 1983 was worth £26 million [Doc 133469].", "latency_ms": 3336.4551067352295, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the gold bullion at the Brink's-Mat security depot on 26 November 1983 was worth £26 million, matching the provided gold answer exactly. It is faithful to the given document citation and does not introduce unsupported details.", "main_total_tokens": 2907, "main_prompt_tokens": 2255, "main_completion_tokens": 652, "judge_total_tokens": 513, "judge_prompt_tokens": 430, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6366.482496261597, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score (80.5772). The generated answer says \"I don't know,\" which does not provide the correct value, so correctness is 0. However, the model is honest about its lack of knowledge and does not hallucinate an answer, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 396, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 354.53009605407715, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1, but the generated answer was 0, so the response is incorrect. The evaluation correctly identifies this mismatch, so it is faithful to the provided information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 453, "judge_prompt_tokens": 385, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "The TV show in which Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight is *Angel Eyes* [Doc 121928].", "latency_ms": 663.1193161010742, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Angel Eyes,' which matches the gold answer exactly. It stays faithful to the given QA pair, only rephrasing the question and providing the correct title, with no unsupported or extraneous claims.", "main_total_tokens": 2638, "main_prompt_tokens": 2599, "main_completion_tokens": 39, "judge_total_tokens": 511, "judge_prompt_tokens": 431, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The context provides information for several movies with different languages:\n\n*   **fr** (French) for *Foreign Language* [Doc 110681]\n*   **nl** (Dutch) for *On Top of the Whale* [Doc 2803]\n*   **en** (English) for *Movie* (2011) [Doc 54173], *Language Lessons* [Doc 99173], *Film* (2000) [Doc 25368], *Film* (2011) [Doc 54687], and *Film* (2023) [Doc 108434]\n*   **es** (Spanish) for *Movie* (2014) [Doc 67258], *I Dream in Another Language* [Doc 78127], and *Spanish Movie* [Doc 45676]\n*   **tr** (Turkish) for *Love in Another Language* [Doc 45031] and *Film* (2011) [Doc 52741]\n*   **ar** (Arabic) for *Film Hindi* [Doc 29800]\n*   **it** (Italian) for *7 km da Gerusalemme* [Doc 40504] and *Film* (2020) [", "latency_ms": 4848.937749862671, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the language of a specific movie, with gold answer Korean, but the generated answer does not identify that movie or state Korean. It instead lists multiple languages for various movies and is also incomplete/cut off, so it is not correct. It is not faithful to the task of answering the question directly and appears to ignore the gold label entirely.", "main_total_tokens": 3310, "main_prompt_tokens": 2288, "main_completion_tokens": 1022, "judge_total_tokens": 736, "judge_prompt_tokens": 633, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 730.1235198974609, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is factually incorrect. The model directly answered the question without adding unsupported details, so it is faithful to its own reasoning/statement even though the content is wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 394, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Based strictly on the provided context, there are no movies listed that were released in 1979.", "latency_ms": 2347.3808765411377, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there are no movies listed that were released in 1979, which aligns with the gold response indicating the database has no movies from 1979. It does not introduce any unsupported information and remains faithful to the provided context.", "main_total_tokens": 2680, "main_prompt_tokens": 2217, "main_completion_tokens": 463, "judge_total_tokens": 497, "judge_prompt_tokens": 415, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The context does not contain information about a movie titled *Back to the Future Part IV* or its release year. The documents refer to the series as a \"trilogy\" [Doc 113788, Doc 28792, Doc 68688, Doc 103759], and one document states that *Back to the Future Part III* (1990) was the \"final installment\" [Doc 10621].", "latency_ms": 3267.5225734710693, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly indicates that there is no information about Back to the Future Part IV and notes that the series is a trilogy with Part III as the final installment. This matches real-world facts and is consistent with the gold answer, which states that Back to the Future Part IV does not exist in the database. The answer is grounded in the provided documents and does not introduce unsupported claims, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3750, "main_prompt_tokens": 3096, "main_completion_tokens": 654, "judge_total_tokens": 604, "judge_prompt_tokens": 486, "judge_completion_tokens": 118}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no movies from 1980 related to the Chernobyl disaster could be found. The generated answer similarly states that it cannot find any movies or shows matching the criteria and hints at checking the event timing, which is accurate since the Chernobyl disaster occurred in 1986, not 1980. Both answers convey that such movies do not exist for that year, so the generated answer is correct and faithful to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 556, "judge_prompt_tokens": 434, "judge_completion_tokens": 122}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 304.72373962402344, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says there are 0 TV shows or movies with 'Astro Boy' in the title, while the gold answer is 4. This is factually incorrect, so correctness is 0. However, the answer is internally consistent and does not contradict itself or the question, so faithfulness to its own reasoning is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 488, "judge_prompt_tokens": 388, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 365.3082847595215, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. It is, however, internally consistent with itself and does not invent any justification or external facts, so it is fully faithful to its own (albeit wrong) conclusion.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 471, "judge_prompt_tokens": 386, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 141.76511764526367, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show about Super Mario Bros released in the 1990s, while the generated answer says 0. This directly contradicts the gold, so the generated answer is incorrect. It is, however, internally consistent with itself, so it is fully faithful to its own reasoning/content.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 389, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 286.81397438049316, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (0), which correctly reflects that no TV shows about Super Mario Bros. were released between 2000 and 2010. The response is both correct with respect to the question and faithful to the provided information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 472, "judge_prompt_tokens": 391, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 216.2494659423828, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer according to the gold is 'Naruto'. The generated answer says 'I don't know', which does not provide the correct information, so correctness is 0. However, the model is honest about its uncertainty and does not fabricate an answer, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 386, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "Based strictly on the context provided, the release date for a Jurassic Park/Power Ranger crossover movie is not mentioned.", "latency_ms": 2661.08775138855, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.5, "justification": "The gold answer clarifies that no such movie exists, so there is no release date. The generated answer instead claims that the release date is simply not mentioned in the context, implying the movie could exist but the date is unknown. This conflicts with the gold fact, so it is not fully faithful. However, it does correctly avoid fabricating a release date, so it is partially aligned in spirit with not inventing information.", "main_total_tokens": 3013, "main_prompt_tokens": 2501, "main_completion_tokens": 512, "judge_total_tokens": 526, "judge_prompt_tokens": 408, "judge_completion_tokens": 118}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Based strictly on the context provided, the movie in which Barbie fights the Predator is not mentioned.", "latency_ms": 3197.457790374756, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer correctly states that the movie was not released, meaning such a movie does not exist. The generated answer claims only that the movie is not mentioned in the context, which fails to answer the question and contradicts the gold implication that no such released movie exists. Therefore it is both incorrect and unfaithful to the gold answer.", "main_total_tokens": 3141, "main_prompt_tokens": 2503, "main_completion_tokens": 638, "judge_total_tokens": 500, "judge_prompt_tokens": 401, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "", "latency_ms": 4783.676385879517, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is empty, so it does not provide the correct movie and does not match the provided gold answer. Therefore it is neither correct nor faithful.", "main_total_tokens": 3523, "main_prompt_tokens": 2500, "main_completion_tokens": 1023, "judge_total_tokens": 452, "judge_prompt_tokens": 390, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The specific title of the work is not mentioned in the question, so the release year cannot be determined.\n\nThe context provides release years for several different titles, including:\n*   *...And the Third Year, He Resuscitated* (1980) [Doc 675]\n*   *1991: The Year Punk Broke* (1992) [Doc 13886]\n*   *1999* (1997) [Doc 19562]\n*   *2002* (2001) [Doc 25738]\n*   *1981* (2009) [Doc 45703]\n*   *1982* (2013) [Doc 62279]\n*   *1992* (201", "latency_ms": 4701.3585567474365, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The gold answer clearly indicates the release year is 1986, but the generated answer claims the year cannot be determined, so it is factually incorrect (score 0 for correctness). The answer is mostly faithful to the provided context in that it explains the ambiguity based on missing title information and cites various example titles from the context. However, it misses the fact that the correct year (1986) is given as the gold answer, so it is not fully faithful to the overall task setup (slightly reduced faithfulness).", "main_total_tokens": 3604, "main_prompt_tokens": 2582, "main_completion_tokens": 1022, "judge_total_tokens": 665, "judge_prompt_tokens": 527, "judge_completion_tokens": 138}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 292.96374320983887, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model said it did not know the answer. The correct rating (per the gold) is 8.466, so the answer is incorrect. However, the statement 'I don't know' is faithful to the model's uncertainty and does not invent information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 398, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The TV show about two comic book fanboys working in a secret hospital for superheroes is called *Mighty Med* [Doc 121125].", "latency_ms": 720.2322483062744, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the show as 'Mighty Med' and faithfully restates the gold answer without adding incorrect information.", "main_total_tokens": 2044, "main_prompt_tokens": 2011, "main_completion_tokens": 33, "judge_total_tokens": 477, "judge_prompt_tokens": 421, "judge_completion_tokens": 56}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "The seven prisoners escaped from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 1662.9669666290283, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the country of the prison from which seven prisoners escaped. The gold answer is Argentina. The generated answer states that the prisoners escaped from the Villa Devoto prison in Buenos Aires, Argentina, which correctly identifies the country as Argentina. The additional detail about the specific prison and city does not contradict the gold answer and remains consistent with it. Therefore, the response is both correct and faithful to the required information.", "main_total_tokens": 2411, "main_prompt_tokens": 2181, "main_completion_tokens": 230, "judge_total_tokens": 527, "judge_prompt_tokens": 412, "judge_completion_tokens": 115}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The apprentice, Li Nai, falls into a trap designed by Jiang Jie [Doc 135231].", "latency_ms": 1622.2317218780518, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The core answer is correct: the apprentice falls into Jiang Jie's trap, matching the gold answer. However, the generated response adds an unverified detail by naming the apprentice as Li Nai without clear support from the provided gold, slightly reducing faithfulness.", "main_total_tokens": 2636, "main_prompt_tokens": 2402, "main_completion_tokens": 234, "judge_total_tokens": 487, "judge_prompt_tokens": 404, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 623.3139038085938, "retrieval_recall": 0.0, "retrieval_mrr": 0.07692307692307693, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is numerically incorrect because the correct answer is 4, but the model output is 0. However, the model did not fabricate any reasoning or unsupported details; it simply produced the wrong number, so it is still faithful to its own (albeit incorrect) computation.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 385, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "37", "latency_ms": 264.6455764770508, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that there are no movies with both Bruce Lee and Steven Seagal, whereas the generated answer says 37. This is factually incorrect. It is also unfaithful to the gold answer, which clearly indicates zero shared movies and uncertainty, not a specific positive number.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 484, "judge_prompt_tokens": 395, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of *Tokyo Mater* (2008) is 6.3 [Doc 42750].", "latency_ms": 756.3726902008057, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (6.3) and does not introduce information beyond the query, so it is both correct and faithful to the provided gold answer.", "main_total_tokens": 2447, "main_prompt_tokens": 2420, "main_completion_tokens": 27, "judge_total_tokens": 471, "judge_prompt_tokens": 405, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The context describes several styles of racing, including:\n\n*   **Street Racing** [Doc 4192", "latency_ms": 5052.868127822876, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks specifically for the style of racing, with the gold answer being 'Drift'. The generated answer instead lists 'Street Racing' and references context without identifying drifting. This is factually incorrect relative to the gold answer and does not faithfully address the specific question.", "main_total_tokens": 3643, "main_prompt_tokens": 2621, "main_completion_tokens": 1022, "judge_total_tokens": 484, "judge_prompt_tokens": 399, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie (or TV show) about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 865.7641410827637, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the title as 'Confronting a Serial Killer.' It is fully supported by the provided information and does not introduce any unsupported claims.", "main_total_tokens": 2458, "main_prompt_tokens": 2403, "main_completion_tokens": 55, "judge_total_tokens": 526, "judge_prompt_tokens": 457, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of *Armageddon* (1998) is 7.838 [Doc 20603].", "latency_ms": 749.4256496429443, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and does not introduce any unsupported information. It is both correct and faithful.", "main_total_tokens": 2519, "main_prompt_tokens": 2489, "main_completion_tokens": 30, "judge_total_tokens": 469, "judge_prompt_tokens": 409, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "", "latency_ms": 5061.033248901367, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is empty, so it does not provide the correct movie title ('The Sixth Sense') and is not faithful to the provided gold answer.", "main_total_tokens": 3380, "main_prompt_tokens": 2357, "main_completion_tokens": 1023, "judge_total_tokens": 453, "judge_prompt_tokens": 392, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The movie *The Transporter* was released in 2002 [Doc 27318].", "latency_ms": 678.6434650421143, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer. It is fully supported by the provided citation, so it is both correct and faithful.", "main_total_tokens": 2581, "main_prompt_tokens": 2557, "main_completion_tokens": 24, "judge_total_tokens": 468, "judge_prompt_tokens": 399, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 movie in 'The Transporter' franchise is *The Transporter Refueled* [Doc 67500].", "latency_ms": 2710.529327392578, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2015 reboot title as 'The Transporter Refueled' and remains faithful to the provided information without adding contradictions.", "main_total_tokens": 3024, "main_prompt_tokens": 2528, "main_completion_tokens": 496, "judge_total_tokens": 489, "judge_prompt_tokens": 421, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The movie 'The Transporter Refueled' was released in 2015 [Doc 67500].", "latency_ms": 1639.9893760681152, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, which matches the gold answer. It does not introduce any contradictions or unsupported claims beyond the cited document reference, so it is both correct and faithful.", "main_total_tokens": 2874, "main_prompt_tokens": 2661, "main_completion_tokens": 213, "judge_total_tokens": 487, "judge_prompt_tokens": 406, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "Based strictly on the provided context, there is no romantic comedy listed in which Abraham Lincoln comes back from the dead. The documents mention films about Lincoln's life, assassination, or supernatural action/horror films where he fights vampires or zombies [Doc 54943, Doc 57213], but none fit the genre of \"romantic comedy\" involving his resurrection.", "latency_ms": 4559.675455093384, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there is no such romantic comedy in the provided context. The generated answer is faithful to the documents it cites and does not introduce unsupported claims. It aligns with the gold response in content and conclusion.", "main_total_tokens": 2961, "main_prompt_tokens": 2109, "main_completion_tokens": 852, "judge_total_tokens": 541, "judge_prompt_tokens": 462, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a mysterious manuscript supposedly written by Ernest Hemingway is *Havana Darkness* [Doc 90923].", "latency_ms": 2317.3348903656006, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'Havana Darkness,' which matches the gold answer. It stays faithful to the question and does not introduce conflicting or incorrect information; mentioning the manuscript as mysterious does not contradict the prompt and remains aligned with the likely source document.", "main_total_tokens": 2550, "main_prompt_tokens": 2163, "main_completion_tokens": 387, "judge_total_tokens": 502, "judge_prompt_tokens": 417, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "The TV show is *Doll & Em* [Doc 122259]. The plot states that \"Doll heads to Hollywood to be with her childhood best friend Em\" [Doc 122259].", "latency_ms": 810.3656768798828, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em' and directly reflects the information given in the question and implied supporting document. There are no contradictions or hallucinations, and the citation aligns with the described plot detail.", "main_total_tokens": 2439, "main_prompt_tokens": 2392, "main_completion_tokens": 47, "judge_total_tokens": 508, "judge_prompt_tokens": 430, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The 2007 movie is *Hallam Foe* [Doc 39389]. The plot description states that \"Hallam's talent for spying on people reveals his darkest fears... Driven to expose the true cause of his mother's death...\" [Doc 39389].", "latency_ms": 2771.907329559326, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Hallam Foe,' which matches the gold answer exactly. It also grounds the reasoning in the cited document, aligning with the question and not introducing unsupported information. Therefore, it is both correct and faithful to the source.", "main_total_tokens": 2864, "main_prompt_tokens": 2392, "main_completion_tokens": 472, "judge_total_tokens": 537, "judge_prompt_tokens": 453, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Based strictly on the provided context, there is no information stating whether a specific person's last name sounds like \"friend\" or \"foe.\"", "latency_ms": 3448.352575302124, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for an interpretation/judgment (answer: 'A foe'), but the generated response claims the context lacks information and does not provide the required answer. This is incorrect relative to the gold. However, the answer is faithful to its own stated reliance on the provided context, so faithfulness is high even though it fails the task.", "main_total_tokens": 3207, "main_prompt_tokens": 2517, "main_completion_tokens": 690, "judge_total_tokens": 514, "judge_prompt_tokens": 414, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "Sam, the main character in the 2017 TV show *Atypical*, is an 18-year-old [Doc 124583].", "latency_ms": 1680.082082748413, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that Sam is 18 years old, which matches the gold answer exactly. It does not introduce any contradictions or unsupported claims beyond an irrelevant document reference tag, so it is both correct and faithful to the gold.", "main_total_tokens": 2466, "main_prompt_tokens": 2205, "main_completion_tokens": 261, "judge_total_tokens": 493, "judge_prompt_tokens": 416, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam, the 18-year-old character in the 2017 TV show *Atypical*, is on the autism spectrum [Doc 124583].", "latency_ms": 1577.221393585205, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which matches the gold answer exactly. It does not introduce conflicting or unsupported information, only adds his age and a document reference that does not affect the core claim. Thus it is both correct and faithful.", "main_total_tokens": 2427, "main_prompt_tokens": 2186, "main_completion_tokens": 241, "judge_total_tokens": 502, "judge_prompt_tokens": 418, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Sandra was born in Chamonix [Doc 59659].", "latency_ms": 1637.5529766082764, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, which matches the gold answer. It is fully faithful to the source information cited.", "main_total_tokens": 2857, "main_prompt_tokens": 2629, "main_completion_tokens": 228, "judge_total_tokens": 471, "judge_prompt_tokens": 410, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481]", "latency_ms": 2021.9531059265137, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and adheres to the prompt without adding unsupported information.", "main_total_tokens": 2997, "main_prompt_tokens": 2712, "main_completion_tokens": 285, "judge_total_tokens": 452, "judge_prompt_tokens": 398, "judge_completion_tokens": 54}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Tabaluga is the crown-prince of Greenland [Doc 114810].", "latency_ms": 2384.714365005493, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which exactly matches the gold answer. It is both factually correct and faithful to the provided gold reference.", "main_total_tokens": 2748, "main_prompt_tokens": 2395, "main_completion_tokens": 353, "judge_total_tokens": 479, "judge_prompt_tokens": 410, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with that description is *One Afternoon* [Doc 96838].", "latency_ms": 3766.2668228149414, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('One Afternoon') and does not introduce any contradictions or unsupported claims beyond a harmless document reference.", "main_total_tokens": 2828, "main_prompt_tokens": 1996, "main_completion_tokens": 832, "judge_total_tokens": 478, "judge_prompt_tokens": 420, "judge_completion_tokens": 58}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The movie is *The Life Ahead* [Doc 93260].\n\nThe plot describes a Holocaust survivor in seaside Italy who \"takes in a 12-year-old street kid who recently robbed her\" [Doc 93260]. It is an Italian language movie [Doc 93260] with a rating of 6.746 [Doc 93260].", "latency_ms": 3823.6587047576904, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the name of the low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her. The gold answer is \"The Life Ahead.\" The generated answer explicitly identifies the movie as \"The Life Ahead\" and correctly mirrors the described plot and attributes (Italian language, the specific plot detail, and the rating information) consistent with the supporting document. There are no contradictions or hallucinations relative to the provided context, and all details align with the gold answer, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3154, "main_prompt_tokens": 2298, "main_completion_tokens": 856, "judge_total_tokens": 615, "judge_prompt_tokens": 469, "judge_completion_tokens": 146}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary “McLaren” is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 804.7561645507812, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the subject as New Zealander Bruce McLaren, founder of the McLaren motor racing team, which matches the gold answer. However, it references an external document ID ([Doc 73859]) that is not present in the provided context, making that specific citation unfaithful.", "main_total_tokens": 2291, "main_prompt_tokens": 2256, "main_completion_tokens": 35, "judge_total_tokens": 512, "judge_prompt_tokens": 417, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "", "latency_ms": 5033.755302429199, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is empty, so it does not provide the correct team name (The McLaren Motor Racing team) and does not address the question at all. Therefore it is neither correct nor faithful.", "main_total_tokens": 3449, "main_prompt_tokens": 2426, "main_completion_tokens": 1023, "judge_total_tokens": 455, "judge_prompt_tokens": 384, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "Based on the context provided, several individuals (\"he\" or \"a man\") have their country of origin mentioned:\n\n*   A man who migrates is from Poland [Doc 10348].\n*   Heinz Stücke is from Germany [Doc 100044].\n*   Karol Wojtyla (Pope John Paul II) is Polish [Doc 1545].\n*   Martin Rauch is an East Germany native [Doc 122771].", "latency_ms": 5001.446008682251, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.7, "justification": "The gold answer says the person is from New Zealand, but the generated answer does not mention New Zealand and instead lists multiple other people from Poland and Germany, so it is incorrect. It is partially faithful to the provided context in that it cites individuals and their origins that plausibly come from some documents, but it does not answer the specific question about the correct person and introduces multiple irrelevant candidates, so faithfulness is limited.", "main_total_tokens": 3111, "main_prompt_tokens": 2104, "main_completion_tokens": 1007, "judge_total_tokens": 583, "judge_prompt_tokens": 466, "judge_completion_tokens": 117}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based strictly on the context provided, the information that John McClane blows up the Great Wall of China in a 2013 movie is not found.\n\nThe 2013 movie featuring John McClane is *A Good Day to Die Hard* [Doc 58791]. However, the plot description states that John McClane travels to Moscow to help his son Jack and battles the Russian underworld [Doc 58791]. There is no mention of the Great Wall of China or its destruction in this document.", "latency_ms": 3666.5124893188477, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly states that there is no 2013 movie in which John McClane blows up the Great Wall of China and correctly identifies *A Good Day to Die Hard* as the 2013 McClane film whose plot does not involve the Great Wall. This aligns with the gold answer that such a movie is not in the database, so correctness is 1. However, the instruction says to base the answer strictly on the provided context, and the model introduces an explicit citation-style reference to [Doc 58791], which is not present in the original gold answer and may not strictly be part of the given context. This slight extrapolation reduces faithfulness somewhat, so faithfulness is scored at 0.8.", "main_total_tokens": 3336, "main_prompt_tokens": 2623, "main_completion_tokens": 713, "judge_total_tokens": 683, "judge_prompt_tokens": 502, "judge_completion_tokens": 181}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with that description is *Tomie: Forbidden Fruit* [Doc 28589].", "latency_ms": 3096.092939376831, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Tomie: Forbidden Fruit' and does not introduce any contradictory or extraneous information. It is fully correct and faithful.", "main_total_tokens": 3024, "main_prompt_tokens": 2386, "main_completion_tokens": 638, "judge_total_tokens": 506, "judge_prompt_tokens": 442, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina, in the 2021 movie *Lei mi parla ancora*, have been married for 65 years [Doc 98566].", "latency_ms": 744.687557220459, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years) and does not introduce any contradictions or extraneous claims beyond the referenced document, so it is both correct and faithful.", "main_total_tokens": 2837, "main_prompt_tokens": 2800, "main_completion_tokens": 37, "judge_total_tokens": 491, "judge_prompt_tokens": 425, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King and Tala search for a legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 1798.2728481292725, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that they search for The Book of Souls, matching the gold answer. Adding 'a legendary relic known as' is consistent with and does not contradict the gold, so it is both correct and faithful.", "main_total_tokens": 2926, "main_prompt_tokens": 2648, "main_completion_tokens": 278, "judge_total_tokens": 493, "judge_prompt_tokens": 417, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for the movie \"Zombies on the Moon\" is not available.", "latency_ms": 2110.4586124420166, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database; the generated answer correctly and faithfully restates that the release year is not available based on the provided context.", "main_total_tokens": 2856, "main_prompt_tokens": 2453, "main_completion_tokens": 403, "judge_total_tokens": 479, "judge_prompt_tokens": 415, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the provided context, the release year for \"The Galactic baker\" is not mentioned.", "latency_ms": 2567.774772644043, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, which means the release year is effectively unknown. The generated answer correctly states that the release year is not mentioned in the provided context and does not fabricate a year. It is therefore correct and faithful to the given information.", "main_total_tokens": 2836, "main_prompt_tokens": 2349, "main_completion_tokens": 487, "judge_total_tokens": 496, "judge_prompt_tokens": 409, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"Return of the Null Pointer\" was released is not mentioned in the provided context.", "latency_ms": 2177.89363861084, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database; the generated answer says the release year is not mentioned in the provided context. Both effectively state that the information is unavailable, which is correct and faithful to the gold response.", "main_total_tokens": 2385, "main_prompt_tokens": 2000, "main_completion_tokens": 385, "judge_total_tokens": 487, "judge_prompt_tokens": 410, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for the movie \"Data Science: The Movie\" is not available.", "latency_ms": 2731.5330505371094, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database. The generated answer says the release year is not available based on the provided context. This is correct given the gold and does not invent a year, and it faithfully reflects the limitation stated in the gold answer.", "main_total_tokens": 3103, "main_prompt_tokens": 2579, "main_completion_tokens": 524, "judge_total_tokens": 499, "judge_prompt_tokens": 415, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for the movie \"Python's Revenge 2\" is not available.", "latency_ms": 2866.598606109619, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold answer states the movie is not in the database. The generated answer correctly says the release year is not available based on the provided context, which matches the gold. No hallucinated year or extra unsupported info is given, so it is both correct and faithful to the context.", "main_total_tokens": 2835, "main_prompt_tokens": 2427, "main_completion_tokens": 408, "judge_total_tokens": 501, "judge_prompt_tokens": 415, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Last Pixel\" is not available in the provided context.", "latency_ms": 2326.7111778259277, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available/unknown, which is correct given the context and do not fabricate a date. The generated answer is faithful to the lack of information in the prompt.", "main_total_tokens": 2582, "main_prompt_tokens": 2126, "main_completion_tokens": 456, "judge_total_tokens": 481, "judge_prompt_tokens": 406, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the provided context, the release year for the title \"The man who ate the sun\" is not available.", "latency_ms": 2681.732177734375, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year is not available, which is consistent with the gold response that the movie is not in the database. It is both factually correct given the context and faithful to the information provided.", "main_total_tokens": 3356, "main_prompt_tokens": 2857, "main_completion_tokens": 499, "judge_total_tokens": 493, "judge_prompt_tokens": 417, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Based on the provided context, the director of the movie \"The Last Days of Lehman Brothers\" is not mentioned [Doc 46069].", "latency_ms": 815.4768943786621, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned in the provided context, matching the gold answer that the database does not provide the director information. It does not hallucinate a director name and remains faithful to the given information and citation.", "main_total_tokens": 2804, "main_prompt_tokens": 2773, "main_completion_tokens": 31, "judge_total_tokens": 505, "judge_prompt_tokens": 426, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not contain information about the movie \"A Love to Die For\" or its director.", "latency_ms": 2570.6796646118164, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not contain information about the director of the movie, which aligns with the gold answer that the database does not provide the director information. It does not introduce unsupported details and remains faithful to the given context.", "main_total_tokens": 3052, "main_prompt_tokens": 2559, "main_completion_tokens": 493, "judge_total_tokens": 496, "judge_prompt_tokens": 416, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Based strictly on the provided context, the director of the movie \"Journey into Gaza\" is not mentioned [Doc 111360].", "latency_ms": 2003.9010047912598, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned in the provided context and aligns with the gold answer that the information is not in the database. It does not introduce unsupported facts and is fully faithful to the given context.", "main_total_tokens": 2742, "main_prompt_tokens": 2430, "main_completion_tokens": 312, "judge_total_tokens": 495, "judge_prompt_tokens": 419, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not contain information about who directed the movie or TV show \"The Last Night at Tremore Beach\" [Doc 135098].", "latency_ms": 2116.666316986084, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not provide information about the director, which matches the gold response indicating that the director information is not in the database. It does not introduce any unsupported facts, so it is both correct and faithful to the given context.", "main_total_tokens": 2462, "main_prompt_tokens": 2139, "main_completion_tokens": 323, "judge_total_tokens": 510, "judge_prompt_tokens": 427, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is *Rise of the Mummy* [Doc 98375].", "latency_ms": 3848.236322402954, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Rise of the Mummy,' which matches the gold answer exactly. It is fully faithful to the given information and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 2904, "main_prompt_tokens": 2136, "main_completion_tokens": 768, "judge_total_tokens": 489, "judge_prompt_tokens": 418, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse and flees is *Stranger in My Bed* [Doc 35237].", "latency_ms": 811.2425804138184, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Stranger in My Bed' and remains faithful to the provided information without adding unsupported details.", "main_total_tokens": 2342, "main_prompt_tokens": 2308, "main_completion_tokens": 34, "judge_total_tokens": 492, "judge_prompt_tokens": 428, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The specific event described—A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years—is associated with the TV show *Game of Silence* [Doc 123938].\n\nNo document identifies a movie with this exact plot description.", "latency_ms": 4689.775466918945, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The question asks for a movie title, and the gold answer is 'Game of Silence.' The generated answer correctly links the description to 'Game of Silence' but incorrectly calls it a TV show and then claims that no document identifies a movie with that plot. Since the task is to give the movie name, the answer is effectively incorrect with respect to the evaluation task. However, it remains largely faithful to the cited document content (it accurately reports that the document associates the plot with a TV show and that no document mentions a movie), so faithfulness is high though not perfect because it overgeneralizes slightly by implying definitiveness across all documents.", "main_total_tokens": 2816, "main_prompt_tokens": 1870, "main_completion_tokens": 946, "judge_total_tokens": 608, "judge_prompt_tokens": 447, "judge_completion_tokens": 161}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie is *The Craigslist Killer* [Doc 53039].\n\nThe description states that the movie is \"Based on a true story\" about Philip Markoff, who \"abuses prostitutes he finds via Craigslist\" [Doc 53039].", "latency_ms": 3287.473440170288, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the description about a killer from Craigslist. The gold answer is 'The Craigslist Killer'. The generated answer also identifies the movie as 'The Craigslist Killer' and cites details consistent with the description. There are no contradictions or hallucinations relative to the provided information, so the response is fully correct and faithful.", "main_total_tokens": 2862, "main_prompt_tokens": 2241, "main_completion_tokens": 621, "judge_total_tokens": 538, "judge_prompt_tokens": 441, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which this happens is *Wood Job!* [Doc 66094].\n\nThe plot states: \"After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.\" [Doc 66094].", "latency_ms": 800.973653793335, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Wood Job!' and accurately restates the plot detail from the provided document. It is both factually correct and faithful to the given source.", "main_total_tokens": 2586, "main_prompt_tokens": 2529, "main_completion_tokens": 57, "judge_total_tokens": 518, "judge_prompt_tokens": 450, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure, is *Sesame Street: The Magical Wand Chase* [Doc 78373].", "latency_ms": 783.6151123046875, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Sesame Street: The Magical Wand Chase' and accurately restates the query. It is fully correct and faithful to the given gold answer.", "main_total_tokens": 2363, "main_prompt_tokens": 2308, "main_completion_tokens": 55, "judge_total_tokens": 528, "judge_prompt_tokens": 460, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which this happens is *Beyond* [Doc 74597].\n\nThe description states: \"When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives\" [Doc 74597].", "latency_ms": 818.4819221496582, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Beyond,' which matches the gold answer. It faithfully restates the given description without adding unsupported details, fully grounded in the provided text.", "main_total_tokens": 2011, "main_prompt_tokens": 1945, "main_completion_tokens": 66, "judge_total_tokens": 533, "judge_prompt_tokens": 466, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman, fired from a financial corporation during the Asia crisis, returns home with no money is *6ixtynin9* [Doc 22478].", "latency_ms": 2599.6930599212646, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', which matches the gold answer. It stays faithful to the given information and does not introduce unsupported details beyond rephrasing the question and citing a document.", "main_total_tokens": 2475, "main_prompt_tokens": 2005, "main_completion_tokens": 470, "judge_total_tokens": 516, "judge_prompt_tokens": 439, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie in which a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort, forcing the staff to stop the violence or hide it, is *Club Dread* [Doc 32410].", "latency_ms": 851.5393733978271, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' matching the gold answer exactly and staying faithful to the provided information without adding incorrect details.", "main_total_tokens": 2393, "main_prompt_tokens": 2346, "main_completion_tokens": 47, "judge_total_tokens": 529, "judge_prompt_tokens": 468, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which this happens is *Toilet* [Doc 48873].", "latency_ms": 3652.1637439727783, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" matching the gold answer exactly. It stays fully consistent with the provided information without adding unsupported details.", "main_total_tokens": 3424, "main_prompt_tokens": 2651, "main_completion_tokens": 773, "judge_total_tokens": 493, "judge_prompt_tokens": 431, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie in which this happens is *Electric Angel* [Doc 1024].\n\nThe description states: \"A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century\" [Doc 1024].", "latency_ms": 925.2707958221436, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Electric Angel') and does not introduce contradictory or extraneous information. It is correct and faithful to the provided gold.", "main_total_tokens": 2353, "main_prompt_tokens": 2283, "main_completion_tokens": 70, "judge_total_tokens": 545, "judge_prompt_tokens": 482, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 listed in the context is *Down Low*, with a rating of 5.207 [Doc 106995].", "latency_ms": 3270.625114440918, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the lowest rated movie from 2023 is 'Mega Ape', but the generated answer claims it is 'Down Low' with a 5.207 rating. This directly contradicts the gold answer, so the generated response is incorrect. However, the generated answer appears internally consistent and grounded in its cited context (Doc 106995), so it is faithful to its own provided evidence, even though that evidence conflicts with the gold label.", "main_total_tokens": 3055, "main_prompt_tokens": 2212, "main_completion_tokens": 843, "judge_total_tokens": 535, "judge_prompt_tokens": 412, "judge_completion_tokens": 123}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 385.00261306762695, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a particular show ('Tea House') as the highest rated TV show from 2010, while the generated answer says 'I don't know.' This is factually incorrect relative to the labeled gold. However, the generated answer does not fabricate any information and honestly admits lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 488, "judge_prompt_tokens": 386, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 592.1797752380371, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct movie title ('The Uncut Story of Mewtwo's Origin'). Therefore it is incorrect. However, it does not invent any false information and honestly expresses uncertainty, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 395, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 listed in the context is *Don't Answer the Phone!*, with a rating of 4.6 [Doc 768]. The language of this movie is English (en) [Doc 768].", "latency_ms": 3731.665849685669, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980. The gold answer is 'English.' The generated answer identifies the lowest-rated movie as 'Don't Answer the Phone!' from 1980 and states that its language is English, matching the gold answer. The reasoning is consistent with the provided context reference [Doc 768], and no unsupported information is introduced. Thus, the answer is both correct and faithful to the context.", "main_total_tokens": 3378, "main_prompt_tokens": 2543, "main_completion_tokens": 835, "judge_total_tokens": 551, "judge_prompt_tokens": 431, "judge_completion_tokens": 120}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show from 1999 with the highest popularity score in the provided context is *Freaks and Geeks*, with a popularity score of 5.8043 [Doc 115187].", "latency_ms": 4092.8690433502197, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold answer states that the TV show is 'Law & Order: Special Victims Unit', while the generated answer claims it is 'Freaks and Geeks'. Additionally, it cites a specific popularity score and document ID that conflict with the gold answer, indicating it is not faithful to the reference answer.", "main_total_tokens": 3457, "main_prompt_tokens": 2570, "main_completion_tokens": 887, "judge_total_tokens": 530, "judge_prompt_tokens": 431, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 780.7683944702148, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and attributes it to a document, remaining faithful to the provided information.", "main_total_tokens": 2450, "main_prompt_tokens": 2429, "main_completion_tokens": 21, "judge_total_tokens": 458, "judge_prompt_tokens": 400, "judge_completion_tokens": 58}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in multiple years, according to the context:\n\n*   1999 [Doc 22481]\n*   2019 [Doc 88811]\n*   2025 [Doc 112077]", "latency_ms": 2464.810609817505, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a single correct year, 2019. The generated answer lists multiple years (1999, 2019, 2025) instead of selecting the correct one, so it is incorrect with respect to the question. However, it appears internally consistent with its referenced context (it attributes each year to a different document), so it is faithful to its own stated context, even though it fails the task requirement.", "main_total_tokens": 2877, "main_prompt_tokens": 2414, "main_completion_tokens": 463, "judge_total_tokens": 553, "judge_prompt_tokens": 436, "judge_completion_tokens": 117}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "2", "latency_ms": 257.4443817138672, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 279 votes, but the generated answer was 2, which is incorrect. There is no indication the model attempted to follow the gold answer, so it is neither correct nor faithful to the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 458, "judge_prompt_tokens": 383, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "11", "latency_ms": 167.4022674560547, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer reports 11 votes, but the gold answer is 12 votes, so the numeric value is incorrect. However, the answer is on-topic and clearly attempts to answer the question about how many votes the item received, so it is faithful to the query.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 468, "judge_prompt_tokens": 383, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 727.6496887207031, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'The Last Days of Lehman Brothers' was released in 2009, which matches the gold answer. It is fully consistent with the provided information and does not introduce contradictions or unsupported claims.", "main_total_tokens": 2771, "main_prompt_tokens": 2744, "main_completion_tokens": 27, "judge_total_tokens": 488, "judge_prompt_tokens": 410, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English (en) [Doc 48035].", "latency_ms": 735.8896732330322, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The movie \"The King's Speech\" is indeed originally in English, matching the gold answer \"en\" and correctly reflecting the provided generation.", "main_total_tokens": 2475, "main_prompt_tokens": 2448, "main_completion_tokens": 27, "judge_total_tokens": 463, "judge_prompt_tokens": 405, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The movie *Wall Street* was released in 1987 [Doc 6993].", "latency_ms": 3025.073766708374, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'Wall Street' was released in 1987, matching the gold answer exactly. The reference to a document does not contradict the question and appears to be a source citation, so the answer is fully faithful to the provided information.", "main_total_tokens": 3417, "main_prompt_tokens": 2972, "main_completion_tokens": 445, "judge_total_tokens": 481, "judge_prompt_tokens": 396, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Brain Fluid Explosion Girl", "latency_ms": 8070.6610679626465, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'Brain Fluid Explosion Girl'. It is both correct and faithful.", "main_total_tokens": 4284, "main_prompt_tokens": 2651, "main_completion_tokens": 1633, "judge_total_tokens": 450, "judge_prompt_tokens": 392, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 5814.471244812012, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Dungeons & Dragons'), which is the correct TV show where six friends rely on a Dungeon Master to get home. It is both correct and fully faithful to the reference.", "main_total_tokens": 3716, "main_prompt_tokens": 2642, "main_completion_tokens": 1074, "judge_total_tokens": 468, "judge_prompt_tokens": 394, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 4699.354887008667, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the movie as 'Nacht der Wölfe.'", "main_total_tokens": 3409, "main_prompt_tokens": 2663, "main_completion_tokens": 746, "judge_total_tokens": 456, "judge_prompt_tokens": 403, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of *Picnic at Hanging Rock* take place in 1900 [Doc 126191].", "latency_ms": 4178.937196731567, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of 'Picnic at Hanging Rock' take place in 1900, which matches the gold answer. It is also faithful, directly answering the question without adding unsupported information.", "main_total_tokens": 3674, "main_prompt_tokens": 2955, "main_completion_tokens": 719, "judge_total_tokens": 481, "judge_prompt_tokens": 407, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "", "latency_ms": 5676.695346832275, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title of a 2004 movie about a young violinist named after a calendar month; the gold answer is 'June'. Since the model output is empty, it does not provide an answer and thus cannot be considered correct. However, the evaluation here is about the gold vs the question, and 'June' is correct and faithful to the described movie. Given the required scoring of the generated answer, which is blank, correctness and faithfulness relative to the gold both effectively fail.", "main_total_tokens": 1141, "main_prompt_tokens": 118, "main_completion_tokens": 1023, "judge_total_tokens": 565, "judge_prompt_tokens": 433, "judge_completion_tokens": 132}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 319.3826675415039, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is known (The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger), but the model responded with 'I don't know,' which is incorrect. However, the model did not hallucinate or provide false information; it admitted uncertainty, so it is fully faithful to its own knowledge state.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 399, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 337.70155906677246, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 7.6, but the generated answer says 'I don't know,' which is incorrect. However, it does not fabricate any information and accurately reflects uncertainty, so it is fully faithful while being factually incorrect.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 467, "judge_prompt_tokens": 389, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 4613.942384719849, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.366123019050001, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The gold answer indicates there are 4 such movies, while the generated answer claims none exist, so it is factually incorrect. The model at least stays within the scope of the question and does not fabricate specific movie details, but its conclusion contradicts the gold label, so correctness is 0. Faithfulness is slightly higher because it follows the query constraints and admits inability rather than hallucinating details.", "main_total_tokens": 2590, "main_prompt_tokens": 1567, "main_completion_tokens": 1023, "judge_total_tokens": 518, "judge_prompt_tokens": 405, "judge_completion_tokens": 113}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 325.26564598083496, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer claims there are none and that it cannot find any, so it is factually incorrect. It is, however, internally consistent and does not contradict its own prior statements, so it is faithful to the (albeit wrong) reasoning it presents.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 495, "judge_prompt_tokens": 404, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 296.1103916168213, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific show, and the correct answer is 'Fantômas'. The generated answer says 'I don't know', which is factually incorrect, so correctness is 0. However, it is faithful to the model's uncertainty without fabricating information, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 389, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise [Doc 11205]", "latency_ms": 5380.622386932373, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Kill Cruise') and does not introduce any extraneous or conflicting information. It is both correct and faithful.", "main_total_tokens": 3863, "main_prompt_tokens": 2812, "main_completion_tokens": 1051, "judge_total_tokens": 454, "judge_prompt_tokens": 393, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor? [Doc 84838]", "latency_ms": 6862.565994262695, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, including punctuation. The bracketed document reference does not alter the correctness or contradict the provided information, so the response is both correct and faithful.", "main_total_tokens": 4501, "main_prompt_tokens": 3199, "main_completion_tokens": 1302, "judge_total_tokens": 482, "judge_prompt_tokens": 414, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The release years for the titles provided in the context are:\n\n*   Won't You Be My Neighbor?: 2018 [Doc 84838]\n*   I Am Not Your Friend: 2009 [Doc 47109]\n*   I Don't Want to Be Friends with You: 2020 [Doc 129223]\n*   I'll Be Next Door for Christmas: 2018 [Doc 85978]\n*   The Neighbor: 2007 [Doc 40654], 20", "latency_ms": 7098.497629165649, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a single year of release, with the gold answer 2018. The generated answer instead lists multiple titles and years and truncates the final entry, never clearly identifying which one answers the original question. It does not provide a clear, correct single year matching the gold answer, and it also fabricates context by expanding beyond what is needed and ending mid-sentence, so it is neither correct nor fully faithful.", "main_total_tokens": 4590, "main_prompt_tokens": 3189, "main_completion_tokens": 1401, "judge_total_tokens": 595, "judge_prompt_tokens": 478, "judge_completion_tokens": 117}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy [Doc 112591]", "latency_ms": 3906.0819149017334, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, correctly identifying 'Astro Boy' as the animated TV series about a young boy robot in 2030 AD. There is no hallucinated or conflicting information; the bracketed citation does not alter the answer, so the response is both correct and faithful.", "main_total_tokens": 3548, "main_prompt_tokens": 2906, "main_completion_tokens": 642, "judge_total_tokens": 486, "judge_prompt_tokens": 395, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "The woman and her young son were held captive for 7 years [Doc 67513].", "latency_ms": 3976.682424545288, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer exactly. It is fully consistent and faithful to the provided information.", "main_total_tokens": 3263, "main_prompt_tokens": 2539, "main_completion_tokens": 724, "judge_total_tokens": 481, "judge_prompt_tokens": 413, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The gold bullion stumbled across at the Brink's-Mat security depot on 26 November 1983 was worth £26m [Doc 133469].", "latency_ms": 1729.1007041931152, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "User answer (Gold: £26m) matches the generated statement that the gold bullion was worth £26m. The generated answer does not introduce contradictory or extraneous claims and stays faithful to the provided fact.", "main_total_tokens": 2507, "main_prompt_tokens": 2268, "main_completion_tokens": 239, "judge_total_tokens": 499, "judge_prompt_tokens": 426, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 554.0452003479004, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score (80.5772). The generated answer says, 'I don't know,' which is incorrect. However, the answer does not invent any facts or contradict the question; it simply expresses ignorance, so it is fully faithful to the prompt and does not hallucinate.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 489, "judge_prompt_tokens": 396, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 349.7731685638428, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 Russian language TV show released in 1997, while the generated answer claims there are none matching the criteria. This is factually incorrect, so correctness is 0. However, the model does not hallucinate a specific incorrect number; it honestly admits inability to find a match, so with respect to its own stated uncertainty it remains faithful to its internal search, yielding high faithfulness.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 510, "judge_prompt_tokens": 394, "judge_completion_tokens": 116}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes [Doc 121928]", "latency_ms": 8067.021369934082, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, correctly identifying 'Angel Eyes' as the TV show. It is fully supported by the provided information and does not introduce any unsupported content, so both correctness and faithfulness are high.", "main_total_tokens": 4826, "main_prompt_tokens": 3230, "main_completion_tokens": 1596, "judge_total_tokens": 479, "judge_prompt_tokens": 403, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The context provides information for several movies. The languages associated with these movies are:\n\n*   English (en) [Doc 14842, Doc 25900, Doc", "latency_ms": 7626.389980316162, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a single movie language (Korean), but the generated answer lists multiple English-language documents and never states Korean. It is therefore incorrect. It also is unfaithful to the gold answer because it does not mention Korean at all and instead introduces unrelated document-language mappings.", "main_total_tokens": 4591, "main_prompt_tokens": 3163, "main_completion_tokens": 1428, "judge_total_tokens": 493, "judge_prompt_tokens": 410, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 649.2817401885986, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer claims there are no such movies, which is incorrect. The model is faithful to its own stated inability but factually wrong compared to the gold label.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 403, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Based strictly on the provided context, there are no movies listed with a release year of 1979.", "latency_ms": 5977.298021316528, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer in substance: it correctly states that, based on the given data/context, there are no movies from 1979 available, so a best-rated movie from that year cannot be identified. It does not introduce external information and is fully faithful to the provided context.", "main_total_tokens": 3968, "main_prompt_tokens": 2851, "main_completion_tokens": 1117, "judge_total_tokens": 506, "judge_prompt_tokens": 416, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The context provided does not contain information regarding a movie titled \"Back to the Future part IV.\" The documents mention \"Back to the Future\" [Doc 4834], \"Back to the Future Part II\" [Doc 9405], and \"Back to the Future Part III\" [Doc 10621], and refer to the series as a \"trilogy\" [Doc 113788, Doc 103759].", "latency_ms": 6423.780202865601, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer correctly states that Back to the Future Part IV does not exist. The generated answer instead focuses on the absence of information in the provided context and lists documents mentioning only the existing trilogy, but it never explicitly answers the user’s question or clearly states that Part IV does not exist or has no release year. Therefore, it is not correct with respect to the user’s query. However, it remains faithful to the given context by accurately describing what the documents contain and not hallucinating a release year or a non-existent film.", "main_total_tokens": 4877, "main_prompt_tokens": 3730, "main_completion_tokens": 1147, "judge_total_tokens": 623, "judge_prompt_tokens": 486, "judge_completion_tokens": 137}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly indicate that there are no movies from 1980 about the Chernobyl disaster, which is accurate because the Chernobyl disaster occurred in 1986. The generated answer is faithful to the question and does not introduce fabricated data; it simply restates the impossibility and suggests rechecking the premise. While phrasing differs, the substantive content matches the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 545, "judge_prompt_tokens": 434, "judge_completion_tokens": 111}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 283.5674285888672, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is factually incorrect because there are indeed multiple TV shows or movies with 'Astro Boy' in the title, and the gold label indicates there are 4. It is also unfaithful to the provided gold answer, which specifies a clear numeric response, whereas the generated answer claims none can be found.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 488, "judge_prompt_tokens": 397, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 5017.281532287598, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer says there are none, so it is factually incorrect relative to the labeled gold. However, given the model’s knowledge cutoff predates 2025 and such a specific count is not knowable from its training data, refusing to fabricate and stating it cannot find any is faithful to its knowledge and limitations.", "main_total_tokens": 4635, "main_prompt_tokens": 3612, "main_completion_tokens": 1023, "judge_total_tokens": 498, "judge_prompt_tokens": 395, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 295.2616214752197, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "Gold states 1 TV show; generated answer claims none and refers to movies or shows. This is factually incorrect relative to the gold. It also contradicts the provided gold label, so it is unfaithful to the given reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 477, "judge_prompt_tokens": 398, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 302.98900604248047, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold answer says 0 shows; generated answer effectively says none exist, which is equivalent. It is correct and aligns with the gold, so both correctness and faithfulness are high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 467, "judge_prompt_tokens": 400, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 231.49347305297852, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Naruto,' a very popular ninja-themed series. The generated answer says 'I don't know,' which is factually incorrect for the question. However, it does not fabricate any information; it simply expresses a lack of knowledge, so it is fully faithful to the model's own output.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 479, "judge_prompt_tokens": 386, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The context does not contain information about a Jurassic Park/Power Ranger crossover movie.", "latency_ms": 4925.616264343262, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the movie was not released. The generated answer instead claims the context lacks information about such a crossover, which does not correctly respond to the question. However, it is faithful to the given context because it does not invent details and accurately reflects that the context has no such information.", "main_total_tokens": 3964, "main_prompt_tokens": 3072, "main_completion_tokens": 892, "judge_total_tokens": 492, "judge_prompt_tokens": 401, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Based strictly on the context provided, there is no movie listed in which Barbie fights the Predator", "latency_ms": 10362.473726272583, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly implies that no such movie exists, aligning with the gold answer that the movie was not released. However, it references 'the context provided' and talks about 'no movie listed,' which slightly shifts the focus from the factual status (unreleased/nonexistent) to an absence in a list, making it slightly less directly faithful to the gold phrasing.", "main_total_tokens": 5197, "main_prompt_tokens": 3229, "main_completion_tokens": 1968, "judge_total_tokens": 507, "judge_prompt_tokens": 400, "judge_completion_tokens": 107}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The 4th oldest Star Wars movie listed in the context is When Star Wars Ruled the World [Doc 32997].\n\n(The chronological order of Star Wars movies provided is:\n1. Return of the Jedi (1983) [Doc 2858]\n2. Empire of Dreams: The Story of the Star Wars Trilogy (2004) [Doc 32312]\n3. The Story of Star Wars (2004) [Doc 32101]\n4. When Star Wars Ruled the World (2004) [Doc 32997])", "latency_ms": 7509.913921356201, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The question asks for the 4th oldest Star Wars movie; the gold label says this is 'Star Wars: Droids - The Great Heep', but the generated answer instead gives 'When Star Wars Ruled the World', so it is incorrect with respect to the gold. However, the answer is internally consistent with and derived from the provided context list and ordering, so it is largely faithful to the context, just misaligned with the gold answer.", "main_total_tokens": 4559, "main_prompt_tokens": 3064, "main_completion_tokens": 1495, "judge_total_tokens": 624, "judge_prompt_tokens": 500, "judge_completion_tokens": 124}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The release years mentioned in the context are:\n\n*   1999 (for *From Star Wars to Star Wars: The Story of Industrial Light & Magic*) [Doc 23388]\n*   2004 (for *When Star Wars Ruled the World* and *Empire of Dreams: The Story of the Star Wars Trilogy*) [Doc 32997, Doc 32312]\n*   2009 (for *Fanboys*) [Doc 45290]\n*   2014 (for *Star Wars Rebels* and *Plastic Galaxy: The Story of Star Wars Toys*) [Doc 121736, Doc 65897]\n*   2015 (for *Star Wars: The Force Awakens*) [Doc 67357]\n*   2019 (for *Star Wars Roll Out* and *Apocalypse: War of Worlds (1945-1991)*) [", "latency_ms": 7145.360231399536, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.2, "justification": "The question expects a single year answer (1986). The generated answer instead lists multiple different release years from some context and never provides 1986, so it is incorrect. It is also not faithful to the specified gold answer; while it may be faithful to some separate context, with respect to the given Q/A pair it fails to answer the question and conflicts with the gold year.", "main_total_tokens": 4446, "main_prompt_tokens": 3105, "main_completion_tokens": 1341, "judge_total_tokens": 670, "judge_prompt_tokens": 560, "judge_completion_tokens": 110}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 309.30256843566895, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the rating of the movie Forrest Gump with a specific gold answer of 8.466. The generated answer says 'I don't know,' which does not provide an incorrect fact; it simply expresses lack of knowledge. Therefore, the generated answer is not factually wrong, but it is unhelpful relative to the task. For correctness relative to factual content, this is acceptable because it avoids hallucination. It is also faithful to the (implied) model behavior of not guessing when it does not know.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 535, "judge_prompt_tokens": 398, "judge_completion_tokens": 137}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med [Doc 121125].", "latency_ms": 6520.317077636719, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the name of the TV show, which is 'Mighty Med'. The generated answer matches the gold answer exactly aside from an irrelevant citation tag, so it is correct and faithful.", "main_total_tokens": 3914, "main_prompt_tokens": 2619, "main_completion_tokens": 1295, "judge_total_tokens": 471, "judge_prompt_tokens": 400, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina [Doc 53582].", "latency_ms": 4135.699033737183, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: both state that the prison was in Argentina. The citation does not contradict the answer, so the response is both correct and faithful.", "main_total_tokens": 3565, "main_prompt_tokens": 2812, "main_completion_tokens": 753, "judge_total_tokens": 464, "judge_prompt_tokens": 398, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "Jiang Jie [Doc 135231].", "latency_ms": 5517.078399658203, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: 'Jiang Jie'. It is correct and fully supported by the provided gold information, with no hallucinated content beyond the answer itself.", "main_total_tokens": 3949, "main_prompt_tokens": 2997, "main_completion_tokens": 952, "judge_total_tokens": 459, "judge_prompt_tokens": 392, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 5010.0672245025635, "retrieval_recall": 0.0, "retrieval_mrr": 0.07692307692307693, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The question has a clear, numeric gold answer (4). The generated answer claims there are no matching movies, so it is factually incorrect relative to the gold. In terms of faithfulness, the answer does not hallucinate any movies or extra details and explicitly acknowledges inability to find matches, which is consistent with a limited-information behavior, so it is mostly faithful to its (implicit) reasoning but still wrong on the task.", "main_total_tokens": 4782, "main_prompt_tokens": 3759, "main_completion_tokens": 1023, "judge_total_tokens": 512, "judge_prompt_tokens": 394, "judge_completion_tokens": 118}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 4597.566366195679, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer is correct: there are no movies featuring both Bruce Lee and Steven Seagal (and it even generalizes to shows). It is faithful to the gold answer, conveying the same core information that no such movies exist.", "main_total_tokens": 3446, "main_prompt_tokens": 2423, "main_completion_tokens": 1023, "judge_total_tokens": 483, "judge_prompt_tokens": 406, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 794.1358089447021, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3 for Tokyo Mater (2008) and clearly attributes the value, so it is both correct and faithful.", "main_total_tokens": 2458, "main_prompt_tokens": 2433, "main_completion_tokens": 25, "judge_total_tokens": 468, "judge_prompt_tokens": 403, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "", "latency_ms": 9855.839729309082, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "No generated answer was provided to evaluate, so it cannot be considered correct or faithful to the gold answer.", "main_total_tokens": 5231, "main_prompt_tokens": 3362, "main_completion_tokens": 1869, "judge_total_tokens": 431, "judge_prompt_tokens": 379, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer [Doc 131298]", "latency_ms": 4352.910041809082, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the title as 'Confronting a Serial Killer', so it is fully correct. However, it appends an extraneous bracketed reference '[Doc 131298]' that is not present in the gold answer and appears to come from outside the user-provided information, reducing faithfulness under the requirement to avoid ungrounded additions.", "main_total_tokens": 3716, "main_prompt_tokens": 3021, "main_completion_tokens": 695, "judge_total_tokens": 521, "judge_prompt_tokens": 418, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838 [Doc 20603]", "latency_ms": 1252.0322799682617, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold value (7.838) with no discrepancies or additions, so it is both correct and faithful to the provided reference.", "main_total_tokens": 2643, "main_prompt_tokens": 2502, "main_completion_tokens": 141, "judge_total_tokens": 457, "judge_prompt_tokens": 395, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "Victim [Doc 22488]", "latency_ms": 6978.119850158691, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct title of the 1999 psychological thriller that alludes to an extra sense is 'The Sixth Sense.' The generated answer 'Victim' is incorrect and does not relate to the provided question or gold answer, so it is neither correct nor faithful.", "main_total_tokens": 4266, "main_prompt_tokens": 2942, "main_completion_tokens": 1324, "judge_total_tokens": 482, "judge_prompt_tokens": 399, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002 [Doc 27318]", "latency_ms": 2542.1297550201416, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter' was released in 2002, matching the gold answer exactly and staying faithful to the information provided.", "main_total_tokens": 3505, "main_prompt_tokens": 3157, "main_completion_tokens": 348, "judge_total_tokens": 451, "judge_prompt_tokens": 389, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled [Doc 67500]", "latency_ms": 5915.910482406616, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly giving the title 'The Transporter Refueled' without adding any contradictory or extraneous content. The bracketed document reference does not alter the correctness or faithfulness.", "main_total_tokens": 4249, "main_prompt_tokens": 3136, "main_completion_tokens": 1113, "judge_total_tokens": 478, "judge_prompt_tokens": 403, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015 [Doc 67500]", "latency_ms": 3497.063636779785, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It does not introduce any contradictions or unsupported claims, so it is both correct and faithful.", "main_total_tokens": 3835, "main_prompt_tokens": 3271, "main_completion_tokens": 564, "judge_total_tokens": 468, "judge_prompt_tokens": 393, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "Based strictly on the context provided, there is no romantic comedy listed in which Abraham Lincoln comes back from the dead. The documents related to Abraham Lincoln are categorized as Documentary, Drama, History, Action, Fantasy, Horror, or Thriller [Doc 60175, Doc 55023, Doc 54943, Doc 57213].", "latency_ms": 7149.732828140259, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that, based on the provided context, there is no romantic comedy where Abraham Lincoln comes back from the dead, which matches the gold answer. It is faithful to the given documents it references and does not introduce unsupported claims.", "main_total_tokens": 4165, "main_prompt_tokens": 2701, "main_completion_tokens": 1464, "judge_total_tokens": 537, "judge_prompt_tokens": 457, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Havana Darkness [Doc 90923]", "latency_ms": 2899.3515968322754, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer provides the correct movie title, 'Havana Darkness', which matches the gold answer exactly, so correctness is 1. However, it adds an extraneous bracketed document reference '[Doc 90923]' not present in the gold answer, so faithfulness is slightly reduced.", "main_total_tokens": 3123, "main_prompt_tokens": 2732, "main_completion_tokens": 391, "judge_total_tokens": 487, "judge_prompt_tokens": 396, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 3812.5834465026855, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'Doll & Em' as the TV show where Doll heads to Hollywood to be with her childhood best friend Em. It is both correct and faithful to the reference.", "main_total_tokens": 3539, "main_prompt_tokens": 3035, "main_completion_tokens": 504, "judge_total_tokens": 468, "judge_prompt_tokens": 392, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam Foe [Doc 39389].", "latency_ms": 4867.262601852417, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Hallam Foe,' which matches the gold answer exactly. The added bracketed reference does not change the correctness or contradict the question, so the response is fully correct and faithful.", "main_total_tokens": 3962, "main_prompt_tokens": 3035, "main_completion_tokens": 927, "judge_total_tokens": 485, "judge_prompt_tokens": 409, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Based on the title *Hallam Foe*, his last name sounds like a foe [Doc 39389].", "latency_ms": 6908.299684524536, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.7, "justification": "The question asks if his last name sounds like friend or foe; the gold answer is simply 'A foe'. The generated answer says the last name sounds like a foe, which is correct. However, it adds a specific reference to the title 'Hallam Foe' and a document ID that are not mentioned in the gold, making the explanation partially speculative relative to the provided information.", "main_total_tokens": 4072, "main_prompt_tokens": 2919, "main_completion_tokens": 1153, "judge_total_tokens": 519, "judge_prompt_tokens": 409, "judge_completion_tokens": 110}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "Sam is 18 years old [Doc 124583].", "latency_ms": 4103.354930877686, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old, which matches the gold answer. There are no contradictions or extraneous claims beyond a harmless document citation, so the response is both correct and faithful.", "main_total_tokens": 3446, "main_prompt_tokens": 2775, "main_completion_tokens": 671, "judge_total_tokens": 472, "judge_prompt_tokens": 399, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam, the 18-year-old character in the 2017 TV show *Atypical*, is on the autism spectrum [Doc 124583].", "latency_ms": 3428.100109100342, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, matching the gold answer exactly. It stays faithful to the provided information and only adds accurate, non-contradictory context (his age and show details).", "main_total_tokens": 3376, "main_prompt_tokens": 2794, "main_completion_tokens": 582, "judge_total_tokens": 494, "judge_prompt_tokens": 418, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix [Doc 59659].", "latency_ms": 4533.297538757324, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly states that Sandra was born in Chamonix, matching the gold answer. However, it adds an unsupported citation-like bracketed note '[Doc 59659]', which is not grounded in the user-provided information, reducing faithfulness to the given context.", "main_total_tokens": 4006, "main_prompt_tokens": 3253, "main_completion_tokens": 753, "judge_total_tokens": 492, "judge_prompt_tokens": 406, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481]", "latency_ms": 4427.691221237183, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and aligns with the given information, so it is both correct and faithful.", "main_total_tokens": 4169, "main_prompt_tokens": 3350, "main_completion_tokens": 819, "judge_total_tokens": 456, "judge_prompt_tokens": 398, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland [Doc 114810].", "latency_ms": 5860.186338424683, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks where Tabaluga is crown prince of. Gold answer is 'Greenland'. The generated answer is 'Greenland [Doc 114810]', which contains the correct content with an extraneous citation marker. Semantically the location is correct and aligns with the gold, and it is faithful to the provided information.", "main_total_tokens": 4018, "main_prompt_tokens": 3005, "main_completion_tokens": 1013, "judge_total_tokens": 497, "judge_prompt_tokens": 401, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon [Doc 96838].", "latency_ms": 7333.877801895142, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, only adding a document identifier in brackets. The identified movie title is correct and the addition does not contradict the gold, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 4006, "main_prompt_tokens": 2605, "main_completion_tokens": 1401, "judge_total_tokens": 485, "judge_prompt_tokens": 412, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The Life Ahead [Doc 93260]", "latency_ms": 6046.017169952393, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('The Life Ahead') and fully addresses the question, so it is both correct and faithful.", "main_total_tokens": 4268, "main_prompt_tokens": 2906, "main_completion_tokens": 1362, "judge_total_tokens": 465, "judge_prompt_tokens": 407, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 2510.11061668396, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and matches the gold answer. The added detail that he is a New Zealander and founder of the McLaren Motor Racing team is accurate and consistent with the question and gold label, so the response is both correct and faithful.", "main_total_tokens": 3217, "main_prompt_tokens": 2816, "main_completion_tokens": 401, "judge_total_tokens": 511, "judge_prompt_tokens": 417, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The McLaren Motor Racing team [Doc 73859].", "latency_ms": 5672.307968139648, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer in content, only adding a citation marker. It is both correct and faithful to the gold.", "main_total_tokens": 4115, "main_prompt_tokens": 3184, "main_completion_tokens": 931, "judge_total_tokens": 453, "judge_prompt_tokens": 395, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "Belgium [Doc 10227].", "latency_ms": 6029.773712158203, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says the person is from Belgium, but the gold answer is New Zealand, so it is factually incorrect. The model clearly picks a specific country from the prompt, so it is faithful to the question context, just wrong in content.", "main_total_tokens": 4269, "main_prompt_tokens": 3239, "main_completion_tokens": 1030, "judge_total_tokens": 464, "judge_prompt_tokens": 383, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based strictly on the context provided, there is no movie listed for the year 2013 that features John McClane blowing up the Great Wall of China.\n\nThe 2013 movie featuring John McClane is *A Good Day to Die Hard* [Doc 58791], but its plot description states that the action takes place in Moscow [Doc 58791].", "latency_ms": 7546.475887298584, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The answer correctly states that there is no 2013 movie in which John McClane blows up the Great Wall of China, aligning with the gold answer that such a movie is not in the database. It faithfully uses the provided context, noting that the 2013 John McClane movie is 'A Good Day to Die Hard' and that its plot is set in Moscow, not at the Great Wall of China. No unsupported claims are made.", "main_total_tokens": 4682, "main_prompt_tokens": 3242, "main_completion_tokens": 1440, "judge_total_tokens": 595, "judge_prompt_tokens": 473, "judge_completion_tokens": 122}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit [Doc 28589]", "latency_ms": 3696.120262145996, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2002 Japanese horror movie as 'Tomie: Forbidden Fruit'. It is both correct and faithful to the provided information.", "main_total_tokens": 3601, "main_prompt_tokens": 3011, "main_completion_tokens": 590, "judge_total_tokens": 496, "judge_prompt_tokens": 428, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 [Doc 98566]", "latency_ms": 3893.637180328369, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years) and does not introduce any extraneous or conflicting information, so it is both correct and faithful.", "main_total_tokens": 4048, "main_prompt_tokens": 3426, "main_completion_tokens": 622, "judge_total_tokens": 464, "judge_prompt_tokens": 402, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King and Tala search for a legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 3451.17449760437, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "Generated answer correctly states they search for The Book of Souls and does not introduce contradictions or unsupported details beyond calling it a legendary relic, which is consistent with the film.", "main_total_tokens": 3847, "main_prompt_tokens": 3282, "main_completion_tokens": 565, "judge_total_tokens": 481, "judge_prompt_tokens": 417, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I am unable to provide the release year for \"Zombies on the Moon\" as that title is not mentioned in the provided context.", "latency_ms": 5001.479625701904, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie, but both the gold and generated answers state that this information is not available (not in the database / not in the provided context). The generated answer correctly avoids fabricating a date and directly acknowledges the lack of information, which is fully consistent with the gold answer. Thus it is both correct relative to the task and faithful to the given information.", "main_total_tokens": 3931, "main_prompt_tokens": 3047, "main_completion_tokens": 884, "judge_total_tokens": 528, "judge_prompt_tokens": 418, "judge_completion_tokens": 110}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I am unable to find the release year for \"The Galactic baker\" as this title is not mentioned in the provided context.", "latency_ms": 4857.2423458099365, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot find the release year and aligns with the gold response that the movie is not in the database. It does not fabricate a year or extra facts and remains faithful to the (lack of) provided context.", "main_total_tokens": 3824, "main_prompt_tokens": 2979, "main_completion_tokens": 845, "judge_total_tokens": 493, "judge_prompt_tokens": 414, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I am unable to answer this question as the title \"Return of the Null Pointer\" is not mentioned in the provided context.", "latency_ms": 4836.607217788696, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is unanswerable; both the gold and the generated response state inability to answer. The generated answer correctly reflects that the title is not present in the context, matching the unanswerable nature. It is factually correct and faithful to the (lack of) context.", "main_total_tokens": 3438, "main_prompt_tokens": 2573, "main_completion_tokens": 865, "judge_total_tokens": 503, "judge_prompt_tokens": 416, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I am unable to find the release year for \"Data Science: The Movie\" in the provided context.", "latency_ms": 5714.494228363037, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about the release year of a non-existent or unknown movie. The gold answer states that the movie is not in the database. The generated answer similarly says it cannot find the release year in the provided context. This does not fabricate information and correctly reflects a lack of knowledge, so it is both correct and faithful to the implied context.", "main_total_tokens": 4219, "main_prompt_tokens": 3180, "main_completion_tokens": 1039, "judge_total_tokens": 512, "judge_prompt_tokens": 412, "judge_completion_tokens": 100}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for the movie \"Python's Revenge 2\" cannot be determined, as that specific title is not listed.", "latency_ms": 5748.157501220703, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database. The generated answer correctly says the release year cannot be determined from the provided context and that the title is not listed. This is both correct and faithful to the gold response.", "main_total_tokens": 4031, "main_prompt_tokens": 3028, "main_completion_tokens": 1003, "judge_total_tokens": 500, "judge_prompt_tokens": 423, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I am unable to provide the release year for \"The Last Pixel\" as that title is not mentioned in the provided context.", "latency_ms": 4995.113372802734, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the system cannot provide the release year / that the movie is not in the database or context. The generated answer is correct (we do not know the year) and faithful to the given information, not hallucinating any date.", "main_total_tokens": 3591, "main_prompt_tokens": 2691, "main_completion_tokens": 900, "judge_total_tokens": 498, "judge_prompt_tokens": 414, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I am unable to find the release year for \"The man who ate the sun\" in the provided context.", "latency_ms": 4692.832231521606, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database. The generated answer states it cannot find the release year in the provided context. Both convey the lack of information without adding incorrect facts, so the response is correct and faithful to the gold.", "main_total_tokens": 4290, "main_prompt_tokens": 3456, "main_completion_tokens": 834, "judge_total_tokens": 494, "judge_prompt_tokens": 414, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not state who directed the movie \"The Last Days of Lehman Brothers\" [Doc 46069].", "latency_ms": 4854.748725891113, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not provide the director information, which aligns with the gold answer that the database lacks this information. It does not hallucinate a director and remains grounded in the provided context, so it is both correct and faithful.", "main_total_tokens": 4186, "main_prompt_tokens": 3361, "main_completion_tokens": 825, "judge_total_tokens": 504, "judge_prompt_tokens": 422, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not contain information about the movie \"A Love to Die For\" or its director.", "latency_ms": 5137.331962585449, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information for the movie is not available in the provided context/database. The generated response does not invent any details and is fully consistent with the gold answer, so it is both correct and faithful.", "main_total_tokens": 4052, "main_prompt_tokens": 3171, "main_completion_tokens": 881, "judge_total_tokens": 495, "judge_prompt_tokens": 416, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Based strictly on the context provided, the director of the movie \"Journey into Gaza\" is not mentioned [Doc 111360].", "latency_ms": 4370.996713638306, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director of the movie is not mentioned, which aligns with the gold answer indicating the database does not provide the director information. It does not introduce any unsupported facts and remains faithful to the context by explicitly noting the lack of information.", "main_total_tokens": 3757, "main_prompt_tokens": 3022, "main_completion_tokens": 735, "judge_total_tokens": 502, "judge_prompt_tokens": 419, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not contain information about who directed the movie or TV show titled \"The Last Night at Tremore Beach\" [Doc 135098].", "latency_ms": 4376.749753952026, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided context/database, and neither attempts to hallucinate a director. The generated response is fully consistent with the context and matches the intent of the gold answer.", "main_total_tokens": 3427, "main_prompt_tokens": 2743, "main_completion_tokens": 684, "judge_total_tokens": 506, "judge_prompt_tokens": 428, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 7128.761768341064, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Rise of the Mummy'), so it is both correct and faithful to the reference.", "main_total_tokens": 4182, "main_prompt_tokens": 2788, "main_completion_tokens": 1394, "judge_total_tokens": 453, "judge_prompt_tokens": 396, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 4152.3277759552, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold reference ('Stranger in My Bed') and correctly identifies the movie based on the plot description. It is both correct and fully faithful to the provided gold answer.", "main_total_tokens": 3533, "main_prompt_tokens": 2957, "main_completion_tokens": 576, "judge_total_tokens": 470, "judge_prompt_tokens": 400, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 6906.46767616272, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title that matches the described plot. The gold answer is \"Game of Silence,\" and the generated answer is exactly \"Game of Silence\" as well. This matches correctly and is fully consistent with the provided gold, so the response is both correct and faithful.", "main_total_tokens": 3749, "main_prompt_tokens": 2496, "main_completion_tokens": 1253, "judge_total_tokens": 487, "judge_prompt_tokens": 400, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 6363.452434539795, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the correct movie title given in the gold response. It is both correct and faithful with no discrepancies.", "main_total_tokens": 3942, "main_prompt_tokens": 2851, "main_completion_tokens": 1091, "judge_total_tokens": 451, "judge_prompt_tokens": 395, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job! [Doc 66094]", "latency_ms": 4945.451021194458, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Wood Job!', correctly identifying the movie based on the plot description. It is fully faithful to the provided information with no additions or deviations.", "main_total_tokens": 4038, "main_prompt_tokens": 3160, "main_completion_tokens": 878, "judge_total_tokens": 476, "judge_prompt_tokens": 409, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 3805.583715438843, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase' with no additions or omissions.", "main_total_tokens": 3576, "main_prompt_tokens": 2970, "main_completion_tokens": 606, "judge_total_tokens": 478, "judge_prompt_tokens": 417, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond [Doc 74597]", "latency_ms": 3810.2965354919434, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title described by the plot about a 7-year-old boy saving his parents from a burning car after a rock fall. The provided gold answer is 'Beyond', and the generated answer is also 'Beyond [Doc 74597]'. The core title matches exactly, so the answer is correct and fully faithful to the gold, with only an extraneous document tag that does not change the meaning.", "main_total_tokens": 3187, "main_prompt_tokens": 2549, "main_completion_tokens": 638, "judge_total_tokens": 531, "judge_prompt_tokens": 415, "judge_completion_tokens": 116}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9 [Doc 22478]", "latency_ms": 3914.0992164611816, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as '6ixtynin9' without adding incorrect or extraneous information. It is both correct and faithful.", "main_total_tokens": 3242, "main_prompt_tokens": 2597, "main_completion_tokens": 645, "judge_total_tokens": 482, "judge_prompt_tokens": 415, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread [Doc 32410]", "latency_ms": 5272.924900054932, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold title exactly ('Club Dread'), and there is no additional incorrect or hallucinated information. Both correctness and faithfulness are perfect.", "main_total_tokens": 3875, "main_prompt_tokens": 2975, "main_completion_tokens": 900, "judge_total_tokens": 496, "judge_prompt_tokens": 433, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet [Doc 48873]", "latency_ms": 5928.076982498169, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot; the gold answer is 'Toilet'. The generated answer is also 'Toilet [Doc 48873]', which correctly identifies the same movie. The added document tag does not change the answer, so it is fully correct and faithful to the gold.", "main_total_tokens": 4432, "main_prompt_tokens": 3279, "main_completion_tokens": 1153, "judge_total_tokens": 512, "judge_prompt_tokens": 421, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel [Doc 1024]", "latency_ms": 6224.093198776245, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot. The gold answer is 'Electric Angel', and the generated answer is 'Electric Angel [Doc 1024]', which clearly identifies the same film with an additional document reference. This is correct with respect to the gold and remains faithful to the given information.", "main_total_tokens": 4019, "main_prompt_tokens": 2871, "main_completion_tokens": 1148, "judge_total_tokens": 515, "judge_prompt_tokens": 424, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is Down Low, with a rating of 5.207 [Doc 106995].", "latency_ms": 5591.541290283203, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect relative to the gold label: it states that the lowest rated movie from 2023 is 'Down Low', while the gold answer is 'Mega Ape'. However, the answer is fully faithful to the cited document (Doc 106995), so faithfulness is high even though the content is wrong with respect to the task.", "main_total_tokens": 4144, "main_prompt_tokens": 2779, "main_completion_tokens": 1365, "judge_total_tokens": 510, "judge_prompt_tokens": 407, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 389.2960548400879, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question asks for the highest rated TV show from 2010, and the gold answer is 'Tea House'. The generated answer says 'I don't know', which is incorrect relative to the gold. However, it does not introduce any contradictory or fabricated content; it simply expresses a lack of knowledge. Thus correctness is 0, while faithfulness (to its own stated uncertainty) is moderate, scored at 0.5.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 500, "judge_prompt_tokens": 386, "judge_completion_tokens": 114}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 378.277063369751, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know', which is incorrect because there is a specific gold answer. It is, however, faithful in that it does not hallucinate any incorrect information; it simply declines to answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 395, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "en [Doc 768]", "latency_ms": 6738.189458847046, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'English' while the generated answer is 'en', which is not an exact match. Additionally, the generated answer includes an unexplained citation-like note '[Doc 768]' that is not supported by the given gold answer context.", "main_total_tokens": 4468, "main_prompt_tokens": 3139, "main_completion_tokens": 1329, "judge_total_tokens": 470, "judge_prompt_tokens": 389, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity among the provided context is Freaks and Geeks (Popularity: 5.8043) [Doc 115187].", "latency_ms": 6291.255235671997, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'Law & Order: Special Victims Unit' as the most popular 1999 TV show, but the generated answer claims it is 'Freaks and Geeks' with a specific popularity score. This directly contradicts the gold answer and thus is factually incorrect in this evaluation context. It also references 'among the provided context', implying reliance on an external ranking not aligned with the gold label, so relative to the task and gold standard, it is unfaithful.", "main_total_tokens": 4391, "main_prompt_tokens": 3147, "main_completion_tokens": 1244, "judge_total_tokens": 557, "judge_prompt_tokens": 426, "judge_completion_tokens": 131}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 1317.765474319458, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and correctly attributes it, so it is both correct and faithful to the provided information.", "main_total_tokens": 2598, "main_prompt_tokens": 2442, "main_completion_tokens": 156, "judge_total_tokens": 461, "judge_prompt_tokens": 400, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in 1999 [Doc 22481], 2019 [Doc 88811], and 2025 [Doc 112077].", "latency_ms": 5478.135108947754, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a single release year of the movie, which according to the gold answer is 2019. The generated answer lists three different years (1999, 2019, 2025) instead of resolving to the correct one, so it is incorrect. It is also unfaithful because it fabricates multiple release years and references to documents (Doc 22481, Doc 88811, Doc 112077) that are not part of the provided information and contradict the gold answer.", "main_total_tokens": 4107, "main_prompt_tokens": 3012, "main_completion_tokens": 1095, "judge_total_tokens": 556, "judge_prompt_tokens": 423, "judge_completion_tokens": 133}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "2", "latency_ms": 3939.3486976623535, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states 279 votes, but the generated answer is 2 votes, which is incorrect. The evaluation correctly identifies the discrepancy, so it is faithful to the provided information but the generated answer itself is not correct.", "main_total_tokens": 3146, "main_prompt_tokens": 2387, "main_completion_tokens": 759, "judge_total_tokens": 459, "judge_prompt_tokens": 383, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "3", "latency_ms": 5104.626655578613, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (3 votes) does not match the gold answer (12 votes), so it is factually incorrect. However, it directly attempts to answer the user question without adding unrelated content, so it is faithful to the query.", "main_total_tokens": 3451, "main_prompt_tokens": 2463, "main_completion_tokens": 988, "judge_total_tokens": 461, "judge_prompt_tokens": 383, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "2009 [Doc 46069]", "latency_ms": 3389.2555236816406, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2009) exactly and does not introduce any extraneous or conflicting information, so it is both correct and faithful.", "main_total_tokens": 3831, "main_prompt_tokens": 3342, "main_completion_tokens": 489, "judge_total_tokens": 458, "judge_prompt_tokens": 396, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gemini-flash-latest", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English (en) [Doc 48035].", "latency_ms": 3461.7223739624023, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the original language of 'The King’s Speech' is English, matching the gold label 'en'. It does not introduce any contradictions or unsupported claims beyond referencing a document, so it is both correct and faithful.", "main_total_tokens": 3626, "main_prompt_tokens": 3075, "main_completion_tokens": 551, "judge_total_tokens": 487, "judge_prompt_tokens": 405, "judge_completion_tokens": 82}
