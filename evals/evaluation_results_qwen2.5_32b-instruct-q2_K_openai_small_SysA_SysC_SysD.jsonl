{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "Wall Street was released in the year 1987 [Doc 6993].", "latency_ms": 9753.327369689941, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, which matches the gold answer. It is consistent and faithful to the provided information.", "main_total_tokens": 3129, "main_prompt_tokens": 3108, "main_completion_tokens": 21, "judge_total_tokens": 456, "judge_prompt_tokens": 394, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The description that matches the scenario where classmates have to play a survival game using their cell phones is found in [Doc 103513], titled \"Perfect Strangers.\" In this movie, seven friends who attend a dinner decide to play a game with their cell phones, which leads to disaster when their dark secrets are revealed.", "latency_ms": 4089.3211364746094, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer specifies the movie \"Brain Fluid Explosion Girl,\" but the generated answer instead identifies \"Perfect Strangers,\" which is incorrect. Moreover, it invents a specific document reference ([Doc 103513]) and description that are not grounded in the provided gold answer. Therefore, the response is both factually incorrect and unfaithful to the source.", "main_total_tokens": 2984, "main_prompt_tokens": 2915, "main_completion_tokens": 69, "judge_total_tokens": 554, "judge_prompt_tokens": 452, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show where six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" from 1983 [Doc 112849].", "latency_ms": 3281.5353870391846, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' which matches the gold answer. It does not introduce contradictions or unsupported claims beyond a minor, plausible detail about the year, so it remains faithful to the provided information.", "main_total_tokens": 3013, "main_prompt_tokens": 2973, "main_completion_tokens": 40, "judge_total_tokens": 502, "judge_prompt_tokens": 423, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie that matches this description is \"Nacht der Wölfe\" [Doc 2590]. This film from 1982 tells the story of two rival gangs in Munich, with one gang composed of German teens and the other of Turkish teens. It is described as a 'Crime' and 'Drama' movie with a rating of 5.0.", "latency_ms": 4319.260597229004, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe', exactly matching the gold answer. The additional details about the film (year, plot, genre, rating) are consistent with the prompt context and do not contradict it, so the response is both correct and faithful.", "main_total_tokens": 3433, "main_prompt_tokens": 3354, "main_completion_tokens": 79, "judge_total_tokens": 560, "judge_prompt_tokens": 471, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of Picnic at Hanging Rock take place in the year 1900 [Doc 126191].", "latency_ms": 3210.4461193084717, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the year as 1900, matching the gold answer exactly and staying faithful to the provided information.", "main_total_tokens": 3114, "main_prompt_tokens": 3085, "main_completion_tokens": 29, "judge_total_tokens": 462, "judge_prompt_tokens": 406, "judge_completion_tokens": 56}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie described is \"June.\" This film centers around June, a young violinist who works in an eccentric coffee shop in Venice and dreams of meeting the perfect partner. The plot matches the description provided where she balances her daytime job with nighttime rehearsals for her musical career while also indulging in daydreams about her ideal man [Doc 32483].", "latency_ms": 4259.616374969482, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'June,' matching the gold answer, so correctness is 1. However, it fabricates additional plot details and a citation ([Doc 32483]) not present in the original question or gold answer, so faithfulness is 0.", "main_total_tokens": 3394, "main_prompt_tokens": 3313, "main_completion_tokens": 81, "judge_total_tokens": 594, "judge_prompt_tokens": 507, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "Based on the provided documents, the highest-rated Russian-language (ru) movie from 1980 is \"Moscow Does Not Believe in Tears\" [Doc 61] with a rating of 7.6.", "latency_ms": 3614.6583557128906, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold label indicates that the highest rated Russian-language movie released in 1980 is 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger', not 'Moscow Does Not Believe in Tears'. The model instead relied on some document-based rating and did not match the gold answer, so it is not faithful to the specified ground truth for this evaluation.", "main_total_tokens": 3340, "main_prompt_tokens": 3293, "main_completion_tokens": 47, "judge_total_tokens": 548, "judge_prompt_tokens": 438, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest Russian-language movie rating in 1999 based on the provided documents is [Doc 22144] \"Features of the Russian Bath\" with a rating of 4.571. [Doc 22144]", "latency_ms": 3479.4681072235107, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 7.6, but the generated answer states 4.571 and identifies a specific movie, so it is numerically incorrect. It also invents a document reference and content not supported by the gold answer, so it is unfaithful to the given ground truth.", "main_total_tokens": 2792, "main_prompt_tokens": 2737, "main_completion_tokens": 55, "judge_total_tokens": 519, "judge_prompt_tokens": 429, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "Based on the provided documents, only one movie meets all the criteria: it was released in 1987, has a rating over 7, and is in Chinese. That movie is \"Last Eunuch In China\" with a rating of 7.8 [Doc 7543].", "latency_ms": 3935.110092163086, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 4 such movies, but the generated answer claims there is only one, so it is factually incorrect. However, the generated answer is consistent with and grounded in its cited document, so it is faithful to its own context.", "main_total_tokens": 3240, "main_prompt_tokens": 3176, "main_completion_tokens": 64, "judge_total_tokens": 533, "judge_prompt_tokens": 450, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "Based on the provided documents, there is one TV show that meets all the criteria:\n\n- **Journey to the West** [Doc 113169]\n    - Year: 1986 (though it's close, it does not meet the year requirement of 1987)\n    - Rating: 8.4\n    - Language: zh\n\nHowever, since \"Journey to the West\" was released in 1986 and not 1987, there are no TV shows that match all three criteria (released in 1987, have a rating over 7, and are in Chinese).", "latency_ms": 5169.286012649536, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 2 such TV shows, but the generated answer concludes there are 0, so it is factually incorrect with respect to the ground truth, giving a correctness of 0. However, the generated answer is internally consistent with the information it cites (it notes the only show found is from 1986 and so does not count), so the reasoning is faithful to the provided evidence, resulting in a faithfulness score of 1.", "main_total_tokens": 3204, "main_prompt_tokens": 3067, "main_completion_tokens": 137, "judge_total_tokens": 636, "judge_prompt_tokens": 512, "judge_completion_tokens": 124}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "Based on the provided documents, there isn't a French language TV show that was specifically released in 1980. The closest matches are movies and TV shows from other years. For example, \"Secrets of a French Maid\" is a comedy movie from 1980 but it's in German [Doc 207]. There is no French language TV show from 1980 mentioned in the provided documents.", "latency_ms": 4659.5399379730225, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the only French language TV show released in 1980, with the gold answer being 'Fantômas'. The generated answer claims there is no such French language TV show in 1980 in the documents and gives an example of a German movie instead, directly contradicting the gold answer, so it is factually incorrect. However, the response is consistent with and constrained to what it says the provided documents contain and does not introduce unrelated fabricated details, so it is faithful to its stated evidence.", "main_total_tokens": 3441, "main_prompt_tokens": 3351, "main_completion_tokens": 90, "judge_total_tokens": 597, "judge_prompt_tokens": 463, "judge_completion_tokens": 134}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie that features two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" [Doc 11205].", "latency_ms": 3349.3409156799316, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise' and matches the gold answer exactly. It is fully supported by the cited document, so both correctness and faithfulness are maximal.", "main_total_tokens": 3202, "main_prompt_tokens": 3171, "main_completion_tokens": 31, "judge_total_tokens": 478, "judge_prompt_tokens": 410, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie where Fred Rogers speaks to children about some of life's most important issues through his television program is \"Won't You Be My Neighbor?\" [Doc 84838]", "latency_ms": 3433.208465576172, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title \"Won't You Be My Neighbor?\" and correctly describes it as the movie about Fred Rogers and his TV series. It remains faithful to the given information without introducing contradictions.", "main_total_tokens": 3282, "main_prompt_tokens": 3240, "main_completion_tokens": 42, "judge_total_tokens": 512, "judge_prompt_tokens": 438, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The document does not specify a single release year for \"YEAR\" but rather describes the context of different movies. However, if you are referring to the documentary by Dmytro Komarov titled \"YEAR,\" it is described as having the year 2023 [Doc 107925].", "latency_ms": 4000.0619888305664, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 2018, but the generated answer claims 2023, which is incorrect. It also fabricates context about a documentary by Dmytro Komarov and a specific document reference that are not grounded in the provided gold answer, so it is unfaithful.", "main_total_tokens": 3374, "main_prompt_tokens": 3310, "main_completion_tokens": 64, "judge_total_tokens": 524, "judge_prompt_tokens": 435, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that fits the description of being an animation about a young boy robot in 2030 AD is \"Astro Boy\" [Doc 112591]. This show describes a setting where, after being abandoned by his creator, a young boy robot is rescued and adopted by Dr Elefun who guides him to use his special powers to aid and protect mankind and robot-kind.", "latency_ms": 4474.487066268921, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy,' matching the gold answer exactly. The added description about the plot and characters (Dr. Elefun, young boy robot, protecting mankind and robot-kind) is consistent with known details of the show and does not contradict the given information, indicating the answer is faithful to the likely source document mention [Doc 112591].", "main_total_tokens": 3586, "main_prompt_tokens": 3503, "main_completion_tokens": 83, "judge_total_tokens": 571, "judge_prompt_tokens": 463, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her young son were held captive for 7 years [Doc 67513].", "latency_ms": 3309.2002868652344, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9463946303571863, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer. It is fully consistent and faithful to the provided information.", "main_total_tokens": 2790, "main_prompt_tokens": 2753, "main_completion_tokens": 37, "judge_total_tokens": 492, "judge_prompt_tokens": 425, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26m before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 3415.6012535095215, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983, matching the provided gold answer exactly. It is faithful to the given information and does not introduce unsupported details.", "main_total_tokens": 3121, "main_prompt_tokens": 3077, "main_completion_tokens": 44, "judge_total_tokens": 514, "judge_prompt_tokens": 428, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "The most popular English-speaking TV show released in 1997, based on the provided documents, is \"Buffy the Vampire Slayer\" with a popularity score of 25.9735 [Doc 114711].", "latency_ms": 4354.3860912323, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer popularity score is 80.5772, whereas the generated answer states 25.9735, so the numeric result is incorrect. It also identifies a specific show (\"Buffy the Vampire Slayer\") and a score that do not match the gold answer, indicating the response is not faithful to the ground truth.", "main_total_tokens": 3950, "main_prompt_tokens": 3897, "main_completion_tokens": 53, "judge_total_tokens": 532, "judge_prompt_tokens": 435, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "Based on the provided documents, there are two Russian language (ru) TV shows that were released in 1997:\n\n1. \"Kamenskaya\" [Doc 115256]\n2. \"Daria\" [Doc 114736] - Although Daria is primarily an English language show, it fits the criteria of being a TV show from 1997.\n\nHowever, only one Russian language (ru) TV show was released in 1997: \"Kamenskaya.\"", "latency_ms": 4433.0949783325195, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says there is 1 Russian language TV show released in 1997, and the generated answer ultimately concludes that only one Russian-language (ru) TV show was released in 1997: 'Kamenskaya.' This matches the gold, so correctness is 1. The reasoning explicitly notes that 'Daria' is primarily an English-language show and therefore does not count as Russian-language, and settles on only Kamenskaya, which is consistent with the gold label, so faithfulness is also 1.", "main_total_tokens": 2698, "main_prompt_tokens": 2582, "main_completion_tokens": 116, "judge_total_tokens": 616, "judge_prompt_tokens": 480, "judge_completion_tokens": 136}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 3376.5251636505127, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Angel Eyes'. It is fully supported by the provided information and does not introduce any unsupported claims.", "main_total_tokens": 3106, "main_prompt_tokens": 3066, "main_completion_tokens": 40, "judge_total_tokens": 495, "judge_prompt_tokens": 429, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French [Doc 110681].", "latency_ms": 2961.9932174682617, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states the language is French, while the gold label says Korean, so the content is factually incorrect. There is no supporting context provided that would justify changing the answer from the gold label, so the answer is also unfaithful.", "main_total_tokens": 3033, "main_prompt_tokens": 3011, "main_completion_tokens": 22, "judge_total_tokens": 474, "judge_prompt_tokens": 393, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "Based on the provided documents, there are no English-speaking (language: en) horror movies from 2009 with a popularity score greater than 7. The highest popularity score for an English-speaking horror movie from 2009 is 2.9925 for \"My Ex\" [Doc 45056], which does not meet the criteria of having a popularity score greater than 7.\n\nTherefore, the answer to your question is 0.", "latency_ms": 4288.974761962891, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "Gold answer is 2 but the generated answer says 0, so it is factually incorrect. However, the reasoning is consistent with and grounded in the provided (though incomplete) documents, so it is faithful to the given context.", "main_total_tokens": 2933, "main_prompt_tokens": 2834, "main_completion_tokens": 99, "judge_total_tokens": 559, "judge_prompt_tokens": 481, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The provided documents do not contain any information about movies released in 1979. Therefore, based on the given context, it is not possible to determine the best-rated movie from that year.", "latency_ms": 3439.1708374023438, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not provide information about 1979 movies and therefore the best-rated movie cannot be determined from the given context. This aligns with the gold response that the database has no movies from 1979. It makes no unsupported claims and stays faithful to the provided context.", "main_total_tokens": 3117, "main_prompt_tokens": 3076, "main_completion_tokens": 41, "judge_total_tokens": 525, "judge_prompt_tokens": 433, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "Based on the provided documents, there is no information about a \"Back to the Future Part IV\" movie release. The documents mention movies up to \"Back to the Future Part III,\" and other related content like documentaries and an animated series, but do not include any details about a fourth installment in the series.", "latency_ms": 4017.655611038208, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is that there is no Back to the Future Part IV movie, matching the gold answer. The generated answer instead frames this as a limitation of the provided documents rather than stating that the film does not exist, which is misleading regarding the real-world fact. It is therefore incorrect. It is also unfaithful to the gold answer, which explicitly says the movie does not exist, while the generated answer only says the documents lack information.", "main_total_tokens": 3451, "main_prompt_tokens": 3388, "main_completion_tokens": 63, "judge_total_tokens": 582, "judge_prompt_tokens": 462, "judge_completion_tokens": 120}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "None of the documents provided contain information on any movies about the Chernobyl disaster from the year 1980. The earliest mentioned is \"Chornobyl: Chronicle of Difficult Weeks\" from 1990 [Doc 11722]. Therefore, based on the given context, there's no popularity score to report for a movie from 1980 about the Chernobyl disaster.", "latency_ms": 4469.460010528564, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly concludes that there is no movie from 1980 about the Chernobyl disaster and thus no popularity score can be given, which matches the gold answer. It also remains faithful to the provided context by explicitly referencing the absence of such a movie in the documents and not introducing any unsupported information.", "main_total_tokens": 3453, "main_prompt_tokens": 3365, "main_completion_tokens": 88, "judge_total_tokens": 574, "judge_prompt_tokens": 481, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are three entries with \"Astro Boy\" in the title:\n\n1. Astro Boy (Movie, 2009) [Doc 44947]\n2. Astro Boy (TV, 1980) [Doc 112591]\n3. Astro Boy (TV, 2003) [Doc 116265]", "latency_ms": 4270.864248275757, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 1.1510196182278052, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.5330476224182168, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there are 4 titles with 'Astro Boy' in the title, but the generated answer states there are only 3, which is incorrect. The answer is internally consistent and aligns with the cited entries, so it is faithful to its own reasoning but factually wrong compared to the gold.", "main_total_tokens": 3282, "main_prompt_tokens": 3199, "main_completion_tokens": 83, "judge_total_tokens": 545, "judge_prompt_tokens": 451, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "Based on the provided documents, there are no movies from 2025 that directly allude to the Holocaust. The only movie from 2025 mentioned is \"We're All Gonna Die,\" which is categorized as a comedy and science fiction film with no connection to the Holocaust. [Doc 111988]", "latency_ms": 4238.335371017456, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that there are 2 such movies, while the generated answer states there are none, making it factually incorrect relative to the gold. However, the generated answer is faithful to the cited document, which apparently mentions only one 2025 movie without Holocaust allusions, and the reasoning follows from the provided evidence, so faithfulness is high.", "main_total_tokens": 3585, "main_prompt_tokens": 3514, "main_completion_tokens": 71, "judge_total_tokens": 548, "judge_prompt_tokens": 445, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "Based on the provided documents, there are three TV shows related to Super Mario Bros that were released in the 1990s:\n\n1. **The Adventures of Super Mario Bros. 3** - Released in 1990 [Doc 113661].\n2. **Super Mario World** - Released in 1991 [Doc 113814].\n\nThese are the only TV shows about Super Mario Bros that were released during this decade according to the provided context.", "latency_ms": 4824.844121932983, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.8010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer says there was 1 TV show in the 1990s, while the generated answer states there are three and then lists only two, concluding these are the only ones. Numerically, this contradicts the gold and is incorrect. In terms of faithfulness to the provided context, the answer references specific docs and release years that appear to come from the context, but it introduces an unsubstantiated claim of three shows while only listing two. This partial inconsistency with the cited context lowers faithfulness, but it is not entirely fabricated, so it is not zero.", "main_total_tokens": 3474, "main_prompt_tokens": 3367, "main_completion_tokens": 107, "judge_total_tokens": 629, "judge_prompt_tokens": 479, "judge_completion_tokens": 150}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "Based on the provided context, there is only one TV show related to \"Super Mario Bros.\" that falls within the specified timeframe (between 2000 and 2010):\n\n- **Title: The Adventures of Super Mario Bros. 3**\n  - Year: 1990\n\nHowever, this TV show was released in 1990, which is outside the requested range of 2000 to 2010.\n\nTherefore, there are no TV shows about \"Super Mario Bros.\" that were released between 2000 and 2010 based on the provided context. [Doc 113661]", "latency_ms": 4996.7522621154785, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a count; the gold answer is 0, but the generated answer says there is only one relevant TV show and never explicitly gives the final numeric answer as 0. It also incorrectly treats a 1990 show as falling within the 2000–2010 timeframe before contradicting itself, so the reasoning and conclusion are muddled and not a correct, direct answer of 0. Additionally, the generated answer references a specific document ([Doc 113661]) in a way that suggests access to external context that is not visible in the prompt, so it is not fully faithful to the provided information.", "main_total_tokens": 3183, "main_prompt_tokens": 3042, "main_completion_tokens": 141, "judge_total_tokens": 665, "judge_prompt_tokens": 509, "judge_completion_tokens": 156}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular TV show or movie about ninjas based on popularity score is [Doc 135161] \"House of Ninjas\" with a popularity score of 4.4317 [Doc 135161].", "latency_ms": 4107.2423458099365, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'Naruto', while the generated answer claims 'House of Ninjas' is the most popular. This is incorrect relative to the gold and also introduces an unsupported notion of a specific popularity score and ranking based on a document reference, which is not grounded in the given gold answer. Thus it is neither correct nor faithful to the intended answer.", "main_total_tokens": 3473, "main_prompt_tokens": 3419, "main_completion_tokens": 54, "judge_total_tokens": 528, "judge_prompt_tokens": 425, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The provided documents do not mention a Jurassic Park/Power Rangers crossover movie, so I cannot provide a release date for such a film based on the given context.", "latency_ms": 3485.2569103240967, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the movie was not released. The generated answer only says it cannot provide a release date based on the documents, not that the movie was never released. This is factually incomplete/incorrect relative to the gold. However, it accurately reflects the information given in the context (the documents don’t mention such a movie), so it is faithful to the provided context.", "main_total_tokens": 3281, "main_prompt_tokens": 3248, "main_completion_tokens": 33, "judge_total_tokens": 525, "judge_prompt_tokens": 417, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "The provided documents do not mention any movie where Barbie fights the Predator. [Doc 82000] describes \"The Predator\" as a movie involving soldiers and scientists, but it does not include Barbie. Similarly, \"[Alien Predator]\" in [Doc 84103] also does not involve Barbie. None of the other documents suggest that there is a movie where Barbie fights the Predator.", "latency_ms": 4308.430910110474, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer is correct that there is no movie where Barbie fights the Predator and it aligns with the gold answer that such a movie was not released. However, it slightly over-relies on the provided documents rather than clearly stating the global fact that no such movie exists, so faithfulness to the intended gold reasoning is high but not perfect.", "main_total_tokens": 3297, "main_prompt_tokens": 3213, "main_completion_tokens": 84, "judge_total_tokens": 559, "judge_prompt_tokens": 458, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "To determine the fourth oldest Star Wars movie among the provided documents, we need to list them by year:\n\n1. **Star Wars: Episode IV - A New Hope** (1977) [Not listed in context]\n2. **The Empire Strikes Back** (1980) [Doc 3]\n3. **Return of the Jedi** (1983) [Doc 2858]\n4. **Star Wars: The Phantom Menace** (1999) [Not listed in context, but is known to be released before other documentaries]\n\nThe fourth oldest movie among those provided directly would be:\n\n- **Return of the Jedi** from 1983 [Doc 2858].\n\nHowever, if we consider only the documents provided and not include outside knowledge about the release dates of the main Star Wars films, then the fourth oldest is:\n\n- **The Story of Star Wars** (2004) [Doc 32101]\n\nTherefore, based strictly on the context provided:\n- The fourth oldest movie listed is \"Return of the Jedi\" from 1983. [Doc 2858]", "latency_ms": 7320.981025695801, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer per the gold label is 'Star Wars: Droids - The Great Heep', while the generated answer concludes 'Return of the Jedi' (and also mentions 'The Story of Star Wars') as the fourth oldest, which is incorrect. The reasoning also introduces and relies on outside films (A New Hope, The Phantom Menace) that are not in the described document set, contradicting the instruction to base the answer on the provided context. Thus it is neither correct nor faithful to the task and gold answer.", "main_total_tokens": 3486, "main_prompt_tokens": 3242, "main_completion_tokens": 244, "judge_total_tokens": 747, "judge_prompt_tokens": 610, "judge_completion_tokens": 137}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The document does not specify a single release year for \"YEAR\" but rather describes the context of different movies. However, if you are referring to the documentary by Dmytro Komarov titled \"YEAR,\" it is described as having the year 2023 [Doc 107925].", "latency_ms": 3963.8547897338867, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 1986, but the generated answer gives 2023, so it is factually incorrect. It also fabricates context about a documentary by Dmytro Komarov titled \"YEAR\" that does not align with the gold answer, so it is unfaithful to the intended question-answer pair.", "main_total_tokens": 3374, "main_prompt_tokens": 3310, "main_completion_tokens": 64, "judge_total_tokens": 531, "judge_prompt_tokens": 435, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The rating of the movie Forrest Gump is 8.466 [Doc 15236].", "latency_ms": 2991.072654724121, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating value (8.466) and does not introduce any unsupported information. It is both correct and faithful to the provided reference.", "main_total_tokens": 2950, "main_prompt_tokens": 2925, "main_completion_tokens": 25, "judge_total_tokens": 477, "judge_prompt_tokens": 413, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 3286.388158798218, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Mighty Med') and correctly states the name of the TV show described. It is fully consistent with the provided information and does not introduce any unsupported content.", "main_total_tokens": 3128, "main_prompt_tokens": 3091, "main_completion_tokens": 37, "judge_total_tokens": 494, "judge_prompt_tokens": 423, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In \"Túnel de los huesos\" (2011), seven prisoners escaped from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 3605.823040008545, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the prison is in Buenos Aires, Argentina, which matches the gold answer specifying Argentina. The added detail about Villa Devoto prison is accurate and does not contradict the source, so the response is both correct and faithful to the gold answer.", "main_total_tokens": 3459, "main_prompt_tokens": 3419, "main_completion_tokens": 40, "judge_total_tokens": 509, "judge_prompt_tokens": 425, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice Li Nai falls into the trap designed by Jiang Jie [Doc 135231].", "latency_ms": 3251.6965866088867, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the apprentice falls into Jiang Jie's trap. The generated answer says the apprentice Li Nai falls into the trap designed by Jiang Jie. The core required fact—whose trap it is—is correctly identified as Jiang Jie, matching the gold. The mention of Li Nai as the apprentice adds detail but does not contradict any provided information, so the answer remains faithful and correct.", "main_total_tokens": 3122, "main_prompt_tokens": 3086, "main_completion_tokens": 36, "judge_total_tokens": 518, "judge_prompt_tokens": 411, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "Based on the provided documents, there are two Korean-language movies that mention assassins after 2023:\n\n1. \"The Assassin\" with a release year of 2023 [Doc 106219].\n2. \"Baby Assassins 3\" which is released in 2024 [Doc 109371].\n\nHowever, only \"Baby Assassins 3\" is strictly after 2023.\n\nTherefore, there is one Korean-language movie mentioning assassins after 2023.", "latency_ms": 4727.0097732543945, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says 4, but the generated answer incorrectly concludes there is only 1 Korean-language movie mentioning assassins after 2023. This contradicts the gold label, so the correctness is 0. The reasoning within the generated answer is consistent with itself and with the described documents, so it is faithful to its own evidence, giving faithfulness a score of 1.", "main_total_tokens": 3105, "main_prompt_tokens": 2990, "main_completion_tokens": 115, "judge_total_tokens": 589, "judge_prompt_tokens": 481, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "The provided documents do not contain any information about movies that feature both Steven Seagal and Bruce Lee. Therefore, based on the given context, there is no answer to how many movies include both actors together.", "latency_ms": 3784.1873168945312, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there is no information (or no movies) featuring both Steven Seagal and Bruce Lee together. The generated answer is consistent with the gold answer and does not introduce unsupported claims.", "main_total_tokens": 3565, "main_prompt_tokens": 3523, "main_completion_tokens": 42, "judge_total_tokens": 509, "judge_prompt_tokens": 434, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 3384.174108505249, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer matches the gold rating (6.3), so it is correct. However, it cites an external document [Doc 42750] that is not provided in the prompt, so we cannot verify that citation, making the faithfulness to given sources unverifiable.", "main_total_tokens": 3316, "main_prompt_tokens": 3290, "main_completion_tokens": 26, "judge_total_tokens": 489, "judge_prompt_tokens": 403, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The provided context does not specify a particular style of racing for \"Fast Track: No Limits.\" It only describes the movie as centered around a group of street racers [Doc 41929]. Therefore, based on the information given, the style of racing in this movie is street racing.", "latency_ms": 3942.293643951416, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Drift,' but the generated answer says 'street racing,' which is incorrect. However, the model remained faithful to the provided context, which mentioned only street racing and not drifting, so it did not hallucinate beyond the given information.", "main_total_tokens": 3405, "main_prompt_tokens": 3343, "main_completion_tokens": 62, "judge_total_tokens": 518, "judge_prompt_tokens": 435, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title that matches your description is \"Confronting a Serial Killer\" [Doc 131298]. This documentary details the relationship between Jillian Lauren and Samuel Little, focusing on her efforts to identify his victims.", "latency_ms": 3609.830617904663, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.95, "justification": "The generated answer gives the correct title, 'Confronting a Serial Killer', which matches the gold answer. The added description about the documentary and its focus is accurate and consistent with known information, but the reference to '[Doc 131298]' suggests an external source not visible here, so a small deduction on faithfulness.", "main_total_tokens": 3306, "main_prompt_tokens": 3258, "main_completion_tokens": 48, "judge_total_tokens": 547, "judge_prompt_tokens": 449, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 3407.8543186187744, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score of 7.838 and clearly attributes it, so it is both correct and faithful to the given information.", "main_total_tokens": 3340, "main_prompt_tokens": 3310, "main_completion_tokens": 30, "judge_total_tokens": 469, "judge_prompt_tokens": 407, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The document provided does not contain the specific details to answer your question directly. However, based on the information given, \"The Seventh Sense\" from [Doc 23090] and [Doc 33889] is a 1999 movie that alludes to an extra sense, but it's categorized as Drama, not a psychological thriller. The plot descriptions do suggest some form of supernatural or extrasensory elements, with one involving a blind musician possibly regaining eyesight and the other about a young man who meets a fortuneteller who helps him read and hear others' thoughts.\n\nThe 1999 movie that fits more closely to your description as a psychological thriller alluding to an extra sense is \"The Sixth Sense\" from [Doc 22098], which involves a child psychologist meeting a boy with hidden secrets, though it does not directly mention the exploration of an extra sense in its plot summary. It's categorized under 'Mystery', 'Thriller', and 'Drama'.", "latency_ms": 6441.678524017334, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6895405204413555, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The question asks for the title of the 1999 psychological thriller movie that alludes to an extra sense; the correct answer is indeed 'The Sixth Sense', which the generated answer ultimately identifies. So correctness is 1. However, the generated answer introduces specific document references ([Doc 23090], [Doc 33889], [Doc 22098]) and plot details that are not present in the user prompt. Since we cannot verify these documents from the given context, some of that content is speculative relative to the provided conversation. Thus faithfulness is slightly reduced to 0.8.", "main_total_tokens": 3116, "main_prompt_tokens": 2903, "main_completion_tokens": 213, "judge_total_tokens": 738, "judge_prompt_tokens": 586, "judge_completion_tokens": 152}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 3178.4777641296387, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer, and it remains faithful to the provided information without introducing unsupported details.", "main_total_tokens": 3243, "main_prompt_tokens": 3220, "main_completion_tokens": 23, "judge_total_tokens": 462, "judge_prompt_tokens": 397, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 3205.6446075439453, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2015 reboot title as 'The Transporter Refueled' and does not introduce any unsupported information beyond a generic document citation.", "main_total_tokens": 3171, "main_prompt_tokens": 3136, "main_completion_tokens": 35, "judge_total_tokens": 490, "judge_prompt_tokens": 420, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in 2015 [Doc 67500].", "latency_ms": 3245.354652404785, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, which matches the gold answer. The citation style does not contradict the information, so the response is both correct and faithful.", "main_total_tokens": 3238, "main_prompt_tokens": 3212, "main_completion_tokens": 26, "judge_total_tokens": 480, "judge_prompt_tokens": 404, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "None of the documents provided contain information about a romantic comedy where Abraham Lincoln comes back from the dead. The plots described for movies involving Abraham Lincoln are either historical, documentaries, or fantasy/horror involving vampires and zombies, but none fit the description of a romantic comedy with Abraham Lincoln coming back from the dead.", "latency_ms": 3671.5588569641113, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no romantic comedy in the provided documents where Abraham Lincoln comes back from the dead, which matches the gold answer. It is accurate and remains grounded in the provided information, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3018, "main_prompt_tokens": 2955, "main_completion_tokens": 63, "judge_total_tokens": 533, "judge_prompt_tokens": 451, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by Ernest Hemingway is \"Havana Darkness\" [Doc 90923].", "latency_ms": 3230.2255630493164, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Havana Darkness') and correctly addresses the question. It is fully faithful to the provided information with no contradictions or additions.", "main_total_tokens": 3149, "main_prompt_tokens": 3116, "main_completion_tokens": 33, "judge_total_tokens": 479, "judge_prompt_tokens": 415, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the British comedy series \"Doll & Em\" [Doc 122259].", "latency_ms": 3281.363010406494, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em', matching the gold answer exactly. It stays faithful to the provided information, only adding that it is a British comedy series, which is accurate and does not contradict the source.", "main_total_tokens": 3142, "main_prompt_tokens": 3107, "main_completion_tokens": 35, "judge_total_tokens": 498, "judge_prompt_tokens": 418, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The 2007 movie where Hallam, who has a talent for spying on people and attempts to find the cause of his mother's death, is \"Hallam Foe\" [Doc 39389].", "latency_ms": 3516.303062438965, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Hallam Foe' and closely matches the gold answer. It is faithful to the given information and does not introduce unsupported details.", "main_total_tokens": 3183, "main_prompt_tokens": 3135, "main_completion_tokens": 48, "judge_total_tokens": 506, "judge_prompt_tokens": 440, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The provided context does not contain any information about a character whose last name could initially be mistaken as sounding like they are a friend or a foe. Therefore, based on the given documents, it is not possible to answer this question.", "latency_ms": 3415.104389190674, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is \"A foe\". The generated answer claims the question cannot be answered from the context, which contradicts the gold answer, so it is factually incorrect with respect to the task. However, given that we are not provided the original context here, the model's answer is internally consistent with its stated lack of information and does not fabricate specific details; it faithfully reflects its (claimed) inability to find the answer in the context.", "main_total_tokens": 3006, "main_prompt_tokens": 2959, "main_completion_tokens": 47, "judge_total_tokens": 553, "judge_prompt_tokens": 432, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "Sam's age in the 2017 TV show \"Atypical\" is 18 years old [Doc 124583].", "latency_ms": 3168.464183807373, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, which matches the gold answer exactly. It does not introduce contradictions, and the added citation-style note does not conflict with the given information, so the response is both correct and faithful.", "main_total_tokens": 2993, "main_prompt_tokens": 2960, "main_completion_tokens": 33, "judge_total_tokens": 503, "judge_prompt_tokens": 413, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 3081.9571018218994, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the TV show Atypical, Sam is on the autism spectrum, which exactly matches the gold answer. It is fully correct and faithful.", "main_total_tokens": 3170, "main_prompt_tokens": 3141, "main_completion_tokens": 29, "judge_total_tokens": 476, "judge_prompt_tokens": 410, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "The provided documents do not contain information about a movie titled \"À corde tendue\" released in 2013, nor do they mention any specific birthplace for a character named Sandra. Therefore, based on the given context, it is not possible to answer the question.", "latency_ms": 3918.3428287506104, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates that the correct response is 'Chamonix', but the generated answer claims the information is not available and that the question cannot be answered from the context. This is incorrect relative to the gold, so correctness is 0. However, the generated answer accurately reflects that, within the provided context, the information was not present and does not fabricate an answer; it clearly states its limitation based on the documents. Thus, it is faithful to the given context, so faithfulness is 1.", "main_total_tokens": 3437, "main_prompt_tokens": 3379, "main_completion_tokens": 58, "judge_total_tokens": 583, "judge_prompt_tokens": 450, "judge_completion_tokens": 133}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "in this japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 3229.1228771209717, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and correctly fills the blank. It is fully faithful to the provided information with no hallucinations or deviations.", "main_total_tokens": 3187, "main_prompt_tokens": 3161, "main_completion_tokens": 26, "judge_total_tokens": 476, "judge_prompt_tokens": 412, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the described German language animated series, Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 3411.6291999816895, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which matches the gold answer exactly. It is fully consistent with the provided information and does not introduce unsupported claims beyond referencing the document ID.", "main_total_tokens": 3311, "main_prompt_tokens": 3276, "main_completion_tokens": 35, "judge_total_tokens": 499, "judge_prompt_tokens": 424, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is titled **One Afternoon** [Doc 96838].", "latency_ms": 3619.2407608032227, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'One Afternoon' matching the gold answer, and it stays faithful to the provided information without adding unsupported claims.", "main_total_tokens": 3055, "main_prompt_tokens": 3004, "main_completion_tokens": 51, "judge_total_tokens": 510, "judge_prompt_tokens": 449, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie where a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" [Doc 93260]. This movie, released in 2020, has a rating of 6.746 and is described as a drama involving a daycare business owner's encounter with a young street child.", "latency_ms": 10138.815641403198, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as \"The Life Ahead,\" matching the gold answer, so correctness is full. It largely stays faithful to the described plot (Holocaust survivor caring for a 12-year-old street kid who robbed her). The small extra details about a daycare business and specific rating are slightly speculative relative to the prompt, so I slightly reduce faithfulness.", "main_total_tokens": 3764, "main_prompt_tokens": 3683, "main_completion_tokens": 81, "judge_total_tokens": 576, "judge_prompt_tokens": 469, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about Bruce McLaren, a New Zealander who founded the McLaren Motor Racing team. [Doc 73859]", "latency_ms": 3624.7684955596924, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and adds accurate contextual detail (that he was a New Zealander and founder of the McLaren racing team). It matches the gold answer without introducing contradictions, so it is both correct and faithful.", "main_total_tokens": 3353, "main_prompt_tokens": 3313, "main_completion_tokens": 40, "judge_total_tokens": 507, "judge_prompt_tokens": 419, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The documents provided do not specify the name of Danny Krueger's racing team in \"Born to Race: Fast Track.\" [Doc 63384]", "latency_ms": 3489.098072052002, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the team as the McLaren Motor Racing team. The generated answer claims the documents do not specify the team name, which is factually incorrect with respect to the gold. However, the generated answer is faithful to the cited document snippet (stating what it sees/notices rather than hallucinating), so faithfulness is high but correctness relative to the gold is zero.", "main_total_tokens": 3439, "main_prompt_tokens": 3404, "main_completion_tokens": 35, "judge_total_tokens": 522, "judge_prompt_tokens": 414, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The provided context does not specify a particular individual's country of origin without additional details about who \"he\" refers to. Could you please clarify or provide more information about the specific individual in question?", "latency_ms": 3478.811740875244, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a known gold answer: New Zealand. The generated answer instead claims the context does not specify and asks for clarification, so it fails to give the correct country and is incorrect (score 0). However, it is faithful to the (implicit) lack of context it perceives and does not hallucinate a country, so in terms of faithfulness to its own stated reasoning, it is fully faithful (score 1).", "main_total_tokens": 3193, "main_prompt_tokens": 3153, "main_completion_tokens": 40, "judge_total_tokens": 532, "judge_prompt_tokens": 414, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "None of the documents provided contain information about a 2013 movie where John McClane blows up the Great Wall of China. The closest reference is to \"A Good Day to Die Hard,\" which is from 2013 and features John McClane, but there's no mention of him blowing up the Great Wall of China [Doc 58791].", "latency_ms": 4095.1313972473145, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no 2013 movie where John McClane blows up the Great Wall of China and identifies the closest relevant film, 'A Good Day to Die Hard', while clarifying that it does not contain that event. This aligns with the gold answer’s core claim that such a movie is not in the database and is consistent with the referenced document, so both correctness and faithfulness are high.", "main_total_tokens": 3229, "main_prompt_tokens": 3151, "main_completion_tokens": 78, "judge_total_tokens": 586, "judge_prompt_tokens": 469, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with the described plot is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 3389.16015625, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Tomie: Forbidden Fruit,' which matches the gold answer. It is fully consistent with the provided information and does not introduce unsupported content.", "main_total_tokens": 3198, "main_prompt_tokens": 3166, "main_completion_tokens": 32, "judge_total_tokens": 510, "judge_prompt_tokens": 443, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years in the 2021 movie \"Lei mi parla ancora\" [Doc 98566].", "latency_ms": 3543.3714389801025, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that Nino and Caterina have been married for 65 years, which matches the gold answer of 65. It is correct and consistent with the provided reference note.", "main_total_tokens": 3498, "main_prompt_tokens": 3459, "main_completion_tokens": 39, "judge_total_tokens": 493, "judge_prompt_tokens": 424, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In \"Scorpion King: Book of Souls\" (2018), The Scorpion King and Tala search for the legendary relic known as The Book of Souls. Their goal is to use this artifact to put an end to an evil warlord [Doc 82582].", "latency_ms": 4234.015703201294, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly states that in 'Scorpion King: Book of Souls' (2018), the Scorpion King and Tala are searching for the Book of Souls, matching the gold answer. The additional detail about using it to end an evil warlord is accurate to the movie, but the bracketed citation-like tag '[Doc 82582]' is fabricated and not grounded in the provided context, slightly reducing faithfulness.", "main_total_tokens": 3867, "main_prompt_tokens": 3806, "main_completion_tokens": 61, "judge_total_tokens": 566, "judge_prompt_tokens": 448, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document provided does not contain information about a movie titled \"Zombies on the Moon\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 3244.365692138672, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the system lacks database information about the movie, while the generated answer claims the limitation is due to the provided document lacking that information. This changes the stated reason for not answering from database absence to context absence, which is not faithful to the gold. Therefore, correctness and faithfulness relative to the gold response are both 0.", "main_total_tokens": 2907, "main_prompt_tokens": 2870, "main_completion_tokens": 37, "judge_total_tokens": 527, "judge_prompt_tokens": 427, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "None of the documents provided contain information about a movie titled \"The Galactic Baker.\" Therefore, the release year for \"The Galactic Baker\" cannot be determined from the given context.", "latency_ms": 3623.7289905548096, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database. The generated answer says the documents do not contain information about the movie and that the release year cannot be determined. Both convey that the system cannot provide the year. This is correct given no information, and it is faithful to the provided context.", "main_total_tokens": 3527, "main_prompt_tokens": 3491, "main_completion_tokens": 36, "judge_total_tokens": 515, "judge_prompt_tokens": 424, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document provided does not contain information about the movie \"Return of the Null Pointer\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 3390.9506797790527, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database and thus no year can be given. The generated answer similarly says the provided document does not contain information about the movie and that it cannot provide the release year from the given context. Both refrain from fabricating a date, so the answer is factually correct relative to the information available and faithfully reflects the lack of information in the context.", "main_total_tokens": 3187, "main_prompt_tokens": 3151, "main_completion_tokens": 36, "judge_total_tokens": 536, "judge_prompt_tokens": 426, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document does not contain information on a movie titled \"Data Science: The Movie\". Therefore, I cannot provide a release year for this specific title based on the given context.", "latency_ms": 3410.7894897460938, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie that does not exist in the provided data. The gold answer states the movie is not in the database, and the generated answer similarly states that the document does not contain information on that movie and therefore it cannot provide a release year. This is factually correct and consistent with the given context, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3122, "main_prompt_tokens": 3086, "main_completion_tokens": 36, "judge_total_tokens": 535, "judge_prompt_tokens": 426, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document provided does not contain information about a movie titled \"Python's Revenge 2\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 3722.9790687561035, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the information about 'Python's Revenge 2' is not available, and thus the release year cannot be provided. This is correct given the lack of data and faithful to the implied context (no such movie in the provided or known database).", "main_total_tokens": 3492, "main_prompt_tokens": 3455, "main_completion_tokens": 37, "judge_total_tokens": 514, "judge_prompt_tokens": 427, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no document provided that contains information about a movie titled \"The Last Pixel\". Therefore, I cannot provide a release year for this specific title based on the given context.", "latency_ms": 3258.145332336426, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database. The generated answer says there is no document with information about that movie and thus cannot provide a release year. Both effectively decline to provide a date and do not hallucinate, so the response is correct and faithful to the gold.", "main_total_tokens": 2834, "main_prompt_tokens": 2798, "main_completion_tokens": 36, "judge_total_tokens": 512, "judge_prompt_tokens": 424, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no document provided that mentions a release year for \"The man who ate the sun\". Therefore, I cannot provide a release year based on the given documents.", "latency_ms": 4812.018156051636, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the system should not provide a release year because the movie is not in the database. The generated answer likewise refrains from guessing and clearly states that no document provides the release year and thus it cannot answer. This is correct with respect to the task and faithful to the (implied) context about available documents.", "main_total_tokens": 3714, "main_prompt_tokens": 3680, "main_completion_tokens": 34, "judge_total_tokens": 522, "judge_prompt_tokens": 425, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The document does not provide information about who directed the movie \"The Last Days of Lehman Brothers\".", "latency_ms": 3257.147789001465, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the document does not provide the director information. This matches the gold answer, which says that the director information is not available in the database, so it is both correct and faithful to the provided context.", "main_total_tokens": 3479, "main_prompt_tokens": 3458, "main_completion_tokens": 21, "judge_total_tokens": 494, "judge_prompt_tokens": 417, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about who directed the movie \"A Love to Die For\". [Doc 131220] through [Doc 88425] detail various movies and TV shows but none of them specify a director for \"A Love to Die For\".", "latency_ms": 3897.648334503174, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both gold and generated answers state that the director information is not available in the provided data. The generated answer accurately reflects the lack of information and does not introduce unsupported details, so it is both correct with respect to the question (given the database limitation) and faithful to the described documents.", "main_total_tokens": 3318, "main_prompt_tokens": 3257, "main_completion_tokens": 61, "judge_total_tokens": 536, "judge_prompt_tokens": 448, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The document does not provide information on who directed the movie \"Journey into Gaza\". [Doc 111360]", "latency_ms": 3302.5147914886475, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the document does not provide information on who directed the movie, which aligns with the gold answer that the director information is not available in the database. It is both factually correct relative to the given context and faithful to the source limitation.", "main_total_tokens": 3408, "main_prompt_tokens": 3380, "main_completion_tokens": 28, "judge_total_tokens": 499, "judge_prompt_tokens": 415, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about who directed the TV show \"The Last Night at Tremore Beach\". [Doc 135098]", "latency_ms": 3101.6600131988525, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not contain information about who directed the work, aligning with the gold response that the director information is not available in the database. It is fully faithful to the provided context because it explicitly notes the absence of that information rather than hallucinating a director.", "main_total_tokens": 2957, "main_prompt_tokens": 2924, "main_completion_tokens": 33, "judge_total_tokens": 514, "judge_prompt_tokens": 425, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The scenario where a group of archaeology students awaken an ancient mummy occurs in the movie \"Rise of the Mummy\" [Doc 98375]. Additionally, a similar situation is described in \"Ancient Evil: Scream of the Mummy,\" where six young archaeology students discover and disturb an ancient Aztec mummy [Doc 24655].", "latency_ms": 4025.5448818206787, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The question asks for a single movie; the gold answer is \"Rise of the Mummy.\" The generated answer correctly identifies \"Rise of the Mummy\" as the movie containing the described event, so correctness is 1. However, it also adds another film, \"Ancient Evil: Scream of the Mummy,\" which is extra information beyond the gold. This is mostly faithful but not perfectly minimal, so faithfulness is slightly reduced.", "main_total_tokens": 3153, "main_prompt_tokens": 3073, "main_completion_tokens": 80, "judge_total_tokens": 581, "judge_prompt_tokens": 459, "judge_completion_tokens": 122}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The scenario where Sarah fakes her own death to escape her abusive spouse is described in the plot of the movie \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 3422.369956970215, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Stranger in My Bed,\" which matches the gold answer. It stays fully consistent with the provided information and cites the document reference without adding unsupported claims.", "main_total_tokens": 3088, "main_prompt_tokens": 3049, "main_completion_tokens": 39, "judge_total_tokens": 501, "judge_prompt_tokens": 431, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The scenario where a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years is described in the plot of \"Game of Silence\" [Doc 123938].", "latency_ms": 3160.658597946167, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question explicitly asks for a movie, but 'Game of Silence' is a TV series, not a film. The generated answer repeats the gold title and is consistent with it and the described plot, so it is faithful to the provided information but factually incorrect with respect to the question's requirement for a movie.", "main_total_tokens": 2751, "main_prompt_tokens": 2708, "main_completion_tokens": 43, "judge_total_tokens": 528, "judge_prompt_tokens": 434, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie in question is \"The Craigslist Killer\" [Doc 53039]. This film is based on a true story about Philip Markoff, a charismatic and popular medical student at Boston University who leads a double life as a brutal and cruel sexual deviant who abuses prostitutes he finds via Craigslist.", "latency_ms": 3643.583059310913, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the description; the gold answer is \"The Craigslist Killer.\" The generated answer correctly identifies the film as \"The Craigslist Killer\" and gives accurate contextual details about Philip Markoff and the premise. It does not contradict the prompt or add unsupported claims; any added detail is consistent with known information about the movie, so both correctness and faithfulness are high.", "main_total_tokens": 2961, "main_prompt_tokens": 2898, "main_completion_tokens": 63, "judge_total_tokens": 558, "judge_prompt_tokens": 451, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The scenario where Yuki Hirano fails his university entrance exam and then decides to become a forestry trainee is from the movie \"Wood Job!\" [Doc 66094].", "latency_ms": 4279.024600982666, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", which matches the gold answer exactly. It is directly grounded in the provided information and does not introduce any unsupported or incorrect details, so both correctness and faithfulness are full.", "main_total_tokens": 4384, "main_prompt_tokens": 4345, "main_completion_tokens": 39, "judge_total_tokens": 512, "judge_prompt_tokens": 434, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The described scenario occurs in the movie titled \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 3350.7378101348877, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Sesame Street: The Magical Wand Chase,' which matches the gold answer exactly. It stays faithful to the question and does not introduce unsupported or contradictory information beyond restating the correct title.", "main_total_tokens": 3435, "main_prompt_tokens": 3406, "main_completion_tokens": 29, "judge_total_tokens": 511, "judge_prompt_tokens": 434, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The described scenario occurs in the movie titled \"Beyond\" [Doc 74597]. In this plot, a 7-year-old boy is faced with the challenge of saving his parents who are trapped inside a burning car due to a rock fall.", "latency_ms": 3424.349546432495, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question’s gold answer is “Beyond,” and the generated answer also identifies the movie as “Beyond” and accurately restates the described scenario. This matches both the query and the provided gold reference, so it is correct and faithful to the given information.", "main_total_tokens": 2923, "main_prompt_tokens": 2870, "main_completion_tokens": 53, "judge_total_tokens": 534, "judge_prompt_tokens": 457, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The plot described happens in the movie \"6ixtynin9\" [Doc 22478].", "latency_ms": 3251.1322498321533, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"6ixtynin9,\" which matches the gold answer exactly. It does not introduce contradictions or extraneous claims beyond a citation-style note, so it is fully faithful to the given information.", "main_total_tokens": 3015, "main_prompt_tokens": 2990, "main_completion_tokens": 25, "judge_total_tokens": 502, "judge_prompt_tokens": 423, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie where a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort, and it's up to the club's staff to stop the violence or at least hide it is \"Club Dread\" [Doc 32410].", "latency_ms": 3736.8462085723877, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' matching the gold answer exactly. It remains faithful to the provided information and does not introduce contradictions or unrelated content.", "main_total_tokens": 3353, "main_prompt_tokens": 3300, "main_completion_tokens": 53, "judge_total_tokens": 539, "judge_prompt_tokens": 473, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The described scenario occurs in the movie titled \"Toilet\" [Doc 48873].", "latency_ms": 3442.2667026519775, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" which matches the gold label. It does not introduce any contradictions or unsupported claims beyond restating the title, so it is both correct and faithful to the given information.", "main_total_tokens": 3667, "main_prompt_tokens": 3645, "main_completion_tokens": 22, "judge_total_tokens": 508, "judge_prompt_tokens": 432, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The description provided matches the plot summary for \"Electric Angel\" [Doc 1024]. This movie is described as an experimental film that uses a variety of audiovisual techniques to create the sense of polymorphic eroticism, inspired by European and Mediterranean cinematography from the 20th century.", "latency_ms": 4063.3063316345215, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Electric Angel,\" which matches the gold answer. It stays faithful to the given description by not adding unsupported claims beyond elaborating on the description already present, so both correctness and faithfulness are high.", "main_total_tokens": 3519, "main_prompt_tokens": 3457, "main_completion_tokens": 62, "judge_total_tokens": 552, "judge_prompt_tokens": 473, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 based on the provided documents is \"Bottom of the Water\" with a rating of 4.2 [Doc 106899].", "latency_ms": 2468.430280685425, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold answer identifies 'Mega Ape' as the lowest rated 2023 movie, while the model claims it is 'Bottom of the Water' with a 4.2 rating. This contradicts the gold answer and the referenced documents, so it is neither correct nor faithful to the provided information.", "main_total_tokens": 2914, "main_prompt_tokens": 2873, "main_completion_tokens": 41, "judge_total_tokens": 512, "judge_prompt_tokens": 415, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "Based on the provided documents, the highest-rated TV show from 2010 is \"Dog's Most Wanted\" with a rating of 10.0 [Doc 128300]. However, its genre information is missing and there is no description available in the given context for this document. Other highly rated shows from 2010 include \"Justified,\" \"Downton Abbey,\" and \"Suits\" which all have a rating of around 8 or above [Doc 119156], [Doc 119146], [Doc 119666].", "latency_ms": 4036.238670349121, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question asks for the highest rated TV show from 2010, and the gold answer is 'Tea House'. The generated answer instead claims 'Dog's Most Wanted' is the highest rated, which directly contradicts the gold answer, so correctness is 0. In terms of faithfulness, the answer refers to specific documents and ratings that appear to come from the given context, but we cannot verify those here; still, it clearly does not align with the gold label and thus is only partially faithful to the task, warranting a mid-range score.", "main_total_tokens": 3402, "main_prompt_tokens": 3270, "main_completion_tokens": 132, "judge_total_tokens": 630, "judge_prompt_tokens": 491, "judge_completion_tokens": 139}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest-rated Japanese language movie from 1999 based on the provided documents is \"Kikujiro\" with a rating of 7.5 [Doc 22213].", "latency_ms": 3655.3549766540527, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'The Uncut Story of Mewtwo's Origin' as the highest rated Japanese language movie from 1999, while the generated answer claims it is 'Kikujiro.' This directly contradicts the gold answer, so it is incorrect. It also shows the model relied on a document reference rather than the gold label for this QA pair, so with respect to the given ground truth it is not faithful.", "main_total_tokens": 3379, "main_prompt_tokens": 3337, "main_completion_tokens": 42, "judge_total_tokens": 545, "judge_prompt_tokens": 427, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 based on the provided documents is \"Read Lips\" with a rating of 4.5 [Doc 354]. The language of this movie is Chinese (cn).", "latency_ms": 3388.2036209106445, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer contradicts the gold answer: it claims the language is Chinese while the gold indicates English, so it is factually incorrect. It also references unspecified documents to justify the wrong movie and language, which shows it is not faithful to the given gold standard.", "main_total_tokens": 2926, "main_prompt_tokens": 2880, "main_completion_tokens": 46, "judge_total_tokens": 508, "judge_prompt_tokens": 424, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "Based on the provided documents, the TV show with the highest popularity in 1999 is \"Hollywood Squares\" with a popularity of 7.0597 [Doc 114940].", "latency_ms": 4451.095342636108, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is factually incorrect relative to the gold label; it states that \"Hollywood Squares\" had the highest popularity in 1999, while the gold answer is \"Law & Order: Special Victims Unit.\" It is also unfaithful to the gold reference because it relies on a different source (Doc 114940) and contradicts the gold answer rather than aligning with it.", "main_total_tokens": 4287, "main_prompt_tokens": 4239, "main_completion_tokens": 48, "judge_total_tokens": 537, "judge_prompt_tokens": 427, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 3261.218309402466, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 and correctly attributes it. It is both correct and faithful to the provided information.", "main_total_tokens": 3147, "main_prompt_tokens": 3125, "main_completion_tokens": 22, "judge_total_tokens": 460, "judge_prompt_tokens": 400, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "There are multiple movies titled \"The Story of Us\" with different release years:\n\n1. [Doc 22481] Year: 1999\n2. [Doc 88811] Year: 2019\n3. [Doc 112077] Year: 2025\n\nSo, depending on which movie you are referring to, the release years are 1999, 2019, or 2025.", "latency_ms": 4598.793506622314, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies 2019, indicating the question refers to that specific film. The generated answer lists 1999, 2019, and 2025 without identifying which matches the question, so it does not correctly answer with 2019. However, it faithfully reflects the information it cites (multiple movies with those years) and does not invent details beyond that, so faithfulness is high even though the final answer is incorrect for the labeled question.", "main_total_tokens": 3165, "main_prompt_tokens": 3056, "main_completion_tokens": 109, "judge_total_tokens": 593, "judge_prompt_tokens": 470, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The document does not provide the exact number of votes \"My Dog Skip\" received; it only provides a popularity score of 1.7749 and a rating of 6.7 [Doc 24114]. Without additional context, we cannot determine the actual number of votes.", "latency_ms": 3940.3140544891357, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that 'My Dog Skip' received 279 votes, but the generated answer incorrectly claims the number of votes cannot be determined. This is factually incorrect relative to the gold. However, the generated answer is faithful to the described document content (it accurately reports what the document provides and notes the absence of the vote count), so faithfulness is high.", "main_total_tokens": 3393, "main_prompt_tokens": 3331, "main_completion_tokens": 62, "judge_total_tokens": 542, "judge_prompt_tokens": 437, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The provided documents do not specify the number of votes for \"Oh My God!\". The rating and popularity scores are given, but these are not equivalent to vote counts. Therefore, based on the context provided, it is not possible to determine the exact number of votes received by \"Oh My God!\".", "latency_ms": 3834.740161895752, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that \"Oh My God!\" received 12 votes, but the generated answer claims that the number of votes cannot be determined. This is factually incorrect with respect to the gold. However, the generated answer is internally consistent with its stated reasoning and does not introduce contradictions or hallucinated details beyond the given (though it overlooks the gold value), so it is faithful to its own cited context.", "main_total_tokens": 3218, "main_prompt_tokens": 3158, "main_completion_tokens": 60, "judge_total_tokens": 549, "judge_prompt_tokens": 440, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 3438.880205154419, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Last Days of Lehman Brothers' was released in 2009, which matches the gold answer. It is fully consistent with the provided information and does not add unsupported claims.", "main_total_tokens": 3480, "main_prompt_tokens": 3451, "main_completion_tokens": 29, "judge_total_tokens": 484, "judge_prompt_tokens": 410, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 3262.8579139709473, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the original language of 'The King's Speech' is English, which matches the gold answer 'en'. It directly reflects the information in the gold without adding unsupported details, so it is both correct and faithful.", "main_total_tokens": 3465, "main_prompt_tokens": 3441, "main_completion_tokens": 24, "judge_total_tokens": 480, "judge_prompt_tokens": 402, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The movie \"Wall Street\" was released in the year 1987 [Doc 6993].", "latency_ms": 2679.58402633667, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'Wall Street' was released in 1987, which matches the gold answer. It is accurate and aligns with the provided information, so it is both correct and faithful.", "main_total_tokens": 2485, "main_prompt_tokens": 2460, "main_completion_tokens": 25, "judge_total_tokens": 472, "judge_prompt_tokens": 398, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "None of the provided documents describe a movie in which classmates play a survival game using their cell phones directly. However, [Doc 90746] describes a scenario where two teenage girls use their phone to summon the supernatural for a game that claims if you lose, you lose your life. This is close but specifies only two individuals rather than a group of classmates. Therefore, based strictly on the provided context, there isn't a movie where classmates play a survival game using their cell phones.", "latency_ms": 3938.694477081299, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the movie title, which is 'Brain Fluid Explosion Girl.' The generated answer claims that no such movie can be identified from the documents, so it fails to provide the correct title and is therefore incorrect. However, it accurately reflects the limitation of the provided documents and does not invent a title or unsupported information, so it is faithful to the given context.", "main_total_tokens": 2286, "main_prompt_tokens": 2185, "main_completion_tokens": 101, "judge_total_tokens": 589, "judge_prompt_tokens": 484, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show where six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" from 1983 [Doc 112849].", "latency_ms": 2880.5155754089355, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the show as the 1983 animated series 'Dungeons & Dragons.' It is fully correct and consistent with the provided information.", "main_total_tokens": 2267, "main_prompt_tokens": 2227, "main_completion_tokens": 40, "judge_total_tokens": 491, "judge_prompt_tokens": 423, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie that is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is \"Nacht der Wölfe\" [Doc 2590].", "latency_ms": 3096.3199138641357, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 'Nacht der Wölfe' as the movie and matches the gold answer exactly. It restates information explicitly given in the source (Doc 2590), so it is both correct and faithful.", "main_total_tokens": 2493, "main_prompt_tokens": 2449, "main_completion_tokens": 44, "judge_total_tokens": 516, "judge_prompt_tokens": 439, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of Picnic at Hanging Rock take place in the year 1900 [Doc 126191].", "latency_ms": 2558.7596893310547, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of Picnic at Hanging Rock take place in 1900, matching the gold answer. It is fully consistent with the provided information and does not introduce unsupported claims.", "main_total_tokens": 2302, "main_prompt_tokens": 2273, "main_completion_tokens": 29, "judge_total_tokens": 477, "judge_prompt_tokens": 406, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The movie described in the question matches the plot summary provided for \"June\" [Doc 32483]. This film, released in 2004, centers around a young violinist named June who works at an oddball Venice coffee shop by day and dreams of romance with a beautiful man destined for her. The description fits perfectly with the details provided in the question about the character's daily routine, aspirations, and romantic circumstances.", "latency_ms": 3915.638208389282, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"June,\" matching the gold answer and the plot details in the question. It remains faithful to the prompt, restating and aligning with the described elements (2004 release, young violinist June, Venice coffee shop, romantic daydreams) without adding unsupported information.", "main_total_tokens": 2637, "main_prompt_tokens": 2546, "main_completion_tokens": 91, "judge_total_tokens": 610, "judge_prompt_tokens": 517, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1515.2842998504639, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific factual movie title. Saying “I don't know” does not provide the correct answer, so correctness is 0. However, the model does not fabricate or contradict the given gold answer; it simply admits not knowing, which is faithful to its own response, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 489, "judge_prompt_tokens": 399, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 405.96818923950195, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a clear numeric answer (7.6), but the generated response says 'I don't know,' which is incorrect, so correctness is 0. The model does not fabricate an answer or contradict the given information; it simply admits not knowing, so it is fully faithful to its own stated uncertainty, giving a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 389, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1140.3477191925049, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.5143371794949736, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.013881288441812, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is incorrect. However, the evaluation here is strictly about that mismatch and does not introduce any additional unsupported assumptions, so it is faithful to the given information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 473, "judge_prompt_tokens": 394, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 327.84152030944824, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is factually incorrect. However, there is no indication that the model contradicted its own reasoning process or the information it had; it simply produced the wrong result. Thus, correctness is 0 while faithfulness to its (unknown) internal reasoning is assumed to be 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 499, "judge_prompt_tokens": 395, "judge_completion_tokens": 104}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1627.9423236846924, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not match the correct answer 'Fantômas,' so it is incorrect. However, it is faithful to the model's uncertainty and does not introduce any hallucinated information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 389, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie that matches your description is \"Kill Cruise\" [Doc 11205]. It describes a plot where two British beauties go to Barbados with a yacht captain who does not know what he's in for.", "latency_ms": 3228.616952896118, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' which matches the gold answer exactly. It remains faithful to the provided information, repeating the title and adding a brief description consistent with the query and not contradicting the gold.", "main_total_tokens": 2518, "main_prompt_tokens": 2470, "main_completion_tokens": 48, "judge_total_tokens": 503, "judge_prompt_tokens": 426, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 2970.3400135040283, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Won't You Be My Neighbor?' and does not introduce contradictions or unsupported content.", "main_total_tokens": 2538, "main_prompt_tokens": 2495, "main_completion_tokens": 43, "judge_total_tokens": 503, "judge_prompt_tokens": 440, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The question \"What year was it released?\" can be answered for any of the movies listed in the context provided. However, without a specific movie title to refer to, I will provide the release year for the first movie mentioned in the documents:\n\nThe documentary titled \"YEAR\" was released in 2023 [Doc 107925].", "latency_ms": 3564.621925354004, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the correct release year is 2018, but the generated answer claims 2023, so it is factually incorrect. It also fabricates context about a documentary titled \"YEAR\" and a specific document reference not present in the prompt, so it is unfaithful to the given information.", "main_total_tokens": 2623, "main_prompt_tokens": 2549, "main_completion_tokens": 74, "judge_total_tokens": 533, "judge_prompt_tokens": 444, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that fits the description of being an animation about a young boy robot in 2030 AD is \"Astro Boy\" [Doc 112591]. However, it should be noted that while Astro Boy is set in 2030 AD and involves a young boy robot, the context provided does not specify this series as an animation. The description matches closely with the query, mentioning a young boy robot who is rescued and adopted by Dr Elefun, guiding him to use his special powers for aiding mankind [Doc 112591].", "latency_ms": 4504.067897796631, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies 'Astro Boy' as the TV series, matching the gold answer, so correctness is 1. The reasoning is mostly grounded in the provided document, accurately describing the boy robot, Dr. Elefun, and the 2030 AD setting, but it incorrectly casts doubt by saying the context does not specify the series as an animation, even though the question itself does; this slightly reduces faithfulness.", "main_total_tokens": 2686, "main_prompt_tokens": 2565, "main_completion_tokens": 121, "judge_total_tokens": 614, "judge_prompt_tokens": 495, "judge_completion_tokens": 119}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, a woman and her young son were held captive for 7 years [Doc 67513].", "latency_ms": 2694.3793296813965, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9463946303571863, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer. It is also fully faithful to the provided information without adding unsupported details.", "main_total_tokens": 2094, "main_prompt_tokens": 2057, "main_completion_tokens": 37, "judge_total_tokens": 495, "judge_prompt_tokens": 425, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26m before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 3007.899045944214, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer directly matches the provided gold value of £26m and correctly restates it as gold bullion worth £26m. It does not introduce contradictions or unsupported details beyond what is given, so it is both correct and faithful to the source.", "main_total_tokens": 2407, "main_prompt_tokens": 2363, "main_completion_tokens": 44, "judge_total_tokens": 509, "judge_prompt_tokens": 428, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6352.419376373291, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score, which is given in the gold answer as 80.5772. The generated answer says only 'I don't know', which does not provide the requested value, so it is incorrect. However, it does not introduce any incorrect factual claims and honestly expresses uncertainty, so it is fully faithful to the model's lack of knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 502, "judge_prompt_tokens": 396, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 280.95483779907227, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that there was 1 Russian language TV show released in 1997, while the generated answer is 0. This directly contradicts the gold answer, so the generated answer is incorrect. The explanation here correctly reflects the discrepancy, so it is faithful to the provided gold and generated answers.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 385, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "The information provided indicates that Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 3048.252582550049, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Angel Eyes', which matches the gold answer exactly. It is also faithful to the provided information, citing the same show and not introducing contradictory or extraneous claims.", "main_total_tokens": 2307, "main_prompt_tokens": 2262, "main_completion_tokens": 45, "judge_total_tokens": 507, "judge_prompt_tokens": 434, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French [Doc 110681].", "latency_ms": 2424.4778156280518, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states the language is French, while the gold answer specifies Korean, so it is factually incorrect. There is no indication in the prompt that supports French; the model appears to rely on an incorrect or fabricated document reference, so it is not faithful to the given information.", "main_total_tokens": 2182, "main_prompt_tokens": 2160, "main_completion_tokens": 22, "judge_total_tokens": 476, "judge_prompt_tokens": 393, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 766.8735980987549, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer is 0, so the result is incorrect. However, the assistant clearly and directly gave the answer 0 without adding fabricated reasoning or extra unsupported claims, so it is faithful to its own (albeit wrong) output.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 394, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Based on the provided context, there is no movie from 1979 with a rating. Therefore, we cannot determine the best-rated movie released in 1979 from this information.", "latency_ms": 3089.9150371551514, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer explicitly states that there are no movies from 1979 in the database. The generated answer adds an extra constraint about ratings and concludes we cannot determine the best-rated movie because there is no movie from 1979 with a rating. This differs subtly but importantly from the gold, which is about absence from the database, not absence of ratings. Thus it is not fully correct with respect to the labeled answer. However, it remains faithful to the provided context (it does not introduce external facts or contradict the context scenario) and stays within that frame, so faithfulness is high.", "main_total_tokens": 2411, "main_prompt_tokens": 2370, "main_completion_tokens": 41, "judge_total_tokens": 580, "judge_prompt_tokens": 431, "judge_completion_tokens": 149}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The provided context does not contain information regarding a fourth part of the Back to the Future series being released. Therefore, based strictly on the given documents, there is no answer to the question about which year Back to the Future Part IV was released [Doc 9405][Doc 10621].", "latency_ms": 3570.5883502960205, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly indicates that there is no information about a fourth part of the Back to the Future series and therefore no release year can be given, aligning with the gold answer that Part IV does not exist in the database. It does not introduce incorrect facts and stays grounded in the provided context, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2742, "main_prompt_tokens": 2677, "main_completion_tokens": 65, "judge_total_tokens": 559, "judge_prompt_tokens": 459, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there are no movies from 1980 about the Chernobyl disaster, which is accurate because the Chernobyl disaster occurred in 1986. The generated answer is faithful to the question and does not invent any movies or scores; it simply notes that nothing matches the criteria, which aligns with the gold answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 537, "judge_prompt_tokens": 434, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 329.6675682067871, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says there are 0 TV shows or movies with 'Astro Boy' in the title, but the gold answer is 4. This is factually incorrect, so correctness is 0. The question is directly about the count in the title and the model clearly attempts to answer that question (it doesn’t change or ignore the task), so it is faithful to the user’s request, yielding a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 509, "judge_prompt_tokens": 388, "judge_completion_tokens": 121}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 265.331506729126, "retrieval_recall": 0.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.17723928678404774, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a clear numeric gold answer (2), but the generated answer is 1, which is incorrect. The model otherwise follows the format and does not invent extraneous information, so it is faithful to its own (incorrect) reasoning but not correct with respect to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 386, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 187.69192695617676, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1 TV show, but the generated answer is 0, so it is factually incorrect. However, the generated answer is internally consistent and directly answers the question without contradictions, so it is faithful to itself.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 467, "judge_prompt_tokens": 389, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 218.46342086791992, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (0) and is fully supported by the given information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 441, "judge_prompt_tokens": 391, "judge_completion_tokens": 50}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 171.88501358032227, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto', but the generated answer says 'I don't know', which is incorrect. However, the model is honest about its lack of knowledge and does not hallucinate, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 462, "judge_prompt_tokens": 386, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The provided documents do not mention any crossover movie between the \"Jurassic Park\" series and the \"Power Rangers.\" Therefore, based on the given context, there is no information about when such a crossover movie was released [None of the Documents].", "latency_ms": 3041.447639465332, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states explicitly that the movie was not released, while the generated answer only claims there is no information in the provided documents about such a crossover being released. This leaves open the possibility that it exists but is undocumented in the context, which conflicts with the gold. Therefore, it is not correct relative to the gold and not fully faithful to the required answer.", "main_total_tokens": 2445, "main_prompt_tokens": 2395, "main_completion_tokens": 50, "judge_total_tokens": 538, "judge_prompt_tokens": 434, "judge_completion_tokens": 104}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "None of the documents provided contain information about a movie where Barbie fights the Predator. [Doc 82000] mentions \"The Predator,\" but it does not involve Barbie. Similarly, there are several entries for Barbie movies in different contexts and genres, but none depict her fighting a Predator. [Doc X].", "latency_ms": 3408.451795578003, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clarifies that such a movie was not released, implying the correct response is that it does not exist. The generated answer instead focuses on the absence of such a movie in the provided documents, not on its real-world (non)existence, so it fails to directly answer the question and is therefore incorrect. However, it remains faithful to the stated lack of evidence in the documents and does not invent any information, so it is fully faithful.", "main_total_tokens": 2484, "main_prompt_tokens": 2419, "main_completion_tokens": 65, "judge_total_tokens": 566, "judge_prompt_tokens": 443, "judge_completion_tokens": 123}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "Based on the provided documents, the 4th oldest Star Wars movie would be \"The Empire Strikes Back\" [Doc 3], which was released in 1980.\n\nHowever, it's important to clarify that this answer is based on the information given and does not include all Star Wars movies. For instance, the original trilogy (Episode IV, V, VI) are part of the main saga, with Episode IV being \"A New Hope\" from 1977 [not directly mentioned in provided docs], followed by \"The Empire Strikes Back\" (Episode V) in 1980, and then \"Return of the Jedi\" (Episode VI) in 1983 [Doc 2858]. Therefore, considering only these documents, \"The Empire Strikes Back\" is listed as the fourth oldest because it's the earliest one mentioned directly.", "latency_ms": 5668.881416320801, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct answer is \"Star Wars: Droids - The Great Heep,\" but the generated answer states \"The Empire Strikes Back,\" so it is factually incorrect, giving correctness 0. The explanation partially reflects the limitation of the provided documents (it notes the answer is based only on given docs and mentions some chronology), so it is somewhat grounded in the described context, but it invents a rationale that makes it the \"fourth oldest\" without explicit support. This partial grounding but incorrect conclusion merits a middling faithfulness score.", "main_total_tokens": 2740, "main_prompt_tokens": 2560, "main_completion_tokens": 180, "judge_total_tokens": 701, "judge_prompt_tokens": 559, "judge_completion_tokens": 142}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The question \"What year was it released?\" can be answered for any of the movies listed in the context provided. However, without a specific movie title to refer to, I will provide the release year for the first movie mentioned in the documents:\n\nThe documentary titled \"YEAR\" was released in 2023 [Doc 107925].", "latency_ms": 3707.4568271636963, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 1986, but the generated answer says 2023, so it is factually incorrect. It is also unfaithful to the gold answer, which explicitly specifies 1986 as the release year, while the model instead invents a different movie and year.", "main_total_tokens": 2623, "main_prompt_tokens": 2549, "main_completion_tokens": 74, "judge_total_tokens": 528, "judge_prompt_tokens": 444, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 234.0834140777588, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct rating is 8.466, but the generated answer says it does not know. This is incorrect. However, the model does not hallucinate a wrong rating; it honestly states ignorance, so it is fully faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 479, "judge_prompt_tokens": 398, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 2677.3719787597656, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med', which matches the gold answer exactly and stays faithful to the provided information.", "main_total_tokens": 2260, "main_prompt_tokens": 2223, "main_completion_tokens": 37, "judge_total_tokens": 482, "judge_prompt_tokens": 423, "judge_completion_tokens": 59}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "The seven prisoners escaped from a prison in Argentina [Doc 53582]. Specifically, they escaped from the Villa Devoto prison in Buenos Aires, Argentina.", "latency_ms": 3005.63383102417, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies the country as Argentina, matching the gold answer. However, it adds an unsupported specific detail about the exact prison (Villa Devoto in Buenos Aires) that is not present in the question or gold reference, making that part potentially unfaithful to the source.", "main_total_tokens": 2691, "main_prompt_tokens": 2656, "main_completion_tokens": 35, "judge_total_tokens": 512, "judge_prompt_tokens": 422, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice, Li Nai, falls into a trap designed by Jiang Jie [Doc 135231].", "latency_ms": 2796.69189453125, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates that the apprentice falls into Jiang Jie's trap. The generated answer explicitly states that the apprentice falls into a trap designed by Jiang Jie, which matches the gold. The extra detail about the apprentice being Li Nai does not contradict the gold and appears plausible, so the response is both correct and faithful to the reference.", "main_total_tokens": 2380, "main_prompt_tokens": 2342, "main_completion_tokens": 38, "judge_total_tokens": 510, "judge_prompt_tokens": 413, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 703.1145095825195, "retrieval_recall": 0.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.19519002499605084, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.2, "ndcg_at_10": 0.3080351663449861, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (4), so it is factually incorrect. However, it is directly answering the user’s question without adding unsupported details, so it is faithful to the prompt itself.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 462, "judge_prompt_tokens": 385, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "46", "latency_ms": 140.6548023223877, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '46' is incorrect since the gold response states there are no movies featuring both Bruce Lee and Steven Seagal together. It is also unfaithful because it directly contradicts the provided gold answer, inventing a specific number without basis in the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 395, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 3043.7943935394287, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that the rating of Tokyo Mater (2008) is 6.3, which exactly matches the gold label of 6.3. It does not introduce any contradictory or extraneous information beyond an internal document reference, so it is both correct and faithful.", "main_total_tokens": 2523, "main_prompt_tokens": 2497, "main_completion_tokens": 26, "judge_total_tokens": 490, "judge_prompt_tokens": 403, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in this movie is street racing [Doc 41929]. This can be inferred from the plot description which mentions \"a group of street racers.\" Additionally, another document also describes movies with similar themes where street racing is a central element [Doc 42313] and [Doc 44013]. However, it's important to note that these descriptions are for different movies. The specific context provided for the question \"Fast Track: No Limits\" points towards street racing as the style of racing in this movie.", "latency_ms": 4297.869682312012, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer specifies 'Drift' as the style of racing, while the generated answer claims 'street racing'. This is incorrect, so correctness is 0. The answer does cite and rely on the provided documents and explains its inference process, but it draws the wrong conclusion and conflates similar movies, so faithfulness to the referenced evidence is only partial, not entirely fabricated or hallucinated beyond the given docs.", "main_total_tokens": 2630, "main_prompt_tokens": 2514, "main_completion_tokens": 116, "judge_total_tokens": 598, "judge_prompt_tokens": 482, "judge_completion_tokens": 116}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 3224.2579460144043, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the title as 'Confronting a Serial Killer' and does not add unsupported information beyond a harmless document reference.", "main_total_tokens": 2505, "main_prompt_tokens": 2455, "main_completion_tokens": 50, "judge_total_tokens": 516, "judge_prompt_tokens": 451, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 2756.1283111572266, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score value of 7.838 for Armageddon (1998) and stays consistent with the provided information.", "main_total_tokens": 2531, "main_prompt_tokens": 2501, "main_completion_tokens": 30, "judge_total_tokens": 469, "judge_prompt_tokens": 407, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098]. This film, with its intriguing plot about a child psychologist meeting a young boy who harbors a dark secret, fits the description provided.", "latency_ms": 3066.4796829223633, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6895405204413555, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the film as 'The Sixth Sense' and does not introduce contradictions or unsupported claims relative to the question. The added plot description is broadly accurate and does not affect correctness or faithfulness.", "main_total_tokens": 2183, "main_prompt_tokens": 2121, "main_completion_tokens": 62, "judge_total_tokens": 520, "judge_prompt_tokens": 447, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 2704.0748596191406, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold year (2002) exactly, and it does not introduce conflicting or extraneous information aside from a benign document reference, so it is both correct and faithful.", "main_total_tokens": 2462, "main_prompt_tokens": 2439, "main_completion_tokens": 23, "judge_total_tokens": 466, "judge_prompt_tokens": 397, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 2844.618797302246, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the 2015 reboot title as 'The Transporter Refueled.' It does not add unsupported information; the bracketed document reference does not conflict with the source.", "main_total_tokens": 2478, "main_prompt_tokens": 2443, "main_completion_tokens": 35, "judge_total_tokens": 491, "judge_prompt_tokens": 420, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The year 'The Transporter Refueled' was released is 2015 [Doc 67500].", "latency_ms": 2666.1837100982666, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It is fully faithful to the provided information and does not introduce any unsupported details.", "main_total_tokens": 2504, "main_prompt_tokens": 2476, "main_completion_tokens": 28, "judge_total_tokens": 479, "judge_prompt_tokens": 406, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "Based on the provided documents, there is no romantic comedy in which Abraham Lincoln comes back from the dead. The closest reference to Abraham Lincoln is found in various genres such as action, fantasy, horror, and documentary, but none of these are romantic comedies [Doc 47970].", "latency_ms": 3216.5000438690186, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a romantic comedy where Abraham Lincoln returns from the dead. The gold answer says such a movie description cannot be found. The generated answer correctly states that there is no romantic comedy in which this happens and notes that references to Abraham Lincoln appear in other genres, matching the gold’s conclusion. It does not invent a nonexistent movie and remains consistent with the provided information, so it is both correct and faithful.", "main_total_tokens": 2321, "main_prompt_tokens": 2260, "main_completion_tokens": 61, "judge_total_tokens": 561, "judge_prompt_tokens": 447, "judge_completion_tokens": 114}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by Ernest Hemingway is \"Havana Darkness\" [Doc 90923]. In this plot, characters are investigating the origins of a mysterious manuscript supposedly written by Ernest Hemingway.", "latency_ms": 3167.487144470215, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'Havana Darkness,' which matches the gold answer. It remains faithful to the information implied by the question and does not introduce contradictions; the added plot detail is plausible and does not conflict with the core fact being evaluated.", "main_total_tokens": 2369, "main_prompt_tokens": 2315, "main_completion_tokens": 54, "judge_total_tokens": 518, "judge_prompt_tokens": 434, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "The TV show in which Doll heads to Hollywood to be with her childhood best friend Em is \"Doll & Em\" [Doc 122259].", "latency_ms": 2818.2425498962402, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em', which matches the gold answer exactly. It is fully supported by the referenced document and does not introduce any unsupported claims.", "main_total_tokens": 2396, "main_prompt_tokens": 2361, "main_completion_tokens": 35, "judge_total_tokens": 488, "judge_prompt_tokens": 419, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "In the movie \"Hallam Foe\" from 2007 [Doc 39389], Hallam has a talent for spying on people which he uses in an attempt to uncover the cause behind his mother's death. However, as described, this endeavor leads him into other unexpected adventures around the city.", "latency_ms": 3343.7535762786865, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the 2007 movie as 'Hallam Foe,' matching the gold answer. It adds extra plot description not present in the question or gold reference, which is somewhat speculative (e.g., 'other unexpected adventures around the city'), so while still broadly true to the film, it slightly exceeds the grounded information, resulting in a small faithfulness deduction.", "main_total_tokens": 2365, "main_prompt_tokens": 2298, "main_completion_tokens": 67, "judge_total_tokens": 568, "judge_prompt_tokens": 459, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The question asks if someone's last name sounds like they are a friend or foe based on the provided context. However, none of the documents provide information about an individual's last name that would make it sound initially like they're either a friend or a foe. The descriptions and plots do not mention any character whose surname could be interpreted this way directly. Therefore, with the given context, there is no basis to determine if a last name sounds at first like he’s a friend or a foe [None of the documents provide relevant information].", "latency_ms": 4043.666362762451, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is a specific label: 'A foe'. The generated answer instead claims there is no basis to determine the answer, which directly contradicts the gold label. Thus it is incorrect. However, the answer is internally consistent and faithful to its own reasoning, not hallucinating details beyond what it claims, so faithfulness is high.", "main_total_tokens": 2289, "main_prompt_tokens": 2181, "main_completion_tokens": 108, "judge_total_tokens": 589, "judge_prompt_tokens": 490, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "In the 2017 TV show \"Atypical,\" Sam is 18 years old [Doc 124583].", "latency_ms": 2600.9693145751953, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, matching the gold answer exactly. It does not introduce any contradictions or unsupported claims beyond citing a document ID, so it is both correct and faithful.", "main_total_tokens": 2207, "main_prompt_tokens": 2176, "main_completion_tokens": 31, "judge_total_tokens": 494, "judge_prompt_tokens": 411, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 2664.4883155822754, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the TV show 'Atypical', Sam is on the autism spectrum, which matches the gold answer exactly. It is fully faithful to the provided gold answer without adding incorrect information.", "main_total_tokens": 2427, "main_prompt_tokens": 2398, "main_completion_tokens": 29, "judge_total_tokens": 484, "judge_prompt_tokens": 410, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "The provided documents do not contain information about a movie titled \"À corde tendue\" released in 2013, nor any details regarding the birthplace of a character named Sandra from such a movie. Therefore, based on the given context, it is not possible to answer the question.", "latency_ms": 3328.6616802215576, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that Sandra was born in Chamonix, but the generated answer claims the question cannot be answered from the provided documents. This is incorrect with respect to the gold answer, so correctness is 0. However, the generated answer accurately reflects that the information is not present in the given context and does not fabricate an answer, so it is faithful to the stated limitation, giving faithfulness a score of 1.", "main_total_tokens": 2558, "main_prompt_tokens": 2497, "main_completion_tokens": 61, "judge_total_tokens": 570, "judge_prompt_tokens": 453, "judge_completion_tokens": 117}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In the Japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 2615.0450706481934, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('rains') and stays faithful to the provided information without adding unsupported details beyond a harmless citation-like note.", "main_total_tokens": 2378, "main_prompt_tokens": 2352, "main_completion_tokens": 26, "judge_total_tokens": 473, "judge_prompt_tokens": 412, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 2993.406295776367, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Greenland as the place where Tabaluga is crown-prince, matching the gold answer exactly and not introducing any contradictions.", "main_total_tokens": 2606, "main_prompt_tokens": 2571, "main_completion_tokens": 35, "judge_total_tokens": 484, "judge_prompt_tokens": 424, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie that matches the given description is \"One Afternoon\" [Doc 96838]. The plot describes a strict father and his two boys living a calm life in an old home, with a pivotal summer moment changing everything for the brothers.", "latency_ms": 3171.055316925049, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the given description, and the gold answer is \"One Afternoon.\" The generated answer also states that the movie is \"One Afternoon\" and reiterates the plot in a manner consistent with the description. There are no contradictions or hallucinations beyond the harmless bracketed document reference, and the answer is both correct and faithful to the prompt and gold.", "main_total_tokens": 2432, "main_prompt_tokens": 2379, "main_completion_tokens": 53, "judge_total_tokens": 557, "judge_prompt_tokens": 452, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie where a Holocaust survivor with a daycare business takes care of a 12-year-old street kid who recently robbed her is titled \"The Life Ahead\" [Doc 93260]. The rating for this movie is 6.746, which can be considered low relative to other highly-rated movies in the provided context such as \"Toto Who Lived Twice\" with a rating of 8.01 [Doc 21954] or \"The Stolen Children\" with a rating of 7.618 [Doc 13582].", "latency_ms": 3922.963857650757, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.4, "justification": "The generated answer correctly identifies the movie as \"The Life Ahead,\" so it answers the question accurately. However, it adds extra claims about the movie’s rating (6.746) and compares it to other films and their specific ratings, none of which are supported by the original QA pair. These numerical details and relative rating discussion are hallucinated with respect to the provided information, reducing faithfulness.", "main_total_tokens": 2932, "main_prompt_tokens": 2804, "main_completion_tokens": 128, "judge_total_tokens": 622, "judge_prompt_tokens": 510, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 2782.1788787841797, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer is factually correct and matches the gold answer, so correctness is 1. However, it introduces an external citation marker [Doc 73859] that is not present in the provided context, so it is not fully faithful to the given information; faithfulness is 0.", "main_total_tokens": 2291, "main_prompt_tokens": 2254, "main_completion_tokens": 37, "judge_total_tokens": 506, "judge_prompt_tokens": 416, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The context provided does not mention any specific racing team associated with an individual referred to as \"his.\" However, one document [Doc 20456] mentions the Newman-Haas (Andretti) racing team in relation to a documentary about building and testing race cars for a season. Without more specific information about who \"he\" is, it's not possible to definitively state which racing team is being referred to.\n\nTherefore, based on the provided context, there isn't enough information to answer the question directly.", "latency_ms": 4290.801048278809, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly identifies the racing team as 'The McLaren Motor Racing team.' The generated answer claims there is not enough information and does not provide this team name, so it is factually incorrect relative to the gold. However, it accurately reflects the (stated) contents of the provided context and does not fabricate a team name, so it is faithful to the given context.", "main_total_tokens": 2648, "main_prompt_tokens": 2540, "main_completion_tokens": 108, "judge_total_tokens": 591, "judge_prompt_tokens": 483, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "None of the documents provided contain information about a specific individual's country of origin referred to as \"he.\" Each document describes different movies with various plots, but none directly answer the question \"What country is he from?\" without additional context about who \"he\" refers to. Therefore, based strictly on the given context, it is not possible to determine what country \"he\" is from.", "latency_ms": 3670.881986618042, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies 'New Zealand' as the correct country, but the generated answer claims the country cannot be determined, so it is factually incorrect relative to the gold label (correctness 0). However, the generated answer accurately reflects that, in the documents it references, there is no clear information about who 'he' is or his country of origin, so it is faithful to the described context (faithfulness 1).", "main_total_tokens": 2492, "main_prompt_tokens": 2414, "main_completion_tokens": 78, "judge_total_tokens": 570, "judge_prompt_tokens": 452, "judge_completion_tokens": 118}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "None of the documents provided contain information about a movie from 2013 where John McClane blows up the Great Wall of China. The closest reference is to \"A Good Day to Die Hard\" [Doc 58791] which involves John McClane but does not mention anything related to blowing up the Great Wall of China. Therefore, based on the provided documents, there is no movie that matches the described scenario.", "latency_ms": 3841.710090637207, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no known 2013 movie where John McClane blows up the Great Wall of China, which aligns with the gold answer that such a movie is not in the database. It is faithful to the premise by referencing the closest relevant film and explicitly concluding that no matching movie exists, matching the gold intent.", "main_total_tokens": 2444, "main_prompt_tokens": 2355, "main_completion_tokens": 89, "judge_total_tokens": 583, "judge_prompt_tokens": 483, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with the described plot is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 2816.988945007324, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the film as 'Tomie: Forbidden Fruit' and does not introduce any contradictory or extraneous information.", "main_total_tokens": 2509, "main_prompt_tokens": 2477, "main_completion_tokens": 32, "judge_total_tokens": 506, "judge_prompt_tokens": 443, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 3181.110382080078, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, which matches the gold answer exactly, and it does not introduce any additional unsupported information.", "main_total_tokens": 2579, "main_prompt_tokens": 2542, "main_completion_tokens": 37, "judge_total_tokens": 490, "judge_prompt_tokens": 423, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls [Doc 82582]. This artifact is sought after because it will allow them to put an end to an evil warlord.", "latency_ms": 3585.3378772735596, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.98, "justification": "The generated answer correctly identifies that The Scorpion King and Tala are searching for the Book of Souls, matching the gold answer. It adds extra context about it being a legendary relic and its purpose in stopping an evil warlord. This additional detail is consistent with the movie but not explicitly present in the provided gold answer; however, it does not contradict it. Thus, correctness is full, and faithfulness is very high but slightly less than perfect due to the added, unreferenced context.", "main_total_tokens": 2861, "main_prompt_tokens": 2799, "main_completion_tokens": 62, "judge_total_tokens": 580, "judge_prompt_tokens": 449, "judge_completion_tokens": 131}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about the release year of \"Zombies on the Moon\". Therefore, based solely on the given context, the answer is not found.", "latency_ms": 2713.252305984497, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer state that the release year (or any info) about 'Zombies on the Moon' is not available in the provided resources/database. The generated answer is correct and faithful to the context, making it fully aligned with the gold.", "main_total_tokens": 2181, "main_prompt_tokens": 2146, "main_completion_tokens": 35, "judge_total_tokens": 510, "judge_prompt_tokens": 425, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain any information regarding the release year of \"The Galactic baker\". [None]", "latency_ms": 3010.4963779449463, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database, i.e., there is no information about its release year. The generated answer correctly states that the provided documents do not contain any information about the release year, which is equivalent in meaning. It does not invent a year and stays faithful to the given information.", "main_total_tokens": 2612, "main_prompt_tokens": 2590, "main_completion_tokens": 22, "judge_total_tokens": 504, "judge_prompt_tokens": 410, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about the release year of \"Return of the Null Pointer\". Therefore, based on the given context, it is not possible to determine in what year \"Return of the Null Pointer\" was released [None of the Documents].", "latency_ms": 3113.7638092041016, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year cannot be determined from the available information. The generated answer correctly notes that the documents do not contain the release year and does not fabricate a date, matching the gold answer in meaning and staying faithful to the context.", "main_total_tokens": 2280, "main_prompt_tokens": 2228, "main_completion_tokens": 52, "judge_total_tokens": 526, "judge_prompt_tokens": 442, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"Data Science: The Movie\". Therefore, based on the given context, it is not possible to determine in what year such a movie was released [None of the Documents].", "latency_ms": 3476.818799972534, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the year of release cannot be determined from the available information. The generated answer is faithful to the context (documents) by explicitly noting their lack of relevant information, aligning with the gold response that the movie is not in the database.", "main_total_tokens": 2393, "main_prompt_tokens": 2346, "main_completion_tokens": 47, "judge_total_tokens": 523, "judge_prompt_tokens": 437, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about \"Python's Revenge 2\". Therefore, the release year for this specific title cannot be determined based on the given context. [Doc None]", "latency_ms": 3215.155839920044, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is unknown. The generated answer faithfully states that the information is not available in the provided documents and that the release year cannot be determined from the context. This is correct (no hallucinated year) and faithful to the situation where the documents contain no relevant info.", "main_total_tokens": 2616, "main_prompt_tokens": 2578, "main_completion_tokens": 38, "judge_total_tokens": 517, "judge_prompt_tokens": 428, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a title called \"The Last Pixel.\" Therefore, based on the given context, the release year for \"The Last Pixel\" is not found. [Doc 107122] - [Doc 58480] (All documents searched).", "latency_ms": 3113.891839981079, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers convey that the release year (or any information) about 'The Last Pixel' is not available. The generated answer correctly states that the documents do not contain information about that title and that the release year is not found, which is effectively equivalent in content to the gold answer. It does not fabricate a year and remains grounded in the described document search, so it is both correct and faithful.", "main_total_tokens": 2068, "main_prompt_tokens": 2005, "main_completion_tokens": 63, "judge_total_tokens": 559, "judge_prompt_tokens": 444, "judge_completion_tokens": 115}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "None of the documents provided contain information about the release year of \"The man who ate the sun\". Therefore, based on the given context, it's not possible to determine in what year \"The man who ate the sun\" was released [Doc 16199].", "latency_ms": 3462.578296661377, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer state that the release year cannot be determined / is not available from the given information. The generated answer correctly notes that the documents do not contain this information, which is faithful to the premise of relying only on the provided context. There is no hallucinated year or conflicting detail, so it is fully correct and faithful.", "main_total_tokens": 2814, "main_prompt_tokens": 2757, "main_completion_tokens": 57, "judge_total_tokens": 546, "judge_prompt_tokens": 444, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about who directed the movie \"The Last Days of Lehman Brothers\". [Doc 46069]", "latency_ms": 2988.9121055603027, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer state that the director information is not available in the provided documents/database. This is factually correct with respect to the given context, and the generated answer does not introduce any unsupported information. Therefore, it is both correct and faithful.", "main_total_tokens": 2811, "main_prompt_tokens": 2780, "main_completion_tokens": 31, "judge_total_tokens": 509, "judge_prompt_tokens": 424, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "None of the documents provided contain information about a movie titled \"A Love to Die For,\" nor do they mention who directed such a film. Therefore, based on the given context, it is not possible to answer the question regarding who directed the movie \"A Love to Die For.\"", "latency_ms": 3427.7567863464355, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director cannot be identified from the given documents, which aligns with the gold answer. It does not introduce any unsupported claims and stays fully grounded in the provided context, so both correctness and faithfulness are maximal.", "main_total_tokens": 2641, "main_prompt_tokens": 2584, "main_completion_tokens": 57, "judge_total_tokens": 530, "judge_prompt_tokens": 451, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not contain information about who directed the movie \"Journey into Gaza\". [Doc 111360]", "latency_ms": 2876.5342235565186, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not provide the director information for the movie, and it does not hallucinate a director. This matches the gold response, which also indicates the database lacks this information. The answer is both correct and faithful to the provided context.", "main_total_tokens": 2652, "main_prompt_tokens": 2623, "main_completion_tokens": 29, "judge_total_tokens": 501, "judge_prompt_tokens": 416, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about the director of the movie \"The Last Night at Tremore Beach\". [Doc 135098]", "latency_ms": 2882.6253414154053, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not contain information about the director, which aligns with the gold answer indicating the database does not provide the director information. It does not introduce unsupported claims and remains faithful to the given context.", "main_total_tokens": 2292, "main_prompt_tokens": 2259, "main_completion_tokens": 33, "judge_total_tokens": 502, "judge_prompt_tokens": 425, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie where a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 2732.414484024048, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy.' There are no contradictions or hallucinations; the bracketed document reference does not alter the factual content, so the response is both correct and faithful to the provided information.", "main_total_tokens": 2398, "main_prompt_tokens": 2365, "main_completion_tokens": 33, "judge_total_tokens": 502, "judge_prompt_tokens": 417, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse is \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 2674.2942333221436, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as 'Stranger in My Bed.' It does not introduce any contradictions or extraneous claims beyond a citation marker, so it is both correct and faithful.", "main_total_tokens": 2357, "main_prompt_tokens": 2324, "main_completion_tokens": 33, "judge_total_tokens": 499, "judge_prompt_tokens": 425, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie in which a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years is \"Game of Silence\" [Doc 123938].", "latency_ms": 2630.284309387207, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Game of Silence') and does not introduce any contradictory or extraneous information, so it is both correct and faithful.", "main_total_tokens": 2087, "main_prompt_tokens": 2048, "main_completion_tokens": 39, "judge_total_tokens": 493, "judge_prompt_tokens": 430, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie that features a plot based on a true story about a killer who uses Craigslist is titled \"The Craigslist Killer\" [Doc 53039]. This film from 2011 describes the story of Philip Markoff, a medical student at Boston University who leads a double life as a brutal sexual deviant targeting prostitutes he contacts through Craigslist.", "latency_ms": 4105.198621749878, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Craigslist Killer,\" matching the gold answer. The additional details about it being a 2011 film based on Philip Markoff and involving Craigslist-targeted crimes are accurate and consistent with known information, and there is no hallucinated contradiction with the given query. Thus, it is both correct and faithful to the prompt.", "main_total_tokens": 2339, "main_prompt_tokens": 2265, "main_completion_tokens": 74, "judge_total_tokens": 563, "judge_prompt_tokens": 460, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie where Yuki Hirano fails his university entrance exam and then sets his eyes on the role of a forestry trainee is \"Wood Job!\" [Doc 66094].", "latency_ms": 3407.8264236450195, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", which matches the gold answer. It does not introduce contradictions or unsupported information beyond restating the question context, so it is both correct and faithful.", "main_total_tokens": 3136, "main_prompt_tokens": 3096, "main_completion_tokens": 40, "judge_total_tokens": 508, "judge_prompt_tokens": 435, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie where a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure is titled \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 3237.5524044036865, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, 'Sesame Street: The Magical Wand Chase', and correctly describes the plot element from the question. It remains faithful to the provided information without adding unsupported details.", "main_total_tokens": 2608, "main_prompt_tokens": 2554, "main_completion_tokens": 54, "judge_total_tokens": 531, "judge_prompt_tokens": 459, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie where a 7 year-old boy must find the hero within him in order to save their lives when a rock fall traps his parents inside a burning car is titled \"Beyond\" [Doc 74597].", "latency_ms": 3222.499132156372, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Beyond,\" matching the gold answer exactly. It does not introduce any contradictions or hallucinations beyond the provided content, so it is fully faithful to the given information.", "main_total_tokens": 2194, "main_prompt_tokens": 2147, "main_completion_tokens": 47, "judge_total_tokens": 523, "judge_prompt_tokens": 451, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie where a woman, fired from a financial corporation during the Asia crisis, returns home with no money is \"6ixtynin9\" [Doc 22478]. This plot description matches exactly with the question asked.", "latency_ms": 3376.0523796081543, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', which matches the gold answer exactly, and it accurately reflects the query description without adding unsupported details.", "main_total_tokens": 2332, "main_prompt_tokens": 2282, "main_completion_tokens": 50, "judge_total_tokens": 514, "judge_prompt_tokens": 448, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie described as having a serial killer interrupting the fun at Coconut Pete's Coconut Beach Resort, where it's up to the club's staff to stop or hide the violence, is titled \"Club Dread\" [Doc 32410].", "latency_ms": 3417.814254760742, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer exactly. It remains faithful to the provided information without adding incorrect details, simply restating and confirming the title.", "main_total_tokens": 2649, "main_prompt_tokens": 2597, "main_completion_tokens": 52, "judge_total_tokens": 543, "judge_prompt_tokens": 472, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is titled \"Toilet.\" [Doc 48873]", "latency_ms": 4115.536689758301, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Toilet,' matching the gold answer exactly. It stays fully consistent with the provided information and does not introduce contradictions or unsupported claims, so it is both correct and faithful.", "main_total_tokens": 2858, "main_prompt_tokens": 2801, "main_completion_tokens": 57, "judge_total_tokens": 540, "judge_prompt_tokens": 466, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie described as a typical experimental film where a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century is \"Electric Angel\" [Doc 1024].", "latency_ms": 3266.432523727417, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Electric Angel,\" which matches the gold answer exactly. It faithfully restates the information without adding unsupported claims or contradictions, and the reference to [Doc 1024] is a plausible citation rather than an invented fact. Therefore, both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2629, "main_prompt_tokens": 2574, "main_completion_tokens": 55, "judge_total_tokens": 560, "judge_prompt_tokens": 466, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 based on the provided documents is \"Bottom of the Water\" with a rating of 4.2 [Doc 106899].", "latency_ms": 2067.4123764038086, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer specifies that the lowest rated movie from 2023 is 'Mega Ape', but the generated answer says it is 'Bottom of the Water' with a rating of 4.2, which directly contradicts the gold. This makes the generated answer incorrect. It is also unfaithful to the provided gold reference, attributing the answer to a different document and film, so both correctness and faithfulness are 0.", "main_total_tokens": 2248, "main_prompt_tokens": 2207, "main_completion_tokens": 41, "judge_total_tokens": 528, "judge_prompt_tokens": 415, "judge_completion_tokens": 113}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 1345.5486297607422, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a specific correct answer (Tea House). The generated answer says 'I don't know,' which is incorrect, so correctness is 0. However, the model does not hallucinate or invent an incorrect fact; it explicitly expresses uncertainty, so it is fully faithful to its own (claimed) knowledge state, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 489, "judge_prompt_tokens": 386, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 691.1587715148926, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a clear gold answer, but the model responded with 'I don't know,' which is incorrect. However, the answer does not fabricate information or contradict the provided data; it simply expresses a lack of knowledge, so it is fully faithful but not correct.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 479, "judge_prompt_tokens": 395, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "Based on the provided documents, the lowest-rated movie from 1980 is \"Read Lips\" [Doc 354] with a rating of 4.5. The language of this movie is cn (Chinese).", "latency_ms": 3082.352638244629, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says the language is English, while the generated answer claims it is Chinese (\"cn\"), so the generated response is factually incorrect. It also fabricates specifics about the movie title, document reference, and rating that contradict the gold label, so it is not faithful to the ground truth.", "main_total_tokens": 2285, "main_prompt_tokens": 2237, "main_completion_tokens": 48, "judge_total_tokens": 519, "judge_prompt_tokens": 426, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "Based on the provided documents, none of them explicitly states which TV show had the highest popularity in 1999. However, among the entries listed for 1999, the document with the highest Popularity score is \"Who Wants to Be a Millionaire?\" with a Popularity score of 2.3756 [Doc 115397]. Other notable mentions include \"The Amanda Show\" with a Popularity score of 3.4197 [Doc 115219] and \"Hollywood Squares\" from 1998, which had a very high Popularity score of 7.0597 [Doc 114940], but it's important to note that the latter is listed under the year 1998.\n\nTherefore, based strictly on the information provided here, \"Who Wants to Be a Millionaire?\" appears to have had one of the highest popularity scores among the entries for 1999. However, this does not necessarily mean it was the most popular show overall in that year as we do not have comprehensive data for all shows from 1999.", "latency_ms": 6992.846488952637, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Law & Order: Special Victims Unit', but the generated answer concludes that 'Who Wants to Be a Millionaire?' had one of the highest popularity scores, so it is incorrect with respect to the gold. However, the assistant consistently bases its reasoning on the described document popularity scores and explicitly notes the limitations of the data, without introducing contradictions or unsupported claims, so it is faithful to the provided context and its own stated assumptions.", "main_total_tokens": 3101, "main_prompt_tokens": 2856, "main_completion_tokens": 245, "judge_total_tokens": 723, "judge_prompt_tokens": 601, "judge_completion_tokens": 122}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 2695.4994201660156, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 and correctly attributes it, with no contradictions or hallucinations relative to the provided information.", "main_total_tokens": 2354, "main_prompt_tokens": 2332, "main_completion_tokens": 22, "judge_total_tokens": 461, "judge_prompt_tokens": 400, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The context provides information for multiple movies titled \"The Story of Us,\" but it does not contain a single definitive release year for all contexts. Instead, there are different years associated with different contexts:\n\n- [Doc 112077] suggests the movie was released in 2025.\n- [Doc 88811] indicates a release year of 2019.\n- [Doc 22481] lists the year as 1999.\n\nTherefore, based on the provided documents, there isn't one specific answer to the question about which year \"The Story of Us\" was released; it depends on which version or context is being referred to.", "latency_ms": 5249.679327011108, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies 2019 as the correct year, implying the question refers to the 2019 film. The generated answer claims there is no single definitive release year and does not pick 2019, so it is incorrect with respect to the gold. However, the generated answer accurately reflects the conflicting years as described in the provided context (2019, 1999, 2025) and does not introduce information beyond that, so it is faithful to the stated context.", "main_total_tokens": 2479, "main_prompt_tokens": 2334, "main_completion_tokens": 145, "judge_total_tokens": 639, "judge_prompt_tokens": 511, "judge_completion_tokens": 128}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 239.14098739624023, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (3) does not match the gold answer (279), so it is incorrect. However, it is a direct attempt to answer the question without contradicting the given context, so it is faithful to the user query.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 461, "judge_prompt_tokens": 383, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "12", "latency_ms": 172.3043918609619, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (12), so it is both correct and faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 433, "judge_prompt_tokens": 383, "judge_completion_tokens": 50}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 2839.8754596710205, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie \"The Last Days of Lehman Brothers\" was released in 2009, which matches the gold answer. It is also faithful to the provided information, explicitly drawing from the referenced document without adding unsupported details.", "main_total_tokens": 2573, "main_prompt_tokens": 2544, "main_completion_tokens": 29, "judge_total_tokens": 491, "judge_prompt_tokens": 410, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 2742.687702178955, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the language is English (en), and the generated answer matches this exactly. There is no contradiction or fabrication beyond the provided information.", "main_total_tokens": 2674, "main_prompt_tokens": 2650, "main_completion_tokens": 24, "judge_total_tokens": 463, "judge_prompt_tokens": 402, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "1987 [Doc 6993]", "latency_ms": 4520.268678665161, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (1987). The citation tag does not contradict the answer, so the response is both correct and faithful.", "main_total_tokens": 3125, "main_prompt_tokens": 3048, "main_completion_tokens": 77, "judge_total_tokens": 448, "judge_prompt_tokens": 387, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Perfect Strangers [Doc 103513]", "latency_ms": 4227.143049240112, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer, 'Perfect Strangers', does not match the correct gold answer, 'Brain Fluid Explosion Girl', so it is incorrect. However, it is a direct attempt to answer the question without inventing extra details beyond naming a movie, so it is faithful to the (incorrect) content it provides.", "main_total_tokens": 2837, "main_prompt_tokens": 2759, "main_completion_tokens": 78, "judge_total_tokens": 490, "judge_prompt_tokens": 397, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 4629.793882369995, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Dungeons & Dragons') and correctly identifies the TV show where six friends rely on a Dungeon Master to get home.", "main_total_tokens": 2968, "main_prompt_tokens": 2884, "main_completion_tokens": 84, "judge_total_tokens": 457, "judge_prompt_tokens": 394, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 4747.662544250488, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'Nacht der Wölfe'. It is both correct and faithful.", "main_total_tokens": 3146, "main_prompt_tokens": 3061, "main_completion_tokens": 85, "judge_total_tokens": 463, "judge_prompt_tokens": 403, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900 [Doc 126191]", "latency_ms": 4689.941167831421, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('1900') and is fully supported by the provided information, so it is both correct and faithful.", "main_total_tokens": 2957, "main_prompt_tokens": 2860, "main_completion_tokens": 97, "judge_total_tokens": 454, "judge_prompt_tokens": 394, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June", "latency_ms": 575.2151012420654, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is a movie quiz whose answer is the name of a month matching the described character. The gold answer is 'June' and the generated answer is also 'June'. This matches exactly and is faithful to the given information.", "main_total_tokens": 146, "main_prompt_tokens": 144, "main_completion_tokens": 2, "judge_total_tokens": 510, "judge_prompt_tokens": 433, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 306.5478801727295, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific movie title, but the generated answer says it does not know, which is factually incorrect relative to the gold. However, the generated response does not hallucinate or contradict provided facts; it simply admits ignorance, so it is fully faithful to its own (lack of) knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 399, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 276.38912200927734, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric rating (7.6). The generated answer says it does not know, which is factually incorrect relative to the gold answer. However, the model does not fabricate any information and honestly states a lack of knowledge, so it is fully faithful to its own (non-)reasoning.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 484, "judge_prompt_tokens": 389, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 2230.867624282837, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.5143371794949736, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.013881288441812, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is incorrect. However, the model directly answers the question and does not contradict or fabricate any reasoning content, so faithfulness to the (albeit wrong) numeric output is preserved.", "main_total_tokens": 1563, "main_prompt_tokens": 1561, "main_completion_tokens": 2, "judge_total_tokens": 478, "judge_prompt_tokens": 394, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 247.00307846069336, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question is about counting TV shows matching given criteria in 1987, with a known gold answer of 2. The generated answer claims there are none, which is factually incorrect relative to the gold label, so correctness is 0. However, there is no evidence of hallucinated details or contradiction with the model’s own stated uncertainty; it simply states it cannot find any, so faithfulness to its own reasoning is maintained, giving a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 526, "judge_prompt_tokens": 404, "judge_completion_tokens": 122}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 295.8252429962158, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct response 'Fantômas,' so it is incorrect. However, it does not invent any incorrect facts and accurately reflects a lack of knowledge, so it is fully faithful to the model's stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 389, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise [Doc 11205]", "latency_ms": 4814.7711753845215, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, correctly identifying 'Kill Cruise' as the movie about two British women going to Barbados with a yacht captain. It is fully correct and faithful to the reference.", "main_total_tokens": 3154, "main_prompt_tokens": 3065, "main_completion_tokens": 89, "judge_total_tokens": 464, "judge_prompt_tokens": 393, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 5149.804353713989, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie title, so it is both correct and faithful.", "main_total_tokens": 3243, "main_prompt_tokens": 3139, "main_completion_tokens": 104, "judge_total_tokens": 462, "judge_prompt_tokens": 408, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018 [Doc 84838]", "latency_ms": 5509.3419551849365, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states the year as 2018, which matches the gold answer exactly and remains faithful to the provided information.", "main_total_tokens": 3115, "main_prompt_tokens": 3001, "main_completion_tokens": 114, "judge_total_tokens": 443, "judge_prompt_tokens": 386, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy [Doc 112591]", "latency_ms": 4828.737735748291, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Astro Boy' correctly identifies the animated TV series about a young boy robot in 2030 AD, matching the gold answer exactly. The citation tag does not contradict or alter the answer, so the response is both correct and faithful.", "main_total_tokens": 3277, "main_prompt_tokens": 3182, "main_completion_tokens": 95, "judge_total_tokens": 477, "judge_prompt_tokens": 395, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 years [Doc 67513]", "latency_ms": 4477.344036102295, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9463946303571863, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states they were held captive for 7 years, which matches the gold answer. It is also faithful as it directly matches the provided gold reference without adding unsupported details.", "main_total_tokens": 2773, "main_prompt_tokens": 2684, "main_completion_tokens": 89, "judge_total_tokens": 471, "judge_prompt_tokens": 403, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m [Doc 133469]", "latency_ms": 2667.863368988037, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer (£26m) matches the gold answer (£26m) exactly, so it is both correct and fully supported by the given information.", "main_total_tokens": 2391, "main_prompt_tokens": 2376, "main_completion_tokens": 15, "judge_total_tokens": 465, "judge_prompt_tokens": 404, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 679.3444156646729, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score, which is 80.5772. The generated answer says “I don't know,” which is incorrect. However, it does not invent any incorrect facts beyond admitting ignorance, so it is fully faithful to the model’s uncertainty but not correct relative to the gold answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 396, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 247.54953384399414, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 1, but the generated answer incorrectly claims there are none, so correctness is 0. The model does not hallucinate a specific incorrect number or show; it honestly states it cannot find any, so its reasoning is faithful to its apparent (though limited) retrieval, giving faithfulness a 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 490, "judge_prompt_tokens": 394, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes [Doc 121928]", "latency_ms": 4765.880823135376, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Angel Eyes') and stays fully consistent with the information provided, so it is both correct and faithful.", "main_total_tokens": 3008, "main_prompt_tokens": 2906, "main_completion_tokens": 102, "judge_total_tokens": 463, "judge_prompt_tokens": 403, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of this movie is English [Doc 14842].", "latency_ms": 4850.073337554932, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states the language is English, but the gold label says Korean, so the answer is factually incorrect. However, the answer is self-consistent and does not contradict its own provided context (it explicitly cites Doc 14842 for English), so it is faithful to its own reasoning/context.", "main_total_tokens": 3243, "main_prompt_tokens": 3164, "main_completion_tokens": 79, "judge_total_tokens": 481, "judge_prompt_tokens": 389, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 765.6087875366211, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because the gold label states that there are 2 such movies, while the model claims there are none. However, the response is internally consistent and does not hallucinate specific movies or fabricated data; it simply states an inability to find matches. Thus, it is unfaithful to the gold answer but faithful to its own stated limitations, warranting a low correctness score and a high faithfulness score.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 516, "judge_prompt_tokens": 403, "judge_completion_tokens": 113}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 1850.126028060913, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the system has no movies from 1979 in its database. The generated answer says simply 'I don't know,' which is consistent with lacking that information. Given the database constraint, not providing a specific movie is correct, and there is no hallucinated content. Therefore, the answer is both correct and faithful to the gold.", "main_total_tokens": 657, "main_prompt_tokens": 610, "main_completion_tokens": 47, "judge_total_tokens": 499, "judge_prompt_tokens": 399, "judge_completion_tokens": 100}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "I don't know.", "latency_ms": 2261.965036392212, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'I don't know.' is factually incomplete compared to the gold answer, which clarifies that 'Back to the Future Part IV' does not exist. Given the task is effectively a QA about a non-existent movie, the correct response should state its non-existence, not uncertainty. It is therefore incorrect. It is also unfaithful to the gold reference, which explicitly gives the nature of the answer rather than simple ignorance.", "main_total_tokens": 701, "main_prompt_tokens": 633, "main_completion_tokens": 68, "judge_total_tokens": 524, "judge_prompt_tokens": 404, "judge_completion_tokens": 120}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both answers correctly state that no such movie is found, which is the gold’s content. The generated answer slightly elaborates by hinting about the event timing, but it does not contradict the gold statement. Therefore it is fully correct and faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 514, "judge_prompt_tokens": 434, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 218.09959411621094, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 titles, while the generated answer incorrectly claims there are none, making it factually wrong relative to the gold. However, the model does not fabricate a specific incorrect number or details; it honestly states it cannot find any, which is faithful to its stated uncertainty and limitations.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 490, "judge_prompt_tokens": 397, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 3311.584949493408, "retrieval_recall": 0.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.17723928678404774, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 2, but the generated answer claims there are none, so it is factually incorrect. However, given the model’s knowledge cutoff and the difficulty of the query, the response is consistent with the model’s stated inability to find such movies, so it is faithful to its own knowledge limits.", "main_total_tokens": 3723, "main_prompt_tokens": 3721, "main_completion_tokens": 2, "judge_total_tokens": 489, "judge_prompt_tokens": 395, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 232.81431198120117, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show, but the generated answer says there were none. This is factually incorrect relative to the gold, so correctness is 0. However, the generated response does not contradict any information it previously provided and remains internally consistent, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 490, "judge_prompt_tokens": 398, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 205.95788955688477, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold answer says 0 shows. The generated answer states it cannot find any movies or shows matching the criteria, which effectively conveys there are none known, aligning with 0. It does not invent extra details or contradict the gold, so it is both correct with respect to the gold and faithful to the query.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 493, "judge_prompt_tokens": 400, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 206.48884773254395, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer identifies 'Naruto' as the most popular ninja-related show. The generated answer says 'I don't know,' which is incorrect relative to the gold. However, it is faithful to the model's (stated) uncertainty and does not hallucinate an alternative, so faithfulness is high even though correctness is zero.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 386, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 1936.9697570800781, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states the movie was not released, while the generated answer says 'I don't know,' which is not factually correct relative to the gold. However, the generated answer accurately reflects the model's lack of knowledge and does not hallucinate, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 606, "main_prompt_tokens": 557, "main_completion_tokens": 49, "judge_total_tokens": 481, "judge_prompt_tokens": 389, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 2214.4479751586914, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that such a movie does not exist or was not released. The generated answer says 'I don't know,' which is factually incorrect given the ground truth, so correctness is 0. However, the model does not invent any misinformation or contradict the provided information; it simply expresses uncertainty, so faithfulness to its own stated knowledge is high, hence faithfulness is 1.", "main_total_tokens": 663, "main_prompt_tokens": 594, "main_completion_tokens": 69, "judge_total_tokens": 496, "judge_prompt_tokens": 386, "judge_completion_tokens": 110}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "I don't know.", "latency_ms": 2196.8696117401123, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the 4th oldest Star Wars movie and the gold answer is a specific title, but the model responded with 'I don't know.' This is not a correct answer. However, it is faithful to the model's uncertainty and does not fabricate information, so faithfulness is high even though correctness is zero.", "main_total_tokens": 659, "main_prompt_tokens": 588, "main_completion_tokens": 71, "judge_total_tokens": 489, "judge_prompt_tokens": 393, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "I don't know.", "latency_ms": 3528.6622047424316, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model said it did not know the answer, which is incorrect because the correct year is 1986. However, the response is fully faithful to the model's uncertainty and does not fabricate an answer.", "main_total_tokens": 816, "main_prompt_tokens": 700, "main_completion_tokens": 116, "judge_total_tokens": 453, "judge_prompt_tokens": 381, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 300.54283142089844, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific rating (8.466), but the generated answer says 'I don't know,' which is incorrect. However, it is faithful in that it does not fabricate an answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 398, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med [Doc 121125]", "latency_ms": 4489.698648452759, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Mighty Med' with no additional incorrect information. It is fully correct and faithful.", "main_total_tokens": 2909, "main_prompt_tokens": 2820, "main_completion_tokens": 89, "judge_total_tokens": 457, "judge_prompt_tokens": 400, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina [Doc 53582]", "latency_ms": 4641.663312911987, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Argentina' exactly matches the gold answer and directly addresses the question without adding unsupported information. It is both correct and faithful to the given gold reference.", "main_total_tokens": 3406, "main_prompt_tokens": 3323, "main_completion_tokens": 83, "judge_total_tokens": 462, "judge_prompt_tokens": 398, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "I don't know.", "latency_ms": 2478.465795516968, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Jiang Jie', but the generated answer is 'I don't know', which is incorrect. However, it is faithful to the model's uncertainty and does not hallucinate an incorrect specific answer.", "main_total_tokens": 646, "main_prompt_tokens": 575, "main_completion_tokens": 71, "judge_total_tokens": 462, "judge_prompt_tokens": 388, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "3", "latency_ms": 3435.0829124450684, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.19519002499605084, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.2, "ndcg_at_10": 0.3080351663449861, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (3) does not match the gold answer (4), so it is incorrect. However, the model directly answered the question without adding unsupported details, so it is faithful to the prompt.", "main_total_tokens": 3846, "main_prompt_tokens": 3839, "main_completion_tokens": 7, "judge_total_tokens": 457, "judge_prompt_tokens": 385, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 2880.920648574829, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says there are no movies with both Bruce Lee and Steven Seagal. The generated answer states it cannot find any movies or shows matching all the criteria, which is effectively the same claim. This is factually correct and fully consistent with the gold response.", "main_total_tokens": 2448, "main_prompt_tokens": 2404, "main_completion_tokens": 44, "judge_total_tokens": 490, "judge_prompt_tokens": 406, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3 [Doc 42750]", "latency_ms": 2428.948402404785, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (6.3) and stays consistent with the provided information, so it is both correct and faithful.", "main_total_tokens": 2523, "main_prompt_tokens": 2510, "main_completion_tokens": 13, "judge_total_tokens": 453, "judge_prompt_tokens": 393, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "drift-style", "latency_ms": 4630.577564239502, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'Drift', while the generated answer is 'drift-style'. Though similar, they are not an exact match and introduce an extra word, so it is not fully correct relative to the gold. The task appears to require exact labeling, so this deviation is also unfaithful to the target answer.", "main_total_tokens": 3294, "main_prompt_tokens": 3225, "main_completion_tokens": 69, "judge_total_tokens": 476, "judge_prompt_tokens": 380, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer", "latency_ms": 5163.790225982666, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct title of the movie, and it is fully consistent with the given information.", "main_total_tokens": 3187, "main_prompt_tokens": 3074, "main_completion_tokens": 113, "judge_total_tokens": 471, "judge_prompt_tokens": 412, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838 [Doc 20603]", "latency_ms": 2495.4781532287598, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) with no extraneous or conflicting information, so it is both correct and faithful.", "main_total_tokens": 2529, "main_prompt_tokens": 2514, "main_completion_tokens": 15, "judge_total_tokens": 456, "judge_prompt_tokens": 395, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense [Doc 22098]", "latency_ms": 4494.635820388794, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6895405204413555, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 1999 psychological thriller as 'The Sixth Sense'. It is both correct and faithful to the reference.", "main_total_tokens": 2809, "main_prompt_tokens": 2717, "main_completion_tokens": 92, "judge_total_tokens": 464, "judge_prompt_tokens": 400, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002 [Doc 27318]", "latency_ms": 3921.4138984680176, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2002) exactly and does not introduce any unsupported information beyond a document reference, so it is both correct and faithful.", "main_total_tokens": 3097, "main_prompt_tokens": 3051, "main_completion_tokens": 46, "judge_total_tokens": 452, "judge_prompt_tokens": 389, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled [Doc 67500]", "latency_ms": 3239.8648262023926, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, including the full correct title of the 2015 reboot of 'The Transporter'. The bracketed document reference does not alter the correctness or contradict the question context, so it remains fully faithful.", "main_total_tokens": 3087, "main_prompt_tokens": 3069, "main_completion_tokens": 18, "judge_total_tokens": 482, "judge_prompt_tokens": 403, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015 [Doc 67500]", "latency_ms": 4473.66189956665, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold year 2015 and does not introduce any conflicting information, so it is both correct and faithful.", "main_total_tokens": 3168, "main_prompt_tokens": 3092, "main_completion_tokens": 76, "judge_total_tokens": 450, "judge_prompt_tokens": 393, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "I don't know.", "latency_ms": 2138.5114192962646, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no such movie description can be found, effectively expressing lack of knowledge about such a film. The generated answer says \"I don't know,\" which is equivalent in content. It is correct relative to the gold and faithful to it, with no contradictions or hallucinations.", "main_total_tokens": 666, "main_prompt_tokens": 605, "main_completion_tokens": 61, "judge_total_tokens": 477, "judge_prompt_tokens": 394, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Havana Darkness [Doc 90923]", "latency_ms": 4469.860792160034, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies the movie title as 'Havana Darkness,' matching the gold answer. However, it adds an extra bracketed artifact '[Doc 90923]' that is not present in the gold, so while the core answer is correct, the response is not fully faithful to the expected output format.", "main_total_tokens": 3006, "main_prompt_tokens": 2930, "main_completion_tokens": 76, "judge_total_tokens": 491, "judge_prompt_tokens": 396, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 5235.811471939087, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Doll & Em'. It is both correct and faithful.", "main_total_tokens": 3139, "main_prompt_tokens": 3025, "main_completion_tokens": 114, "judge_total_tokens": 451, "judge_prompt_tokens": 392, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam Foe [Doc 39389]", "latency_ms": 4562.785387039185, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Hallam Foe') and correctly identifies the 2007 movie. The added bracketed document reference does not alter the content of the answer, so it is fully correct and faithful.", "main_total_tokens": 3026, "main_prompt_tokens": 2939, "main_completion_tokens": 87, "judge_total_tokens": 486, "judge_prompt_tokens": 409, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "I don't know.", "latency_ms": 3054.3594360351562, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'A foe'. The generated answer is 'I don't know', which is incorrect. However, it does not hallucinate and accurately reflects uncertainty, so it is faithful to the model's knowledge.", "main_total_tokens": 835, "main_prompt_tokens": 746, "main_completion_tokens": 89, "judge_total_tokens": 464, "judge_prompt_tokens": 390, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18 [Doc 124583]", "latency_ms": 4330.981969833374, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (18) exactly, and it is directly supported by the given information without adding unsupported claims.", "main_total_tokens": 2888, "main_prompt_tokens": 2801, "main_completion_tokens": 87, "judge_total_tokens": 452, "judge_prompt_tokens": 395, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam is on the autism spectrum [Doc 124583].", "latency_ms": 4276.788234710693, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which exactly matches the gold answer. It is factually correct and fully supported by the provided gold reference.", "main_total_tokens": 3099, "main_prompt_tokens": 3026, "main_completion_tokens": 73, "judge_total_tokens": 464, "judge_prompt_tokens": 399, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "I don't know.", "latency_ms": 2368.112802505493, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Chamonix'. The generated answer is 'I don't know,' which is incorrect. However, it accurately reflects the model's uncertainty instead of hallucinating a specific location, so it is fully faithful to its own statement of not knowing.", "main_total_tokens": 676, "main_prompt_tokens": 605, "main_completion_tokens": 71, "judge_total_tokens": 479, "judge_prompt_tokens": 401, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 1]", "latency_ms": 3681.9121837615967, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and stays consistent with the provided information.", "main_total_tokens": 2992, "main_prompt_tokens": 2942, "main_completion_tokens": 50, "judge_total_tokens": 448, "judge_prompt_tokens": 397, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland [Doc 114810]", "latency_ms": 4326.64942741394, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "Answer matches the gold exactly and is supported by the provided information.", "main_total_tokens": 3279, "main_prompt_tokens": 3209, "main_completion_tokens": 70, "judge_total_tokens": 445, "judge_prompt_tokens": 401, "judge_completion_tokens": 44}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon [Doc 96838]", "latency_ms": 4649.784803390503, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, so it is correct. It is also fully faithful to the provided information without adding unsupported details.", "main_total_tokens": 3086, "main_prompt_tokens": 2993, "main_completion_tokens": 93, "judge_total_tokens": 471, "judge_prompt_tokens": 412, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The Life Ahead [Doc 93260]", "latency_ms": 3023.124933242798, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('The Life Ahead') and correctly identifies the movie described in the question. It is fully faithful to the provided gold answer and contains no contradictions or extraneous information.", "main_total_tokens": 3483, "main_prompt_tokens": 3429, "main_completion_tokens": 54, "judge_total_tokens": 479, "judge_prompt_tokens": 407, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary \"McLaren\" is about Bruce McLaren [Doc 73859].", "latency_ms": 4636.075735092163, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the 2016 documentary 'McLaren' is about Bruce McLaren, matching the gold answer without adding contradictions or extraneous claims.", "main_total_tokens": 2946, "main_prompt_tokens": 2851, "main_completion_tokens": 95, "judge_total_tokens": 468, "judge_prompt_tokens": 404, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren Motor Racing team [Doc 73859]", "latency_ms": 5690.767526626587, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer in content, only omitting the article 'The' and adding a citation tag. The core entity 'McLaren Motor Racing team' is correct and faithful to the gold.", "main_total_tokens": 3381, "main_prompt_tokens": 3263, "main_completion_tokens": 118, "judge_total_tokens": 467, "judge_prompt_tokens": 394, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "I don't know.", "latency_ms": 2271.292209625244, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is New Zealand, but the model said 'I don't know,' which is incorrect. However, it did not hallucinate or contradict known information; it simply expressed uncertainty, so faithfulness is high.", "main_total_tokens": 698, "main_prompt_tokens": 643, "main_completion_tokens": 55, "judge_total_tokens": 454, "judge_prompt_tokens": 380, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "I don't know.", "latency_ms": 3556.6277503967285, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that there is no such movie in the database. The generated answer says 'I don't know,' which does not assert any incorrect fact and is consistent with the gold response. Therefore it is fully correct and faithful to the gold.", "main_total_tokens": 721, "main_prompt_tokens": 582, "main_completion_tokens": 139, "judge_total_tokens": 482, "judge_prompt_tokens": 402, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit [Doc 28589]", "latency_ms": 4683.120250701904, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as 'Tomie: Forbidden Fruit'. It is correct and fully faithful to the reference.", "main_total_tokens": 3224, "main_prompt_tokens": 3139, "main_completion_tokens": 85, "judge_total_tokens": 489, "judge_prompt_tokens": 428, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 [Doc 98566]", "latency_ms": 4776.023864746094, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that Nino and Caterina have been married for 65 years, which exactly matches the gold answer of 65. The response is both correct and faithful.", "main_total_tokens": 3267, "main_prompt_tokens": 3183, "main_completion_tokens": 84, "judge_total_tokens": 469, "judge_prompt_tokens": 402, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King and Tala search for The Book of Souls [Doc 82582].", "latency_ms": 5548.084259033203, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Scorpion King and Tala search for the Book of Souls, matching the gold answer. The extra bracketed reference does not change the meaning and appears to be a citation marker, so the response is both correct and faithful to the gold.", "main_total_tokens": 3576, "main_prompt_tokens": 3460, "main_completion_tokens": 116, "judge_total_tokens": 497, "judge_prompt_tokens": 412, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2425.447463989258, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the system does not have information about the movie. The generated answer, \"I don't know,\" is consistent with this and is an appropriate response. It is therefore correct relative to the gold and faithful to the lack of knowledge.", "main_total_tokens": 664, "main_prompt_tokens": 589, "main_completion_tokens": 75, "judge_total_tokens": 475, "judge_prompt_tokens": 395, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1691.7576789855957, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer and the generated answer are semantically equivalent: both state a lack of knowledge about the movie. The generated response does not introduce any extra, incorrect, or hallucinated information and matches the intended uncertainty, so it is fully correct and faithful.", "main_total_tokens": 646, "main_prompt_tokens": 603, "main_completion_tokens": 43, "judge_total_tokens": 475, "judge_prompt_tokens": 393, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2290.842294692993, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates that the movie is not in the database, effectively conveying a lack of knowledge about the release year. The generated answer, 'I don't know,' is consistent with this and does not introduce incorrect information. Therefore, it is both correct relative to the gold and faithful to the available information.", "main_total_tokens": 638, "main_prompt_tokens": 563, "main_completion_tokens": 75, "judge_total_tokens": 487, "judge_prompt_tokens": 395, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1784.1992378234863, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a non-existent or unknown movie. The gold answer appropriately indicates lack of knowledge. The generated answer also correctly expresses not knowing. This matches the gold’s intent and is factually appropriate, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 617, "main_prompt_tokens": 579, "main_completion_tokens": 38, "judge_total_tokens": 481, "judge_prompt_tokens": 395, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1942.073106765747, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers appropriately indicate a lack of knowledge about the movie. The generated answer \"I don't know.\" is correct given the missing information and is semantically faithful to the gold response, which states the movie is not in the database.", "main_total_tokens": 648, "main_prompt_tokens": 601, "main_completion_tokens": 47, "judge_total_tokens": 476, "judge_prompt_tokens": 395, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1485.8839511871338, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer state lack of knowledge about the movie rather than fabricating a year. This is correct given the task, and the generated answer is fully faithful to the gold intent (expressing uncertainty/not in database).", "main_total_tokens": 575, "main_prompt_tokens": 539, "main_completion_tokens": 36, "judge_total_tokens": 472, "judge_prompt_tokens": 393, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1677.2472858428955, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer admits not knowing, which is effectively equivalent in correctness to the gold response stating it is not in the database. It does not introduce any incorrect facts and remains faithful to the gold intent of lacking information.", "main_total_tokens": 648, "main_prompt_tokens": 607, "main_completion_tokens": 41, "judge_total_tokens": 470, "judge_prompt_tokens": 396, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 3001.234292984009, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer explicitly states that the director information is not available in the database. The generated answer says \"I don't know,\" which is equivalent in meaning: it does not hallucinate a director and aligns with the lack of information. Therefore it is both correct relative to the task and faithful to the database limitation.", "main_total_tokens": 705, "main_prompt_tokens": 600, "main_completion_tokens": 105, "judge_total_tokens": 494, "judge_prompt_tokens": 401, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 2106.074571609497, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not available in the database. The generated answer says \"I don't know,\" which is equivalent in content and does not hallucinate or add unsupported information. It is therefore correct and fully faithful to the gold.", "main_total_tokens": 671, "main_prompt_tokens": 611, "main_completion_tokens": 60, "judge_total_tokens": 480, "judge_prompt_tokens": 399, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 2592.087984085083, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates that the database does not provide the director information, so not knowing is correct. The generated answer, 'I don't know,' matches this uncertainty and does not invent any information.", "main_total_tokens": 663, "main_prompt_tokens": 575, "main_completion_tokens": 88, "judge_total_tokens": 467, "judge_prompt_tokens": 397, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 2250.094413757324, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not available; the generated answer says 'I don't know,' which is equivalent in meaning. It does not hallucinate a director and aligns with the lack of information. Therefore it is both correct and faithful.", "main_total_tokens": 673, "main_prompt_tokens": 603, "main_completion_tokens": 70, "judge_total_tokens": 477, "judge_prompt_tokens": 401, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 4493.301868438721, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Rise of the Mummy'). It correctly identifies the movie and is fully consistent with the provided reference.", "main_total_tokens": 3084, "main_prompt_tokens": 3000, "main_completion_tokens": 84, "judge_total_tokens": 456, "judge_prompt_tokens": 396, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 4566.418886184692, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Stranger in My Bed' and faithfully reflecting the provided reference.", "main_total_tokens": 3042, "main_prompt_tokens": 2959, "main_completion_tokens": 83, "judge_total_tokens": 459, "judge_prompt_tokens": 400, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 4832.339763641357, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the work as 'Game of Silence'. The description provided is indeed the premise of the TV series 'Game of Silence', so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2767, "main_prompt_tokens": 2660, "main_completion_tokens": 107, "judge_total_tokens": 476, "judge_prompt_tokens": 400, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 4912.276029586792, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'The Craigslist Killer'. It is both factually correct and faithful to the reference.", "main_total_tokens": 3029, "main_prompt_tokens": 2928, "main_completion_tokens": 101, "judge_total_tokens": 457, "judge_prompt_tokens": 395, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 4491.337537765503, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Wood Job!') and correctly identifies the movie described, so it is both fully correct and faithful.", "main_total_tokens": 3805, "main_prompt_tokens": 3750, "main_completion_tokens": 55, "judge_total_tokens": 463, "judge_prompt_tokens": 403, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The Magical Wand Chase", "latency_ms": 4722.156524658203, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer omits the key series name 'Sesame Street' from the movie title, making it incomplete and not an exact match to the gold answer. It is therefore not fully correct or faithful to the reference.", "main_total_tokens": 3347, "main_prompt_tokens": 3268, "main_completion_tokens": 79, "judge_total_tokens": 489, "judge_prompt_tokens": 414, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond [Doc 74597]", "latency_ms": 4453.488111495972, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot. The gold answer is 'Beyond', and the generated answer is also 'Beyond [Doc 74597]'. This correctly identifies the movie and remains faithful to the provided information, with the extra bracketed doc reference not altering the correctness.", "main_total_tokens": 2846, "main_prompt_tokens": 2771, "main_completion_tokens": 75, "judge_total_tokens": 503, "judge_prompt_tokens": 415, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9 [Doc 22478]", "latency_ms": 4474.301815032959, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('6ixtynin9') and does not introduce any extraneous or fabricated information, so it is both correct and faithful.", "main_total_tokens": 2989, "main_prompt_tokens": 2905, "main_completion_tokens": 84, "judge_total_tokens": 481, "judge_prompt_tokens": 415, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread [Doc 32410]", "latency_ms": 4544.576644897461, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer exactly. There is no added or conflicting information beyond a harmless document tag, so the response is both correct and faithful to the prompt and gold label.", "main_total_tokens": 3337, "main_prompt_tokens": 3261, "main_completion_tokens": 76, "judge_total_tokens": 512, "judge_prompt_tokens": 433, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Title: Toilet [Doc 48873]", "latency_ms": 4567.38543510437, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer gives the correct movie title, 'Toilet,' which matches the gold answer exactly. It is fully consistent with the provided information and does not introduce unsupported details, so both correctness and faithfulness are maximal.", "main_total_tokens": 3526, "main_prompt_tokens": 3456, "main_completion_tokens": 70, "judge_total_tokens": 498, "judge_prompt_tokens": 423, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel [Doc 1024]", "latency_ms": 4519.692659378052, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Electric Angel', matching the gold answer exactly. It is fully consistent with the provided information and does not introduce unsupported content.", "main_total_tokens": 3262, "main_prompt_tokens": 3186, "main_completion_tokens": 76, "judge_total_tokens": 488, "judge_prompt_tokens": 424, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is \"Bottom of the Water\" with a rating of 4.2 [Doc 106899].", "latency_ms": 3418.4341430664062, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'Mega Ape' as the lowest rated movie from 2023, while the generated answer claims it is 'Bottom of the Water' with a rating of 4.2. This directly contradicts the gold label, so the generated answer is factually incorrect relative to the provided ground truth. It also appears to invent a specific rating and citation, indicating it is not faithful to the reference information implied by the gold answer.", "main_total_tokens": 2860, "main_prompt_tokens": 2758, "main_completion_tokens": 102, "judge_total_tokens": 530, "judge_prompt_tokens": 410, "judge_completion_tokens": 120}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 312.511682510376, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer according to the gold is 'Tea House'. The generated answer says 'I don't know', which is incorrect. However, it does not introduce any incorrect factual claim about the world; it simply expresses lack of knowledge, so it is fully faithful to the (lack of) information provided.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 477, "judge_prompt_tokens": 386, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 298.28476905822754, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific movie title as the highest-rated Japanese language movie from 1999, but the generated answer simply states uncertainty and provides no attempt at an answer. This is incorrect with respect to the gold. However, saying 'I don't know' does not introduce any fabricated information, so it is fully faithful to the model's own stated knowledge limits.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 499, "judge_prompt_tokens": 395, "judge_completion_tokens": 104}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "I don't know.", "latency_ms": 2732.1791648864746, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'English' but the generated answer says 'I don't know,' which is incorrect. The model is faithful to its own uncertainty but does not provide the correct factual answer.", "main_total_tokens": 643, "main_prompt_tokens": 549, "main_completion_tokens": 94, "judge_total_tokens": 456, "judge_prompt_tokens": 387, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The Amanda Show [Doc 115219]", "latency_ms": 6372.5340366363525, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold answer is 'Law & Order: Special Victims Unit', but the model answered 'The Amanda Show'. There is no evidence or citation supporting 'The Amanda Show' as the most popular TV show in 1999, so it is not faithful to the provided gold answer or any referenced documents.", "main_total_tokens": 3625, "main_prompt_tokens": 3469, "main_completion_tokens": 156, "judge_total_tokens": 497, "judge_prompt_tokens": 399, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2 [Doc 79355]", "latency_ms": 2378.976583480835, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 and does not introduce any unsupported information; the document reference does not affect faithfulness to the gold label.", "main_total_tokens": 2358, "main_prompt_tokens": 2345, "main_completion_tokens": 13, "judge_total_tokens": 457, "judge_prompt_tokens": 392, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "2025 [Doc 112077]", "latency_ms": 4273.669719696045, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states the movie was released in 2025, but the gold label says 2019, so the answer is factually incorrect. Since the evaluation is against the provided gold answer, the model output is not faithful to the labeled ground truth either.", "main_total_tokens": 2991, "main_prompt_tokens": 2921, "main_completion_tokens": 70, "judge_total_tokens": 477, "judge_prompt_tokens": 393, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "30", "latency_ms": 3765.120506286621, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (30) does not match the gold answer (279), so it is incorrect. However, it attempts to answer the question directly without adding unrelated information, so it is faithful to the query itself.", "main_total_tokens": 2485, "main_prompt_tokens": 2404, "main_completion_tokens": 81, "judge_total_tokens": 457, "judge_prompt_tokens": 383, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "2", "latency_ms": 2067.2671794891357, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 12 votes, but the generated answer said 2 votes, which is numerically incorrect. The evaluation correctly identifies the mismatch, so it is faithful to the comparison but the generated answer itself is not correct.", "main_total_tokens": 2451, "main_prompt_tokens": 2447, "main_completion_tokens": 4, "judge_total_tokens": 455, "judge_prompt_tokens": 383, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "2009 [Doc 46069]", "latency_ms": 3824.0997791290283, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer '2009' exactly matches the gold answer '2009' and does not introduce any extraneous or conflicting information, so it is both correct and faithful.", "main_total_tokens": 3080, "main_prompt_tokens": 3034, "main_completion_tokens": 46, "judge_total_tokens": 462, "judge_prompt_tokens": 396, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b-instruct-q2_K", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is en [Doc 48035].", "latency_ms": 4664.616107940674, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the original language as English (\"en\"), matching the gold answer. However, it cites a specific document ID ([Doc 48035]) that is not present in the provided context, so the justification is not faithful to the given information.", "main_total_tokens": 3350, "main_prompt_tokens": 3271, "main_completion_tokens": 79, "judge_total_tokens": 487, "judge_prompt_tokens": 402, "judge_completion_tokens": 85}
