{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "Wall Street was released in 1987 [Doc 6993].", "latency_ms": 27547.447443008423, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer exactly. It is also faithful to the cited document identifier and does not introduce unsupported information.", "main_total_tokens": 3127, "main_prompt_tokens": 3108, "main_completion_tokens": 19, "judge_total_tokens": 460, "judge_prompt_tokens": 392, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie where classmates have to play a survival game using their cell phones is \"Antisocial\" [Doc 60217]. However, it's important to note that while the participants use their cell phones and other tech devices, the situation does not present itself as a \"game\" but rather as a survival scenario due to an epidemic outbreak. Another movie where classmates play a game with their cell phones leading to disaster is \"Perfect Strangers\" [Doc 103513], but this movie is in Indonesian (id).", "latency_ms": 9245.86820602417, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'Brain Fluid Explosion Girl,' but the generated response incorrectly names 'Antisocial' and 'Perfect Strangers' instead, so it is factually wrong. It also does not adhere to the gold answer and introduces unrelated movies, so it is unfaithful to the reference.", "main_total_tokens": 3027, "main_prompt_tokens": 2915, "main_completion_tokens": 112, "judge_total_tokens": 582, "judge_prompt_tokens": 491, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show where six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" from 1983 [Doc 112849].", "latency_ms": 5004.674434661865, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which TV show features six friends who must rely on a Dungeon Master to get home. The gold answer is \"Dungeons & Dragons.\" The generated answer correctly identifies the show as the 1983 TV series \"Dungeons & Dragons,\" matching the gold answer and not adding any contradictory information. Thus it is both correct and faithful.", "main_total_tokens": 3013, "main_prompt_tokens": 2973, "main_completion_tokens": 40, "judge_total_tokens": 523, "judge_prompt_tokens": 423, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie that tells the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is \"Nacht der Wölfe\" [Doc 2590].", "latency_ms": 5752.81286239624, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, naming 'Nacht der Wölfe' as the movie about rival German and Turkish teen gangs in Munich. It is both correct and directly grounded in the provided gold information.", "main_total_tokens": 3397, "main_prompt_tokens": 3354, "main_completion_tokens": 43, "judge_total_tokens": 513, "judge_prompt_tokens": 438, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of Picnic at Hanging Rock take place in the year 1900 [Doc 126191].", "latency_ms": 5009.279251098633, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of Picnic at Hanging Rock take place in 1900, which matches the gold answer. It is also faithful to the provided information and does not introduce any unsupported details.", "main_total_tokens": 3114, "main_prompt_tokens": 3085, "main_completion_tokens": 29, "judge_total_tokens": 479, "judge_prompt_tokens": 406, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie that matches this description is \"June\" [Doc 32483].", "latency_ms": 4250.293016433716, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question’s gold answer is the movie title 'June'. The generated answer also states that the movie is 'June', matching exactly. There are no extraneous or conflicting details, so the response is both correct and faithful to the gold.", "main_total_tokens": 3338, "main_prompt_tokens": 3313, "main_completion_tokens": 25, "judge_total_tokens": 530, "judge_prompt_tokens": 451, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "The highest rated Russian-language movie released in 1980, based on the provided documents, is \"Moscow Does Not Believe in Tears\" with a rating of 7.6 [Doc 61].", "latency_ms": 6179.3999671936035, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger', while the generated answer is 'Moscow Does Not Believe in Tears'. This is factually incorrect with respect to the labeled gold. Additionally, the generated answer references 'the provided documents' and a specific rating [Doc 61], but no such documents are available in this evaluation context, so the claim is not grounded in the given evidence, making it unfaithful for this task.", "main_total_tokens": 3338, "main_prompt_tokens": 3293, "main_completion_tokens": 45, "judge_total_tokens": 563, "judge_prompt_tokens": 436, "judge_completion_tokens": 127}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest Russian-language movie rating in 1999 based on the provided documents is 4.571, which belongs to the movie \"Features of the Russian Bath\" [Doc 22144].", "latency_ms": 5244.380235671997, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the highest Russian-language movie rating in 1999 is 7.6, whereas the generated answer claims it is 4.571 for a specific movie, so the numeric result is incorrect. Additionally, the generated answer references “the provided documents” and a specific document ID, but in this evaluation setting we cannot verify that claim; the answer directly contradicts the known gold value, so it is not faithful to the ground truth for this question.", "main_total_tokens": 2784, "main_prompt_tokens": 2737, "main_completion_tokens": 47, "judge_total_tokens": 545, "judge_prompt_tokens": 424, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "Based on the provided documents, two movies from 1987 have a rating over 7 and are in Chinese:\n\n1. \"A Chinese Ghost Story\" with a rating of 7.3 [Doc 7064].\n2. \"Last Eunuch In China\" with a rating of 7.8 [Doc 7754].", "latency_ms": 7102.392673492432, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 such movies, but the generated answer claims there are only 2. This directly contradicts the gold label, so the correctness is 0. However, within the generated answer, it consistently bases its conclusion on the documents it references (listing exactly the movies it finds), so it is faithful to its own cited evidence, giving a faithfulness score of 1.", "main_total_tokens": 3264, "main_prompt_tokens": 3188, "main_completion_tokens": 76, "judge_total_tokens": 572, "judge_prompt_tokens": 460, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "Based on the provided documents, there is one TV show that meets all the criteria:\n\n- **Journey to the West** [Doc 113169]\n  - Year: 1986 (Note: The second season was released in 1999, but the original series was first broadcast in 1986)\n  - Rating: 8.4\n  - Language: zh\n\nHowever, since the question specifically asks for TV shows released in 1987, and \"Journey to the West\" was first broadcast in 1986, there are no TV shows that meet all the criteria listed.", "latency_ms": 10623.256921768188, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer is 2, but the generated answer concludes there are no such TV shows, so the numerical result is incorrect. The reasoning is partially grounded in the provided document about 'Journey to the West' (noting its 1986 release and thus excluding it), which is faithful to that document, but it then extrapolates that there are no qualifying TV shows at all, which is not supported by the full ground truth. Thus correctness is 0; faithfulness is partial at 0.5.", "main_total_tokens": 3096, "main_prompt_tokens": 2959, "main_completion_tokens": 137, "judge_total_tokens": 650, "judge_prompt_tokens": 513, "judge_completion_tokens": 137}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "The provided documents do not list any French language TV shows released specifically in the year 1980. [Doc]", "latency_ms": 4246.371746063232, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the only French language TV show released in 1980, which the gold answer identifies as 'Fantômas.' The generated answer incorrectly states that no such show is listed, directly contradicting the gold answer and thus is factually incorrect relative to the labeled data. However, the generated answer is consistent with the information it claims about the provided documents (it does not invent another show, but instead says the documents do not list one), so it remains faithful to its stated source limitations.", "main_total_tokens": 3377, "main_prompt_tokens": 3351, "main_completion_tokens": 26, "judge_total_tokens": 540, "judge_prompt_tokens": 408, "judge_completion_tokens": 132}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" [Doc 11205].", "latency_ms": 4355.231285095215, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise', which matches the gold answer. It stays faithful to the information implied by the question and adds a minimal citation-like note without introducing contradictions or unsupported claims.", "main_total_tokens": 3201, "main_prompt_tokens": 3171, "main_completion_tokens": 30, "judge_total_tokens": 482, "judge_prompt_tokens": 409, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 5098.067283630371, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Won't You Be My Neighbor?'. It is fully correct and faithful to the given information.", "main_total_tokens": 3283, "main_prompt_tokens": 3240, "main_completion_tokens": 43, "judge_total_tokens": 506, "judge_prompt_tokens": 440, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The document does not specify a single release year for \"it\" as multiple movies are listed with different release years. Please specify which movie you are asking about.", "latency_ms": 4613.351106643677, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates a specific year (2018), so the correct response should have been that the item in question was released in 2018. The generated answer incorrectly claims ambiguity and does not provide the year. This contradicts the gold answer and is therefore both incorrect and unfaithful to the target answer.", "main_total_tokens": 3343, "main_prompt_tokens": 3310, "main_completion_tokens": 33, "judge_total_tokens": 503, "judge_prompt_tokens": 409, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is \"Astro Boy\" [Doc 112591].", "latency_ms": 4974.130392074585, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 'Astro Boy' as the TV series described and does not introduce unsupported information beyond restating the question with the answer and a document reference.", "main_total_tokens": 3539, "main_prompt_tokens": 3503, "main_completion_tokens": 36, "judge_total_tokens": 481, "judge_prompt_tokens": 416, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her young son were held captive for 7 years [Doc 67513].", "latency_ms": 4527.72331237793, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9463946303571863, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer exactly, and it does not introduce any extraneous or contradictory information.", "main_total_tokens": 2790, "main_prompt_tokens": 2753, "main_completion_tokens": 37, "judge_total_tokens": 494, "judge_prompt_tokens": 425, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26m before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 5198.967695236206, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer directly matches the gold amount given in the question (£26m) and does not introduce any unsupported information beyond restating it with minor wording changes, so it is both correct and faithful.", "main_total_tokens": 3121, "main_prompt_tokens": 3077, "main_completion_tokens": 44, "judge_total_tokens": 499, "judge_prompt_tokens": 428, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "The most popular English-speaking TV show released in 1997, based on the provided documents, is \"Buffy the Vampire Slayer\" with a popularity score of 25.9735 [Doc 114711].", "latency_ms": 7350.956439971924, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer popularity score is 80.5772, whereas the generated answer gives 25.9735, which is incorrect. It also asserts that Buffy the Vampire Slayer is the most popular show with that score, contradicting the gold value for the most popular show, so the reasoning is not faithful to the ground truth.", "main_total_tokens": 3950, "main_prompt_tokens": 3897, "main_completion_tokens": 53, "judge_total_tokens": 532, "judge_prompt_tokens": 435, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "Based on the provided documents, there is one Russian language TV show that was released in 1997: \"Kamenskaya\" [Doc 115256].", "latency_ms": 5037.222385406494, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the number of Russian language TV shows released in 1997, and the gold answer is 1. The generated answer states that there is one such show, matching the gold numerically. It also bases this on the provided documents, so it does not invent extra facts beyond what is implied; thus it is both correct and faithful to the given information.", "main_total_tokens": 2623, "main_prompt_tokens": 2582, "main_completion_tokens": 41, "judge_total_tokens": 521, "judge_prompt_tokens": 415, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 5381.958246231079, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Angel Eyes') and correctly identifies the TV show. It is fully consistent with the provided information and does not introduce any unsupported claims.", "main_total_tokens": 3106, "main_prompt_tokens": 3066, "main_completion_tokens": 40, "judge_total_tokens": 495, "judge_prompt_tokens": 429, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French [Doc 110681].", "latency_ms": 4273.966073989868, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states that the movie language is French, while the gold label says Korean, so the content is factually incorrect. It also introduces a movie title and citation not grounded in the provided gold answer, so it is not faithful to the given information.", "main_total_tokens": 3033, "main_prompt_tokens": 3011, "main_completion_tokens": 22, "judge_total_tokens": 476, "judge_prompt_tokens": 393, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "None of the English-speaking Horror movies listed from 2009 have a popularity score greater than 7. The movies from 2009 are:\n\n1. \"In the Market\" with a popularity score of 0.5359 [Doc 47301]\n2. \"Perkins' 14\" with a popularity score of 1.1125 [Doc 46432]\n3. \"Scare Zone\" with a popularity score of 0.3206 [Doc 47582]\n4. \"Vampiro\" with a popularity score of 0.3315 [Doc 47574]\n\nNone of these scores are greater than 7.", "latency_ms": 12845.213651657104, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 2 such movies, but the generated answer concludes there are none, so the numerical result is incorrect. However, the generated answer is consistent with the provided document scores it cites and does not invent unseen data, so it is faithful to the given evidence.", "main_total_tokens": 2993, "main_prompt_tokens": 2834, "main_completion_tokens": 159, "judge_total_tokens": 613, "judge_prompt_tokens": 525, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The provided documents do not contain information about any movies released in 1979. Therefore, I cannot determine the best-rated movie from that year based on the given context.", "latency_ms": 5423.457622528076, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that there is no information about movies released in 1979 in the available data. The generated answer correctly and faithfully reflects the limitation of the provided documents and does not introduce external information or guesses, so it is both correct and faithful to the context.", "main_total_tokens": 3113, "main_prompt_tokens": 3076, "main_completion_tokens": 37, "judge_total_tokens": 517, "judge_prompt_tokens": 429, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The provided documents do not mention a movie titled \"Back to the Future Part IV\" or its release year. [Doc 10621], [Doc 9405], and other related documents only discuss the original trilogy consisting of three parts, with the last installment, Back to the Future Part III, released in 1990. There is no information about a fourth part being made or released.", "latency_ms": 8928.330659866333, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly explains that there is no information about a 'Back to the Future Part IV' and notes that only the original trilogy exists, matching the gold answer's statement that such a movie does not exist in the database. It does not invent a release year and stays aligned with the provided information, so it is both correct and faithful.", "main_total_tokens": 3475, "main_prompt_tokens": 3388, "main_completion_tokens": 87, "judge_total_tokens": 579, "judge_prompt_tokens": 479, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "None of the provided documents contain information about a movie from 1980 regarding the Chernobyl disaster. The earliest documentary related to Chernobyl is \"Chornobyl: Chronicle of Difficult Weeks\" from 1990 [Doc 11722]. Therefore, based on the given context, there is no answer to this question.", "latency_ms": 7394.262075424194, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The gold answer states that no movies from 1980 related to the Chernobyl disaster could be found. The generated answer instead extrapolates from provided documents and mentions a 1990 documentary, concluding there is no answer based on the given context. This changes the scope from an information-retrieval failure (\"could not find\") to a context-limited statement and introduces a specific documentary not in the gold, so it does not match the gold answer. Thus, it is not fully correct. However, it remains mostly faithful to the given (hypothetical) documents by stating that none of them mention a 1980 Chernobyl movie and that no answer can be derived from them, so faithfulness is relatively high but not perfect due to the extra claim about the earliest documentary.", "main_total_tokens": 3441, "main_prompt_tokens": 3365, "main_completion_tokens": 76, "judge_total_tokens": 665, "judge_prompt_tokens": 472, "judge_completion_tokens": 193}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are five TV shows or movies with 'Astro Boy' in the title based on the provided documents:\n\n1. Astro Boy (Movie, 2009) [Doc 44947]\n2. Astro Boy (TV, 1980) [Doc 112591]\n3. Astro Boy (TV, 2003) [Doc 116265]\n4. Astro Boy vs The Junkyard Pirates (Movie, 2010) [Doc 49121]\n5. Astro Note (TV, 2024) [Doc 135375]", "latency_ms": 11307.386636734009, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 1.1510196182278052, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.5330476224182168, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer says there are 4 titles with 'Astro Boy' in the title. The generated answer claims 5, so it is numerically incorrect. It also incorrectly includes 'Astro Note', which does not have 'Astro Boy' in the title, making the reasoning partially unfaithful to the question requirement, even though it may follow some provided documents. Thus correctness is 0 and faithfulness is partial.", "main_total_tokens": 3340, "main_prompt_tokens": 3199, "main_completion_tokens": 141, "judge_total_tokens": 618, "judge_prompt_tokens": 498, "judge_completion_tokens": 120}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "Based on the provided documents, there are two movies from 2025 that allude to the Holocaust:\n\n1. \"Everyone Is Going to Die\" - This movie does not directly relate to the Holocaust but is included for completeness. However, it does not allude to the Holocaust in its description [Doc 111564].\n   \n2. \"The Future Awaits\" - This movie tells the story of a 13-year-old Jewish girl and her parents hiding during the Vel' d'Hiv' Roundup of Jewish families in Paris, which is directly related to the Holocaust [Doc 112009].\n\nTherefore, only one movie from 2025 alludes to the Holocaust.", "latency_ms": 16646.713972091675, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.3, "justification": "The question asks how many movies in 2025 allude to the Holocaust; the gold answer is 2. The generated answer concludes that only one movie alludes to the Holocaust, contradicting the gold, so the correctness is 0. In addition, the generated answer internally contradicts itself: it first states there are two movies from 2025 that allude to the Holocaust, then describes one as not alluding to the Holocaust and concludes that only one movie alludes to the Holocaust. This inconsistency and misinterpretation of the given documents reduce faithfulness, though the answer does attempt to ground its reasoning in the described documents, so it is not entirely unfaithful.", "main_total_tokens": 3665, "main_prompt_tokens": 3514, "main_completion_tokens": 151, "judge_total_tokens": 691, "judge_prompt_tokens": 520, "judge_completion_tokens": 171}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "Based on the provided documents, there are three TV shows about Super Mario Bros that were released in the 1990s:\n\n1. The Adventures of Super Mario Bros. 3 [Doc 113661], which was released in 1990.\n2. Super Mario World [Doc 113814], which was released in 1991.\n3. The Super Mario Bros. Super Show! [Doc 113517], although this show is listed as being from 1989, it is worth noting for context but does not count towards the 1990s tally.\n\nTherefore, two TV shows about Super Mario Bros were released in the 1990s. [Doc 113661], [Doc 113814]", "latency_ms": 17775.05874633789, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.8010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer states there was 1 TV show, but the generated answer concludes there were 2, so it is factually incorrect relative to the gold. The reasoning stays mostly within the given documents and does not hallucinate additional shows, but it misinterprets or miscounts based on those documents. Thus, faithfulness is partial rather than full.", "main_total_tokens": 3545, "main_prompt_tokens": 3367, "main_completion_tokens": 178, "judge_total_tokens": 636, "judge_prompt_tokens": 532, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "Based on the provided documents, there are no TV shows about Super Mario Bros that were released between the years 2000 and 2010. The closest entries are \"The Adventures of Super Mario Bros. 3\" from 1990 and \"Super Mario World\" from 1991, both falling outside the specified time frame [Doc 113661] [Doc 113814].", "latency_ms": 7500.072956085205, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0 TV shows, and the generated answer states that there are no TV shows about Super Mario Bros released between 2000 and 2010, which matches the gold. It also correctly notes older shows outside the range. The reasoning stays grounded in the provided documents, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3137, "main_prompt_tokens": 3042, "main_completion_tokens": 95, "judge_total_tokens": 566, "judge_prompt_tokens": 467, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular TV show or movie about ninjas, based on the provided popularity scores, is \"House of Ninjas\" with a popularity score of 4.4317 [Doc 135161].", "latency_ms": 4813.7030601501465, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question expects the specific gold answer 'Naruto' as the most popular ninja-related show. The generated answer instead selects 'House of Ninjas' based on document popularity scores, which conflicts with the gold label, so it is incorrect. However, the answer is faithful to the cited document and uses the provided popularity scores as its basis, so faithfulness is high.", "main_total_tokens": 3467, "main_prompt_tokens": 3419, "main_completion_tokens": 48, "judge_total_tokens": 528, "judge_prompt_tokens": 423, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The provided documents do not mention a Jurassic Park/Power Rangers crossover movie. Therefore, based on the given information, there is no answer to this question.", "latency_ms": 4536.490201950073, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.0, "faithfulness_score": 1.0, "justification": "The gold answer states that the movie was not released. The generated answer instead claims there is no answer based on the documents, which does not convey the correct factual resolution that the movie does not exist/was not released. Thus, it is unfaithful to the gold answer in terms of correctness. However, it accurately reflects its own stated limitations (based on documents) and does not fabricate a release date, so it is faithful to the information it cites, but not correct relative to the gold.", "main_total_tokens": 3280, "main_prompt_tokens": 3248, "main_completion_tokens": 32, "judge_total_tokens": 551, "judge_prompt_tokens": 416, "judge_completion_tokens": 135}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "None of the provided documents mention Barbie fighting the Predator. [Doc 68094] to [Doc 104474] detail various Barbie movies, but none involve a confrontation with the Predator. The Predator is featured in other movies like \"The Predator\" [Doc 82000] and \"Alien Predator\" [Doc 84103], but Barbie does not appear in these films.", "latency_ms": 8092.388868331909, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the movie was not released, implying that such a movie exists conceptually but never came out. The generated answer instead claims that no such movie exists among the provided documents and shifts focus to other Predator movies without addressing the release status of a hypothetical Barbie vs. Predator movie. This conflicts with the gold answer and does not follow it, so it is neither correct relative to the gold nor faithful to it.", "main_total_tokens": 3304, "main_prompt_tokens": 3213, "main_completion_tokens": 91, "judge_total_tokens": 574, "judge_prompt_tokens": 458, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The provided context does not contain a list of all Star Wars movies in order of release, so it's not possible to definitively state which is the 4th oldest Star Wars movie based solely on this information. The documents primarily focus on documentaries, specials, and some animated series rather than the main film series. [Doc 32101] [Doc 2858] [Doc 39908] [Doc 117312] [Doc 67472] [Doc 94808] [Doc 32997] [Doc 31821] [Doc 3] [Doc 133041] [Doc 128111] [Doc 124704] [Doc 121736] [Doc 93599] [Doc 54577] [Doc 3568] [Doc 41419] [Doc 32312]", "latency_ms": 19506.021738052368, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because it claims the question cannot be answered from the context, while the gold answer explicitly identifies the 4th oldest Star Wars movie as 'Star Wars: Droids - The Great Heep'. However, the answer is internally consistent with the (stated) limitation of the provided documents and does not hallucinate an alternative movie title, so it is faithful to its own reasoning about the context it describes.", "main_total_tokens": 3467, "main_prompt_tokens": 3242, "main_completion_tokens": 225, "judge_total_tokens": 675, "judge_prompt_tokens": 558, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The document does not specify a single release year for \"it\" as multiple movies are listed with different release years. Please specify which movie you are asking about.", "latency_ms": 4952.117204666138, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly indicates the intended reference is to a single item released in 1986, so the correct response should be the year 1986. The generated answer claims the document lists multiple movies and asks for clarification instead of providing 1986. This is incorrect relative to the gold. However, given only the user question and no explicit mention of the film title in the question itself, the model’s response is consistent with the limited context it sees, so it is faithful to its own (insufficient) context but not correct with respect to the gold answer.", "main_total_tokens": 3343, "main_prompt_tokens": 3310, "main_completion_tokens": 33, "judge_total_tokens": 554, "judge_prompt_tokens": 409, "judge_completion_tokens": 145}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The rating of the movie Forrest Gump is 8.466 [Doc 15236].", "latency_ms": 4174.121856689453, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating value 8.466 and directly answers the question about the movie Forrest Gump. It is fully consistent with the provided gold reference, so it is both correct and faithful.", "main_total_tokens": 2950, "main_prompt_tokens": 2925, "main_completion_tokens": 25, "judge_total_tokens": 487, "judge_prompt_tokens": 413, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 5116.6887283325195, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med,' which matches the gold answer exactly. It is fully consistent with the provided information, so both correctness and faithfulness are maximal.", "main_total_tokens": 3128, "main_prompt_tokens": 3091, "main_completion_tokens": 37, "judge_total_tokens": 493, "judge_prompt_tokens": 423, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "The seven prisoners escaped from a prison in Argentina in the movie Túnel de los huesos (2011) [Doc 53582].", "latency_ms": 5125.227928161621, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the seven prisoners in 'Túnel de los huesos' (2011) escaped from a prison in Argentina, which matches the gold answer. It does not introduce any contradictions or extraneous, unfounded details beyond a minor citation marker, so it is both correct and faithful to the provided information.", "main_total_tokens": 3454, "main_prompt_tokens": 3419, "main_completion_tokens": 35, "judge_total_tokens": 518, "judge_prompt_tokens": 420, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice Li Nai falls into the trap designed by Jiang Jie, the \"Chef Assassin.\" [Doc 135231]", "latency_ms": 5482.5439453125, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The gold answer is simply 'Jiang Jie.' The generated answer states that the apprentice Li Nai falls into the trap designed by Jiang Jie. This directly matches the gold regarding whose trap it is, so the core factual answer is correct. The added description 'the \"Chef Assassin\"' and mention of a document citation are extra details not supported by the gold; they may or may not be accurate, so there is slight over-specification relative to the provided reference. Thus correctness is 1, and faithfulness is slightly reduced to 0.9 due to the additional unsupported embellishment.", "main_total_tokens": 3128, "main_prompt_tokens": 3086, "main_completion_tokens": 42, "judge_total_tokens": 568, "judge_prompt_tokens": 417, "judge_completion_tokens": 151}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "Based on the provided documents, there are no Korean-language movies mentioning assassins that are released after 2023. The most recent Korean-language movie related to assassins is \"The Assassin\" from 2023 [Doc 106219].", "latency_ms": 6103.531837463379, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says 4, while the generated answer says there are none, so it is factually incorrect relative to the gold. However, the generated answer is consistent with and grounded in the provided document it cites (Doc 106219), so it is faithful to the given context.", "main_total_tokens": 3047, "main_prompt_tokens": 2990, "main_completion_tokens": 57, "judge_total_tokens": 520, "judge_prompt_tokens": 431, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "The provided documents do not contain any information about Steven Seagal and Bruce Lee appearing together in movies. Therefore, based on the given context, I cannot provide a number of movies that feature both actors. [No Direct Source]", "latency_ms": 5808.791637420654, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information in the provided documents about movies featuring both Steven Seagal and Bruce Lee together, and therefore does not fabricate a number. This matches the gold answer in substance and is fully grounded in the given context, so it is both correct and faithful.", "main_total_tokens": 3569, "main_prompt_tokens": 3523, "main_completion_tokens": 46, "judge_total_tokens": 527, "judge_prompt_tokens": 438, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 4645.395994186401, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3 and attributes it to a document, so it is both correct and faithful to the provided information.", "main_total_tokens": 3316, "main_prompt_tokens": 3290, "main_completion_tokens": 26, "judge_total_tokens": 466, "judge_prompt_tokens": 403, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The provided context does not specify the style of racing for any particular movie that matches a unique description of its racing style. Each movie mentions racing but does not detail the specific style beyond general terms like \"illegal street race,\" \"cross-country road race,\" or \"stock car track.\" Therefore, based solely on the given information, it is not possible to determine a specific style of racing for any one movie. [Doc 41929]-[Doc 62979]", "latency_ms": 8719.141960144043, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question expects the specific answer 'Drift' as the style of racing. The generated response states that it is not possible to determine the style, which is incorrect. However, the answer is faithful to the provided (though limited) context it cites, as it accurately reflects that the context segments mentioned do not specify drifting, and it refrains from inventing details.", "main_total_tokens": 3443, "main_prompt_tokens": 3343, "main_completion_tokens": 100, "judge_total_tokens": 576, "judge_prompt_tokens": 471, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The document provided does not contain a movie title about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little. The information given is related to a TV show titled \"Confronting a Serial Killer\" from 2021 [Doc 131298].", "latency_ms": 6803.555965423584, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The correct answer is \"Confronting a Serial Killer.\" The generated answer incorrectly claims there is no movie title and refers only to a TV show, thus failing to provide the correct title. However, it does accurately reference the related work and its context, so it is mostly faithful to the provided document but not correct with respect to the question.", "main_total_tokens": 3328, "main_prompt_tokens": 3258, "main_completion_tokens": 70, "judge_total_tokens": 571, "judge_prompt_tokens": 469, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 4427.810430526733, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and does not introduce any contradictions or unsupported claims. It is correct and faithful to the provided information.", "main_total_tokens": 3340, "main_prompt_tokens": 3310, "main_completion_tokens": 30, "judge_total_tokens": 472, "judge_prompt_tokens": 407, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 4465.431690216064, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6895405204413555, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Sixth Sense' and remains faithful to the information implied by the question and gold answer, with no contradictions or hallucinations.", "main_total_tokens": 2938, "main_prompt_tokens": 2903, "main_completion_tokens": 35, "judge_total_tokens": 480, "judge_prompt_tokens": 420, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 3907.9768657684326, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: it states that The Transporter was released in 2002. The citation style does not contradict the claim, so the response is both correct and faithful.", "main_total_tokens": 3243, "main_prompt_tokens": 3220, "main_completion_tokens": 23, "judge_total_tokens": 468, "judge_prompt_tokens": 397, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 4506.664276123047, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2015 reboot title as 'The Transporter Refueled' and does not introduce unsupported information beyond a benign document reference.", "main_total_tokens": 3171, "main_prompt_tokens": 3136, "main_completion_tokens": 35, "judge_total_tokens": 489, "judge_prompt_tokens": 420, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in 2015 [Doc 67500].", "latency_ms": 4134.771347045898, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold year 2015 and clearly states it, so it is correct. It is also faithful to the provided information, without adding unsupported details.", "main_total_tokens": 3238, "main_prompt_tokens": 3212, "main_completion_tokens": 26, "judge_total_tokens": 469, "judge_prompt_tokens": 404, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "None of the provided documents mention a romantic comedy where Abraham Lincoln comes back from the dead. The romantic comedies listed do not involve Abraham Lincoln or any supernatural elements related to him returning from the dead. [Doc 47970], [Doc 3294], and [Doc 94894] are the documents that describe romantic comedies, but none of them include Abraham Lincoln as a character or any plot involving his resurrection.", "latency_ms": 7963.377952575684, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says it cannot find such a movie. The generated answer likewise states that none of the documents mention a romantic comedy where Abraham Lincoln comes back from the dead and that no such plot is present. This matches the evidence and does not invent information. Both correctness and faithfulness are therefore fully satisfied.", "main_total_tokens": 3049, "main_prompt_tokens": 2955, "main_completion_tokens": 94, "judge_total_tokens": 567, "judge_prompt_tokens": 475, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by Ernest Hemingway is \"Havana Darkness\" [Doc 90923].", "latency_ms": 4470.957517623901, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying 'Havana Darkness' as the movie title. It is correct and directly based on the provided information, with no hallucinations or extraneous claims.", "main_total_tokens": 3149, "main_prompt_tokens": 3116, "main_completion_tokens": 33, "judge_total_tokens": 486, "judge_prompt_tokens": 415, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show \"Doll & Em\" [Doc 122259].", "latency_ms": 4535.375118255615, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Doll & Em,\" matching the gold answer exactly and staying faithful to the provided information without adding unsupported details.", "main_total_tokens": 3141, "main_prompt_tokens": 3107, "main_completion_tokens": 34, "judge_total_tokens": 480, "judge_prompt_tokens": 417, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The 2007 movie in which Hallam has a talent for spying on people and attempts to find the cause of his mother's death is titled \"Hallam Foe\" [Doc 39389].", "latency_ms": 5409.440755844116, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Hallam Foe,\" which matches the gold answer exactly. It is fully supported by the provided gold reference and stays faithful to it without adding incorrect information.", "main_total_tokens": 3182, "main_prompt_tokens": 3135, "main_completion_tokens": 47, "judge_total_tokens": 510, "judge_prompt_tokens": 439, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The provided documents do not contain information about any character whose last name sounds like they could be a friend or a foe. Therefore, based solely on the given context, there is no answer to this question.", "latency_ms": 4926.775932312012, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'A foe', indicating the question is answerable and the correct label is that the last name sounds like a foe. The generated answer claims there is no answer based on the context, which contradicts the gold answer, so it is incorrect. However, the response is faithful to the (purported) provided context, as it does not invent an answer and explicitly states that the information is not present, so faithfulness is high.", "main_total_tokens": 3001, "main_prompt_tokens": 2959, "main_completion_tokens": 42, "judge_total_tokens": 549, "judge_prompt_tokens": 427, "judge_completion_tokens": 122}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "The age of Sam in the 2017 TV show Atypical is 18. [Doc 124583]", "latency_ms": 4193.619966506958, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: Sam is 18. It does not introduce any contradictions or unsupported claims beyond referencing a document ID, so it is both correct and faithful.", "main_total_tokens": 2991, "main_prompt_tokens": 2960, "main_completion_tokens": 31, "judge_total_tokens": 479, "judge_prompt_tokens": 411, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 4206.02011680603, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, stating that Sam is on the autism spectrum. This is correct and fully supported by the provided information.", "main_total_tokens": 3170, "main_prompt_tokens": 3141, "main_completion_tokens": 29, "judge_total_tokens": 470, "judge_prompt_tokens": 410, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "The provided documents do not contain information about a movie titled \"À corde tendue\" or any details regarding a character named Sandra being born in a specific place. Therefore, based on the given context, this information cannot be found. [Information not found in provided sources]", "latency_ms": 6604.840278625488, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for Sandra's birthplace (Chamonix). The generated answer claims the information cannot be found in the provided documents, which contradicts the gold answer and is therefore incorrect. However, it accurately reflects the assistant's stated limitation of relying only on the provided context, so it is faithful to the (assumed) context constraints.", "main_total_tokens": 3435, "main_prompt_tokens": 3379, "main_completion_tokens": 56, "judge_total_tokens": 550, "judge_prompt_tokens": 451, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "in this japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 4141.031265258789, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, filling the blank with 'rains' and preserving the sentence meaning without adding unsupported information.", "main_total_tokens": 3187, "main_prompt_tokens": 3161, "main_completion_tokens": 26, "judge_total_tokens": 470, "judge_prompt_tokens": 412, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the German language animation \"Tabaluga\", Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 4819.897413253784, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Greenland') and stays within the information given, citing the same location without adding unsupported claims.", "main_total_tokens": 3313, "main_prompt_tokens": 3276, "main_completion_tokens": 37, "judge_total_tokens": 485, "judge_prompt_tokens": 426, "judge_completion_tokens": 59}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is \"One Afternoon\" [Doc 96838].", "latency_ms": 5474.099636077881, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'One Afternoon,' which matches the gold answer exactly. It does not introduce contradictions or unsupported claims beyond referencing a document ID, so it is both correct and faithful to the given information.", "main_total_tokens": 3054, "main_prompt_tokens": 3004, "main_completion_tokens": 50, "judge_total_tokens": 524, "judge_prompt_tokens": 448, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie where a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" [Doc 93260].", "latency_ms": 10552.29377746582, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title of a low-rated Italian language movie where a Holocaust survivor cares for a 12-year-old street kid who had robbed her. Both the gold answer and the generated answer identify \"The Life Ahead\" as the movie. This matches exactly, so the generated answer is fully correct and faithful to the gold.", "main_total_tokens": 3724, "main_prompt_tokens": 3683, "main_completion_tokens": 41, "judge_total_tokens": 531, "judge_prompt_tokens": 434, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary \"McLaren\" is about Bruce McLaren, a New Zealander who founded the McLaren Motor Racing team. [Doc 73859]", "latency_ms": 5106.1742305755615, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies that the 2016 documentary 'McLaren' is about Bruce McLaren and stays consistent with the provided gold answer. It adds accurate contextual detail (New Zealander who founded the McLaren Motor Racing team) without contradicting the source.", "main_total_tokens": 3353, "main_prompt_tokens": 3313, "main_completion_tokens": 40, "judge_total_tokens": 504, "judge_prompt_tokens": 419, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The provided documents do not mention the name of Danny Krueger's racing team. [Doc 52333] and [Doc 63384] both discuss movies featuring a character named Danny Krueger who is involved in racing, but neither document specifies the name of his racing team.", "latency_ms": 6690.71364402771, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the racing team is the McLaren Motor Racing team. The generated answer claims the documents do not mention the team name and therefore does not provide it. This is incorrect relative to the gold, so correctness is 0. However, the response is internally consistent with its own claim about the documents and does not invent a team name, so it is faithful to the information it cites, giving faithfulness 1.", "main_total_tokens": 3469, "main_prompt_tokens": 3404, "main_completion_tokens": 65, "judge_total_tokens": 554, "judge_prompt_tokens": 441, "judge_completion_tokens": 113}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The question \"What country is he from?\" does not have a specific answer based on the provided documents, as there is no clear subject (\"he\") referred to in the context that has a specified country of origin. Each document describes different movies with various characters and settings but does not provide information about a particular individual's country of origin without additional context.", "latency_ms": 6802.08945274353, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'New Zealand', so the question does have a specific correct answer. The generated answer incorrectly claims that there is no specific answer based on the documents and does not provide 'New Zealand'. Therefore it is incorrect. It is also unfaithful to the gold label because it contradicts the known correct answer.", "main_total_tokens": 3224, "main_prompt_tokens": 3153, "main_completion_tokens": 71, "judge_total_tokens": 541, "judge_prompt_tokens": 445, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "None of the provided documents mention John McClane blowing up the Great Wall of China in a 2013 movie. The 2013 movie featuring John McClane is \"A Good Day to Die Hard,\" but it does not involve him blowing up the Great Wall of China [Doc 58791].", "latency_ms": 6891.651630401611, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a 2013 movie where John McClane blows up the Great Wall of China. The gold answer correctly states that there is no such movie in the database. The generated answer correctly identifies the actual 2013 John McClane film as 'A Good Day to Die Hard' and correctly notes that it does not feature him blowing up the Great Wall of China. This directly addresses the question by implying that no such film exists. It is consistent with the gold answer and does not introduce unsupported claims, so it is both correct and faithful to the provided information.", "main_total_tokens": 3219, "main_prompt_tokens": 3151, "main_completion_tokens": 68, "judge_total_tokens": 608, "judge_prompt_tokens": 460, "judge_completion_tokens": 148}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with the given description is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 4557.868003845215, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as \"Tomie: Forbidden Fruit\". It does not introduce any contradictory or extraneous claims, so it is both correct and faithful to the given information.", "main_total_tokens": 3198, "main_prompt_tokens": 3166, "main_completion_tokens": 32, "judge_total_tokens": 519, "judge_prompt_tokens": 443, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years in the 2021 movie \"Lei mi parla ancora\" [Doc 98566].", "latency_ms": 5223.699569702148, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, matching the gold answer. It is faithful to the provided information without adding unsupported details.", "main_total_tokens": 3498, "main_prompt_tokens": 3459, "main_completion_tokens": 39, "judge_total_tokens": 491, "judge_prompt_tokens": 424, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In \"Scorpion King: Book of Souls\" (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls, which will allow them to put an end to an evil warlord [Doc 82582].", "latency_ms": 7255.953311920166, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in \"Scorpion King: Book of Souls\" (2018), The Scorpion King and Tala search for the Book of Souls, matching the gold answer. The extra description about it being a legendary relic that can end an evil warlord is consistent with the movie and does not contradict the question, so it remains faithful to the source information.", "main_total_tokens": 3864, "main_prompt_tokens": 3806, "main_completion_tokens": 58, "judge_total_tokens": 551, "judge_prompt_tokens": 445, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document provided does not contain information about a movie titled \"Zombies on the Moon\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 4966.877222061157, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database and thus does not give a year. The generated answer similarly states that the provided document does not contain information about that movie and that it cannot provide the release year. This is correct given the lack of information and is faithful to the context and the question's limitations.", "main_total_tokens": 2907, "main_prompt_tokens": 2870, "main_completion_tokens": 37, "judge_total_tokens": 522, "judge_prompt_tokens": 427, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain any information about a movie titled \"The Galactic baker\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 5002.244234085083, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database. The generated answer correctly states that the documents do not contain information about the movie and that it cannot provide the release year. This is accurate and aligns with the given context, without inventing a year or extra facts.", "main_total_tokens": 3527, "main_prompt_tokens": 3491, "main_completion_tokens": 36, "judge_total_tokens": 510, "judge_prompt_tokens": 424, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"Return of the Null Pointer\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 4810.707569122314, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about a fictional or unknown movie not present in the database. The gold answer states that the movie is not in the database. The generated answer correctly states that the provided documents do not contain information about the movie and that it cannot provide the release year, which is effectively equivalent. It does not hallucinate a year or contradict the context, so it is both correct and faithful to the given information.", "main_total_tokens": 3188, "main_prompt_tokens": 3151, "main_completion_tokens": 37, "judge_total_tokens": 540, "judge_prompt_tokens": 427, "judge_completion_tokens": 113}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"Data Science: The Movie\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 4937.037944793701, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, while the generated answer states that the provided documents do not contain information. These are meaningfully different contexts; the generated answer implies a limitation to the given documents, not the overall database. Therefore it is not correct relative to the gold. However, the generated answer is faithful to the information in the prompt and does not hallucinate a release year or extra facts, so faithfulness is high.", "main_total_tokens": 3123, "main_prompt_tokens": 3086, "main_completion_tokens": 37, "judge_total_tokens": 549, "judge_prompt_tokens": 427, "judge_completion_tokens": 122}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document does not contain information about a movie titled \"Python's Revenge 2\". [Doc X]", "latency_ms": 4067.599058151245, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there is no information about a movie titled 'Python's Revenge 2'. The generated answer is faithful to the prompt and does not hallucinate a release year or other details; it simply notes the absence of information in the document, which aligns with the gold response meaningfully.", "main_total_tokens": 3477, "main_prompt_tokens": 3455, "main_completion_tokens": 22, "judge_total_tokens": 507, "judge_prompt_tokens": 412, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no movie titled \"The Last Pixel\" in the provided documents.", "latency_ms": 3249.5765686035156, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The gold answer correctly states that the movie is not in the database. The generated answer states it is not in the provided documents, which is semantically similar but not identical to the database notion, so it is mostly but not perfectly faithful to the gold phrasing. Factually, both convey that the system has no information about such a movie, so correctness is 1.", "main_total_tokens": 2814, "main_prompt_tokens": 2798, "main_completion_tokens": 16, "judge_total_tokens": 512, "judge_prompt_tokens": 404, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"The man who ate the sun\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 6042.1881675720215, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the release year cannot be provided, and the generated answer stays faithful to the given context by explicitly noting that the documents lack this information.", "main_total_tokens": 3718, "main_prompt_tokens": 3680, "main_completion_tokens": 38, "judge_total_tokens": 496, "judge_prompt_tokens": 429, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not specify who directed the movie \"The Last Days of Lehman Brothers\". [Doc 46069]", "latency_ms": 5888.124465942383, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not specify the director and does not invent an answer. This matches the gold response, which also indicates the information is not available in the database, so it is both correct and faithful to the source.", "main_total_tokens": 3487, "main_prompt_tokens": 3458, "main_completion_tokens": 29, "judge_total_tokens": 502, "judge_prompt_tokens": 422, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about who directed the movie \"A Love to Die For\". [Doc 24763] mentions a movie titled \"A Love That Kills\" from the year 2000, and [Doc 117179] describes a TV series titled \"A Love to Kill\" from 2005, but neither document specifies a director or matches the title \"A Love to Die For\". Therefore, based on the given context, there is no information available about who directed the movie \"A Love to Die For\".", "latency_ms": 11669.518232345581, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the director information for the movie 'A Love to Die For' is not available in the provided context/database. The generated answer remains faithful to the documents by explicitly noting that referenced titles differ and no director is specified, and it does not hallucinate any director name.", "main_total_tokens": 3376, "main_prompt_tokens": 3257, "main_completion_tokens": 119, "judge_total_tokens": 596, "judge_prompt_tokens": 503, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not specify who directed the movie \"Journey into Gaza\". [Doc 111360]", "latency_ms": 4902.670383453369, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not specify the director of the movie, which aligns with the gold answer that the director is not provided in the database. It does not introduce any unsupported information and remains faithful to the given evidence.", "main_total_tokens": 3407, "main_prompt_tokens": 3380, "main_completion_tokens": 27, "judge_total_tokens": 493, "judge_prompt_tokens": 414, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not specify who directed the TV show \"The Last Night at Tremore Beach\". [Doc 135098]", "latency_ms": 4143.739223480225, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer incorrectly refers to the work as a TV show, while the question and gold both specify it is a movie. More importantly, it claims the documents do not specify the director, which contradicts the gold answer that the database (and thus documents) simply lack the director information; the model should say it cannot answer, not assert something about the documents. Therefore it is neither correct nor faithful to the gold response.", "main_total_tokens": 2955, "main_prompt_tokens": 2924, "main_completion_tokens": 31, "judge_total_tokens": 539, "judge_prompt_tokens": 423, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "This scenario occurs in the movie \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 4288.58494758606, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Rise of the Mummy', which matches the gold answer exactly. It does not introduce contradictions or unsupported claims beyond a harmless bracketed document reference, so it is both correct and faithful.", "main_total_tokens": 3097, "main_prompt_tokens": 3073, "main_completion_tokens": 24, "judge_total_tokens": 487, "judge_prompt_tokens": 410, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The scenario where Sarah fakes her own death to escape an abusive spouse occurs in the movie \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 4524.863719940186, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Stranger in My Bed', which matches the gold answer exactly. It is faithful to the provided information, with no contradictions or unsupported additions beyond a generic document reference.", "main_total_tokens": 3084, "main_prompt_tokens": 3049, "main_completion_tokens": 35, "judge_total_tokens": 500, "judge_prompt_tokens": 427, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The scenario where a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years is described in the plot of \"Game of Silence\" [Doc 123938].", "latency_ms": 4816.612243652344, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the title as 'Game of Silence,' which matches the gold answer. It does not contradict the provided information and remains grounded in the given description, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2751, "main_prompt_tokens": 2708, "main_completion_tokens": 43, "judge_total_tokens": 509, "judge_prompt_tokens": 434, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie in which a true story about a killer from the site Craigslist is depicted is \"The Craigslist Killer\" [Doc 53039].", "latency_ms": 4275.529623031616, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer', which matches the gold answer exactly. It does not introduce contradictions or irrelevant information, and stays faithful to the query and the provided gold response.", "main_total_tokens": 2930, "main_prompt_tokens": 2898, "main_completion_tokens": 32, "judge_total_tokens": 492, "judge_prompt_tokens": 420, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The described plot happens in the movie \"Wood Job!\" [Doc 66094].", "latency_ms": 4886.852741241455, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", which matches the gold answer exactly. It does not introduce contradictions or unsupported claims, aside from a benign reference tag, so it is both correct and faithful to the given QA pair.", "main_total_tokens": 4366, "main_prompt_tokens": 4345, "main_completion_tokens": 21, "judge_total_tokens": 497, "judge_prompt_tokens": 417, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The event described occurs in the movie \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 4878.410816192627, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title \"Sesame Street: The Magical Wand Chase\" and does not introduce contradictions or unsupported details. It is correct and faithful.", "main_total_tokens": 3434, "main_prompt_tokens": 3406, "main_completion_tokens": 28, "judge_total_tokens": 497, "judge_prompt_tokens": 433, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The plot you described occurs in the movie \"Beyond\" from 2016 [Doc 74597].", "latency_ms": 5141.345262527466, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot, with the gold answer 'Beyond'. The generated answer states that the movie is 'Beyond' and even specifies the year 2016, which does not contradict the gold. Thus, it correctly identifies the film and remains faithful to the provided gold information, with no hallucinated contradictions or deviations.", "main_total_tokens": 2897, "main_prompt_tokens": 2870, "main_completion_tokens": 27, "judge_total_tokens": 528, "judge_prompt_tokens": 429, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The described scenario occurs in the movie \"6ixtynin9\" [Doc 22478].", "latency_ms": 4830.816745758057, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9,' which matches the gold answer exactly and does not introduce any unsupported information.", "main_total_tokens": 3015, "main_prompt_tokens": 2990, "main_completion_tokens": 25, "judge_total_tokens": 484, "judge_prompt_tokens": 423, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The described scenario happens in the movie \"Club Dread\" [Doc 32410].", "latency_ms": 5601.4368534088135, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot; the gold answer is \"Club Dread.\" The generated answer also identifies the movie as \"Club Dread.\" This matches exactly and does not introduce contradictions or extra unsupported claims. Both correctness and faithfulness are therefore fully satisfied.", "main_total_tokens": 3321, "main_prompt_tokens": 3300, "main_completion_tokens": 21, "judge_total_tokens": 528, "judge_prompt_tokens": 442, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The described scenario happens in the movie \"Toilet\" [Doc 48873].", "latency_ms": 6009.8817348480225, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer identifies the movie as \"Toilet,\" which matches the gold answer exactly, so it is correct. It also directly restates the information from the prompt without adding unsupported details, so it is faithful to the given data.", "main_total_tokens": 3666, "main_prompt_tokens": 3645, "main_completion_tokens": 21, "judge_total_tokens": 509, "judge_prompt_tokens": 431, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The described scenario happens in the movie \"Electric Angel\" [Doc 1024].", "latency_ms": 5358.431339263916, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as 'Electric Angel' with no added contradictions or hallucinated details. It is both correct and faithful to the given information.", "main_total_tokens": 3477, "main_prompt_tokens": 3457, "main_completion_tokens": 20, "judge_total_tokens": 501, "judge_prompt_tokens": 433, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 based on the provided documents is \"Dark\" with a rating of 4.5 [Doc 108151].", "latency_ms": 3476.7892360687256, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the lowest rated 2023 movie is 'Mega Ape', while the generated answer claims it is 'Dark' with a rating of 4.5, which directly contradicts the gold. Therefore the generated answer is factually incorrect with respect to the reference. In addition, it cites a document and rating that do not match the gold answer, so it is not faithful to the provided ground truth.", "main_total_tokens": 2911, "main_prompt_tokens": 2873, "main_completion_tokens": 38, "judge_total_tokens": 528, "judge_prompt_tokens": 412, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "Based on the provided documents, the highest rated TV show from 2010 is \"Justified\" with a rating of 8.0 [Doc 119156]. However, it's important to note that there are other shows from 2010 with higher ratings such as \"Sherlock\" (2010) with a rating of 8.511 [Doc 119150], but since the question specifically asks for the highest rated TV show from 2010 and \"Justified\" is the only one explicitly stated to be from 2010 in the provided context, \"Justified\" is the answer according to the given information.", "latency_ms": 9364.239692687988, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Tea House', but the generated answer states 'Justified' as the highest rated TV show from 2010, which does not match the gold. Therefore, the correctness is 0. The answer is, however, consistent with its own cited documents and reasoning (it clearly explains why it chose 'Justified' given the constraints it assumes), so faithfulness to its described sources and logic is 1.", "main_total_tokens": 3419, "main_prompt_tokens": 3270, "main_completion_tokens": 149, "judge_total_tokens": 627, "judge_prompt_tokens": 509, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest rated Japanese language movie from 1999 based on the provided documents is \"Kikujiro\" with a rating of 7.5 [Doc 22213].", "latency_ms": 7039.729356765747, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer identifies \"Kikujiro\" as the highest rated Japanese language movie from 1999, but the gold answer is \"The Uncut Story of Mewtwo's Origin,\" so it is factually incorrect with respect to the task. It also explicitly bases its claim on a document reference that conflicts with the gold label, so relative to the gold, the reasoning is not faithful to the required ground truth signal.", "main_total_tokens": 3379, "main_prompt_tokens": 3337, "main_completion_tokens": 42, "judge_total_tokens": 544, "judge_prompt_tokens": 427, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 based on the provided documents is \"Read Lips\" with a rating of 4.5, and its language is cn (Chinese) [Doc 354].", "latency_ms": 6851.543188095093, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says the language is English, but the generated answer claims it is Chinese (cn), so it is factually incorrect. It also introduces a specific movie title and rating that are not implied by the gold answer, so the reasoning is not faithful to the reference.", "main_total_tokens": 2925, "main_prompt_tokens": 2880, "main_completion_tokens": 45, "judge_total_tokens": 509, "judge_prompt_tokens": 423, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "Based on the provided documents, the TV show from 1999 with the highest popularity is \"Passions\" with a popularity score of 2.44 [Doc 115264].", "latency_ms": 8085.963726043701, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect: the gold label indicates that the TV show with the highest popularity in 1999 was 'Law & Order: Special Victims Unit,' but the model responded with 'Passions.' Additionally, the answer references 'the provided documents' and a specific document ID and score that are not present in the user’s query context, so the claim is not faithful to the given information.", "main_total_tokens": 4284, "main_prompt_tokens": 4239, "main_completion_tokens": 45, "judge_total_tokens": 539, "judge_prompt_tokens": 427, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 5199.066877365112, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 for 'Bluefin' and does not introduce any contradictions or unsupported claims, so it is both correct and faithful to the given information.", "main_total_tokens": 3147, "main_prompt_tokens": 3125, "main_completion_tokens": 22, "judge_total_tokens": 471, "judge_prompt_tokens": 400, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in multiple years according to the documents:\n\n- In 1999 [Doc 22481]\n- In 2019 [Doc 88811]\n- In 2025 [Doc 112077]", "latency_ms": 8264.687061309814, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the movie \"The Story of Us\" was released in 2019, but the generated answer claims it was released in multiple years: 1999, 2019, and 2025. This contradicts the gold answer, so correctness is 0. Additionally, the generated answer references multiple years \"according to the documents\" without aligning with the single correct year indicated by the gold standard, so it is not faithful to the required answer; faithfulness is 0.", "main_total_tokens": 3124, "main_prompt_tokens": 3056, "main_completion_tokens": 68, "judge_total_tokens": 567, "judge_prompt_tokens": 435, "judge_completion_tokens": 132}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The provided documents do not contain specific information about the number of votes \"My Dog Skip\" received. [Doc 24114]", "latency_ms": 5606.06837272644, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that 'My Dog Skip' received 279 votes. The generated answer claims the documents do not contain this information, which contradicts the gold answer and is therefore incorrect. However, the response is faithful to the (incorrect) retrieval it cites and does not fabricate a number, so it is fully faithful to the referenced document context.", "main_total_tokens": 3361, "main_prompt_tokens": 3331, "main_completion_tokens": 30, "judge_total_tokens": 503, "judge_prompt_tokens": 407, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The provided documents do not contain specific information about the number of votes for the movie \"Oh My God!\". [Doc 85434] only provides a rating of 3.8 and popularity of 0.8047, but does not specify the number of votes received.", "latency_ms": 7786.189794540405, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that \"Oh My God!\" received 12 votes. The generated answer claims that the documents do not contain information about the number of votes and therefore does not provide the correct value. This makes the generated answer factually incorrect relative to the gold (correctness 0). However, the answer is faithful to the described content of the cited document, stating it shows rating and popularity but not vote count, and does not invent a number, so it is faithful to the given context (faithfulness 1).", "main_total_tokens": 3219, "main_prompt_tokens": 3158, "main_completion_tokens": 61, "judge_total_tokens": 568, "judge_prompt_tokens": 436, "judge_completion_tokens": 132}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 4720.610857009888, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, stating the movie was released in 2009. It does not introduce contradictions or unsupported claims and remains faithful to the provided information.", "main_total_tokens": 3480, "main_prompt_tokens": 3451, "main_completion_tokens": 29, "judge_total_tokens": 476, "judge_prompt_tokens": 410, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-small_qwen2.5:32b", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 4375.023603439331, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'en' (English), and the generated answer states that the original language is English, which is correct and consistent with the gold label and the source document reference.", "main_total_tokens": 3465, "main_prompt_tokens": 3441, "main_completion_tokens": 24, "judge_total_tokens": 470, "judge_prompt_tokens": 402, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "Wall Street was released in the year 1987 [Doc 6993].", "latency_ms": 3351.6719341278076, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, which matches the gold answer. It is also faithful to the provided information, only adding a bracketed reference that does not contradict the content.", "main_total_tokens": 2481, "main_prompt_tokens": 2460, "main_completion_tokens": 21, "judge_total_tokens": 468, "judge_prompt_tokens": 394, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "None of the provided documents explicitly describe a movie where classmates have to play a survival game using their cell phones. However, \"47 Hours to Live\" involves two teenage girls using their phone to summon the supernatural as part of a game they find on the internet, which somewhat aligns with the query but does not fully match the description of a survival game played by classmates [Doc 90746].", "latency_ms": 7592.504024505615, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question is asking for the specific movie title, which is 'Brain Fluid Explosion Girl.' The generated answer instead discusses '47 Hours to Live' and concludes that none of the documents match the description, never providing the correct title. This makes the answer factually incorrect. However, the answer appears to be consistent with (and limited to) the information described as coming from the provided document, so it is faithful to the cited source, just incomplete with respect to the gold answer.", "main_total_tokens": 2269, "main_prompt_tokens": 2185, "main_completion_tokens": 84, "judge_total_tokens": 593, "judge_prompt_tokens": 466, "judge_completion_tokens": 127}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show where six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" from 1983 [Doc 112849].", "latency_ms": 5554.64506149292, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Dungeons & Dragons,\" matching the gold answer exactly. It does not introduce any contradictions or unsupported claims beyond a harmless reference to a document ID, so it is both correct and faithful.", "main_total_tokens": 2267, "main_prompt_tokens": 2227, "main_completion_tokens": 40, "judge_total_tokens": 501, "judge_prompt_tokens": 423, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie that is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is \"Nacht der Wölfe\" [Doc 2590].", "latency_ms": 6512.847423553467, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Nacht der Wölfe') and restates the question correctly without adding unsupported information. It is both correct and faithful to the provided data.", "main_total_tokens": 2493, "main_prompt_tokens": 2449, "main_completion_tokens": 44, "judge_total_tokens": 503, "judge_prompt_tokens": 439, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of Picnic at Hanging Rock take place in the year 1900 [Doc 126191].", "latency_ms": 4731.026411056519, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of Picnic at Hanging Rock take place in 1900, matching the gold answer, and it remains faithful to the provided information without adding unsupported details.", "main_total_tokens": 2302, "main_prompt_tokens": 2273, "main_completion_tokens": 29, "judge_total_tokens": 475, "judge_prompt_tokens": 406, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie that matches the description is \"June.\" The plot details a young violinist named June who works in a coffee shop, dreams of a professional music career, and daydreams about her destined love, all while dealing with quirky situations and characters. [Doc 32483]", "latency_ms": 8049.631357192993, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title of the 2004 movie; the gold answer is \"June.\" The generated answer correctly identifies the movie as \"June\" and its description is consistent with the question, without adding contradictions. Thus it is both correct and faithful to the gold answer.", "main_total_tokens": 2612, "main_prompt_tokens": 2546, "main_completion_tokens": 66, "judge_total_tokens": 579, "judge_prompt_tokens": 492, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1423.281192779541, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific movie title, while the generated answer says 'I don't know.' This is factually incorrect relative to the gold answer, so correctness is 0. However, the generated response does not hallucinate or contradict itself; it simply states lack of knowledge, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 493, "judge_prompt_tokens": 399, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 363.93070220947266, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric rating, and the gold answer is 7.6. The generated answer says 'I don't know,' which does not provide the correct rating, so correctness is 0. However, the answer does not invent or distort information; it simply admits lack of knowledge, so it is fully faithful to the model's stated knowledge and gets a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 501, "judge_prompt_tokens": 389, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1222.2929000854492, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.5143371794949736, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.013881288441812, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 4, but the generated answer is 18, which is incorrect. The evaluation correctly compares the two, so it is faithful to the given information but not correct relative to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 467, "judge_prompt_tokens": 394, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 425.83584785461426, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer ('0') does not match the gold answer ('2'), so it is factually incorrect. There is no evidence of misinterpretation of the question itself; it simply gives the wrong count, so faithfulness to the prompt is fine but correctness is not.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 395, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1753.464698791504, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct response 'Fantômas,' so it is incorrect. However, it does not fabricate any information and accurately reflects uncertainty, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 389, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie that is about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" from 1990 [Doc 11205].", "latency_ms": 5303.791522979736, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise' and matches the gold answer exactly. It is fully consistent with the provided information and does not introduce unsupported details, so both correctness and faithfulness are maximal.", "main_total_tokens": 2508, "main_prompt_tokens": 2470, "main_completion_tokens": 38, "judge_total_tokens": 489, "judge_prompt_tokens": 415, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 5864.6721839904785, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as \"Won't You Be My Neighbor?\", which matches the gold answer exactly, and it remains faithful to the information provided without adding unsupported details.", "main_total_tokens": 2538, "main_prompt_tokens": 2495, "main_completion_tokens": 43, "judge_total_tokens": 509, "judge_prompt_tokens": 440, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The question \"What year was it released?\" can be answered for multiple movies in the provided context. Here are the years of release for each movie mentioned:\n\n- YEAR: 2023 [Doc 107925]\n- The Disco Years: 1994 [Doc 15797]\n- 663114: 2012 [Doc 58592]\n- Mine: 2013 [Doc 62815]\n- My Favorite Year: 1982 [Doc 2118]\n- 1: 2008 [Doc 44750]\n- Paper Year: 2018 [Doc 85553]\n- Year of Grace: 2011 [Doc 53000]\n- A Year and Change: 2015 [Doc 69571]\n- The Year of the Deer: 1995 [Doc 17784]\n- Year of the Gun: 1991 [Doc 11881]\n- Another Year: 2014 [Doc 62952]\n- What It Was: 2014 [Doc 65131]\n- Let It Be: 2005 [Doc 35791]", "latency_ms": 28783.002614974976, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer specifies a single year: 2018. The generated answer lists many films and their release years, including 2018 for 'Paper Year', but it does not identify which film corresponds to the question and does not provide a single clear answer. Therefore it is not correctly answering the specific question. It is partially faithful in that it uses the context to derive years for multiple movies but fails to focus on the intended one.", "main_total_tokens": 2848, "main_prompt_tokens": 2549, "main_completion_tokens": 299, "judge_total_tokens": 721, "judge_prompt_tokens": 601, "judge_completion_tokens": 120}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is \"Astro Boy\" [Doc 112591].", "latency_ms": 10460.790634155273, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy,' which matches the gold answer exactly. It is faithful to the provided information and does not introduce any contradictions or unsupported details.", "main_total_tokens": 2601, "main_prompt_tokens": 2565, "main_completion_tokens": 36, "judge_total_tokens": 485, "judge_prompt_tokens": 416, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 4929.940700531006, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9463946303571863, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer. It is also faithful to the provided information without adding unsupported details.", "main_total_tokens": 2093, "main_prompt_tokens": 2057, "main_completion_tokens": 36, "judge_total_tokens": 493, "judge_prompt_tokens": 424, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26m before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 5719.339847564697, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the provided gold value of £26m and directly restates it without adding unsupported information. It is both correct and faithful to the source.", "main_total_tokens": 2407, "main_prompt_tokens": 2363, "main_completion_tokens": 44, "judge_total_tokens": 491, "judge_prompt_tokens": 428, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6649.819612503052, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a numeric popularity score (80.5772). The generated answer says, 'I don't know,' which does not provide the correct value, so correctness is 0. However, it does not fabricate any information and honestly acknowledges a lack of knowledge, so it is fully faithful to the model's uncertainty, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 496, "judge_prompt_tokens": 396, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 409.6255302429199, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer ('0') does not match the gold answer ('1'), so it is incorrect. It is, however, a direct and consistent response to the question, with no hallucinated reasoning, so it is fully faithful to itself.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 464, "judge_prompt_tokens": 385, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 5616.980791091919, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Angel Eyes' and remains faithful to the provided information.", "main_total_tokens": 2302, "main_prompt_tokens": 2262, "main_completion_tokens": 40, "judge_total_tokens": 487, "judge_prompt_tokens": 429, "judge_completion_tokens": 58}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French [Doc 110681].", "latency_ms": 5010.648012161255, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer says the movie language is French, but the gold label shows it is Korean, so it is factually incorrect. It also introduces a specific movie title and citation not supported by the gold answer, so it is not faithful to the reference.", "main_total_tokens": 2182, "main_prompt_tokens": 2160, "main_completion_tokens": 22, "judge_total_tokens": 475, "judge_prompt_tokens": 393, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 701.7934322357178, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is factually incorrect. However, there is no indication that the model contradicted or ignored any user-provided context; it simply produced an incorrect numeric result, so faithfulness to the (limited) prompt is preserved.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 486, "judge_prompt_tokens": 394, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The provided documents do not contain information about any movies released in 1979. Therefore, based solely on the given context, it is not possible to determine the best-rated movie from that year. [Doc None]", "latency_ms": 7564.01515007019, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the best-rated movie released in 1979, but the gold answer states that there are no movies from 1979 in the database. The generated answer correctly notes that the provided documents do not contain information about movies from 1979 and that it cannot determine the best-rated movie based on the given context. This matches the limitation indicated by the gold answer and does not introduce any unsupported claims. Therefore, it is both correct with respect to the context and faithful to the provided information.", "main_total_tokens": 2416, "main_prompt_tokens": 2370, "main_completion_tokens": 46, "judge_total_tokens": 570, "judge_prompt_tokens": 438, "judge_completion_tokens": 132}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The provided context does not contain any information about a movie titled \"Back to the Future Part IV\" or its release year. [Doc X]", "latency_ms": 4960.466623306274, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct answer is that 'Back to the Future Part IV' does not exist; the generated answer implies only that the context lacks information, not that the movie is nonexistent, so it is factually incorrect with respect to the question. It is partially faithful to the context in saying the context has no info, but it fails to convey the key gold fact that the movie does not exist.", "main_total_tokens": 2707, "main_prompt_tokens": 2677, "main_completion_tokens": 30, "judge_total_tokens": 540, "judge_prompt_tokens": 429, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.9992122650146484, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly indicates that there are no movies or shows matching the specified criteria, which aligns with the gold answer stating that no movies from 1980 about the Chernobyl disaster can be found. The event (Chernobyl disaster) occurred in 1986, so a 1980 movie about it is anachronistic. The response remains faithful to the user query by directly addressing the impossibility and does not introduce unsupported details.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 555, "judge_prompt_tokens": 434, "judge_completion_tokens": 121}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 274.8143672943115, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states there are 0 TV shows or movies with 'Astro Boy' in the title, while the gold answer is 4. This is factually incorrect, so correctness is 0. The answer is at least internally consistent and directly addresses the question without contradicting itself, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 388, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 266.2947177886963, "retrieval_recall": 0.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.17723928678404774, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric answer (how many movies in 2025 allude to the Holocaust). The gold answer is 2, while the generated answer is 1. Since the numeric value does not match, the generated answer is factually incorrect, so correctness is 0. The model directly answered the question with a number and did not introduce extraneous or conflicting information beyond that single value, so its response is fully based on the question and thus faithful in form, giving faithfulness a 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 516, "judge_prompt_tokens": 386, "judge_completion_tokens": 130}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 236.47117614746094, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show released in the 1990s about Super Mario Bros, while the generated answer is 0. This conflicts with the gold, so correctness is 0. The generated answer is at least internally consistent and not self-contradictory, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 486, "judge_prompt_tokens": 389, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 208.7874412536621, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (0), so it is correct. The question is factual and the response directly aligns with the reference, indicating full correctness and faithfulness.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 457, "judge_prompt_tokens": 391, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 256.7484378814697, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto', which is a specific, correct response. The generated answer says 'I don't know,' which does not provide the correct information and therefore is incorrect. However, it is faithful to the model's stated uncertainty and does not hallucinate, so faithfulness is high even though correctness is zero.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 482, "judge_prompt_tokens": 386, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The provided documents do not contain information about a Jurassic Park/Power Ranger crossover movie. Therefore, based on the given context, there is no release date for such a crossover movie.", "latency_ms": 5174.489259719849, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.5, "faithfulness_score": 1, "justification": "The gold answer clarifies that the movie itself does not exist, while the generated answer only says the documents do not contain information and therefore there is no release date. This implies lack of evidence rather than non-existence. It is faithful to the given context (no invented details), but not fully correct relative to the gold answer, which explicitly states the movie was not released.", "main_total_tokens": 2432, "main_prompt_tokens": 2395, "main_completion_tokens": 37, "judge_total_tokens": 529, "judge_prompt_tokens": 421, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Based on the provided context, there is no movie where Barbie fights the Predator. [Doc 82000] mentions \"The Predator,\" but it does not involve Barbie. None of the other documents list a movie featuring Barbie fighting a Predator either.", "latency_ms": 6521.566390991211, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer is effectively correct: there is no such movie, which matches the gold statement that the movie was not released. However, it adds a fabricated reference to document [Doc 82000] mentioning 'The Predator' without evidence from the prompt context, so while the conclusion is faithful to the gold answer, it is not fully faithful to the (unstated) source documents, slightly reducing faithfulness.", "main_total_tokens": 2472, "main_prompt_tokens": 2419, "main_completion_tokens": 53, "judge_total_tokens": 546, "judge_prompt_tokens": 431, "judge_completion_tokens": 115}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The provided context does not contain information about the release order of all Star Wars movies necessary to determine the 4th oldest Star Wars movie. The documents mostly consist of documentaries, TV series, and some feature films but do not provide a comprehensive list of all theatrical releases in chronological order [Doc 32101][Doc 2858][Doc 39908][Doc 117312][Doc 67472][Doc 94808][Doc 32997][Doc 31821][Doc 3][Doc 133041][Doc 128111][Doc 124704][Doc 121736].", "latency_ms": 14806.870698928833, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because it claims that the question cannot be answered from the context, while the gold answer shows that there is a known correct response. However, the model does not fabricate a specific movie title and instead accurately describes its own lack of information and reliance on the provided documents, so it is faithful to the (limited) context it references.", "main_total_tokens": 2722, "main_prompt_tokens": 2560, "main_completion_tokens": 162, "judge_total_tokens": 614, "judge_prompt_tokens": 510, "judge_completion_tokens": 104}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The question \"What year was it released?\" can be answered for multiple movies in the provided context. Here are the years of release for each movie mentioned:\n\n- YEAR: 2023 [Doc 107925]\n- The Disco Years: 1994 [Doc 15797]\n- 663114: 2012 [Doc 58592]\n- Mine: 2013 [Doc 62815]\n- My Favorite Year: 1982 [Doc 2118]\n- 1: 2008 [Doc 44750]\n- Paper Year: 2018 [Doc 85553]\n- Year of Grace: 2011 [Doc 53000]\n- A Year and Change: 2015 [Doc 69571]\n- The Year of the Deer: 1995 [Doc 17784]\n- Year of the Gun: 1991 [Doc 11881]\n- Another Year: 2014 [Doc 62952]\n- What It Was: 2014 [Doc 65131]\n- Let It Be: 2005 [Doc 35791]", "latency_ms": 25117.228507995605, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer specifies a single correct year: 1986. The generated answer instead lists many different movies with their release years and does not identify 1986 or clearly answer the specific question. It is therefore incorrect with respect to the gold and also unfaithful to the implied context, which expects one specific year, not a catalog of unrelated films.", "main_total_tokens": 2848, "main_prompt_tokens": 2549, "main_completion_tokens": 299, "judge_total_tokens": 703, "judge_prompt_tokens": 601, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 290.1453971862793, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific numerical rating (8.466). The generated answer says 'I don't know,' which does not match the correct value, so it is incorrect. However, it is faithful to the model's stated uncertainty and does not fabricate information, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 398, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 5010.904312133789, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the show as 'Mighty Med' without adding unsupported information.", "main_total_tokens": 2260, "main_prompt_tokens": 2223, "main_completion_tokens": 37, "judge_total_tokens": 478, "judge_prompt_tokens": 423, "judge_completion_tokens": 55}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "The seven prisoners escaped from a prison in Argentina. Specifically, they escaped from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 4562.596559524536, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies Argentina as the country, matching the gold answer. However, it adds specific details about Villa Devoto prison and a document reference [Doc 53582] that are not supported by the question context and may not be grounded in the provided information, slightly reducing faithfulness.", "main_total_tokens": 2691, "main_prompt_tokens": 2656, "main_completion_tokens": 35, "judge_total_tokens": 514, "judge_prompt_tokens": 422, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In Sweet Trap (2024), the apprentice Li Nai falls into a trap designed by Jiang Jie [Doc 135231].", "latency_ms": 3906.4626693725586, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The core answer is correct: the apprentice falls into Jiang Jie's trap, matching the gold (`Jiang Jie`). However, the generated answer adds an extra, likely hallucinated detail by naming the apprentice as Li Nai and referencing a specific document ID, neither of which are supported by the gold reference. Therefore, correctness is full, but faithfulness is slightly reduced due to unnecessary, unverified embellishment.", "main_total_tokens": 2376, "main_prompt_tokens": 2342, "main_completion_tokens": 34, "judge_total_tokens": 523, "judge_prompt_tokens": 409, "judge_completion_tokens": 114}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 632.6751708984375, "retrieval_recall": 0.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.19519002499605084, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.2, "ndcg_at_10": 0.3080351663449861, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (4), so it is factually incorrect for this question. However, the answer is a single numeric response without reasoning, and there is no evidence it contradicts its own internal logic or misrepresents any prior context; thus it is fully faithful to itself.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 385, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "46", "latency_ms": 185.16874313354492, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '46' is factually incorrect because there are no movies featuring both Bruce Lee and Steven Seagal together, matching the gold response. It is also unfaithful to the gold answer, which explicitly states that no such movies can be found.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 395, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 3968.4994220733643, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3 and attributes it to a document, indicating it is grounded in the provided source.", "main_total_tokens": 2523, "main_prompt_tokens": 2497, "main_completion_tokens": 26, "judge_total_tokens": 463, "judge_prompt_tokens": 403, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in \"Fast Track: No Limits\" is street racing [Doc 41929]. Similarly, the movies \"Street Racer\" also depict street racing scenarios [Doc 42313] [Doc 44013].", "latency_ms": 5913.582801818848, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.4, "justification": "The question asks for the style of racing in the movie, and the gold answer is 'Drift'. The generated answer states the style is 'street racing', which is incorrect relative to the gold. Therefore correctness is 0. In terms of faithfulness, the answer references specific documents, but it introduces 'street racing' instead of 'drift', suggesting either misinterpretation or incorrect retrieval. It partially grounds itself in cited docs but does not faithfully reflect the expected style, so faithfulness is low but not zero.", "main_total_tokens": 2571, "main_prompt_tokens": 2514, "main_completion_tokens": 57, "judge_total_tokens": 562, "judge_prompt_tokens": 424, "judge_completion_tokens": 138}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 5413.649082183838, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the title as 'Confronting a Serial Killer' and is consistent with the referenced document. There are no inaccuracies or hallucinations.", "main_total_tokens": 2506, "main_prompt_tokens": 2455, "main_completion_tokens": 51, "judge_total_tokens": 521, "judge_prompt_tokens": 452, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 5235.39924621582, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold value (7.838) and stays within the given information, even citing the same document reference. Both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2531, "main_prompt_tokens": 2501, "main_completion_tokens": 30, "judge_total_tokens": 474, "judge_prompt_tokens": 407, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 5291.23854637146, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6895405204413555, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Sixth Sense' and adds only a neutral citation-like note. It matches the gold answer exactly, with no contradictions or hallucinations, so it is both correct and faithful.", "main_total_tokens": 2156, "main_prompt_tokens": 2121, "main_completion_tokens": 35, "judge_total_tokens": 495, "judge_prompt_tokens": 420, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 4577.136278152466, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer and staying faithful to the provided information.", "main_total_tokens": 2462, "main_prompt_tokens": 2439, "main_completion_tokens": 23, "judge_total_tokens": 456, "judge_prompt_tokens": 397, "judge_completion_tokens": 59}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 6021.740198135376, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2015 reboot title as 'The Transporter Refueled' and does not introduce contradictions or unsupported content.", "main_total_tokens": 2478, "main_prompt_tokens": 2443, "main_completion_tokens": 35, "judge_total_tokens": 486, "judge_prompt_tokens": 420, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The movie 'The Transporter Refueled' was released in the year 2015 [Doc 67500].", "latency_ms": 4897.881507873535, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, which matches the gold answer. It is also supported by the cited document, so it is both correct and faithful.", "main_total_tokens": 2506, "main_prompt_tokens": 2476, "main_completion_tokens": 30, "judge_total_tokens": 483, "judge_prompt_tokens": 408, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "None of the provided documents mention Abraham Lincoln coming back from the dead in a romantic comedy. [Doc 47970] describes \"A Romantic Comedy\" but does not involve Abraham Lincoln or any scenario where he comes back from the dead. Therefore, based on the given context, there is no romantic comedy where Abraham Lincoln comes back from the dead.", "latency_ms": 6472.027540206909, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about identifying a romantic comedy where Abraham Lincoln comes back from the dead. The gold answer states that no such movie description can be found. The generated answer correctly concludes that none of the provided documents mention such a film and therefore there is no romantic comedy matching that description, which aligns with the gold. It accurately reflects the given context and does not invent a movie, so it is both correct and faithful to the source information.", "main_total_tokens": 2333, "main_prompt_tokens": 2260, "main_completion_tokens": 73, "judge_total_tokens": 577, "judge_prompt_tokens": 459, "judge_completion_tokens": 118}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by Ernest Hemingway is \"Havana Darkness\" [Doc 90923].", "latency_ms": 3883.891820907593, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 'Havana Darkness' as the movie title that alludes to a manuscript written by Ernest Hemingway, matching the gold answer exactly and staying faithful to the given information.", "main_total_tokens": 2348, "main_prompt_tokens": 2315, "main_completion_tokens": 33, "judge_total_tokens": 484, "judge_prompt_tokens": 415, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "The TV show in which Doll heads to Hollywood to be with her childhood best friend Em is titled \"Doll & Em\" [Doc 122259].", "latency_ms": 4025.495767593384, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em' and matches the gold answer exactly, with no added unsupported information. It is both correct and faithful to the provided information.", "main_total_tokens": 2397, "main_prompt_tokens": 2361, "main_completion_tokens": 36, "judge_total_tokens": 490, "judge_prompt_tokens": 420, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "In the 2007 movie \"Hallam Foe,\" Hallam has a talent for spying on people and attempts to find the cause of his mother’s death [Doc 39389].", "latency_ms": 4437.017440795898, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the 2007 movie as 'Hallam Foe', matching the gold answer, so correctness is 1. However, it cites a specific document ID [Doc 39389] that is not present in the provided context, making that reference ungrounded; thus faithfulness is 0.", "main_total_tokens": 2342, "main_prompt_tokens": 2298, "main_completion_tokens": 44, "judge_total_tokens": 533, "judge_prompt_tokens": 437, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The provided context does not contain any information about an individual's last name that might suggest they are a friend or foe based on its sound. Therefore, the question cannot be answered based on the given documents.", "latency_ms": 4282.744884490967, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'A foe'. The generated answer says the question cannot be answered from context, which contradicts the gold label, so it is factually incorrect for this QA task. However, it is faithful to the (unstated) premise that the model should only use the provided context; it does not hallucinate an answer and instead honestly states insufficiency of information, so faithfulness is high.", "main_total_tokens": 2223, "main_prompt_tokens": 2181, "main_completion_tokens": 42, "judge_total_tokens": 539, "judge_prompt_tokens": 426, "judge_completion_tokens": 113}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "In the 2017 TV show \"Atypical,\" Sam is 18 years old [Doc 124583].", "latency_ms": 3699.831485748291, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, matching the gold answer exactly and staying fully consistent with the provided information.", "main_total_tokens": 2207, "main_prompt_tokens": 2176, "main_completion_tokens": 31, "judge_total_tokens": 479, "judge_prompt_tokens": 411, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 3683.637857437134, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the 2017 TV show Atypical, Sam is on the autism spectrum, which exactly matches the gold answer. It is both factually correct and fully consistent with the given gold reference.", "main_total_tokens": 2427, "main_prompt_tokens": 2398, "main_completion_tokens": 29, "judge_total_tokens": 487, "judge_prompt_tokens": 410, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "The provided documents do not contain information about a movie titled \"À corde tendue\" released in 2013 or any details regarding the birthplace of a character named Sandra. Therefore, based on the given context, it is not possible to answer the question. [No Direct Source]", "latency_ms": 5549.349784851074, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that Sandra was born in Chamonix, but the generated answer claims the question cannot be answered from the documents and does not provide Chamonix. This is incorrect with respect to the gold answer. However, given the model explicitly adheres to the constraint of only using the provided documents and states that the information is not present, it remains faithful to its available context and does not hallucinate an answer.", "main_total_tokens": 2558, "main_prompt_tokens": 2497, "main_completion_tokens": 61, "judge_total_tokens": 569, "judge_prompt_tokens": 453, "judge_completion_tokens": 116}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In this Japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 3565.1659965515137, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly fills in the blank with 'rains', matching the gold answer exactly and remaining faithful to the given information.", "main_total_tokens": 2378, "main_prompt_tokens": 2352, "main_completion_tokens": 26, "judge_total_tokens": 469, "judge_prompt_tokens": 412, "judge_completion_tokens": 57}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 3609.522819519043, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the place where Tabaluga is crown-prince. The gold answer is 'Greenland'. The generated answer states 'Greenland' as well and adds only an inline document reference, which does not contradict the gold. Therefore it is correct and faithful to the gold answer.", "main_total_tokens": 2598, "main_prompt_tokens": 2571, "main_completion_tokens": 27, "judge_total_tokens": 506, "judge_prompt_tokens": 416, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is titled \"One Afternoon\" [Doc 96838].", "latency_ms": 5198.179483413696, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'One Afternoon' and directly restates the provided description without adding unsupported information. It is fully correct and faithful to the given data.", "main_total_tokens": 2430, "main_prompt_tokens": 2379, "main_completion_tokens": 51, "judge_total_tokens": 515, "judge_prompt_tokens": 449, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie where a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is titled \"The Life Ahead\" [Doc 93260].", "latency_ms": 3866.2991523742676, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Life Ahead,\" which matches the gold answer exactly. It stays faithful to the provided information and does not introduce contradictions or extraneous fabricated details beyond a citation-style reference.", "main_total_tokens": 2846, "main_prompt_tokens": 2804, "main_completion_tokens": 42, "judge_total_tokens": 509, "judge_prompt_tokens": 435, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary “McLaren” is about Bruce McLaren, a New Zealander who founded the McLaren Motor Racing team. [Doc 73859]", "latency_ms": 4332.914590835571, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary, matching the gold answer. The added detail about him being a New Zealander and founder of the McLaren Motor Racing team is accurate and consistent with known facts, so the response is both correct and faithful to the source indication.", "main_total_tokens": 2294, "main_prompt_tokens": 2254, "main_completion_tokens": 40, "judge_total_tokens": 514, "judge_prompt_tokens": 419, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The context provided does not mention any specific racing team name associated with a particular individual in the given documents. Therefore, based strictly on the information provided, there is no name of his racing team mentioned.", "latency_ms": 4443.669080734253, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The gold answer states the name of the racing team as 'The McLaren Motor Racing team.' The generated answer denies that any specific racing team name is mentioned, which directly contradicts the gold. Thus it is incorrect with respect to the gold answer. However, the generated answer appears to be faithful to its stated constraint of relying only on the (implied) context it had, asserting that no team name was in that context, so it is mostly faithful but not to the gold.", "main_total_tokens": 2581, "main_prompt_tokens": 2540, "main_completion_tokens": 41, "judge_total_tokens": 553, "judge_prompt_tokens": 423, "judge_completion_tokens": 130}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The provided context does not specify a particular individual for whom a country of origin is being asked. The question \"What country is he from?\" cannot be answered without identifying the specific person referred to as \"he.\" Therefore, based on the given information, it's not possible to determine which country the unspecified individual is from [Doc X].", "latency_ms": 5932.296991348267, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'New Zealand', so the question is answerable, but the generated answer incorrectly claims it cannot be answered. Thus, it is factually incorrect relative to the gold. However, given the model only had access to the provided context (which apparently did not mention the person or New Zealand), its reasoning about the lack of information is faithful to that context, so faithfulness is high.", "main_total_tokens": 2482, "main_prompt_tokens": 2414, "main_completion_tokens": 68, "judge_total_tokens": 554, "judge_prompt_tokens": 442, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based on the provided context, there is no movie from 2013 where John McClane blows up the Great Wall of China. However, John McClane does appear in a 2013 movie titled \"A Good Day to Die Hard,\" but the plot summary does not mention anything about blowing up the Great Wall [Doc 58791].", "latency_ms": 6387.877941131592, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says there is no such movie in the database, which is correct. The generated answer also states that there is no 2013 movie where John McClane blows up the Great Wall of China, and correctly notes that the 2013 McClane film is 'A Good Day to Die Hard' without that plot element. This matches the gold intent and does not introduce contradictions, so it is both correct and faithful to the gold answer.", "main_total_tokens": 2431, "main_prompt_tokens": 2355, "main_completion_tokens": 76, "judge_total_tokens": 591, "judge_prompt_tokens": 468, "judge_completion_tokens": 123}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie that matches the given description is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 3963.5298252105713, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Tomie: Forbidden Fruit', which matches the gold answer. It does not introduce contradictions or unsupported information beyond a brief citation-style note, so it is both correct and faithful.", "main_total_tokens": 2510, "main_prompt_tokens": 2477, "main_completion_tokens": 33, "judge_total_tokens": 519, "judge_prompt_tokens": 444, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie \"Lei mi parla ancora,\" Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 4361.206293106079, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that Nino and Caterina have been married for 65 years, which matches the gold answer of 65. It does not introduce contradictions or unsupported claims beyond referencing a document ID, so it is both correct and faithful.", "main_total_tokens": 2581, "main_prompt_tokens": 2542, "main_completion_tokens": 39, "judge_total_tokens": 504, "judge_prompt_tokens": 424, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for the legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 4800.440311431885, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, stating that they search for the Book of Souls. It adds a minor descriptor ('legendary relic') and a document reference, but does not change the meaning. Therefore it is both fully correct and faithful.", "main_total_tokens": 2841, "main_prompt_tokens": 2799, "main_completion_tokens": 42, "judge_total_tokens": 510, "judge_prompt_tokens": 429, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about the release year of \"Zombies on the Moon\". [Doc N/A]", "latency_ms": 3254.1587352752686, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly indicate that the release year is not available / not in the documents. The generated answer is faithful to the (absent) documents and matches the gold answer in substance.", "main_total_tokens": 2171, "main_prompt_tokens": 2146, "main_completion_tokens": 25, "judge_total_tokens": 487, "judge_prompt_tokens": 415, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided context does not contain any information about a release year for \"The Galactic baker\". [Doc X]", "latency_ms": 3361.158847808838, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer correctly states that the movie is not in the database, while the generated answer incorrectly references a provided context and a document, which are not mentioned in the question. It invents the existence of context and Doc X, making it unfaithful to the task setup and not the appropriate response (it should instead acknowledge lack of data).", "main_total_tokens": 2613, "main_prompt_tokens": 2590, "main_completion_tokens": 23, "judge_total_tokens": 511, "judge_prompt_tokens": 411, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain any information about a movie titled \"Return of the Null Pointer\". Therefore, the release year cannot be determined from the given context.", "latency_ms": 3839.768409729004, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database. The generated answer says the documents do not contain information about that movie and thus the release year cannot be determined from the context, which is effectively equivalent. It does not invent a year or contradict the gold and correctly reflects the lack of information, so it is both correct and faithful.", "main_total_tokens": 2261, "main_prompt_tokens": 2228, "main_completion_tokens": 33, "judge_total_tokens": 522, "judge_prompt_tokens": 423, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about the release year of \"Data Science: The Movie\". [Doc 76666] to [Doc 110664] list various documentaries and movies related to data, science, and other topics but do not mention \"Data Science: The Movie\" specifically. Therefore, based on the given context, the release year of \"Data Science: The Movie\" is not found.", "latency_ms": 7406.6431522369385, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both gold and generated answers correctly state that the information about the movie is not available. The generated answer remains grounded in the given context (documents do not mention the movie) and does not hallucinate a release year, so it is both correct and faithful.", "main_total_tokens": 2436, "main_prompt_tokens": 2346, "main_completion_tokens": 90, "judge_total_tokens": 555, "judge_prompt_tokens": 473, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain any information about the release year of \"Python's Revenge 2\". [Doc X]", "latency_ms": 3513.49139213562, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The question is unanswerable from the given data, and the generated answer correctly states that the documents do not contain the information. This matches the gold intent (unable to answer), so it's correct. Very slight faithfulness deduction only because it explicitly references 'provided documents' and a placeholder [Doc X] without being shown those documents here, but it still does not hallucinate a release year.", "main_total_tokens": 2603, "main_prompt_tokens": 2578, "main_completion_tokens": 25, "judge_total_tokens": 527, "judge_prompt_tokens": 415, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided context does not contain any information about a release year for \"The Last Pixel\". [Doc X]", "latency_ms": 3075.5786895751953, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer incorrectly implies there is some provided context and that it has checked it, which is not in the gold answer. It also fails to match the gold response that the movie is not in the database. Thus it is not faithful to the gold and not correct relative to the expected output.", "main_total_tokens": 2028, "main_prompt_tokens": 2005, "main_completion_tokens": 23, "judge_total_tokens": 501, "judge_prompt_tokens": 411, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about the release year of \"The man who ate the sun\". [Doc Not Found]", "latency_ms": 3776.6048908233643, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the release year (or the movie itself) is not available in the provided information. The generated answer is faithful to the constraint of using only the provided documents and explicitly notes their lack of information. No hallucinated year or extra data is added.", "main_total_tokens": 2783, "main_prompt_tokens": 2757, "main_completion_tokens": 26, "judge_total_tokens": 506, "judge_prompt_tokens": 417, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not specify who directed the movie \"The Last Days of Lehman Brothers\". [Doc 46069]", "latency_ms": 3961.1401557922363, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks who directed the movie. The gold answer states that the director information is not in the database and thus cannot be answered. The generated answer says the provided documents do not specify the director. This is consistent with the gold rationale and does not hallucinate an answer, so it is both correct and faithful to the given information.", "main_total_tokens": 2809, "main_prompt_tokens": 2780, "main_completion_tokens": 29, "judge_total_tokens": 520, "judge_prompt_tokens": 422, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about the director of the movie \"A Love to Die For\". [Doc X]", "latency_ms": 3577.336311340332, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the system cannot answer because the database lacks the director information. The generated answer instead asserts that the provided documents do not contain the director information, which is a stronger claim about the contents of the documents and not just the database limitation. This introduces an ungrounded assumption about the documents, so it is not faithful to the gold rationale. Therefore, both correctness and faithfulness scores are 0.", "main_total_tokens": 2609, "main_prompt_tokens": 2584, "main_completion_tokens": 25, "judge_total_tokens": 533, "judge_prompt_tokens": 419, "judge_completion_tokens": 114}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not specify who directed the movie \"Journey into Gaza\". [Doc 111360]", "latency_ms": 3863.0917072296143, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not specified and aligns with the gold answer that the database does not provide the director information. It does not introduce any unsupported claims.", "main_total_tokens": 2650, "main_prompt_tokens": 2623, "main_completion_tokens": 27, "judge_total_tokens": 480, "judge_prompt_tokens": 414, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about the director of \"The Last Night at Tremore Beach\". [Doc 135098]", "latency_ms": 3710.8852863311768, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided documents/database. The generated answer is factually consistent with the gold response and does not introduce unsupported information, so it is fully correct and faithful.", "main_total_tokens": 2290, "main_prompt_tokens": 2259, "main_completion_tokens": 31, "judge_total_tokens": 500, "judge_prompt_tokens": 423, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie where a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 3963.258743286133, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy' and does not introduce any contradictions or hallucinations beyond a benign document reference.", "main_total_tokens": 2398, "main_prompt_tokens": 2365, "main_completion_tokens": 33, "judge_total_tokens": 484, "judge_prompt_tokens": 417, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The scenario where Sarah fakes her own death to escape her abusive spouse occurs in the movie \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 4001.239538192749, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Stranger in My Bed,\" which matches the gold answer. It is also faithful to the (implied) source document reference [Doc 35237] and does not introduce unsupported claims.", "main_total_tokens": 2359, "main_prompt_tokens": 2324, "main_completion_tokens": 35, "judge_total_tokens": 505, "judge_prompt_tokens": 427, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie in which a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years is \"Game of Silence\" [Doc 123938].", "latency_ms": 3970.430374145508, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie (actually a TV series) where a successful Atlanta attorney’s long-lost childhood friends reappear after 25 years. The gold answer is \"Game of Silence,\" and the generated answer exactly matches this, with no contradictions or additions beyond restating the question. Therefore, it is both correct and faithful to the gold.", "main_total_tokens": 2087, "main_prompt_tokens": 2048, "main_completion_tokens": 39, "judge_total_tokens": 531, "judge_prompt_tokens": 430, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie where the plot is based on a true story about a killer from the site Craigslist is titled \"The Craigslist Killer\" [Doc 53039].", "latency_ms": 3939.021348953247, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, 'The Craigslist Killer,' and correctly identifies the movie. It does not introduce contradictions or hallucinations; the added citation-style bracket does not change the content. Thus it is both correct and faithful.", "main_total_tokens": 2300, "main_prompt_tokens": 2265, "main_completion_tokens": 35, "judge_total_tokens": 502, "judge_prompt_tokens": 423, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which Yuki Hirano sets his eyes on the role of a forestry trainee after failing his university entrance exam is \"Wood Job!\" [Doc 66094].", "latency_ms": 4850.379467010498, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", which matches the gold answer exactly. It does not introduce any extraneous or contradictory information and remains faithful to the question and gold reference.", "main_total_tokens": 3136, "main_prompt_tokens": 3096, "main_completion_tokens": 40, "judge_total_tokens": 506, "judge_prompt_tokens": 435, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure is titled \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 5267.634868621826, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, correctly identifying the movie as \"Sesame Street: The Magical Wand Chase.\" It restates the question and provides the correct title without adding unsupported information, so it is both correct and faithful to the source.", "main_total_tokens": 2609, "main_prompt_tokens": 2554, "main_completion_tokens": 55, "judge_total_tokens": 541, "judge_prompt_tokens": 460, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie where a 7-year-old boy must find the hero within him to save his parents who are trapped inside a burning car due to a rock fall is titled \"Beyond\" [Doc 74597].", "latency_ms": 4491.421461105347, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Beyond,\" which matches the gold answer. It stays faithful to the provided information without adding contradictions or unsupported details.", "main_total_tokens": 2193, "main_prompt_tokens": 2147, "main_completion_tokens": 46, "judge_total_tokens": 512, "judge_prompt_tokens": 450, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie where a woman, fired from a financial corporation during the Asia crisis, returns home with no money is titled \"6ixtynin9\" [Doc 22478].", "latency_ms": 4345.881938934326, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as \"6ixtynin9\" and does not introduce any extraneous or conflicting information.", "main_total_tokens": 2323, "main_prompt_tokens": 2282, "main_completion_tokens": 41, "judge_total_tokens": 503, "judge_prompt_tokens": 439, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie where the following happens is \"Club Dread\" (2004). The plot description matches exactly with the provided context: \"When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!\" [Doc 32410].", "latency_ms": 7068.302631378174, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread' and aligns fully with the provided gold answer. The explanation about the plot matching the description is accurate and clearly grounded in the given context, with no invented details beyond citing the document reference.", "main_total_tokens": 2681, "main_prompt_tokens": 2597, "main_completion_tokens": 84, "judge_total_tokens": 583, "judge_prompt_tokens": 502, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is titled \"Toilet\" [Doc 48873].", "latency_ms": 5658.125162124634, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" matching the gold answer exactly. It stays fully consistent with the given information and does not introduce contradictions or extraneous claims.", "main_total_tokens": 2858, "main_prompt_tokens": 2801, "main_completion_tokens": 57, "judge_total_tokens": 533, "judge_prompt_tokens": 466, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie described is \"Electric Angel\" [Doc 1024].", "latency_ms": 3218.7089920043945, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the given description. The gold answer is 'Electric Angel', and the generated answer is also 'Electric Angel'. This is correct and fully consistent with the provided gold, so both correctness and faithfulness are 1.", "main_total_tokens": 2591, "main_prompt_tokens": 2574, "main_completion_tokens": 17, "judge_total_tokens": 510, "judge_prompt_tokens": 430, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 based on the provided context is \"Bottom of the Water\" with a rating of 4.2 [Doc 106899].", "latency_ms": 3111.7169857025146, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'Mega Ape' as the lowest rated movie from 2023, while the generated answer gives a different title, 'Bottom of the Water,' with a rating of 4.2. This directly contradicts the gold and is therefore incorrect. It is also unfaithful to the reference information, since it relies on a different document/context than the one implied by the gold answer.", "main_total_tokens": 2248, "main_prompt_tokens": 2207, "main_completion_tokens": 41, "judge_total_tokens": 522, "judge_prompt_tokens": 415, "judge_completion_tokens": 107}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 1345.8807468414307, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the highest rated TV show from 2010 is 'Tea House'. The generated answer says 'I don't know,' which does not match the correct answer, so it is factually incorrect. However, it does not invent any incorrect facts and honestly indicates a lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 488, "judge_prompt_tokens": 386, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 685.6687068939209, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the highest rated Japanese language movie from 1999, with the gold answer being 'The Uncut Story of Mewtwo's Origin'. The generated answer says 'I don't know,' which is incorrect relative to the gold. However, it does not invent any information and accurately reflects uncertainty, so it is fully faithful to the model's own knowledge state.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 501, "judge_prompt_tokens": 395, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is \"Read Lips\" with a rating of 4.5 [Doc 354]. The language of this movie is cn (Chinese).", "latency_ms": 4361.51909828186, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer claims the language is Chinese (cn), but the gold label says English, so the factual answer is incorrect. It also directly contradicts the given gold answer, so it is not faithful to the reference.", "main_total_tokens": 2278, "main_prompt_tokens": 2237, "main_completion_tokens": 41, "judge_total_tokens": 494, "judge_prompt_tokens": 419, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "Based on the provided context, the TV show in 1999 with the highest popularity is \"Popular\" with a popularity score of 4.5113 [Doc 115196].", "latency_ms": 5024.850606918335, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer claims that the most popular 1999 TV show was \"Popular\" with a specific score, but the gold answer is \"Law & Order: Special Victims Unit\". This is factually incorrect relative to the gold, so correctness is 0. However, the model explicitly attributes its answer to the provided context ([Doc 115196]) and does not invent conflicting sources, so it is faithful to the given context; faithfulness is 1.", "main_total_tokens": 2902, "main_prompt_tokens": 2856, "main_completion_tokens": 46, "judge_total_tokens": 552, "judge_prompt_tokens": 427, "judge_completion_tokens": 125}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 3231.9915294647217, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and attributes it correctly. It is both correct and faithful to the provided information.", "main_total_tokens": 2354, "main_prompt_tokens": 2332, "main_completion_tokens": 22, "judge_total_tokens": 460, "judge_prompt_tokens": 400, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in multiple years according to the provided documents: 1999 [Doc 22481], 2019 [Doc 88811], and a future release year of 2025 [Doc 112077].", "latency_ms": 5904.067516326904, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that \"The Story of Us\" was released in 2019. The generated answer lists multiple years (1999, 2019, and 2025) and does not commit to 2019 as the release year, so it is incorrect with respect to the gold. It is also unfaithful because it introduces a 2025 future release year without support from the gold answer and fails to select the single correct year requested.", "main_total_tokens": 2403, "main_prompt_tokens": 2334, "main_completion_tokens": 69, "judge_total_tokens": 559, "judge_prompt_tokens": 436, "judge_completion_tokens": 123}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 269.1836357116699, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 279 votes, while the generated answer is 3 votes. This is numerically incorrect. The model did faithfully provide a specific answer, but it does not match the gold fact, so correctness is 0. Faithfulness to its own output is fine, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 477, "judge_prompt_tokens": 383, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "12", "latency_ms": 153.3641815185547, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (12), so it is both correct and faithful to the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 436, "judge_prompt_tokens": 383, "judge_completion_tokens": 53}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 3684.2033863067627, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie was released in 2009, which matches the gold answer. It is also faithful to the provided information, simply restating the year without adding unsupported details.", "main_total_tokens": 2452, "main_prompt_tokens": 2423, "main_completion_tokens": 29, "judge_total_tokens": 481, "judge_prompt_tokens": 410, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 3605.41033744812, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'en' (English). The generated answer states that the original language of the movie 'The King's Speech' is English, which is correct and matches the gold. It is also faithful to the given information without adding contradictions.", "main_total_tokens": 2674, "main_prompt_tokens": 2650, "main_completion_tokens": 24, "judge_total_tokens": 482, "judge_prompt_tokens": 402, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "1987 [Doc 6993]", "latency_ms": 3462.3494148254395, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer gives the correct release year, 1987, which matches the gold answer exactly. It is also faithful, as it does not introduce any contradictory or extraneous information beyond citing a document reference.", "main_total_tokens": 3063, "main_prompt_tokens": 3048, "main_completion_tokens": 15, "judge_total_tokens": 460, "judge_prompt_tokens": 387, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Odds Are [Doc 84255]", "latency_ms": 3270.3099250793457, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer, 'Odds Are', is incorrect because the movie described (classmates forced to play a survival game using their cell phones) is 'Brain Fluid Explosion Girl'. The model consistently refers to the (incorrect) generated answer and does not contradict itself or the given information, so faithfulness to its own reasoning is high, but correctness relative to the gold answer is zero.", "main_total_tokens": 2774, "main_prompt_tokens": 2759, "main_completion_tokens": 15, "judge_total_tokens": 503, "judge_prompt_tokens": 396, "judge_completion_tokens": 107}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 3893.087863922119, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the TV show as 'Dungeons & Dragons' in which six friends rely on a Dungeon Master to get home.", "main_total_tokens": 2906, "main_prompt_tokens": 2884, "main_completion_tokens": 22, "judge_total_tokens": 459, "judge_prompt_tokens": 394, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 4233.030796051025, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which correctly identifies 'Nacht der Wölfe' as the movie about two rival gangs in Munich composed of German and Turkish teens.", "main_total_tokens": 3085, "main_prompt_tokens": 3061, "main_completion_tokens": 24, "judge_total_tokens": 470, "judge_prompt_tokens": 403, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900 [Doc 126191]", "latency_ms": 3396.6636657714844, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and is fully supported by the provided information.", "main_total_tokens": 2877, "main_prompt_tokens": 2860, "main_completion_tokens": 17, "judge_total_tokens": 442, "judge_prompt_tokens": 394, "judge_completion_tokens": 48}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June", "latency_ms": 532.4687957763672, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'June' matches the gold answer exactly and correctly identifies the movie. It is both correct and faithful to the expected output.", "main_total_tokens": 146, "main_prompt_tokens": 144, "main_completion_tokens": 2, "judge_total_tokens": 492, "judge_prompt_tokens": 433, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 247.26629257202148, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific movie title, while the generated answer claims not to know. This contradicts the gold and is therefore incorrect. However, the generated response does not fabricate any information and accurately reflects uncertainty, so it is fully faithful to the model's own output.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 399, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 225.61216354370117, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric value (the highest Russian-language movie rating in 1999, which is 7.6 according to the gold answer). The generated answer says 'I don't know,' which does not provide the correct value, so correctness is 0. However, the model is honest about its lack of knowledge and does not hallucinate an answer, so it is fully faithful to its own stated uncertainty, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 514, "judge_prompt_tokens": 389, "judge_completion_tokens": 125}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 9583.458185195923, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.5143371794949736, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.013881288441812, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is incorrect. However, it directly answers the user's question without adding extra, hallucinated information, so it is faithful to the question asked.", "main_total_tokens": 1693, "main_prompt_tokens": 1561, "main_completion_tokens": 132, "judge_total_tokens": 470, "judge_prompt_tokens": 394, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 298.99024963378906, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the number of TV shows matching specific criteria, and the gold answer is 2. The generated answer says there are none, which is incorrect, so correctness is 0. However, the model does not hallucinate specific incorrect shows or numbers; it simply states an inability to find matches, which is consistent with its claimed limitations, so it is faithful to its own stated knowledge limits, giving a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 525, "judge_prompt_tokens": 404, "judge_completion_tokens": 121}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 271.6054916381836, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which is incorrect because the correct answer is 'Fantômas'. However, the model does not hallucinate or provide conflicting information; it honestly admits not knowing, so it is fully faithful to its own (lack of) knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 389, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise", "latency_ms": 2944.8935985565186, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly. The movie described by the question is indeed 'Kill Cruise', so the response is both correct and fully faithful to the reference.", "main_total_tokens": 3070, "main_prompt_tokens": 3065, "main_completion_tokens": 5, "judge_total_tokens": 452, "judge_prompt_tokens": 387, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor? [Doc 84838]", "latency_ms": 3790.2920246124268, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, including punctuation. The added document reference tag does not change the correctness or faithfulness of the movie title itself.", "main_total_tokens": 3158, "main_prompt_tokens": 3139, "main_completion_tokens": 19, "judge_total_tokens": 476, "judge_prompt_tokens": 414, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018 [Doc 84838]", "latency_ms": 4673.873424530029, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (2018) and introduces no unsupported information; the bracketed doc reference does not contradict anything.", "main_total_tokens": 3039, "main_prompt_tokens": 3009, "main_completion_tokens": 30, "judge_total_tokens": 445, "judge_prompt_tokens": 386, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy [Doc 112591]", "latency_ms": 3746.002674102783, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as Astro Boy, matching the gold answer exactly. It is fully faithful to the provided information without adding unsupported details.", "main_total_tokens": 3198, "main_prompt_tokens": 3182, "main_completion_tokens": 16, "judge_total_tokens": 457, "judge_prompt_tokens": 395, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 years [Doc 67513]", "latency_ms": 3268.005132675171, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9463946303571863, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, matching the gold answer without adding unsupported details.", "main_total_tokens": 2698, "main_prompt_tokens": 2684, "main_completion_tokens": 14, "judge_total_tokens": 462, "judge_prompt_tokens": 403, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m [Doc 133469]", "latency_ms": 3790.1766300201416, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer (£26m) matches the provided gold answer (£26m) exactly, and it is consistent with the cited document reference. Therefore it is both correct and faithful.", "main_total_tokens": 2391, "main_prompt_tokens": 2376, "main_completion_tokens": 15, "judge_total_tokens": 471, "judge_prompt_tokens": 404, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 528.4368991851807, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score. The gold answer provides this numeric value (80.5772). The generated answer says, 'I don't know,' which does not provide the requested value and is therefore incorrect. However, it does not invent or distort any facts; it simply expresses a lack of knowledge, so it is fully faithful to the model’s knowledge state.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 503, "judge_prompt_tokens": 396, "judge_completion_tokens": 107}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 401.40604972839355, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.75, "justification": "The gold answer is 1, but the generated answer says there are none, so it is factually incorrect (score 0). The model does not hallucinate a specific wrong number; it instead admits inability to find any, which is partially faithful to its (apparent) lack of information, but still contradicts the gold by implying zero, so faithfulness is moderate rather than full.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 506, "judge_prompt_tokens": 394, "judge_completion_tokens": 112}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes [Doc 121928]", "latency_ms": 3405.730962753296, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, identifying 'Angel Eyes' as the TV show where Soo-wan regains her sight through an eye transplant. There is no contradiction or fabrication beyond the provided information.", "main_total_tokens": 2921, "main_prompt_tokens": 2906, "main_completion_tokens": 15, "judge_total_tokens": 476, "judge_prompt_tokens": 403, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "en", "latency_ms": 4186.418771743774, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer says the language is English (\"en\"), but the gold label says Korean. This is factually incorrect and not faithful to the provided gold answer.", "main_total_tokens": 3178, "main_prompt_tokens": 3156, "main_completion_tokens": 22, "judge_total_tokens": 442, "judge_prompt_tokens": 377, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 590.1694297790527, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer claims there are no such movies, which is factually incorrect, so correctness is 0. The answer is internally consistent with the model’s stated inability to find matches and does not fabricate specific data, so it is faithful to its own reasoning, giving a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 506, "judge_prompt_tokens": 403, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 805.5160045623779, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.8, "faithfulness_score": 1, "justification": "The gold answer specifies that there are no movies from 1979 in the database. The generated answer says simply \"I don't know,\" which is effectively equivalent in this context (the system cannot provide the requested information). This makes it generally correct with respect to not inventing an answer. However, it is slightly less precise than the gold, hence not perfectly aligned with the specified behavior. The answer is fully faithful to the given information and does not add hallucinated content.", "main_total_tokens": 612, "main_prompt_tokens": 610, "main_completion_tokens": 2, "judge_total_tokens": 521, "judge_prompt_tokens": 399, "judge_completion_tokens": 122}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "I don't know.", "latency_ms": 4529.847621917725, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer should state that 'Back to the Future Part IV' does not exist, whereas the generated answer simply says 'I don\\'t know,' which is not factually correct given the available information. It also does not faithfully follow the gold answer's content or specificity.", "main_total_tokens": 699, "main_prompt_tokens": 633, "main_completion_tokens": 66, "judge_total_tokens": 492, "judge_prompt_tokens": 404, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot find any movies or shows matching the criteria, which aligns with the gold answer indicating no such movies from 1980 related to Chernobyl exist. The answer is faithful to the query and does not introduce unsupported details, only adding a reasonable suggestion to check the event timing.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 528, "judge_prompt_tokens": 434, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 285.30168533325195, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 titles, but the generated response incorrectly claims there are none, so it is factually wrong. However, the model does not fabricate specific numbers or titles and instead admits inability to find matches, so it remains faithful to its own (albeit limited) retrieval rather than hallucinating details.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 493, "judge_prompt_tokens": 397, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 3249.1869926452637, "retrieval_recall": 0.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.17723928678404774, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 2, but the generated answer claims there are none. This is factually incorrect relative to the provided gold answer, so correctness is 0. The model does, however, accurately reflect its own stated limitation (inability to find matches) and does not hallucinate specific movies or data, so within its own reasoning it is faithful, giving faithfulness a 1.", "main_total_tokens": 3723, "main_prompt_tokens": 3721, "main_completion_tokens": 2, "judge_total_tokens": 500, "judge_prompt_tokens": 395, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 226.55916213989258, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show, but the generated answer claims there were none. This is factually incorrect relative to the gold. However, the model does not hallucinate any incorrect specific titles or numbers; it simply admits it cannot find any, so it remains faithful to its own (albeit limited) information. Therefore, correctness is 0, faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 509, "judge_prompt_tokens": 398, "judge_completion_tokens": 111}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 215.99435806274414, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0 shows, and the generated answer states that it cannot find any movies or shows matching those criteria, which is consistent with there being 0 such shows. It does not introduce conflicting information or hallucinate specific titles, so it is both correct relative to the gold and faithful to the question.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 494, "judge_prompt_tokens": 400, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 185.00399589538574, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer should be 'Naruto'. The generated answer says 'I don't know,' which is incorrect for the question but is internally consistent and not hallucinated, so it is fully faithful to the model’s stated uncertainty but not correct.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 386, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 3302.89888381958, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states definitively that the movie does not exist, so the correct response should convey that there was no such release. The generated answer says only 'I don't know,' which is not factually correct relative to the gold, so correctness is 0. However, it is faithful in that it does not invent a release date or false information; it honestly indicates uncertainty, so faithfulness is 1.", "main_total_tokens": 600, "main_prompt_tokens": 557, "main_completion_tokens": 43, "judge_total_tokens": 503, "judge_prompt_tokens": 389, "judge_completion_tokens": 114}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 4109.065055847168, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that no such movie exists or was not released. The generated answer, 'I don't know,' does not provide this information, so it is incorrect. However, it does not contradict the truth or hallucinate a wrong movie, so it is faithful to the model's uncertainty.", "main_total_tokens": 652, "main_prompt_tokens": 594, "main_completion_tokens": 58, "judge_total_tokens": 476, "judge_prompt_tokens": 386, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "Return of the Jedi [Doc 2858]", "latency_ms": 8544.080972671509, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect: the 4th oldest Star Wars movie per the provided gold label is 'Star Wars: Droids - The Great Heep', not 'Return of the Jedi'. It is also unfaithful to the given gold answer, so both correctness and faithfulness are 0.", "main_total_tokens": 3263, "main_prompt_tokens": 3161, "main_completion_tokens": 102, "judge_total_tokens": 491, "judge_prompt_tokens": 399, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "1983", "latency_ms": 3907.5820446014404, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '1983' does not match the gold answer '1986', so it is factually incorrect. There is no indication it is faithful to any provided context; it directly contradicts the reference answer.", "main_total_tokens": 3230, "main_prompt_tokens": 3213, "main_completion_tokens": 17, "judge_total_tokens": 455, "judge_prompt_tokens": 380, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 254.04071807861328, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific numerical rating (8.466), while the generated answer states lack of knowledge. This is factually incorrect relative to the gold, so correctness is 0. However, the model honestly reflects its uncertainty and does not hallucinate a rating, so faithfulness to its own stated knowledge is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 495, "judge_prompt_tokens": 398, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med [Doc 121125]", "latency_ms": 3441.863775253296, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Mighty Med' exactly matches the gold answer and correctly identifies the TV show. It is fully correct and faithful to the reference.", "main_total_tokens": 2837, "main_prompt_tokens": 2820, "main_completion_tokens": 17, "judge_total_tokens": 461, "judge_prompt_tokens": 400, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina [Doc 53582]", "latency_ms": 3424.154758453369, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Argentina as the country, matching the gold answer exactly and staying fully consistent with the provided information.", "main_total_tokens": 3336, "main_prompt_tokens": 3323, "main_completion_tokens": 13, "judge_total_tokens": 453, "judge_prompt_tokens": 398, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "I don't know.", "latency_ms": 3701.9124031066895, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies 'Jiang Jie', but the generated answer says 'I don't know', which is incorrect. However, the model is honest about its lack of knowledge and does not fabricate an answer, so it is fully faithful to its own reasoning.", "main_total_tokens": 625, "main_prompt_tokens": 575, "main_completion_tokens": 50, "judge_total_tokens": 470, "judge_prompt_tokens": 388, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "2", "latency_ms": 3434.6132278442383, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.19519002499605084, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.2, "ndcg_at_10": 0.3080351663449861, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric answer (how many Korean-language movies mention assassins after 2023). The gold answer is 4, but the generated answer is 2, which is numerically incorrect. There is no evidence of hallucinated reasoning beyond the wrong number; it is simply an incorrect value relative to the gold. Thus correctness is 0. However, with only the final number given and no conflicting explanation, we cannot say it is unfaithful to its own (unstated) reasoning, so faithfulness is scored as 1.", "main_total_tokens": 3844, "main_prompt_tokens": 3839, "main_completion_tokens": 5, "judge_total_tokens": 528, "judge_prompt_tokens": 385, "judge_completion_tokens": 143}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 2123.8315105438232, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly reflects the gold answer: both state that no movies can be found with Bruce Lee and Steven Seagal together. The wording is slightly different but semantically equivalent, and there is no hallucinated information. Thus it is both correct and faithful to the gold.", "main_total_tokens": 2406, "main_prompt_tokens": 2404, "main_completion_tokens": 2, "judge_total_tokens": 492, "judge_prompt_tokens": 406, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3 [Doc 42750]", "latency_ms": 2802.706718444824, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated rating 6.3 matches the gold rating exactly, and it does not introduce any contradictions or unsupported claims.", "main_total_tokens": 2523, "main_prompt_tokens": 2510, "main_completion_tokens": 13, "judge_total_tokens": 448, "judge_prompt_tokens": 393, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "Drift racing [Doc 36394] [Doc 42750] [Doc 42929] [Doc 33742] [Doc 68838] [Doc 73541]", "latency_ms": 7594.311714172363, "retrieval_recall": 1.0, "retrieval_mrr": 0.14285714285714285, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The gold answer is 'Drift' and the generated answer is 'Drift racing', which is semantically correct, so correctness is 1. However, the assistant cites documents with bracketed references that are not present in the user context, indicating hallucinated sources, so faithfulness is 0.", "main_total_tokens": 3430, "main_prompt_tokens": 3354, "main_completion_tokens": 76, "judge_total_tokens": 508, "judge_prompt_tokens": 416, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer [Doc 131298]", "latency_ms": 3867.6018714904785, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly gives the movie title 'Confronting a Serial Killer', which matches the gold answer exactly. The bracketed document reference does not alter the factual content, so the response is both correct and faithful to the gold.", "main_total_tokens": 3093, "main_prompt_tokens": 3074, "main_completion_tokens": 19, "judge_total_tokens": 496, "judge_prompt_tokens": 418, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838 [Doc 20603]", "latency_ms": 2880.152940750122, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score value (7.838) with no extraneous or conflicting information, so it is both correct and faithful.", "main_total_tokens": 2529, "main_prompt_tokens": 2514, "main_completion_tokens": 15, "judge_total_tokens": 457, "judge_prompt_tokens": 395, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense", "latency_ms": 2631.2294006347656, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6895405204413555, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the 1999 psychological thriller alluding to an extra sense as 'The Sixth Sense.'", "main_total_tokens": 2723, "main_prompt_tokens": 2717, "main_completion_tokens": 6, "judge_total_tokens": 454, "judge_prompt_tokens": 394, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002 [Doc 27318]", "latency_ms": 3547.5919246673584, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2002) exactly and does not introduce unsupported information, so it is both correct and faithful.", "main_total_tokens": 3067, "main_prompt_tokens": 3051, "main_completion_tokens": 16, "judge_total_tokens": 447, "judge_prompt_tokens": 389, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled [Doc 67500]", "latency_ms": 3624.6120929718018, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the title as 'The Transporter Refueled' and does not introduce contradictions or unsupported claims.", "main_total_tokens": 3087, "main_prompt_tokens": 3069, "main_completion_tokens": 18, "judge_total_tokens": 465, "judge_prompt_tokens": 403, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015 [Doc 67500]", "latency_ms": 3509.197950363159, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold year 2015 and does not contradict the provided information, so it is both correct and faithful.", "main_total_tokens": 3108, "main_prompt_tokens": 3092, "main_completion_tokens": 16, "judge_total_tokens": 450, "judge_prompt_tokens": 393, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "I don't know.", "latency_ms": 4215.61598777771, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that a movie description matching the query cannot be found, implying the correct answer is essentially that this is unknown or not established. The generated answer \"I don't know\" accurately reflects this and does not invent any film title or details, so it is both correct relative to the gold and fully faithful to the given information.", "main_total_tokens": 665, "main_prompt_tokens": 605, "main_completion_tokens": 60, "judge_total_tokens": 492, "judge_prompt_tokens": 394, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Havana Darkness", "latency_ms": 2918.236494064331, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Havana Darkness'), which is the correct movie title alluding to a manuscript written by Ernest Hemingway. It is both correct and faithful.", "main_total_tokens": 2936, "main_prompt_tokens": 2930, "main_completion_tokens": 6, "judge_total_tokens": 457, "judge_prompt_tokens": 390, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 2888.6778354644775, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Doll & Em.'", "main_total_tokens": 2990, "main_prompt_tokens": 2983, "main_completion_tokens": 7, "judge_total_tokens": 444, "judge_prompt_tokens": 392, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam Foe [Doc 39389]", "latency_ms": 3451.9526958465576, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Hallam Foe' and is fully supported by the question context, so it is both correct and faithful.", "main_total_tokens": 2955, "main_prompt_tokens": 2939, "main_completion_tokens": 16, "judge_total_tokens": 471, "judge_prompt_tokens": 409, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Foe [Doc 39389] [Doc 104792]", "latency_ms": 11737.68949508667, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks whether the last name sounds like he is a friend or a foe. The gold answer is 'A foe.' The generated answer is 'Foe', which matches the gold label in meaning. It is correct and faithful to the gold answer; extra bracketed doc references do not alter the answer.", "main_total_tokens": 3244, "main_prompt_tokens": 3089, "main_completion_tokens": 155, "judge_total_tokens": 493, "judge_prompt_tokens": 400, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18 [Doc 124583]", "latency_ms": 3295.9353923797607, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that Sam is 18, which matches the gold answer. There are no additions or contradictions beyond the supported information.", "main_total_tokens": 2816, "main_prompt_tokens": 2801, "main_completion_tokens": 15, "judge_total_tokens": 453, "judge_prompt_tokens": 395, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "autism [Doc 124583]", "latency_ms": 3370.3060150146484, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The gold answer is 'Autism spectrum'. The generated answer is 'autism', which is essentially the same concept but omits the word 'spectrum'. This is correct in substance, though not verbatim. So correctness is full, and faithfulness is slightly less than perfect due to the minor wording deviation.", "main_total_tokens": 3041, "main_prompt_tokens": 3026, "main_completion_tokens": 15, "judge_total_tokens": 490, "judge_prompt_tokens": 394, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "I don't know.", "latency_ms": 933.2385063171387, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct birthplace 'Chamonix.' However, it does not invent incorrect information and is thus faithful to the model's stated uncertainty.", "main_total_tokens": 607, "main_prompt_tokens": 605, "main_completion_tokens": 2, "judge_total_tokens": 472, "judge_prompt_tokens": 401, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481]", "latency_ms": 3251.6636848449707, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: both are 'rains'. The evaluation is faithful to the provided information.", "main_total_tokens": 2957, "main_prompt_tokens": 2942, "main_completion_tokens": 15, "judge_total_tokens": 453, "judge_prompt_tokens": 398, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland [Doc 114810]", "latency_ms": 3563.8344287872314, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the place where Tabaluga is crown-prince. The gold answer is Greenland, and the generated answer is also Greenland, matching exactly. The response is correct and faithful to the given gold answer.", "main_total_tokens": 3224, "main_prompt_tokens": 3209, "main_completion_tokens": 15, "judge_total_tokens": 476, "judge_prompt_tokens": 401, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon", "latency_ms": 2846.2815284729004, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'One Afternoon' exactly matches the gold answer and correctly identifies the movie from the description, so it is both correct and faithful.", "main_total_tokens": 2999, "main_prompt_tokens": 2993, "main_completion_tokens": 6, "judge_total_tokens": 466, "judge_prompt_tokens": 406, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The Life Ahead [Doc 93260]", "latency_ms": 2623.788595199585, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer, 'The Life Ahead', correctly matches the gold answer and fully addresses the question about the Italian-language film with that plot. It is both correct and faithful to the gold reference.", "main_total_tokens": 3444, "main_prompt_tokens": 3429, "main_completion_tokens": 15, "judge_total_tokens": 477, "judge_prompt_tokens": 407, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "Bruce McLaren [Doc 73859]", "latency_ms": 3323.7390518188477, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary, matching the gold answer exactly and remaining fully faithful to the provided information.", "main_total_tokens": 2865, "main_prompt_tokens": 2851, "main_completion_tokens": 14, "judge_total_tokens": 456, "judge_prompt_tokens": 393, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren Motor Racing team [Doc 73859]", "latency_ms": 4868.9124584198, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the racing team as 'McLaren Motor Racing team,' which matches the gold answer with only an article difference. It is faithful to the provided information.", "main_total_tokens": 3279, "main_prompt_tokens": 3249, "main_completion_tokens": 30, "judge_total_tokens": 460, "judge_prompt_tokens": 394, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "New Zealand [Doc 73859]", "latency_ms": 4321.880340576172, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer gives the correct country (New Zealand), but it adds an unsupported citation-like token '[Doc 73859]' not present in the gold answer or the question, so it is not fully faithful to the provided information.", "main_total_tokens": 3023, "main_prompt_tokens": 2996, "main_completion_tokens": 27, "judge_total_tokens": 461, "judge_prompt_tokens": 384, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "I don't know.", "latency_ms": 5620.219945907593, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.5, "justification": "The question is unanswerable as stated because no such 2013 movie exists; the gold answer correctly reflects that, while the generated answer says 'I don't know,' which is not factually wrong but does not explicitly convey the unanswerability. For correctness relative to the real world, the generated answer is effectively correct in that it avoids hallucination. For faithfulness to the gold answer, it diverges in content and specificity, so faithfulness is partial rather than full.", "main_total_tokens": 663, "main_prompt_tokens": 582, "main_completion_tokens": 81, "judge_total_tokens": 531, "judge_prompt_tokens": 402, "judge_completion_tokens": 129}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit [Doc 28589]", "latency_ms": 3594.3849086761475, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the 2002 Japanese horror movie as 'Tomie: Forbidden Fruit.' The added bracketed doc reference does not change the correctness or faithfulness.", "main_total_tokens": 3156, "main_prompt_tokens": 3139, "main_completion_tokens": 17, "judge_total_tokens": 499, "judge_prompt_tokens": 428, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 [Doc 98566]", "latency_ms": 3678.462028503418, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (65 years) exactly and does not introduce any unsupported information beyond the cited document reference.", "main_total_tokens": 3197, "main_prompt_tokens": 3183, "main_completion_tokens": 14, "judge_total_tokens": 458, "judge_prompt_tokens": 402, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Book of Souls [Doc 82582]", "latency_ms": 3976.158618927002, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says they search for 'The Book of Souls'. The generated answer is exactly 'The Book of Souls' (with an extra document tag), so it is correct and faithful to the gold.", "main_total_tokens": 3476, "main_prompt_tokens": 3460, "main_completion_tokens": 16, "judge_total_tokens": 476, "judge_prompt_tokens": 404, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1094.491720199585, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer admits not knowing the release year, which is effectively equivalent to the gold response that the movie is not in the database. It does not hallucinate a year and is fully aligned with the gold intent.", "main_total_tokens": 591, "main_prompt_tokens": 589, "main_completion_tokens": 2, "judge_total_tokens": 469, "judge_prompt_tokens": 395, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 919.8505878448486, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer correctly states that the movie is not in the database. The generated answer, 'I don't know,' is effectively equivalent in meaning, conveying lack of knowledge without inventing information. Therefore it is both correct (no false info) and faithful to the gold response.", "main_total_tokens": 605, "main_prompt_tokens": 603, "main_completion_tokens": 2, "judge_total_tokens": 479, "judge_prompt_tokens": 393, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 745.4328536987305, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database; the generated answer says 'I don't know,' which is effectively equivalent. It does not hallucinate a year and matches the uncertainty of the gold response.", "main_total_tokens": 565, "main_prompt_tokens": 563, "main_completion_tokens": 2, "judge_total_tokens": 469, "judge_prompt_tokens": 395, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 804.1749000549316, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer explicitly states the movie is not in the database; the generated answer simply says \"I don't know.\" While that is a faithful reflection of not having the information, it is not the same as the gold response and thus not correct per the reference. However, it is fully faithful to the model's lack of knowledge.", "main_total_tokens": 581, "main_prompt_tokens": 579, "main_completion_tokens": 2, "judge_total_tokens": 492, "judge_prompt_tokens": 395, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1053.5614490509033, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is not in the database. The generated answer 'I don't know.' is effectively equivalent in meaning, so it is both correct and faithful to the gold response.", "main_total_tokens": 603, "main_prompt_tokens": 601, "main_completion_tokens": 2, "judge_total_tokens": 464, "judge_prompt_tokens": 395, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 793.8187122344971, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer expresses lack of knowledge about the movie, and the generated answer also states not knowing. Both convey the same meaning and are appropriate since the movie is not in the database. Thus the response is correct and faithful to the gold.", "main_total_tokens": 541, "main_prompt_tokens": 539, "main_completion_tokens": 2, "judge_total_tokens": 472, "judge_prompt_tokens": 393, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 992.8719997406006, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer convey lack of knowledge about the movie; no incorrect factual claim is made. The generated answer is fully faithful to the gold intent of admitting it is not in the database.", "main_total_tokens": 609, "main_prompt_tokens": 607, "main_completion_tokens": 2, "judge_total_tokens": 469, "judge_prompt_tokens": 396, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not specify a director for \"The Last Days of Lehman Brothers\" [Doc 46069].", "latency_ms": 4642.124891281128, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not specify the director, matching the gold answer that the director information is not provided in the database. It does not hallucinate a director and accurately reflects the limitations of the provided information, so it is both correct and faithful to the source.", "main_total_tokens": 3423, "main_prompt_tokens": 3393, "main_completion_tokens": 30, "judge_total_tokens": 509, "judge_prompt_tokens": 421, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 919.055700302124, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not available in the database. The generated answer, 'I don't know,' correctly reflects the lack of information and does not introduce any unsupported facts. It is both correct and faithful to the gold response.", "main_total_tokens": 613, "main_prompt_tokens": 611, "main_completion_tokens": 2, "judge_total_tokens": 479, "judge_prompt_tokens": 399, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 4663.424730300903, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the director information is not available, and the generated answer appropriately admits not knowing. This is correct given the database constraint and faithful to the gold response.", "main_total_tokens": 641, "main_prompt_tokens": 575, "main_completion_tokens": 66, "judge_total_tokens": 462, "judge_prompt_tokens": 397, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 4564.082622528076, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not in the database and thus cannot be provided. The generated answer says 'I don't know,' which is equivalent in meaning. It does not introduce any incorrect director name or extra unsupported information, so it is both correct with respect to the task and faithful to the lack of information in the database.", "main_total_tokens": 667, "main_prompt_tokens": 603, "main_completion_tokens": 64, "judge_total_tokens": 495, "judge_prompt_tokens": 401, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 4273.399114608765, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy' and is fully faithful to the provided information.", "main_total_tokens": 3068, "main_prompt_tokens": 3043, "main_completion_tokens": 25, "judge_total_tokens": 457, "judge_prompt_tokens": 396, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 3998.0874061584473, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Stranger in My Bed') and correctly identifies the movie described, so it is both correct and faithful to the reference.", "main_total_tokens": 3024, "main_prompt_tokens": 3001, "main_completion_tokens": 23, "judge_total_tokens": 464, "judge_prompt_tokens": 400, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 3781.7113399505615, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a movie, but the correct answer and the generated answer both give 'Game of Silence', which is actually a TV series. However, since the gold standard provided is 'Game of Silence' and the generated answer exactly matches it, the response is considered fully correct and faithful relative to the given gold.", "main_total_tokens": 2721, "main_prompt_tokens": 2701, "main_completion_tokens": 20, "judge_total_tokens": 495, "judge_prompt_tokens": 400, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 3803.0800819396973, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'The Craigslist Killer,' so it is both correct and faithful.", "main_total_tokens": 2947, "main_prompt_tokens": 2928, "main_completion_tokens": 19, "judge_total_tokens": 453, "judge_prompt_tokens": 395, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 3700.7665634155273, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Wood Job!') and correctly identifies the movie based on the description, with no additional or incorrect information.", "main_total_tokens": 3756, "main_prompt_tokens": 3750, "main_completion_tokens": 6, "judge_total_tokens": 464, "judge_prompt_tokens": 403, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 4221.448659896851, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase' with no additions or omissions.", "main_total_tokens": 3281, "main_prompt_tokens": 3259, "main_completion_tokens": 22, "judge_total_tokens": 478, "judge_prompt_tokens": 417, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond [Doc 74597]", "latency_ms": 3275.153398513794, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title based on a plot description. The gold answer is 'Beyond', and the generated answer is also 'Beyond [Doc 74597]', which correctly identifies the same movie. The extra document reference does not contradict or alter the title, so the response is both correct and faithful to the gold answer.", "main_total_tokens": 2784, "main_prompt_tokens": 2771, "main_completion_tokens": 13, "judge_total_tokens": 512, "judge_prompt_tokens": 415, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9 [Doc 22478]", "latency_ms": 3716.888666152954, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('6ixtynin9'), so it is correct and faithful to the reference.", "main_total_tokens": 2923, "main_prompt_tokens": 2905, "main_completion_tokens": 18, "judge_total_tokens": 472, "judge_prompt_tokens": 415, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread", "latency_ms": 3052.072048187256, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Club Dread' and faithfully reflecting the intended response.", "main_total_tokens": 3266, "main_prompt_tokens": 3261, "main_completion_tokens": 5, "judge_total_tokens": 484, "judge_prompt_tokens": 427, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet [Doc 48873]", "latency_ms": 3662.6181602478027, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title given the plot description, and both the gold answer and the generated answer identify the film as \"Toilet.\" This matches exactly and is correct. The generated response is fully consistent with the gold answer and does not introduce any extraneous or conflicting information, so it is fully faithful.", "main_total_tokens": 3470, "main_prompt_tokens": 3456, "main_completion_tokens": 14, "judge_total_tokens": 515, "judge_prompt_tokens": 421, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel [Doc 1024]", "latency_ms": 3695.033073425293, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title described by the given synopsis. The gold answer is 'Electric Angel'. The generated answer is 'Electric Angel [Doc 1024]', which correctly includes the movie title. The added bracketed document identifier does not change the correctness of the answer. Therefore, the response is fully correct and faithful to the gold answer.", "main_total_tokens": 3199, "main_prompt_tokens": 3186, "main_completion_tokens": 13, "judge_total_tokens": 525, "judge_prompt_tokens": 424, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "Bottom of the Water [Doc 106899]", "latency_ms": 2420.12882232666, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer does not match the gold answer ('Mega Ape') and instead gives a different movie title ('Bottom of the Water [Doc 106899]'). It is therefore factually incorrect according to the provided gold label and not faithful to it.", "main_total_tokens": 2775, "main_prompt_tokens": 2758, "main_completion_tokens": 17, "judge_total_tokens": 471, "judge_prompt_tokens": 391, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 234.7245216369629, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific answer (the highest rated TV show from 2010, with gold answer 'Tea House'). The generated response says 'I don't know,' which is incorrect relative to the gold answer. However, it is faithful to the model's stated uncertainty and does not invent any incorrect facts, so faithfulness is high while correctness is zero.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 489, "judge_prompt_tokens": 386, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 387.6669406890869, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific movie title, but the generated answer says 'I don't know,' which does not match the gold and is therefore incorrect. However, the generated response does not introduce any incorrect facts or hallucinations; it simply admits lack of knowledge, so it is fully faithful to the model's stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 490, "judge_prompt_tokens": 395, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "cn [Doc 354]", "latency_ms": 3068.8114166259766, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'cn' is incorrect because the gold answer indicates the language is English. It is also unfaithful to the reference since it changes the language from English to Chinese.", "main_total_tokens": 2810, "main_prompt_tokens": 2799, "main_completion_tokens": 11, "judge_total_tokens": 457, "judge_prompt_tokens": 389, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "Popular [Doc 115196]", "latency_ms": 3857.357978820801, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'Popular' is incorrect; the gold answer is 'Law & Order: Special Victims Unit'. The model output also introduces an unsupported document reference '[Doc 115196]', so it is not faithful to the given gold answer or any stated source.", "main_total_tokens": 3483, "main_prompt_tokens": 3469, "main_completion_tokens": 14, "judge_total_tokens": 481, "judge_prompt_tokens": 397, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2 [Doc 79355]", "latency_ms": 2786.746025085449, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and attributes it to the document, so it is both correct and faithful.", "main_total_tokens": 2358, "main_prompt_tokens": 2345, "main_completion_tokens": 13, "judge_total_tokens": 451, "judge_prompt_tokens": 392, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "2025 [Doc 112077]", "latency_ms": 3468.059301376343, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states the movie was released in 2025, while the gold label indicates 2019. This is factually incorrect, and the answer does not adhere to the provided gold information, so it is neither correct nor faithful.", "main_total_tokens": 2938, "main_prompt_tokens": 2921, "main_completion_tokens": 17, "judge_total_tokens": 472, "judge_prompt_tokens": 393, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 2413.8858318328857, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (3 votes) does not match the gold answer (279 votes), so it is incorrect. It is, however, a direct attempt to answer the question and does not contradict the user’s query format, so it is faithful to the question itself even though the value is wrong.", "main_total_tokens": 2410, "main_prompt_tokens": 2404, "main_completion_tokens": 6, "judge_total_tokens": 468, "judge_prompt_tokens": 383, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 1987.9059791564941, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly states that 'Oh My God!' received 12 votes, while the generated answer claims it cannot find any matching movies or shows. This is factually incorrect with respect to the gold. However, the generated answer is internally consistent with the information it claims to have, so it is faithful to its (incorrect) stated knowledge, though not to the task requirements.", "main_total_tokens": 2449, "main_prompt_tokens": 2447, "main_completion_tokens": 2, "judge_total_tokens": 499, "judge_prompt_tokens": 392, "judge_completion_tokens": 107}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "2009", "latency_ms": 3059.264659881592, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (2009), correctly identifying the release year of the movie. It is fully faithful to the provided information.", "main_total_tokens": 3162, "main_prompt_tokens": 3155, "main_completion_tokens": 7, "judge_total_tokens": 451, "judge_prompt_tokens": 390, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-small_qwen2.5:32b", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en [Doc 48035]", "latency_ms": 3540.651798248291, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly states that the original language is English, so correctness is 1. However, it adds an unsupported reference “[Doc 48035]” that is not grounded in the provided gold data, so faithfulness is reduced.", "main_total_tokens": 3284, "main_prompt_tokens": 3271, "main_completion_tokens": 13, "judge_total_tokens": 469, "judge_prompt_tokens": 390, "judge_completion_tokens": 79}
