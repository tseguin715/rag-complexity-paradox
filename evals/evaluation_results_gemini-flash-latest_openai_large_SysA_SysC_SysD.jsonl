{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The movie *Wall Street* was released in 1987 [Doc 6993].", "latency_ms": 13245.754957199097, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer. It does not introduce contradictions or unsupported claims; the citation format does not affect faithfulness here, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3203, "main_prompt_tokens": 3181, "main_completion_tokens": 22, "judge_total_tokens": 476, "judge_prompt_tokens": 396, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The following movies involve classmates playing a survival game using cell phones:\n\n*   ***Brain Fluid Explosion Girl*** (2015): Hana Ichii and her classmates are locked in a cage and \"forced to take part in a survival game using their cellphones\" [Doc 69929].\n*   ***Tokyo Gore School*** (2009): A mysterious violent game is being played out in high schools, spread virally via students’ mobile phones [Doc 45096].\n*   ***One Missed Call 3: Final*** (2006): Asuka plans revenge by sending her classmates a cursed phone message they can either pass on or die [Doc 36853].\n*   ***Ousama Game The Animation*** (2017): A text message from someone calling themselves the \"King\" is sent to everyone in class, signaling the start of a death game [Doc 124945].", "latency_ms": 1522.8068828582764, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0.5, "faithfulness_score": 0, "justification": "The question asks for a single movie; the correct answer is 'Brain Fluid Explosion Girl'. The generated answer includes that film but also lists several additional, unnecessary titles, making the response only partially correct for the specific question. Faithfulness is 0 because the instructions for this task require selecting the best short answer matching the gold; the model instead added extra movies beyond what the question and gold specify.", "main_total_tokens": 3330, "main_prompt_tokens": 3125, "main_completion_tokens": 205, "judge_total_tokens": 680, "judge_prompt_tokens": 568, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show where six friends are transported into the Dungeons & Dragons realm and must try to find a way home with the help of their guide 'Dungeon Master' is titled *Dungeons & Dragons* [Doc 112849].", "latency_ms": 1027.7016162872314, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.4306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.719741384391281, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' which matches the gold answer exactly. It is fully faithful to the information in the provided content and does not introduce contradictions or unsupported claims.", "main_total_tokens": 3361, "main_prompt_tokens": 3309, "main_completion_tokens": 52, "judge_total_tokens": 510, "judge_prompt_tokens": 437, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is titled *Nacht der Wölfe* [Doc 2590].", "latency_ms": 783.029317855835, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe', which matches the gold answer exactly. It faithfully restates the information without adding unsupported details, and the reference to a document ID does not contradict the given gold answer.", "main_total_tokens": 3114, "main_prompt_tokens": 3072, "main_completion_tokens": 42, "judge_total_tokens": 518, "judge_prompt_tokens": 438, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of *Picnic at Hanging Rock* (2018 TV series) take place on Valentine's Day, 1900 [Doc 126191].", "latency_ms": 882.9445838928223, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies the year 1900, matching the gold answer. However, it specifies the 2018 TV series adaptation and cites a particular document, which may or may not align with the intended source scope of the question (possibly the original novel or film), making it slightly less than fully faithful to the likely intended reference context.", "main_total_tokens": 2940, "main_prompt_tokens": 2899, "main_completion_tokens": 41, "judge_total_tokens": 518, "judge_prompt_tokens": 416, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie is titled *June* [Doc 32483]. It is about a young, innocent, and quirky violinist named June (Felicia Day) who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and all-hours daydreaming about the beautiful man destined just for her [Doc 32483].", "latency_ms": 1016.394853591919, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'June' and accurately restates the plot details provided in the question without adding contradicting or extraneous information. It remains faithful to the implied source citation and matches the gold answer exactly.", "main_total_tokens": 3442, "main_prompt_tokens": 3357, "main_completion_tokens": 85, "judge_total_tokens": 587, "judge_prompt_tokens": 510, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "The highest rated Russian-language movie released in 1980 is *Moscow Does Not Believe in Tears*, with a rating of 7.6 [Doc 61].", "latency_ms": 766.0629749298096, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer names 'Moscow Does Not Believe in Tears' as the highest rated Russian-language movie from 1980, but the gold answer is 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger.' Therefore the content is factually incorrect relative to the gold. It also explicitly cites a document [Doc 61] as evidence for the incorrect claim, so its reasoning and use of sources are not faithful to the correct answer.", "main_total_tokens": 3099, "main_prompt_tokens": 3062, "main_completion_tokens": 37, "judge_total_tokens": 550, "judge_prompt_tokens": 429, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest Russian-language movie rating in 1999 is 7.117, for the movie *The Rifleman of the Voroshilov Regiment* [Doc 22123].", "latency_ms": 2531.96120262146, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the highest Russian-language movie rating in 1999 is 7.6, while the generated answer claims it is 7.117 and names a specific movie. This is numerically incorrect relative to the gold. Additionally, the generated answer introduces a specific film and document reference not supported by the provided gold answer, so it is not faithful to the given information.", "main_total_tokens": 3379, "main_prompt_tokens": 2963, "main_completion_tokens": 416, "judge_total_tokens": 530, "judge_prompt_tokens": 422, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "There are two movies released in 1987 that have a rating over 7.0 and are in Chinese:\n\n1.  **A Chinese Ghost Story** (Rating: 7.3, Language: cn) [Doc 7064]\n2.  **Last Eunuch In China** (Rating: 7.8, Language: cn) [Doc 7754]", "latency_ms": 4530.6761264801025, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer states there are 4 such movies, but the generated answer claims there are only 2, listing them explicitly; thus it is factually incorrect relative to the gold. It is partially faithful in that it appears to follow the query constraints (year 1987, rating >7, Chinese language) and does not introduce obviously irrelevant information, but it omits other qualifying movies and contradicts the gold count, so faithfulness is only partial.", "main_total_tokens": 4003, "main_prompt_tokens": 3052, "main_completion_tokens": 951, "judge_total_tokens": 590, "judge_prompt_tokens": 470, "judge_completion_tokens": 120}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "One TV show was released in 1987, has a rating over 7, and is in Chinese:\n\n1.  Dream of the Red Chamber (Rating: 9.4, Language: zh) [Doc 113285]", "latency_ms": 3743.2124614715576, "retrieval_recall": 0.0, "retrieval_mrr": 0.07142857142857142, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a count (2), but the answer gives 1 and lists only a single show, so it is numerically incorrect. It also contradicts the provided gold answer directly, so it is not faithful to the given information.", "main_total_tokens": 3699, "main_prompt_tokens": 3013, "main_completion_tokens": 686, "judge_total_tokens": 521, "judge_prompt_tokens": 441, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "The only French language TV show released in 1980 is *Fantômas* [Doc 112642].", "latency_ms": 2445.9805488586426, "retrieval_recall": 1.0, "retrieval_mrr": 0.2, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.38685280723454163, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.38685280723454163, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 'Fantômas' as the only French language TV show released in 1980, matching the gold answer exactly. It is also explicitly grounded in the cited document, so it is both correct and faithful.", "main_total_tokens": 3370, "main_prompt_tokens": 2954, "main_completion_tokens": 416, "judge_total_tokens": 486, "judge_prompt_tokens": 408, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is *Kill Cruise* [Doc 11205].", "latency_ms": 907.8428745269775, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' which matches the gold answer exactly. It does not introduce contradictions or hallucinated content beyond the provided information, so it is both correct and faithful.", "main_total_tokens": 3203, "main_prompt_tokens": 3176, "main_completion_tokens": 27, "judge_total_tokens": 481, "judge_prompt_tokens": 409, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The movie about the TV series in which Fred Rogers speaks directly to young children about some of life's most important issues is titled *Won't You Be My Neighbor?* [Doc 84838].", "latency_ms": 969.2633152008057, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Won't You Be My Neighbor?' and remains faithful to the provided information.", "main_total_tokens": 3600, "main_prompt_tokens": 3555, "main_completion_tokens": 45, "judge_total_tokens": 502, "judge_prompt_tokens": 440, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "", "latency_ms": 4602.5214195251465, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The model produced no answer, so the year of release was neither stated nor supported. Therefore it is incorrect and unfaithful to the gold answer of 2018.", "main_total_tokens": 4343, "main_prompt_tokens": 3320, "main_completion_tokens": 1023, "judge_total_tokens": 443, "judge_prompt_tokens": 378, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is *Astro Boy* (1980) [Doc 112591].", "latency_ms": 773.2865810394287, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 'Astro Boy' as the TV series and remains faithful to the provided information, adding only a plausible year/variant without contradicting the gold answer.", "main_total_tokens": 3622, "main_prompt_tokens": 3581, "main_completion_tokens": 41, "judge_total_tokens": 487, "judge_prompt_tokens": 420, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "The woman and her young son were held captive for 7 years in the movie *Room* [Doc 67513].", "latency_ms": 1386.0242366790771, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer exactly. It is fully consistent with the given information and does not introduce any contradictions or extraneous claims, so it is both correct and faithful.", "main_total_tokens": 2960, "main_prompt_tokens": 2762, "main_completion_tokens": 198, "judge_total_tokens": 503, "judge_prompt_tokens": 419, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The context states that six armed men broke into the Brink's-Mat security depot on 26 November 1983, \"stumbling across gold bullion worth £26m\" [Doc 133469]. The context does not specify the total amount of gold the depot was holding before the break-in, only the value of the gold bullion that was discovered [Doc 133469].", "latency_ms": 1056.981086730957, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question explicitly asks how much gold the depot was holding before it was broken into, and the provided answer instead talks about what the context does or does not specify, without actually answering the question. Even if the context is limited, the requested output was simply the amount of gold given (Gold: £26m). Thus the answer is not correct relative to the question, but it is faithful to the provided context because it accurately describes what the document states and does not introduce unsupported information.", "main_total_tokens": 3161, "main_prompt_tokens": 3071, "main_completion_tokens": 90, "judge_total_tokens": 599, "judge_prompt_tokens": 471, "judge_completion_tokens": 128}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "The popularity score of the most popular English-speaking TV show released in 1997 is 18.4612, which belongs to the show *The Practice* [Doc 114712].", "latency_ms": 4298.541307449341, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The generated answer gives a specific popularity score (18.4612) and ties it to a show (The Practice), but the gold answer indicates the correct score is 80.5772. Therefore the core factual claim is incorrect, so correctness is 0. The answer is mostly grounded in the provided document reference [Doc 114712] and does not introduce obvious extraneous fabrication beyond misidentifying the correct score, so faithfulness is relatively high but not perfect.", "main_total_tokens": 3857, "main_prompt_tokens": 2882, "main_completion_tokens": 975, "judge_total_tokens": 558, "judge_prompt_tokens": 430, "judge_completion_tokens": 128}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "Based strictly on the context provided, there were 0 Russian language TV shows released in 1997.", "latency_ms": 4042.2751903533936, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates there was 1 Russian language TV show released in 1997, while the generated answer states there were 0. This is factually incorrect. Additionally, the generated answer claims to be based strictly on provided context, but the context (gold) contradicts it, so it is not faithful.", "main_total_tokens": 3627, "main_prompt_tokens": 2835, "main_completion_tokens": 792, "judge_total_tokens": 499, "judge_prompt_tokens": 404, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "The TV show in which Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight is *Angel Eyes* [Doc 121928].", "latency_ms": 790.1911735534668, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Angel Eyes' and matches the gold answer exactly. It is fully faithful to the provided information and does not introduce unsupported details.", "main_total_tokens": 3405, "main_prompt_tokens": 3366, "main_completion_tokens": 39, "judge_total_tokens": 497, "judge_prompt_tokens": 431, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "", "latency_ms": 4771.361589431763, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "There is no generated answer to evaluate, so we cannot confirm correctness or faithfulness.", "main_total_tokens": 4280, "main_prompt_tokens": 3257, "main_completion_tokens": 1023, "judge_total_tokens": 425, "judge_prompt_tokens": 377, "judge_completion_tokens": 48}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "Based strictly on the context provided, there are zero English-speaking Horror movies released in 2009 that have a popularity score greater than 7.", "latency_ms": 3416.274309158325, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "Gold answer says 2, but the generated answer says 0. This contradicts the gold, so the answer is incorrect. However, it appears to follow the given context as stated, so it is faithful to that context.", "main_total_tokens": 3595, "main_prompt_tokens": 2959, "main_completion_tokens": 636, "judge_total_tokens": 498, "judge_prompt_tokens": 421, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Answer based strictly on the context. Do not use outside knowledge. Cite sources for every fact using the format [Doc X]. If answer is not found, say so.\n\nNo movies released in 1979 are listed in the provided context.", "latency_ms": 2814.3978118896484, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that there are no movies from 1979 in the database and therefore cannot answer. The generated answer correctly follows the instruction to rely only on the provided context, explicitly notes that no 1979 movies are listed, and thus that the answer is not found. This matches the gold response and does not introduce any outside information, so it is both correct and fully faithful to the context and instructions.", "main_total_tokens": 3595, "main_prompt_tokens": 3078, "main_completion_tokens": 517, "judge_total_tokens": 557, "judge_prompt_tokens": 443, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "Based on the context provided, there is no movie titled \"Back to the Future Part IV\" listed. The context mentions \"Back to the Future\" (1985) [Doc 4834], \"Back to the Future Part II\" (1989) [Doc 9405], and \"Back to the Future Part III\" (1990) [Doc 10621].", "latency_ms": 1034.0900421142578, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a non-existent movie. The gold answer correctly states that 'Back to the Future Part IV' does not exist in the database. The generated answer similarly notes that there is no movie titled 'Back to the Future Part IV' listed, and correctly references only Parts I–III with accurate years from the provided context. It does not fabricate a year and is consistent with the context, so it is both correct and faithful.", "main_total_tokens": 3717, "main_prompt_tokens": 3627, "main_completion_tokens": 90, "judge_total_tokens": 600, "judge_prompt_tokens": 477, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "Based strictly on the context provided, there are no movies released in 1980 that are about the nuclear Chernobyl disaster. The only movie listed for 1980, *Cataclysm* [Doc 744], is about a police detective investigating a death related to a Nazi concentration camp [Doc 744].", "latency_ms": 3757.564067840576, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The answer correctly states that there are no movies from 1980 about the Chernobyl disaster, matching the gold response. It is faithful to the given context by explicitly referencing that the only 1980 movie listed, Cataclysm, is unrelated to Chernobyl and instead concerns a Nazi concentration camp, which is consistent with the provided document.", "main_total_tokens": 4114, "main_prompt_tokens": 3337, "main_completion_tokens": 777, "judge_total_tokens": 573, "judge_prompt_tokens": 471, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "Based strictly on the context provided, four TV shows or movies have \"Astro Boy\" in the title:\n\n1.  *Astro Boy* (Movie, 2009) [Doc 44947]\n2.  *Astro Boy* (TV, 2003) [Doc 116265]\n3.  *Astro Boy* (TV, 1980) [Doc 112591]\n4.  *Astro Boy vs The Junkyard Pirates* (Movie, 2010) [Doc 49121]", "latency_ms": 4072.4802017211914, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 1.1510196182278052, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.773714921545009, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the count, which is 4, matching the gold answer. The generated answer explicitly lists four titles and bases the count strictly on the provided context, so it is both correct and faithful to the context.", "main_total_tokens": 3966, "main_prompt_tokens": 3156, "main_completion_tokens": 810, "judge_total_tokens": 573, "judge_prompt_tokens": 497, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "Based strictly on the provided context, there are no movies released in 2025 that allude to the Holocaust.", "latency_ms": 3000.797986984253, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there are 2 such movies, while the generated answer claims there are none, so it is factually incorrect. It also explicitly contradicts the given gold label, so it is unfaithful to the provided information.", "main_total_tokens": 3937, "main_prompt_tokens": 3420, "main_completion_tokens": 517, "judge_total_tokens": 486, "judge_prompt_tokens": 407, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "There were two TV shows released in the 1990s about Super Mario Bros.:\n\n1.  *The Adventures of Super Mario Bros. 3* (1990) [Doc 113661]\n2.  *Super Mario World* (1991) [Doc 113814]", "latency_ms": 4589.895486831665, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there was 1 TV show, but the generated answer claims there were 2, listing The Adventures of Super Mario Bros. 3 and Super Mario World. This directly contradicts the gold label, so the generated answer is incorrect. It also invents a second show relative to the gold, so it is not faithful to the provided reference answer.", "main_total_tokens": 4294, "main_prompt_tokens": 3377, "main_completion_tokens": 917, "judge_total_tokens": 551, "judge_prompt_tokens": 446, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "Based strictly on the context provided, there were zero TV shows released between 2000 and 2010 about Super Mario Bros.", "latency_ms": 4035.0425243377686, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the number of TV shows about Super Mario Bros released between 2000 and 2010; the gold answer is 0. The generated answer explicitly states that there were zero such TV shows, which matches the gold exactly. It does not introduce extra unsupported claims beyond restating the count, so it is both correct and faithful to the provided gold answer.", "main_total_tokens": 3629, "main_prompt_tokens": 2907, "main_completion_tokens": 722, "judge_total_tokens": 521, "judge_prompt_tokens": 415, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular movie or TV show about ninjas, based on the provided context, is the movie *Ninja Assassin*, with a popularity rating of 5.8597 [Doc 44828].", "latency_ms": 4898.525953292847, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer per the gold standard is 'Naruto', but the generated answer states 'Ninja Assassin', which is factually incorrect relative to the task. It also contradicts the provided gold context and thus is not faithful to it.", "main_total_tokens": 4164, "main_prompt_tokens": 3278, "main_completion_tokens": 886, "judge_total_tokens": 502, "judge_prompt_tokens": 423, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "Based strictly on the provided context, there is no mention of a Jurassic Park/Power Ranger crossover movie [Doc 19050, Doc 114047, Doc 19350, Doc 16623, Doc 86272, Doc 76939, Doc 14060, Doc 44738, Doc 26302, Doc 89797, Doc 18805, Doc 105397, Doc 84799, Doc 110701].", "latency_ms": 3589.5566940307617, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that such a movie does not exist or was not released. The generated answer instead says there is no mention of it in the provided context, which does not explicitly state that the movie was never released, so it does not correctly answer the question. However, it is faithful to the provided context, as it accurately reflects that the context does not mention such a crossover movie.", "main_total_tokens": 3821, "main_prompt_tokens": 3144, "main_completion_tokens": 677, "judge_total_tokens": 585, "judge_prompt_tokens": 476, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Based strictly on the context provided, the movie in which Barbie fights the Predator is not mentioned.", "latency_ms": 3818.217992782593, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is that the movie was not released. The generated answer instead claims the movie is not mentioned, which does not match the gold answer and is therefore incorrect. It also fails to be faithful to the provided gold reference.", "main_total_tokens": 4222, "main_prompt_tokens": 3500, "main_completion_tokens": 722, "judge_total_tokens": 478, "judge_prompt_tokens": 401, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "Based strictly on the provided context, there are no movies titled \"Star Wars\" listed, so the 4th oldest Star Wars movie cannot be determined.", "latency_ms": 3760.0951194763184, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a particular title as the 4th oldest Star Wars movie, while the generated answer claims it cannot be determined from the provided context. Given the instruction to base the answer strictly on the provided context and that the context evidently did not list any Star Wars movies, the generated answer is faithful to the context but factually incorrect relative to the gold answer. Thus, correctness is 0, faithfulness is 1.", "main_total_tokens": 4108, "main_prompt_tokens": 3480, "main_completion_tokens": 628, "judge_total_tokens": 538, "judge_prompt_tokens": 420, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "", "latency_ms": 4579.825639724731, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is missing/blank, so it does not state the correct year (1986) and thus is neither correct nor meaningfully grounded in the given gold answer.", "main_total_tokens": 4343, "main_prompt_tokens": 3320, "main_completion_tokens": 1023, "judge_total_tokens": 444, "judge_prompt_tokens": 378, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The rating of the movie *Forrest Gump* is 8.466 [Doc 15236].", "latency_ms": 1607.4860095977783, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating value of 8.466 and directly answers the user’s question without adding unsupported or conflicting information. It is both correct and faithful to the provided gold data.", "main_total_tokens": 3401, "main_prompt_tokens": 3200, "main_completion_tokens": 201, "judge_total_tokens": 487, "judge_prompt_tokens": 416, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The TV show about two comic book fanboys who discover a secret hospital for superheroes and are offered a job there is titled *Mighty Med* [Doc 121125].", "latency_ms": 749.6051788330078, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med' and matches the gold answer. It remains faithful to the question, only adding an accurate brief description consistent with the query.", "main_total_tokens": 2949, "main_prompt_tokens": 2910, "main_completion_tokens": 39, "judge_total_tokens": 496, "judge_prompt_tokens": 427, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "The seven prisoners in *Túnel de los huesos* (2011) escaped from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 886.8741989135742, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Argentina as the country where the prison is located, matching the gold answer. The mention of Villa Devoto prison in Buenos Aires is an extra detail but is consistent with the source citation and does not contradict the question. Both correctness and faithfulness are therefore fully satisfied.", "main_total_tokens": 3648, "main_prompt_tokens": 3609, "main_completion_tokens": 39, "judge_total_tokens": 515, "judge_prompt_tokens": 426, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The apprentice, Li Nai, falls into a trap designed by Jiang Jie [Doc 135231].", "latency_ms": 613.8803958892822, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'Jiang Jie'. The generated answer states that the apprentice, Li Nai, falls into a trap designed by Jiang Jie. This directly matches the gold regarding whose trap it is, so it is correct. The mention of the apprentice's name adds detail but does not contradict the gold. The answer is faithful to the provided information.", "main_total_tokens": 3202, "main_prompt_tokens": 3178, "main_completion_tokens": 24, "judge_total_tokens": 505, "judge_prompt_tokens": 404, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "Based strictly on the context provided, there are no Korean-language movies that mention assassins and were released after 2023.", "latency_ms": 4836.1334800720215, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the number of Korean-language movies mentioning assassins after 2023. The gold answer is 4, while the generated answer claims there are none, so it is factually incorrect. However, the answer is consistent with the context constraint it cites (\"Based strictly on the context provided\"), so it is faithful to that context even though the context is incomplete relative to the gold.", "main_total_tokens": 4114, "main_prompt_tokens": 3098, "main_completion_tokens": 1016, "judge_total_tokens": 520, "judge_prompt_tokens": 408, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "Based strictly on the provided context, there is no information about movies featuring both Steven Seagal and Bruce Lee.", "latency_ms": 816.5445327758789, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information about movies featuring both Steven Seagal and Bruce Lee, which aligns with the gold answer. It is both factually correct in context and faithful to the provided gold response.", "main_total_tokens": 3329, "main_prompt_tokens": 3307, "main_completion_tokens": 22, "judge_total_tokens": 490, "judge_prompt_tokens": 415, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of *Tokyo Mater* (2008) is 6.3 [Doc 42750].", "latency_ms": 778.0864238739014, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (6.3) and does not introduce any contradictions or extraneous, unfounded details. It is both correct and faithful.", "main_total_tokens": 3122, "main_prompt_tokens": 3095, "main_completion_tokens": 27, "judge_total_tokens": 470, "judge_prompt_tokens": 405, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "", "latency_ms": 5423.362016677856, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "No generated answer was provided to evaluate, so correctness and faithfulness cannot be confirmed.", "main_total_tokens": 4587, "main_prompt_tokens": 3564, "main_completion_tokens": 1023, "judge_total_tokens": 427, "judge_prompt_tokens": 379, "judge_completion_tokens": 48}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the TV series about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is *Confronting a Serial Killer* [Doc 131298].", "latency_ms": 987.8623485565186, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the title as 'Confronting a Serial Killer,' which matches the gold answer. It stays faithful to the given information, only adding that it is a TV series, which is accurate and does not contradict the source.", "main_total_tokens": 3199, "main_prompt_tokens": 3148, "main_completion_tokens": 51, "judge_total_tokens": 533, "judge_prompt_tokens": 453, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of *Armageddon* (1998) is 7.838 [Doc 20603].", "latency_ms": 737.3111248016357, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and does not introduce any contradictions. It is both correct and faithful to the provided information.", "main_total_tokens": 3364, "main_prompt_tokens": 3334, "main_completion_tokens": 30, "judge_total_tokens": 472, "judge_prompt_tokens": 409, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 movie that is a thriller and alludes to an extra sense is *The Sixth Sense* [Doc 22098]. It is categorized under the genres 'Mystery', 'Thriller', and 'Drama' [Doc 22098].", "latency_ms": 4196.84100151062, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'The Sixth Sense', matching the gold answer, and all additional details are consistent with the likely source document. There is no contradiction or hallucination relative to the prompt and cited doc, so both correctness and faithfulness are full.", "main_total_tokens": 3976, "main_prompt_tokens": 3110, "main_completion_tokens": 866, "judge_total_tokens": 532, "judge_prompt_tokens": 446, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "*The Transporter* was released in 2002 [Doc 27318].", "latency_ms": 929.1326999664307, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter' was released in 2002, which matches the gold answer. It does not contradict the provided information and appears faithful to the source citation.", "main_total_tokens": 3310, "main_prompt_tokens": 3288, "main_completion_tokens": 22, "judge_total_tokens": 467, "judge_prompt_tokens": 397, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 film in 'The Transporter' series is *The Transporter Refueled* [Doc 67500].", "latency_ms": 2196.5270042419434, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2015 reboot title as 'The Transporter Refueled' and stays faithful to the provided information.", "main_total_tokens": 3794, "main_prompt_tokens": 3433, "main_completion_tokens": 361, "judge_total_tokens": 486, "judge_prompt_tokens": 421, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in 2015 [Doc 67500].", "latency_ms": 994.931697845459, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It does not introduce contradictory or extraneous information beyond a benign reference marker, so it is fully correct and faithful.", "main_total_tokens": 3356, "main_prompt_tokens": 3331, "main_completion_tokens": 25, "judge_total_tokens": 484, "judge_prompt_tokens": 404, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "Based strictly on the provided context, there is no movie classified as a \"romantic comedy\" (a film containing both the 'Romance' and 'Comedy' genres) that explicitly states Abraham Lincoln comes back from the dead.\n\nHowever, the context does mention:\n1.  *Re-Elected* (Genres: Horror, Comedy) where \"Friends battle former U.S. presidents when they come back from the dead as zombies\" [Doc 94414]. This movie is a comedy about undead presidents, but it is not categorized as a romantic comedy.\n2.  *Abraham Lincoln vs. Zombies* (Genres: Horror, Action, Thriller) where Lincoln faces \"", "latency_ms": 5193.254232406616, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.9, "faithfulness_score": 0.9, "justification": "The generated answer correctly concludes, in line with the gold answer, that there is no romantic comedy in the provided context where Abraham Lincoln comes back from the dead. It also correctly notes that the films mentioned are not romantic comedies. This is faithful to the given context. The slight deduction is because the question expects essentially the more direct statement that no such movie can be found, but the generated answer adds extra detail that is somewhat speculative in describing genre classification rules beyond the minimal needed response.", "main_total_tokens": 4041, "main_prompt_tokens": 3019, "main_completion_tokens": 1022, "judge_total_tokens": 658, "judge_prompt_tokens": 526, "judge_completion_tokens": 132}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The movie \"Genius\" alludes to Ernest Hemingway as one of the literary greats signed by editor Max Perkins [Doc 73141].\n\nThe movie \"The Rum Diary\" alludes to Hemingway's \"The Lost Generation\" [Doc 52139].\n\nThe movie \"Hello Hemingway\" alludes to Ernest Hemingway's novel \"The Old Man and the Sea\" [Doc 11652].\n\nThe movie \"Garden of Eden\" is based on a novel by Ernest Hemingway [Doc 43581].\n\nThe movie \"Across the River and into the Trees\" is based on a novel by Ernest Hemingway [Doc 106490].\n\nThe movie \"The Old Man and the Sea\" (1990) is based on the novel by Ernest Hemingway [Doc 11238].\n\nThe context does not explicitly mention a movie title that alludes to a manuscript *written by* Ernest Hemingway, but several movies are based on his novels or refer to his works [Doc 43581, Doc 106490, Doc 11238, Doc 11652, Doc 52139]. \"Genius\" mentions that Max Perkins signed Ernest Hemingway, but does not specify a manuscript [Doc 73141].", "latency_ms": 1962.3098373413086, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the title of a specific movie that alludes to a manuscript written by Ernest Hemingway, whose correct answer is 'Havana Darkness'. The generated answer instead lists several other Hemingway-related films and explicitly states that the context does not mention a movie title that alludes to a manuscript, failing to identify 'Havana Darkness'. Thus it is incorrect. However, the answer remains faithful to the provided context, accurately describing what the documents say and not inventing unsupported claims, so faithfulness is high.", "main_total_tokens": 3340, "main_prompt_tokens": 3055, "main_completion_tokens": 285, "judge_total_tokens": 759, "judge_prompt_tokens": 626, "judge_completion_tokens": 133}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show *Doll & Em* [Doc 122259].", "latency_ms": 982.555627822876, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em', matching the gold answer exactly. It does not introduce any contradictions or unsupported claims, so it is both correct and faithful.", "main_total_tokens": 2985, "main_prompt_tokens": 2954, "main_completion_tokens": 31, "judge_total_tokens": 487, "judge_prompt_tokens": 417, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The 2007 movie is *Hallam Foe* [Doc 39389]. The description states that Hallam's talent for spying on people is revealed, and he is \"Driven to expose the true cause of his mother's death\" [Doc 39389].", "latency_ms": 1211.2536430358887, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which 2007 movie features Hallam, who spies on people and seeks the cause of his mother’s death. The generated answer correctly identifies the film as 'Hallam Foe' and accurately cites the description that matches the plot, without adding unsupported details. Both correctness and faithfulness are therefore fully satisfied.", "main_total_tokens": 3259, "main_prompt_tokens": 3194, "main_completion_tokens": 65, "judge_total_tokens": 549, "judge_prompt_tokens": 453, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Based strictly on the provided context, there is no specific person mentioned whose last name sounds like \"friend\" or \"foe.\"", "latency_ms": 5053.777694702148, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a clear gold answer: 'A foe.' The generated answer claims there is no specific person mentioned and thus does not select between friend or foe, which is incorrect relative to the labeled gold answer, so correctness is 0. However, given only the user-visible context (the question and gold), the model’s statement that no last name is provided is consistent with the text it sees, so it is faithful to its own reasoning and the visible context, yielding a faithfulness score of 1.", "main_total_tokens": 4188, "main_prompt_tokens": 3248, "main_completion_tokens": 940, "judge_total_tokens": 545, "judge_prompt_tokens": 412, "judge_completion_tokens": 133}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "Sam is 18 years old in the 2017 TV show *Atypical* [Doc 124583].", "latency_ms": 777.1944999694824, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that Sam is 18 years old, which matches the gold answer exactly. There is no additional conflicting or hallucinated information; the citation tag does not affect factuality, so the response is both correct and faithful.", "main_total_tokens": 3034, "main_prompt_tokens": 3004, "main_completion_tokens": 30, "judge_total_tokens": 489, "judge_prompt_tokens": 411, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show *Atypical*, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 724.637508392334, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the TV show 'Atypical', Sam is on the autism spectrum, matching the gold answer exactly. It is fully faithful to the provided information without adding misleading content.", "main_total_tokens": 2784, "main_prompt_tokens": 2755, "main_completion_tokens": 29, "judge_total_tokens": 483, "judge_prompt_tokens": 411, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "In the movie *À corde tendue*, released in 2013, Sandra was born in Chamonix [Doc 59659].", "latency_ms": 815.3815269470215, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, matching the gold answer exactly. It is fully grounded in the provided information and does not introduce extraneous or contradictory details.", "main_total_tokens": 3671, "main_prompt_tokens": 3638, "main_completion_tokens": 33, "judge_total_tokens": 494, "judge_prompt_tokens": 425, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "in this japanese language movie, Hazuki clips her nails every time it **rains** [Doc 100481].", "latency_ms": 2485.248327255249, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly fills in the blank with 'rains', exactly matching the gold answer. It is fully faithful to the provided information and restates it accurately without adding contradictions.", "main_total_tokens": 3574, "main_prompt_tokens": 3320, "main_completion_tokens": 254, "judge_total_tokens": 482, "judge_prompt_tokens": 415, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Tabaluga is the crown-prince of Greenland [Doc 114810].", "latency_ms": 3578.2296657562256, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, correctly stating that Tabaluga is the crown-prince of Greenland and does not introduce any contradictory or extraneous information.", "main_total_tokens": 3985, "main_prompt_tokens": 3443, "main_completion_tokens": 542, "judge_total_tokens": 474, "judge_prompt_tokens": 410, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with that description is *One Afternoon* [Doc 96838].", "latency_ms": 784.3277454376221, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'One Afternoon' and does not introduce any contradictions or unsupported details. It is correct and faithful.", "main_total_tokens": 2845, "main_prompt_tokens": 2826, "main_completion_tokens": 19, "judge_total_tokens": 479, "judge_prompt_tokens": 420, "judge_completion_tokens": 59}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The Italian language movie where a Holocaust survivor with a daycare business takes in a 12-year-old street kid who recently robbed her is *The Life Ahead* [Doc 93260].\n\nIts rating is 6.746 [Doc 93260].", "latency_ms": 7584.113597869873, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies the movie as 'The Life Ahead', matching the gold answer, so correctness is 1. However, it adds extra details not present in the question or gold reference: mentioning a daycare business, a specific numerical rating (6.746), and citing a hypothetical document [Doc 93260]. These additions go beyond the provided gold answer and source, reducing faithfulness, hence 0.8.", "main_total_tokens": 3235, "main_prompt_tokens": 3173, "main_completion_tokens": 62, "judge_total_tokens": 566, "judge_prompt_tokens": 448, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary titled \"McLaren\" is about the story of New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859]. The documentary shows how a man of humble beginnings could compete against and defeat the elite of motor racing [Doc 73859].", "latency_ms": 1084.9244594573975, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies that the 2016 documentary 'McLaren' is about Bruce McLaren, matching the gold answer, so correctness is 1. However, it cites a specific source '[Doc 73859]' that was not provided in the prompt, so the explanation is not fully faithful to the given context, resulting in a faithfulness score of 0.", "main_total_tokens": 3422, "main_prompt_tokens": 3355, "main_completion_tokens": 67, "judge_total_tokens": 552, "judge_prompt_tokens": 446, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "", "latency_ms": 5347.750425338745, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is empty, so it does not provide the correct team name and cannot be assessed as faithful to the question or gold answer.", "main_total_tokens": 4600, "main_prompt_tokens": 3577, "main_completion_tokens": 1023, "judge_total_tokens": 443, "judge_prompt_tokens": 384, "judge_completion_tokens": 59}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "Based on the context provided, here are the countries of origin mentioned for various individuals or films:\n\n*   **Karol Wojtyla** (later Pope John Paul II) is a Polish Roman Catholic [Doc 1545].\n*   **Henryk Greenberg** is a Polish-born American [Doc 13962].\n*   **Cyriaque Kouenou** was forced to leave his country and fled to the Netherlands [Doc 67245].\n*   **Xavi** (a soccer player) moved to Spain, but he and Rosana are friends from his childhood in Uruguay [Doc 43287].\n*   **Guy Bennett** is an American journalist interviewing him in Moscow [Doc 4064].\n*   **Momo** (a blind hitman) is followed in Berlin [Doc 88121].\n*   **Ullaas** and **Laya** are characters in a film whose language is Malayalam (ml) [Doc 70953].\n*   **Bode Donovan** is a young convict in Northern California [Doc 131499].\n*   A **Sunni Arab doctor** is followed as he prepares to run for elections in Iraq [Doc 38478].\n*   **Anatoly** fought in France [Doc 8202].\n*   The film *What a Country!* features a minister in the Croatian government [Doc 85756].\n*   *Born in Syria* follows Syrian children refugees [Doc 75442].\n*   *Sudani from Nigeria* features a foreign player from Nigeria [Doc 85630].\n*   *Born in Absurdistan* involves an Austrian couple and a Turkish family of immigrant workers, who are deported to their native village in Turkey [Doc 22786].\n*   *Free Country* is set in former East Germany [Doc 92281].", "latency_ms": 2919.4953441619873, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the country of a specific male individual, whose gold answer is New Zealand. The generated answer does not identify any person from New Zealand and instead lists multiple unrelated people and films with various countries, never answering who \"he\" is or his country. Therefore it is incorrect. It is also unfaithful to the task because it does not address the target entity at all and fabricates a list of context items that do not resolve the question.", "main_total_tokens": 3727, "main_prompt_tokens": 3296, "main_completion_tokens": 431, "judge_total_tokens": 872, "judge_prompt_tokens": 749, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based strictly on the context provided, the information regarding a 2013 movie where John McClane blows up the Great Wall of China is not available [Doc 58791, Doc 38837, Doc 8154, Doc 10612, Doc 16494].\n\nThe 2013 movie featuring John McClane is *A Good Day to Die Hard*, where he travels to Moscow to help his son stop a nuclear weapons heist [Doc 58791].", "latency_ms": 3556.870698928833, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that no such movie exists in the database, implying the correct response is to say the information is not available. The generated answer invents a specific movie context (A Good Day to Die Hard and its Moscow plot) and presents it as an answer to the Great Wall of China question, which is incorrect for this query. It also goes beyond the gold answer by adding plot details that are not supported by the gold, so it is not faithful to the provided gold response.", "main_total_tokens": 4172, "main_prompt_tokens": 3539, "main_completion_tokens": 633, "judge_total_tokens": 623, "judge_prompt_tokens": 494, "judge_completion_tokens": 129}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with that description is *Tomie: Forbidden Fruit* [Doc 28589].", "latency_ms": 845.7694053649902, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Tomie: Forbidden Fruit' and stays faithful to the question and provided information without adding unsupported details.", "main_total_tokens": 3424, "main_prompt_tokens": 3394, "main_completion_tokens": 30, "judge_total_tokens": 502, "judge_prompt_tokens": 442, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie *Lei mi parla ancora*, Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 738.8408184051514, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years) and does not introduce any unsupported information beyond a generic document reference, so it is both correct and faithful.", "main_total_tokens": 3765, "main_prompt_tokens": 3730, "main_completion_tokens": 35, "judge_total_tokens": 488, "judge_prompt_tokens": 424, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In *Scorpion King: Book of Souls* (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 844.2187309265137, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in 'Scorpion King: Book of Souls' (2018), the Scorpion King and Tala are searching for the Book of Souls, matching the gold answer. It aligns with the given information and does not introduce contradictions or irrelevant details.", "main_total_tokens": 3519, "main_prompt_tokens": 3478, "main_completion_tokens": 41, "judge_total_tokens": 516, "judge_prompt_tokens": 431, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Answer based strictly on the context. Do not use outside knowledge. Cite sources for every fact using the format [Doc X]. If answer is not found, say so.\n\nThe release year for \"Zombies on the Moon\" is not found in the provided context.", "latency_ms": 2380.0480365753174, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold answer says the movie is not in the database. The generated answer correctly states that the release year is not found in the provided context, which is equivalent in meaning. It does not introduce outside knowledge and adheres to the constraint to base the answer on context only. Therefore it is both correct and faithful.", "main_total_tokens": 3405, "main_prompt_tokens": 2968, "main_completion_tokens": 437, "judge_total_tokens": 537, "judge_prompt_tokens": 444, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Galactic baker\" is not available in the provided context.", "latency_ms": 2366.837978363037, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database. The generated answer says the release year is not available in the provided context. Both effectively convey that the system cannot provide the release year, so the generated answer is correct and faithful to the gold.", "main_total_tokens": 3682, "main_prompt_tokens": 3277, "main_completion_tokens": 405, "judge_total_tokens": 489, "judge_prompt_tokens": 406, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Answer based strictly on the context. Do not use outside knowledge. Cite sources for every fact using the format [Doc X]. If answer is not found, say so.\n\nThe release year for \"Return of the Null Pointer\" is not found in the provided context.", "latency_ms": 2433.577537536621, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold says the movie is not in the database. The generated answer says the release year is not found in the provided context, which is equivalent. It follows the instruction to answer based strictly on context and to say so if the answer is not found. Therefore, it is both correct and faithful to the gold and instructions.", "main_total_tokens": 3709, "main_prompt_tokens": 3235, "main_completion_tokens": 474, "judge_total_tokens": 539, "judge_prompt_tokens": 444, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for the movie \"Data Science: The Movie\" is not available in the provided context.", "latency_ms": 2280.658960342407, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The gold answer admits the movie is not in the database. The generated answer claims the information is not in the provided context, which is a different limitation and misrepresents the source of its uncertainty. Factually, both convey that the year is unknown, so correctness is fine, but faithfulness to the gold/reference behavior is not.", "main_total_tokens": 3668, "main_prompt_tokens": 3227, "main_completion_tokens": 441, "judge_total_tokens": 510, "judge_prompt_tokens": 412, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the movie \"Python's Revenge 2\" is not listed.", "latency_ms": 2890.317916870117, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database. The generated answer correctly and faithfully echoes that the movie is not listed, without inventing a year or additional details. It is both correct and faithful to the given context.", "main_total_tokens": 3789, "main_prompt_tokens": 3205, "main_completion_tokens": 584, "judge_total_tokens": 489, "judge_prompt_tokens": 411, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the movie \"The Last Pixel\" is not listed.", "latency_ms": 3077.805757522583, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie is not listed, which aligns with the gold answer indicating it is not in the database. It does not fabricate a release year and stays within the provided context, so it is both correct and faithful.", "main_total_tokens": 3505, "main_prompt_tokens": 2946, "main_completion_tokens": 559, "judge_total_tokens": 486, "judge_prompt_tokens": 407, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for the movie \"The man who ate the sun\" is not available.", "latency_ms": 2278.752326965332, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year is not available and does not hallucinate a date. It is fully consistent with the gold answer, so it is both correct and faithful to the provided context.", "main_total_tokens": 4017, "main_prompt_tokens": 3607, "main_completion_tokens": 410, "judge_total_tokens": 489, "judge_prompt_tokens": 417, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not state who directed the movie \"The Last Days of Lehman Brothers\" [Doc 46069].", "latency_ms": 824.6166706085205, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the provided context does not include the director, which aligns with the gold answer indicating the database lacks that information. It does not hallucinate a director and remains faithful to the given context.", "main_total_tokens": 3698, "main_prompt_tokens": 3671, "main_completion_tokens": 27, "judge_total_tokens": 496, "judge_prompt_tokens": 422, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Answer not found in the provided context.", "latency_ms": 3099.677801132202, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the answer is not found in the provided context, matching the gold response that the director information is not available in the database. It does not introduce any unsupported information and remains faithful to the given context.", "main_total_tokens": 3646, "main_prompt_tokens": 3111, "main_completion_tokens": 535, "judge_total_tokens": 480, "judge_prompt_tokens": 403, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context does not provide the name of the director for the movie \"Journey into Gaza\" [Doc 111360].", "latency_ms": 1944.0913200378418, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not provide the name of the director, which is equivalent in meaning to the gold answer that the director information is not in the database. It is both correct and faithful to the given gold response.", "main_total_tokens": 3638, "main_prompt_tokens": 3368, "main_completion_tokens": 270, "judge_total_tokens": 496, "judge_prompt_tokens": 417, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not contain information about who directed the movie \"The Last Night at Tremore Beach\" [Doc 135098].", "latency_ms": 680.4699897766113, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not contain information about the director, which aligns with the gold answer that the director information is not provided in the database. It does not fabricate any details and remains faithful to the given context.", "main_total_tokens": 2974, "main_prompt_tokens": 2943, "main_completion_tokens": 31, "judge_total_tokens": 502, "judge_prompt_tokens": 424, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The following movies involve a group of archaeology students or similar groups awakening or discovering a mummy/ancient evil:\n\n*   In *Rise of the Mummy* (2021), a group of archaeology students awaken an ancient mummy [Doc 98375].\n*   In *Ancient Evil: Scream of the Mummy* (2000), six young archaeology students discover the remains of an ancient Aztec mummy and accidentally unleash the fury of an evil god [Doc 24655].\n*   In *The Mummy Resurrected* (2014), a team of archaeologists discover an ancient mummy, unleashing a deadly curse [Doc 65341].\n*   In *Tale of the Mummy* (1998), a group of archaeologists break the cursed seal of a tomb on a greedy search for treasure, causing every man to vanish [Doc 21073].\n*   In *The Tomb* (2006), a group of young archaeologists flies to Mexico to study the Mayan culture and finds trouble when they stumble upon an unknown cursed temple [Doc 38091].\n*   In *The Circle* (2017), a University professor takes four archaeology students on a field trip where they unearth clues to an ancient mystery [Doc 80953].", "latency_ms": 1757.8952312469482, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.6, "justification": "The question asks for a single movie: the correct answer is 'Rise of the Mummy,' which the generated answer does include, so the core question is answered correctly. However, the response adds multiple other films that involve archaeologists and mummies, implying they might also match the described plot without clarifying that only 'Rise of the Mummy' is the direct answer. This extra, somewhat speculative list reduces faithfulness to the user’s specific, narrow query.", "main_total_tokens": 3258, "main_prompt_tokens": 2975, "main_completion_tokens": 283, "judge_total_tokens": 777, "judge_prompt_tokens": 651, "judge_completion_tokens": 126}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The following movie description matches the scenario:\n\n*   **Stranger in My Bed** (2005): To escape her abusive spouse, Sarah fakes her own death and flees [Doc 35237].", "latency_ms": 903.3985137939453, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Stranger in My Bed,' which matches the gold answer exactly and aligns with the described plot of Sarah faking her death to escape her abusive spouse. It stays fully faithful to the given information without introducing contradictions or unsupported details.", "main_total_tokens": 3025, "main_prompt_tokens": 2978, "main_completion_tokens": 47, "judge_total_tokens": 523, "judge_prompt_tokens": 438, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The event described happens in the TV show *Game of Silence* [Doc 123938].", "latency_ms": 898.5059261322021, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a movie, and the gold answer is the TV series 'Game of Silence.' The generated answer identifies it as a TV show rather than a movie, so it does not correctly answer the user’s question and is therefore incorrect. It also contradicts the provided gold answer’s framing (movie vs TV show), so it is not faithful to the gold.", "main_total_tokens": 2695, "main_prompt_tokens": 2672, "main_completion_tokens": 23, "judge_total_tokens": 521, "judge_prompt_tokens": 416, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie described as being \"Based on a true story. Philip Markoff, a charismatic and popular med student at Boston University, leads a double life as a brutal and cruel sexual deviant who abuses prostitutes he finds via Craigslist\" is titled *The Craigslist Killer* [Doc 53039].", "latency_ms": 960.3416919708252, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer,' which matches the gold answer exactly. It remains faithful to the given information, including the description and title, without introducing contradictions or unsupported claims.", "main_total_tokens": 3115, "main_prompt_tokens": 3054, "main_completion_tokens": 61, "judge_total_tokens": 523, "judge_prompt_tokens": 451, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "This happens in the movie *Wood Job!* [Doc 66094].", "latency_ms": 805.1323890686035, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Wood Job!', which matches the gold answer exactly. It is fully faithful to the question and does not introduce any extraneous or incorrect information beyond a harmless citation token.", "main_total_tokens": 3510, "main_prompt_tokens": 3491, "main_completion_tokens": 19, "judge_total_tokens": 488, "judge_prompt_tokens": 415, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The event described happens in the movie *Sesame Street: The Magical Wand Chase* [Doc 78373].", "latency_ms": 910.9611511230469, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Sesame Street: The Magical Wand Chase,' which matches the gold answer exactly, and does not introduce any unsupported information beyond a citation placeholder.", "main_total_tokens": 3506, "main_prompt_tokens": 3480, "main_completion_tokens": 26, "judge_total_tokens": 501, "judge_prompt_tokens": 433, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a 7 year-old boy must find the hero within him to save his parents, who are trapped inside a burning car by a rock fall, is *Beyond* [Doc 74597].", "latency_ms": 893.7978744506836, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question’s gold answer is \"Beyond\" and the generated answer correctly identifies the movie as \"Beyond\" with consistent plot details. It does not introduce contradictions or unsupported claims.", "main_total_tokens": 3109, "main_prompt_tokens": 3062, "main_completion_tokens": 47, "judge_total_tokens": 517, "judge_prompt_tokens": 451, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie is *6ixtynin9* [Doc 22478].", "latency_ms": 714.7138118743896, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', which matches the gold answer exactly. It does not add any contradictory or extraneous information and remains faithful to the provided data.", "main_total_tokens": 3218, "main_prompt_tokens": 3198, "main_completion_tokens": 20, "judge_total_tokens": 491, "judge_prompt_tokens": 419, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie in which this happens is *Club Dread* [Doc 32410].", "latency_ms": 714.9467468261719, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer exactly. It does not introduce any contradictions or fabricate information beyond citing a document reference, so it is both correct and faithful to the given information.", "main_total_tokens": 3202, "main_prompt_tokens": 3182, "main_completion_tokens": 20, "judge_total_tokens": 520, "judge_prompt_tokens": 442, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie is **Toilet** [Doc 48873].", "latency_ms": 650.5427360534668, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold label exactly: the movie is \"Toilet.\" There are no contradictions or unsupported additions beyond restating the correct title, so it is both correct and faithful.", "main_total_tokens": 3835, "main_prompt_tokens": 3820, "main_completion_tokens": 15, "judge_total_tokens": 496, "judge_prompt_tokens": 427, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie described is *Electric Angel* [Doc 1024].", "latency_ms": 817.7952766418457, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title that matches the given plot description. The gold answer is 'Electric Angel'. The generated answer explicitly identifies the movie as 'Electric Angel', matching the gold exactly. There are no contradictions or hallucinations; referencing '[Doc 1024]' does not alter the correctness of the title. Thus the answer is fully correct and faithful.", "main_total_tokens": 3390, "main_prompt_tokens": 3374, "main_completion_tokens": 16, "judge_total_tokens": 533, "judge_prompt_tokens": 430, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 in the context provided is *Doomsday Stories*, with a rating of 2.167 [Doc 108366].", "latency_ms": 3134.82928276062, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the lowest rated movie from 2023 is 'Mega Ape', while the generated answer claims it is 'Doomsday Stories' with a rating of 2.167. This contradicts the gold answer, so the generated response is incorrect. However, the generated answer refers explicitly to a document ID and rating, indicating it is grounded in the provided context rather than being invented, so it is faithful to its cited source.", "main_total_tokens": 3796, "main_prompt_tokens": 3040, "main_completion_tokens": 756, "judge_total_tokens": 532, "judge_prompt_tokens": 415, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "The highest rated TV show from 2010 listed in the context is *Downton Abbey*, with a rating of 8.1 [Doc 119146].", "latency_ms": 766.8702602386475, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the highest rated TV show from 2010, and the gold answer is 'Tea House'. The generated answer instead claims it is 'Downton Abbey' with a rating of 8.1, which directly contradicts the gold answer. There is no provided context here to justify choosing 'Downton Abbey', so the response is both incorrect relative to the gold answer and unfaithful to the (unseen) source context.", "main_total_tokens": 3726, "main_prompt_tokens": 3686, "main_completion_tokens": 40, "judge_total_tokens": 536, "judge_prompt_tokens": 415, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest rated Japanese language movie from 1999 is *Kikujiro*, with a rating of 7.5 [Doc 22213].", "latency_ms": 3355.5872440338135, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect with respect to the gold label: it claims the highest rated Japanese language movie from 1999 is 'Kikujiro', but the gold answer is 'The Uncut Story of Mewtwo's Origin'. It is also unfaithful to the provided gold answer, directly contradicting it by naming a different movie.", "main_total_tokens": 3619, "main_prompt_tokens": 3032, "main_completion_tokens": 587, "judge_total_tokens": 523, "judge_prompt_tokens": 422, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is *Cataclysm* (Rating: 3.889) [Doc 744], and its language is English (en) [Doc 744].", "latency_ms": 4550.5406856536865, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980. The gold answer is 'English'. The generated answer states that the lowest-rated movie is 'Cataclysm' and explicitly says its language is English, matching the gold. There is no contradiction or hallucination relative to the provided gold; it directly answers with the same language. Thus both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3947, "main_prompt_tokens": 3096, "main_completion_tokens": 851, "judge_total_tokens": 537, "judge_prompt_tokens": 424, "judge_completion_tokens": 113}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show from 1999 with the highest popularity was *The Sopranos*, with a popularity score of 59.4284 [Doc 115146].", "latency_ms": 4504.78982925415, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the gold label indicates that the 1999 TV show with the highest popularity is 'Law & Order: Special Victims Unit', whereas the generated answer claims it is 'The Sopranos'. However, the generated answer does not contradict the cited document; it attributes its claim and popularity score directly to Doc 115146, so it is faithful to the source it references.", "main_total_tokens": 4425, "main_prompt_tokens": 3454, "main_completion_tokens": 971, "judge_total_tokens": 536, "judge_prompt_tokens": 424, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 669.8706150054932, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 and attributes it to a document, indicating it is grounded in the provided context. Both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3283, "main_prompt_tokens": 3262, "main_completion_tokens": 21, "judge_total_tokens": 469, "judge_prompt_tokens": 400, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in multiple years: 1999 [Doc 22481], 2019 [Doc 88811], and 2025 [Doc 112077].", "latency_ms": 876.2331008911133, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer indicates 2019, but the generated answer lists three different years (1999, 2019, 2025) instead of giving the single correct year, so it is not correct. It is partially faithful in that it does include the correct year 2019 among its answers, but it adds incorrect years and presents them all as valid release years, which reduces faithfulness.", "main_total_tokens": 3230, "main_prompt_tokens": 3172, "main_completion_tokens": 58, "judge_total_tokens": 539, "judge_prompt_tokens": 426, "judge_completion_tokens": 113}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The context does not specify the number of votes \"My Dog Skip\" received. It only provides the rating (6.7) and popularity (1.7749) [Doc 24114].", "latency_ms": 3879.4825077056885, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that 'My Dog Skip' received 279 votes, but the generated answer claims the context does not specify the number of votes. This is incorrect given the gold. However, the generated answer is faithful to what it asserts about the provided context (it does not hallucinate a number), so faithfulness is high but correctness relative to the gold is low.", "main_total_tokens": 3748, "main_prompt_tokens": 3341, "main_completion_tokens": 407, "judge_total_tokens": 522, "judge_prompt_tokens": 421, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The context does not provide information on how many votes the movie \"Oh My God!\" received. It only lists the rating (3.8) and popularity (0.8047) [Doc 85434].", "latency_ms": 912.3139381408691, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states the movie received 12 votes, but the generated answer claims the context does not provide the number of votes. This is incorrect given the gold answer, so correctness is 0. However, the generated answer appears consistent with the context as it describes it (rating and popularity only), so faithfulness is 1.", "main_total_tokens": 3112, "main_prompt_tokens": 3064, "main_completion_tokens": 48, "judge_total_tokens": 522, "judge_prompt_tokens": 424, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 728.5840511322021, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie was released in 2009, matching the gold answer exactly. It is fully supported by the provided information and does not introduce unsupported claims.", "main_total_tokens": 3145, "main_prompt_tokens": 3118, "main_completion_tokens": 27, "judge_total_tokens": 477, "judge_prompt_tokens": 410, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-large_gemini-flash-latest", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English (en) [Doc 48035].", "latency_ms": 745.7122802734375, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the original language of 'The King's Speech' is English, matching the gold answer 'en'. It is also faithful, simply restating the gold information with a small clarification and a document reference.", "main_total_tokens": 3619, "main_prompt_tokens": 3592, "main_completion_tokens": 27, "judge_total_tokens": 481, "judge_prompt_tokens": 405, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The movie *Wall Street* was released in 1987 [Doc 6993].", "latency_ms": 869.8909282684326, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer. It does not introduce contradictions or unsupported claims; the bracketed citation does not affect factuality.", "main_total_tokens": 2430, "main_prompt_tokens": 2408, "main_completion_tokens": 22, "judge_total_tokens": 466, "judge_prompt_tokens": 396, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie in which a character wakes up locked in a cage with her classmates and is forced to take part in a survival game using their cellphones is *Brain Fluid Explosion Girl* [Doc 69929].", "latency_ms": 830.6331634521484, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie where classmates must play a survival game using their cell phones. The gold answer is 'Brain Fluid Explosion Girl.' The generated answer also identifies the movie as 'Brain Fluid Explosion Girl' and adds consistent detail about the plot. This matches the gold exactly and does not contradict it, so it is both correct and faithful.", "main_total_tokens": 2391, "main_prompt_tokens": 2346, "main_completion_tokens": 45, "judge_total_tokens": 530, "judge_prompt_tokens": 430, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show in which six friends are transported into the *Dungeons & Dragons* realm and must try to find a way home with the help of their guide 'Dungeon Master' is **Dungeons & Dragons** [Doc 112849].", "latency_ms": 874.577522277832, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.4306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.719741384391281, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' matching the gold answer. It remains faithful to the provided information, only adding accurate contextual detail about the premise of the show.", "main_total_tokens": 2477, "main_prompt_tokens": 2423, "main_completion_tokens": 54, "judge_total_tokens": 510, "judge_prompt_tokens": 439, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is *Nacht der Wölfe* [Doc 2590].", "latency_ms": 726.5918254852295, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 'Nacht der Wölfe' as the movie about two rival gangs in Munich, one German and one Turkish. It matches the gold answer exactly and does not introduce any extraneous or incorrect information, so it is both correct and faithful.", "main_total_tokens": 2313, "main_prompt_tokens": 2272, "main_completion_tokens": 41, "judge_total_tokens": 522, "judge_prompt_tokens": 437, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of *Picnic at Hanging Rock* take place on Valentine's Day, 1900 [Doc 126191].", "latency_ms": 1842.2644138336182, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events occur in 1900, which matches the gold answer. It does not introduce contradictory or extraneous information and remains faithful to the implied source citation.", "main_total_tokens": 2409, "main_prompt_tokens": 2131, "main_completion_tokens": 278, "judge_total_tokens": 479, "judge_prompt_tokens": 410, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie described is *June* [Doc 32483]. The plot details state that the protagonist, June (whose name is a calendar month), is a violinist who \"spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and all-hours daydreaming about the beautiful man that's destined just for her\" [Doc 32483].", "latency_ms": 3856.6713333129883, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'June,' matching the gold answer. It accurately reflects the described plot and explicitly connects the character's name to a calendar month, consistent with the given description and citation, without adding unsupported details.", "main_total_tokens": 3342, "main_prompt_tokens": 2559, "main_completion_tokens": 783, "judge_total_tokens": 594, "judge_prompt_tokens": 516, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1488.257646560669, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific factual answer (the highest rated Russian-language movie released in 1980, which is given in the gold). The generated answer is simply 'I don't know,' which is not correct, but it does not invent or misrepresent any facts, so it is fully faithful to the model's stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 496, "judge_prompt_tokens": 399, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 425.2135753631592, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric rating (7.6). The generated answer says 'I don't know,' which is not the correct value, so correctness is 0. However, the model does not fabricate an incorrect rating and honestly states lack of knowledge, so it is fully faithful to its own uncertainty, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 389, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1472.1200466156006, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 0.9828920819566879, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3364045224324153, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is incorrect. The model, however, simply produced a numeric answer without obvious discrepancies with its own reasoning, so it remains faithful to itself.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 470, "judge_prompt_tokens": 394, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 345.0613021850586, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. However, the model did not introduce any additional claims beyond giving the wrong count, so it is faithful to its own (albeit incorrect) reasoning.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 476, "judge_prompt_tokens": 395, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1669.7273254394531, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer admits not knowing and does not provide an incorrect alternative, so it is not correct with respect to the gold answer 'Fantômas.' However, it is faithful to the model's uncertainty and does not hallucinate information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 467, "judge_prompt_tokens": 389, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is *Kill Cruise* [Doc 11205].", "latency_ms": 1027.144193649292, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' which matches the gold answer exactly. It does not introduce any contradictory or extraneous information beyond citing a document, so it is fully faithful to the given QA pair.", "main_total_tokens": 2321, "main_prompt_tokens": 2294, "main_completion_tokens": 27, "judge_total_tokens": 485, "judge_prompt_tokens": 409, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is *Won't You Be My Neighbor?* [Doc 84838].", "latency_ms": 835.5803489685059, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'Won't You Be My Neighbor?' which matches the gold answer exactly. It is faithful to the information given and adds only a harmless citation marker without altering the content.", "main_total_tokens": 2535, "main_prompt_tokens": 2490, "main_completion_tokens": 45, "judge_total_tokens": 515, "judge_prompt_tokens": 441, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The context provides release years for several titles:\n\n*   **1997** for *Year of the Horse* [Doc 19452]\n*   **2023** for *YEAR* [Doc 107925] and *Greatest Geek Year Ever! 1982* [Doc 134707]\n*   **2014** for *What It Was* [Doc 65131] and *Another Year* [Doc 62952]\n*   **2002** for *Year of the Devil* [Doc 28274] and *The Last Year* [Doc 28876]\n*   **2007** for *The Year After* [Doc 40870] and *The Year of the Wolf* [Doc 39815]\n*   **1984** for *Year of the Jellyfish* [Doc 4092]\n*   **1994** for *The Year of the Dog* [Doc 15249]\n*   **1986** for *Is It Easy to Be Young?* [Doc 6805]\n*   **1995** for *The Year of the Deer* [Doc 17784]\n*   **1985** for *Year of the Dragon* [Doc 5019]\n*   **2005** for *Let It Be* [Doc 35791]", "latency_ms": 1894.4098949432373, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the release year is 2018, but the generated answer lists multiple different years and never mentions 2018 or clearly answers the specific question. It is therefore incorrect. It is also unfaithful because it fabricates a list of titles and years without tying them to the actual question or gold target, and does not provide the correct year from the gold.", "main_total_tokens": 2952, "main_prompt_tokens": 2600, "main_completion_tokens": 352, "judge_total_tokens": 757, "judge_prompt_tokens": 650, "judge_completion_tokens": 107}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is *Astro Boy* (1980) [Doc 112591].\n\nThe description states: \"In 2030 AD, after being abandoned by his creator, a young boy robot is rescued by an Android Advocate, Dr Elefun, who adopts him as his own and guides him in the use of his special powers to aid and protect mankind and robot-kind\" [Doc 112591]. The genres include 'Animation' and the type is 'TV' [Doc 112591].", "latency_ms": 1161.8962287902832, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy', which matches the gold answer. The explanation is fully grounded in the provided description, accurately quoting and referencing the document without adding unsupported claims.", "main_total_tokens": 2707, "main_prompt_tokens": 2571, "main_completion_tokens": 136, "judge_total_tokens": 575, "judge_prompt_tokens": 503, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "The woman and her young son were held captive for 7 years in the movie \"Room\" released in 2015 [Doc 67513].", "latency_ms": 1587.5813961029053, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, matching the gold answer exactly. It is fully faithful to the information provided and does not introduce any contradictions or extraneous claims.", "main_total_tokens": 2300, "main_prompt_tokens": 2055, "main_completion_tokens": 245, "judge_total_tokens": 499, "judge_prompt_tokens": 424, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The gold bullion stumbled across by the robbers at the Brink's-Mat security depot on 26 November 1983 was worth £26m [Doc 133469].", "latency_ms": 1936.399221420288, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the value of gold held at the Brink's-Mat security depot before the 26 November 1983 robbery. The generated answer states that the gold bullion was worth £26m, which matches the provided gold answer (£26m). It does not introduce contradictions or extra unsupported details beyond rephrasing, so it is both correct and faithful to the given information.", "main_total_tokens": 2599, "main_prompt_tokens": 2323, "main_completion_tokens": 276, "judge_total_tokens": 538, "judge_prompt_tokens": 430, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6416.198253631592, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is a numeric value (80.5772), indicating that the correct response exists and is known. The generated answer says \"I don't know,\" which is incorrect given the gold. However, the model does not hallucinate a value and honestly admits uncertainty, so it is fully faithful to its own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 396, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 494.128942489624, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 1 but the generated answer is 0, so the answer is incorrect. The evaluation correctly compares the given gold and generated answers without adding unsupported information, so it is faithful to the provided data.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 455, "judge_prompt_tokens": 385, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "The TV show in which Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight is *Angel Eyes* [Doc 121928].", "latency_ms": 835.874080657959, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Angel Eyes') and correctly identifies the TV show. It stays faithful to the information, with only a minor citation-style addition that does not contradict the content.", "main_total_tokens": 2589, "main_prompt_tokens": 2550, "main_completion_tokens": 39, "judge_total_tokens": 502, "judge_prompt_tokens": 431, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The specific movie is not identified in the question. However, the context provides information on several movies and their languages:\n\n*   *Foreign Language* (2024) is in French (fr) [Doc 110681].\n*   *Lingua Franca* (2020) is in English (en) [Doc 94134].\n*   *Sexo* (2001) is in English (en) [Doc 25596].\n*   *Machete Language* (2012) is in Spanish (es) [Doc 57266].\n*   *On Top of the Whale* (1982) is in Dutch (nl) [Doc 2803].\n*   *I Don't Speak English* (1995) is in Italian (it) [Doc 16759].\n*   *Holy Tongue* (2000) is in Italian (it) [Doc 24678].\n*   *Broken English* (2007) is in English (en) [Doc 39024].\n*   *El idioma imposible* (2010) is in Spanish (es) [Doc 51083].\n*   *Broken English* (1996) is in English (en) [Doc 18043].\n*   *El lenguaje del tiempo* (2011) is in Spanish (es) [Doc 51629].\n*   *Spanish Movie* (2009) is in Spanish (es) [Doc 45676].\n*   *I Dream in Another Language* (", "latency_ms": 4968.761444091797, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the language of a specific movie, with the gold answer being Korean. The generated answer does not identify or provide Korean as the language and instead lists multiple unrelated movies and their languages while also claiming the movie is not identified. This is incorrect relative to the gold answer and unfaithful to the task, which expects a single language, not a discussion of multiple films.", "main_total_tokens": 3406, "main_prompt_tokens": 2384, "main_completion_tokens": 1022, "judge_total_tokens": 793, "judge_prompt_tokens": 685, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 715.7425880432129, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.5, "precision_at_5": 0.2, "ndcg_at_5": 0.6131471927654584, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.6131471927654584, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a count of movies (2), but the generated answer is 1, which is numerically incorrect. The model stayed on topic (counting movies matching the described criteria), so it is faithful to the query, but the final value is wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 479, "judge_prompt_tokens": 394, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Based strictly on the provided context, there are no movies listed that were released in 1979. Therefore, the best-rated movie from that year cannot be determined.", "latency_ms": 2686.5992546081543, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Given the gold answer that the database contains no movies from 1979, the generated answer correctly and faithfully explains that, based on the provided context, no best-rated movie from 1979 can be determined. It does not introduce external information and adheres to the same limitation, so it is both correct and faithful to the given context.", "main_total_tokens": 2909, "main_prompt_tokens": 2426, "main_completion_tokens": 483, "judge_total_tokens": 527, "judge_prompt_tokens": 428, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "Answer not found in the context.", "latency_ms": 4312.762975692749, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly indicates that the answer cannot be found in the given context, which is consistent with the gold response that the movie does not exist in the database. It does not introduce fabricated information and remains faithful to the provided context and question limitations.", "main_total_tokens": 3531, "main_prompt_tokens": 2745, "main_completion_tokens": 786, "judge_total_tokens": 488, "judge_prompt_tokens": 407, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there are no movies from 1980 about the Chernobyl disaster, which is accurate because the Chernobyl disaster occurred in 1986. The generated answer is faithful to the question’s information and does not introduce any unsupported claims, just an additional suggestion to check the event time period.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 533, "judge_prompt_tokens": 434, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 262.82286643981934, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer ('0') is factually incorrect because the gold answer is 4. However, the answer is internally consistent and does not contradict itself, so it is fully faithful to its own content and reasoning.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 462, "judge_prompt_tokens": 388, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 373.7034797668457, "retrieval_recall": 0.0, "retrieval_mrr": 0.125, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.19342640361727081, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, while the generated answer is 1, so the content is factually incorrect relative to the labeled truth. However, the model directly answered the question without adding extraneous or hallucinated details, so it is faithful to its own (albeit incorrect) claim.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 476, "judge_prompt_tokens": 386, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 299.37195777893066, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show about Super Mario Bros released in the 1990s, while the generated answer is 0. This is factually incorrect relative to the gold, so correctness is 0. The generated answer does not conflict with any information it previously introduced or fabricate a rationale; it just gives a wrong number, so faithfulness to its own content is preserved, giving a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 507, "judge_prompt_tokens": 389, "judge_completion_tokens": 118}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 224.2140769958496, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (0) and directly responds to the question without adding unsupported claims, so it is both correct and faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 451, "judge_prompt_tokens": 391, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 241.0275936126709, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer given the gold label is 'Naruto'. The generated answer says 'I don't know', which is incorrect. However, it is faithful to the model's stated uncertainty and does not hallucinate an answer, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 467, "judge_prompt_tokens": 386, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "Based strictly on the context provided, the release date for a Jurassic Park/Power Ranger crossover movie is not mentioned. The documents list individual titles for *Jurassic Park* [Doc 14060] and various *Power Rangers* movies [Doc 16623, Doc 19350, Doc 76939], but none describe a crossover between the two franchises.", "latency_ms": 2264.787197113037, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates that the correct response is that such a movie was not released. The generated answer instead states that the release date is not mentioned in the provided documents, implying uncertainty about the movie’s existence rather than affirming that it does not exist. Therefore it does not correctly answer the question and is not faithful to the gold answer’s core fact.", "main_total_tokens": 2812, "main_prompt_tokens": 2425, "main_completion_tokens": 387, "judge_total_tokens": 560, "judge_prompt_tokens": 458, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Based strictly on the provided context, there is no movie listed where Barbie fights the Predator.", "latency_ms": 2892.1799659729004, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "Both answers convey that no such movie exists. The gold answer explicitly states the movie was not released, while the generated answer says there is no movie listed where Barbie fights the Predator. This is effectively correct in context, but slightly less direct than the gold, so faithfulness is high but not perfect.", "main_total_tokens": 3300, "main_prompt_tokens": 2759, "main_completion_tokens": 541, "judge_total_tokens": 493, "judge_prompt_tokens": 400, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "Based strictly on the provided context, there is no information regarding the release years or titles of any Star Wars movies [Doc 72959, Doc 72779, Doc 19112, Doc 5364, Doc 10943, Doc 14714, Doc 44637, Doc 5908, Doc 21169, Doc 20060, Doc 5237].", "latency_ms": 3049.7944355010986, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the 4th oldest Star Wars movie, with the gold answer being a specific title. The generated answer claims there is no information in the provided context about release years or titles, which contradicts the known gold answer. Thus, it is factually incorrect with respect to the task. However, it is faithful to the stated constraint of using only the provided documents, explicitly basing its response on the lack of information in them, so faithfulness is high.", "main_total_tokens": 3376, "main_prompt_tokens": 2763, "main_completion_tokens": 613, "judge_total_tokens": 594, "judge_prompt_tokens": 467, "judge_completion_tokens": 127}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The context provides release years for multiple titles:\n\n*   *Year of the Horse* was released in 1997 [Doc 19452].\n*   *YEAR* was released in 2023 [Doc 107925].\n*   *What It Was* was released in 2014 [Doc 65131].\n*   *Year of the Devil* was released in", "latency_ms": 4715.765714645386, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question expects the single year 1986, but the generated answer lists several different titles and years and never mentions 1986, so it is incorrect. It also fabricates or incompletely cites context (e.g., 'Year of the Devil' without a year) and does not clearly answer the specific question based on the gold answer, so it is only partially faithful to the intended context.", "main_total_tokens": 3622, "main_prompt_tokens": 2600, "main_completion_tokens": 1022, "judge_total_tokens": 561, "judge_prompt_tokens": 453, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 351.99904441833496, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is a specific numeric rating (8.466). The generated answer says 'I don't know,' which is not the correct rating, so correctness is 0. However, the model is honest about its lack of knowledge rather than fabricating an answer, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 489, "judge_prompt_tokens": 398, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The TV show about two comic book fanboys working in a secret hospital for superheroes is *Mighty Med* [Doc 121125].", "latency_ms": 639.5635604858398, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the show as 'Mighty Med' and matches the gold answer exactly, with no extra unsupported claims. Therefore it is both correct and faithful.", "main_total_tokens": 2229, "main_prompt_tokens": 2197, "main_completion_tokens": 32, "judge_total_tokens": 485, "judge_prompt_tokens": 420, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "The seven prisoners escaped from a prison in Argentina [Doc 53582]. Specifically, the escape was from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 1923.743486404419, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the country of the prison from which seven prisoners escaped. Both the gold answer and the generated answer state Argentina, and there is no contradiction or fabrication beyond what is implied by the reference. The added detail about Villa Devoto prison in Buenos Aires is consistent with the cited document and does not conflict with the gold answer.", "main_total_tokens": 3032, "main_prompt_tokens": 2767, "main_completion_tokens": 265, "judge_total_tokens": 526, "judge_prompt_tokens": 428, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The apprentice, Li Nai, falls into a trap designed by Jiang Jie [Doc 135231].", "latency_ms": 1848.3021259307861, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the apprentice falls into Jiang Jie's trap, matching the gold answer. The added detail naming the apprentice as Li Nai does not contradict the given information, so the response is both correct and faithful to the source.", "main_total_tokens": 2628, "main_prompt_tokens": 2353, "main_completion_tokens": 275, "judge_total_tokens": 483, "judge_prompt_tokens": 404, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 552.9501438140869, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.7095272044910244, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.7095272044910244, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer per the gold is 4, but the generated answer is 0, which is numerically incorrect. The evaluation itself is faithful to the provided gold and generated outputs.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 453, "judge_prompt_tokens": 385, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "47", "latency_ms": 219.70462799072266, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '47' is factually incorrect given the gold statement that there are no movies with both Bruce Lee and Steven Seagal, and it is not faithful to the gold reference response, which explicitly states the inability to find any such movies.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 476, "judge_prompt_tokens": 395, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of *Tokyo Mater* (2008) is 6.3 [Doc 42750].", "latency_ms": 674.2527484893799, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the rating of Tokyo Mater (2008) is 6.3, matching the gold answer exactly. It is also faithful because it directly uses the provided information without adding unsupported details.", "main_total_tokens": 2365, "main_prompt_tokens": 2338, "main_completion_tokens": 27, "judge_total_tokens": 479, "judge_prompt_tokens": 405, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "", "latency_ms": 4911.4580154418945, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "No generated answer was provided to evaluate, so correctness and faithfulness cannot be confirmed.", "main_total_tokens": 3569, "main_prompt_tokens": 2546, "main_completion_tokens": 1023, "judge_total_tokens": 427, "judge_prompt_tokens": 379, "judge_completion_tokens": 48}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie (TV show) about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 823.1260776519775, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the title as 'Confronting a Serial Killer,' which matches the gold answer exactly. It remains faithful to the question and does not introduce any contradictory or extraneous claims beyond clarifying it as a movie/TV show, which is accurate context.", "main_total_tokens": 2405, "main_prompt_tokens": 2351, "main_completion_tokens": 54, "judge_total_tokens": 542, "judge_prompt_tokens": 456, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of *Armageddon* (1998) is 7.838 [Doc 20603].", "latency_ms": 745.7365989685059, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and directly reflects the information given, so it is both correct and faithful.", "main_total_tokens": 2532, "main_prompt_tokens": 2502, "main_completion_tokens": 30, "judge_total_tokens": 464, "judge_prompt_tokens": 409, "judge_completion_tokens": 55}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].\n\nIt is classified as a Movie, released in 1999, and its genres include 'Mystery', 'Thriller', and 'Drama' [Doc 22098].", "latency_ms": 909.8191261291504, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'The Sixth Sense,' which matches the gold answer exactly. The additional details about its classification, release year, and genres are consistent with common knowledge about the film and do not contradict the question or hallucinate irrelevant facts. Thus, the answer is both correct and faithful to the implied source.", "main_total_tokens": 2317, "main_prompt_tokens": 2244, "main_completion_tokens": 73, "judge_total_tokens": 553, "judge_prompt_tokens": 455, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The movie *The Transporter* was released in 2002 [Doc 27318].", "latency_ms": 787.726640701294, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, which matches the gold answer. It is also faithful, as it directly uses the provided information without introducing unsupported details.", "main_total_tokens": 2570, "main_prompt_tokens": 2546, "main_completion_tokens": 24, "judge_total_tokens": 469, "judge_prompt_tokens": 399, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 movie in 'The Transporter' franchise is *The Transporter Refueled* [Doc 67500].", "latency_ms": 2299.471378326416, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, 'The Transporter Refueled', and does not introduce any contradictions or extraneous fabricated details. It is both correct and faithful to the reference.", "main_total_tokens": 2973, "main_prompt_tokens": 2572, "main_completion_tokens": 401, "judge_total_tokens": 491, "judge_prompt_tokens": 421, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The movie 'The Transporter Refueled' was released in 2015 [Doc 67500].", "latency_ms": 1617.4259185791016, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, which matches the gold answer. It does not introduce any contradictions or unsupported claims relative to the provided information, so it is both correct and faithful.", "main_total_tokens": 2678, "main_prompt_tokens": 2447, "main_completion_tokens": 231, "judge_total_tokens": 487, "judge_prompt_tokens": 406, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "Based strictly on the context provided, there is no movie classified as a \"Romantic Comedy\" in which Abraham Lincoln comes back from the dead.\n\nThe documents mention movies featuring Abraham Lincoln and the undead, such as *Abraham Lincoln vs. Zombies* (Genres: Horror, Action, Thriller) [Doc 57213] and *Abraham Lincoln: Vampire Hunter* (Genres: Action, Fantasy, Horror) [Doc 54943].\n\nA movie where former U.S. presidents come back from the dead as zombies is listed (*Re-Elected*), but its genres are Horror and Comedy, not Romantic Comedy [Doc 94414].", "latency_ms": 4769.478559494019, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a romantic comedy where Abraham Lincoln comes back from the dead. The gold answer says such a movie description cannot be found. The generated answer correctly states that, based on the provided context, there is no romantic comedy fitting that description, and it supports this by listing related films and their genres, noting they are horror/action/comedy rather than romantic comedy. This is correct with respect to the question and faithful to the given documents, without fabricating a nonexistent movie.", "main_total_tokens": 3122, "main_prompt_tokens": 2245, "main_completion_tokens": 877, "judge_total_tokens": 646, "judge_prompt_tokens": 519, "judge_completion_tokens": 127}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by Ernest Hemingway is *Hello Hemingway* [Doc 11652]. The plot describes a young girl whose aspirations are \"paralleled with her reading of Ernest Hemingway's 'The Old Man and the Sea'\" [Doc 11652].", "latency_ms": 4703.566789627075, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The question asks for a movie that alludes to a manuscript written by Ernest Hemingway. The gold answer is 'Havana Darkness', but the generated answer gives 'Hello Hemingway', which is a different film and refers to Hemingway’s published work 'The Old Man and the Sea', not an unpublished manuscript. Thus the answer is incorrect. However, the answer is internally consistent and grounded in the cited document, so it is largely faithful to its own source, though it misinterprets the question’s requirement about a manuscript.", "main_total_tokens": 3103, "main_prompt_tokens": 2247, "main_completion_tokens": 856, "judge_total_tokens": 583, "judge_prompt_tokens": 447, "judge_completion_tokens": 136}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "The TV show in which Doll heads to Hollywood to be with her childhood best friend Em is *Doll & Em* [Doc 122259].", "latency_ms": 915.0140285491943, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em' and directly matches the gold answer. It is fully consistent with the given information and does not introduce unsupported content.", "main_total_tokens": 2265, "main_prompt_tokens": 2232, "main_completion_tokens": 33, "judge_total_tokens": 487, "judge_prompt_tokens": 419, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The 2007 movie is *Hallam Foe* [Doc 39389]. The plot description states that \"Hallam's talent for spying on people... [is] Driven to expose the true cause of his mother's death\" [Doc 39389].", "latency_ms": 3919.2521572113037, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2007 movie as 'Hallam Foe,' which matches the gold answer. It is faithful to the provided document citation and does not invent unsupported details beyond what is referenced.", "main_total_tokens": 3250, "main_prompt_tokens": 2486, "main_completion_tokens": 764, "judge_total_tokens": 525, "judge_prompt_tokens": 452, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Based strictly on the provided context, the answer cannot be found. The documents discuss titles and plots involving friends and foes, but they do not mention a specific person whose last name is analyzed to sound like \"friend\" or \"foe.\"", "latency_ms": 2596.8565940856934, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is a specific label ('A foe'), so the question clearly has a determinate answer. The generated response incorrectly claims that the answer cannot be found from the provided context, which contradicts the gold. Thus, it is factually incorrect with respect to the task. However, it is faithful in that it does not invent any details beyond stating that it cannot find the answer in the documents; it stays within the given (though unspecified here) context and does not hallucinate specific content.", "main_total_tokens": 2781, "main_prompt_tokens": 2403, "main_completion_tokens": 378, "judge_total_tokens": 565, "judge_prompt_tokens": 434, "judge_completion_tokens": 131}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "Sam, the main character in the 2017 TV show *Atypical*, is 18 years old [Doc 124583].", "latency_ms": 1592.6790237426758, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, which matches the gold answer. It does not introduce contradictions or unsupported details beyond a generic citation marker, so it is both correct and faithful.", "main_total_tokens": 2435, "main_prompt_tokens": 2210, "main_completion_tokens": 225, "judge_total_tokens": 497, "judge_prompt_tokens": 415, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam, the 18-year-old character in the 2017 TV show *Atypical*, is on the autism spectrum [Doc 124583].", "latency_ms": 661.2441539764404, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which matches the gold answer. It is fully supported by the provided information and does not introduce contradictions or unsupported claims.", "main_total_tokens": 2111, "main_prompt_tokens": 2072, "main_completion_tokens": 39, "judge_total_tokens": 485, "judge_prompt_tokens": 418, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Sandra was born in Chamonix [Doc 59659].", "latency_ms": 1702.1524906158447, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, matching the gold answer exactly, and does not introduce any unsupported information beyond citing a document reference.", "main_total_tokens": 2915, "main_prompt_tokens": 2690, "main_completion_tokens": 225, "judge_total_tokens": 474, "judge_prompt_tokens": 410, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481]", "latency_ms": 1970.6578254699707, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and does not introduce any unsupported information. It is both correct and faithful.", "main_total_tokens": 2899, "main_prompt_tokens": 2567, "main_completion_tokens": 332, "judge_total_tokens": 456, "judge_prompt_tokens": 398, "judge_completion_tokens": 58}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Tabaluga is the crown-prince of Greenland [Doc 114810].", "latency_ms": 1972.7537631988525, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which matches the gold answer exactly. It is faithful to the given information and does not introduce contradictions or extraneous claims.", "main_total_tokens": 2823, "main_prompt_tokens": 2505, "main_completion_tokens": 318, "judge_total_tokens": 482, "judge_prompt_tokens": 410, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers\" is \"One Afternoon\" [Doc 96838].", "latency_ms": 780.5342674255371, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'One Afternoon,' matching the gold answer exactly. It also remains faithful to the given description and does not introduce contradictions or unsupported claims.", "main_total_tokens": 2161, "main_prompt_tokens": 2114, "main_completion_tokens": 47, "judge_total_tokens": 514, "judge_prompt_tokens": 448, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The Italian language movie is *The Life Ahead* [Doc 93260]. The plot describes a Holocaust survivor with a daycare business in seaside Italy who \"takes in a 12-year-old street kid who recently robbed her\" [Doc 93260]. Its rating is 6.746 [Doc 93260].", "latency_ms": 4036.9603633880615, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title of a low-rated Italian-language movie whose plot involves a Holocaust survivor taking care of a 12-year-old street kid who recently robbed her. Both the gold answer and the generated answer identify the film as \"The Life Ahead.\" The generated answer correctly describes the plot details and notes it is Italian-language and low-rated, matching the gold answer. There are no contradictions or hallucinations relative to the provided information, so the response is both correct and faithful.", "main_total_tokens": 3193, "main_prompt_tokens": 2305, "main_completion_tokens": 888, "judge_total_tokens": 589, "judge_prompt_tokens": 462, "judge_completion_tokens": 127}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 778.003454208374, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and adds accurate contextual detail about him founding the McLaren racing team, which is consistent with the gold answer and does not contradict it.", "main_total_tokens": 2493, "main_prompt_tokens": 2458, "main_completion_tokens": 35, "judge_total_tokens": 492, "judge_prompt_tokens": 417, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The context does not specify who \"he\" refers to, and therefore the name of", "latency_ms": 5722.996950149536, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates that the correct response is 'The McLaren Motor Racing team.' The generated answer instead claims the context does not specify who 'he' is and does not provide the team name, so it is factually incorrect relative to the gold. However, given only the partial generated text and no source passage, it appears internally consistent with what it asserts (it does not introduce unsupported facts) and thus is faithful to its own reasoning, even though it is wrong with respect to the gold answer.", "main_total_tokens": 3651, "main_prompt_tokens": 2629, "main_completion_tokens": 1022, "judge_total_tokens": 532, "judge_prompt_tokens": 400, "judge_completion_tokens": 132}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "", "latency_ms": 5244.786739349365, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "No answer was generated to compare against the gold label, so the response is neither correct nor faithful to the provided reference.", "main_total_tokens": 3562, "main_prompt_tokens": 2539, "main_completion_tokens": 1023, "judge_total_tokens": 432, "judge_prompt_tokens": 377, "judge_completion_tokens": 55}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based strictly on the context provided, there is no mention of John McClane blowing up the Great Wall of China in any 2013 movie [Doc 58791, Doc 58760, Doc 58753, Doc 61845].\n\nThe 2013 movie featuring John McClane, *A Good Day to Die Hard*, describes him traveling to Moscow to stop a nuclear weapons heist [Doc 58791].", "latency_ms": 3131.230354309082, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.5, "justification": "The question is essentially asking for a movie that does not exist; the correct response is to say there is no such movie, which the Generated answer does. So correctness with respect to the user query is high. However, the evaluation context suggests the system only has the Gold answer ('I do not have a movie like this in my database') as its source, and the Generated response introduces detailed information about *A Good Day to Die Hard* plus document IDs that are not present in the provided gold. Because the instructions say to judge faithfulness strictly based on the provided context, referencing specific documents and plot details that are not in the given Gold answer is not fully grounded, so faithfulness is only partial.", "main_total_tokens": 3230, "main_prompt_tokens": 2637, "main_completion_tokens": 593, "judge_total_tokens": 659, "judge_prompt_tokens": 485, "judge_completion_tokens": 174}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with that description is *Tomie: Forbidden Fruit* [Doc 28589].", "latency_ms": 3304.7993183135986, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the film as 'Tomie: Forbidden Fruit'. There are no inaccuracies or hallucinations, and the citation style does not contradict the given information.", "main_total_tokens": 3211, "main_prompt_tokens": 2533, "main_completion_tokens": 678, "judge_total_tokens": 513, "judge_prompt_tokens": 442, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 677.3333549499512, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, stating that Nino and Caterina have been married for 65 years. It does not introduce any unsupported information beyond what is in the provided gold answer, so it is both correct and faithful.", "main_total_tokens": 2875, "main_prompt_tokens": 2853, "main_completion_tokens": 22, "judge_total_tokens": 491, "judge_prompt_tokens": 412, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In the movie *Scorpion King: Book of Souls* (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 994.0359592437744, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly states that in 'Scorpion King: Book of Souls' (2018), the Scorpion King and Tala search for the Book of Souls, matching the gold answer. However, it adds an unsupported citation/reference '[Doc 82582]' which is fabricated given the context, so it is not fully faithful to the provided information.", "main_total_tokens": 2751, "main_prompt_tokens": 2708, "main_completion_tokens": 43, "judge_total_tokens": 534, "judge_prompt_tokens": 433, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for the movie \"Zombies on the Moon\" is not available.", "latency_ms": 2277.951955795288, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, i.e., the release year is not available. The generated answer correctly reflects that the release year is not available based strictly on the given context. It does not introduce unsupported facts and aligns with the gold reasoning, so it is both correct and faithful.", "main_total_tokens": 2720, "main_prompt_tokens": 2272, "main_completion_tokens": 448, "judge_total_tokens": 509, "judge_prompt_tokens": 415, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Galactic baker\" is not provided in the context documents.", "latency_ms": 1882.0412158966064, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the release year is not available; the model does not hallucinate a year and explicitly notes the lack of information in the provided context, so it is both correct and faithful.", "main_total_tokens": 2908, "main_prompt_tokens": 2566, "main_completion_tokens": 342, "judge_total_tokens": 481, "judge_prompt_tokens": 406, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for \"Return of the Null Pointer\" is not available.", "latency_ms": 2311.117649078369, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year is not available, which matches the gold answer indicating the movie is not in the database. It does not introduce any new or fabricated information and is fully aligned with the given context.", "main_total_tokens": 2910, "main_prompt_tokens": 2508, "main_completion_tokens": 402, "judge_total_tokens": 489, "judge_prompt_tokens": 413, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for the movie titled \"Data Science: The Movie\" is not available.", "latency_ms": 2495.8810806274414, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year is not available and aligns with the gold response indicating the movie is not in the database. It is both correct and faithful to the provided context.", "main_total_tokens": 2968, "main_prompt_tokens": 2496, "main_completion_tokens": 472, "judge_total_tokens": 485, "judge_prompt_tokens": 416, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for the movie \"Python's Revenge 2\" is not provided in the context. The context includes a movie titled \"Python 2\" released in 2002 [Doc 28408], and several movies with \"Revenge\" in the title, but not the exact combination requested.", "latency_ms": 3381.0386657714844, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database. The generated answer claims the release year is not provided in the context and talks about context documents; this assumes access to external context that is not mentioned in the query. It also fails to directly answer in terms of database presence. Therefore, it is not correct relative to the gold. However, it is internally consistent with its own stated context and does not hallucinate a specific year, so it is largely faithful to its own reasoning.", "main_total_tokens": 3032, "main_prompt_tokens": 2369, "main_completion_tokens": 663, "judge_total_tokens": 582, "judge_prompt_tokens": 453, "judge_completion_tokens": 129}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for the movie \"The Last Pixel\" is not available.", "latency_ms": 2595.698118209839, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year is not available and does not fabricate a date. It is fully consistent with the gold response, which says the movie is not in the database, so the model remains faithful to the provided context.", "main_total_tokens": 2654, "main_prompt_tokens": 2193, "main_completion_tokens": 461, "judge_total_tokens": 490, "judge_prompt_tokens": 411, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for the movie \"The man who ate the sun\" is not available.", "latency_ms": 2101.2651920318604, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year is not available and does not hallucinate a date. This matches the gold response in content and is fully aligned with the provided context, so it is both correct and faithful.", "main_total_tokens": 3092, "main_prompt_tokens": 2741, "main_completion_tokens": 351, "judge_total_tokens": 492, "judge_prompt_tokens": 417, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Based strictly on the provided context, the director of the movie \"The Last Days of Lehman Brothers\" is not mentioned [Doc 46069].", "latency_ms": 2290.752410888672, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the director information is not provided in the database. The generated answer correctly states that, based strictly on the provided context, the director is not mentioned. It does not hallucinate a director and stays within the given context, so it is both correct and faithful.", "main_total_tokens": 3133, "main_prompt_tokens": 2779, "main_completion_tokens": 354, "judge_total_tokens": 514, "judge_prompt_tokens": 427, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Based strictly on the context provided, the movie \"A Love to Die For\" is not listed, and no director information is available for any of the movies mentioned.", "latency_ms": 2565.4704570770264, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says the director info is not provided in the database, so it only claims that the director cannot be answered from the database. The generated answer adds an incorrect claim that the movie is not listed and that no director information is available for any movies mentioned. This is not supported by the gold reference and overgeneralizes beyond the given context, so it is not fully correct or faithful.", "main_total_tokens": 2824, "main_prompt_tokens": 2357, "main_completion_tokens": 467, "judge_total_tokens": 538, "judge_prompt_tokens": 428, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not contain information about who directed the movie \"Journey into Gaza.\"", "latency_ms": 2175.5106449127197, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not contain information about the director of 'Journey into Gaza,' aligning with the gold answer that the director information is not provided in the database. It does not introduce any unsupported details and is fully faithful to the available information.", "main_total_tokens": 2849, "main_prompt_tokens": 2504, "main_completion_tokens": 345, "judge_total_tokens": 494, "judge_prompt_tokens": 410, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Based strictly on the provided context, the director of the movie \"The Last Night at Tremore Beach\" is not mentioned [Doc 135098].", "latency_ms": 996.2589740753174, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned in the provided context and explicitly bases this on the given document, matching the gold answer that the database does not provide this information. It does not introduce external facts and remains faithful to the constraint of using only the provided context.", "main_total_tokens": 2111, "main_prompt_tokens": 2077, "main_completion_tokens": 34, "judge_total_tokens": 514, "judge_prompt_tokens": 427, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The event described happens in the movie *Rise of the Mummy* [Doc 98375].\n\n*   **Rise of the Mummy** (2021): \"A group of archaeology students awaken an ancient mummy\" [Doc 98375].\n\nA similar event occurs in *Ancient Evil: Scream of the Mummy*:\n\n*   **Ancient Evil: Scream of the Mummy** (2000): \"Six young archaeology students discover the remains of an ancient Aztec mummy and accidentally unleash the fury of an evil god\" [Doc 24655].", "latency_ms": 1273.6778259277344, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 'Rise of the Mummy' as the movie matching the described plot, which aligns with the gold answer. It also faithfully reflects the provided documents, citing both the main match and a related similar film without contradicting the source content.", "main_total_tokens": 2365, "main_prompt_tokens": 2243, "main_completion_tokens": 122, "judge_total_tokens": 590, "judge_prompt_tokens": 507, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse and flees is *Stranger in My Bed* [Doc 35237].\n\nThe plot description states: \"To escape her abusive spouse, Sarah fakes her own death and flees\" [Doc 35237].", "latency_ms": 907.9515933990479, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Stranger in My Bed,' matching the gold answer exactly. It is faithful to the provided document citation and does not introduce unsupported information; it directly paraphrases the given plot description.", "main_total_tokens": 2406, "main_prompt_tokens": 2340, "main_completion_tokens": 66, "judge_total_tokens": 532, "judge_prompt_tokens": 456, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Based strictly on the context provided, the event described happens in the TV show", "latency_ms": 5014.79697227478, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the described event happens in the TV series 'Game of Silence', not a generic, unspecified TV show. It is also unfaithful to the gold answer, which explicitly names 'Game of Silence', whereas the generated response omits this and remains vague.", "main_total_tokens": 3032, "main_prompt_tokens": 2010, "main_completion_tokens": 1022, "judge_total_tokens": 500, "judge_prompt_tokens": 412, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie is **The Craigslist Killer** [Doc 53039].\n\nThe plot description states it is \"Based on a true story. Philip Markoff, a charismatic and popular med student at Boston University, leads a double life as a brutal and cruel sexual deviant who abuses prostitutes he finds via Craigslist\" [Doc 53039].", "latency_ms": 1160.8500480651855, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Craigslist Killer,\" which matches the gold answer. The explanation accurately reflects the plot description and does not introduce contradictions or unsupported claims, so it is both correct and faithful to the provided information.", "main_total_tokens": 2252, "main_prompt_tokens": 2179, "main_completion_tokens": 73, "judge_total_tokens": 537, "judge_prompt_tokens": 459, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which this happens is *Wood Job!* [Doc 66094].\n\nThe plot description states: \"After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.\" [Doc 66094].", "latency_ms": 856.2569618225098, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\" and directly supports it with the quoted plot description that matches the question. There are no contradictions or hallucinations relative to the provided gold answer and cited document.", "main_total_tokens": 2670, "main_prompt_tokens": 2612, "main_completion_tokens": 58, "judge_total_tokens": 526, "judge_prompt_tokens": 451, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The event described happens in the movie *Sesame Street: The Magical Wand Chase* [Doc 78373].", "latency_ms": 2887.047290802002, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase' and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 3161, "main_prompt_tokens": 2639, "main_completion_tokens": 522, "judge_total_tokens": 498, "judge_prompt_tokens": 433, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a 7-year-old boy must find the hero within him to save his parents trapped inside a burning car by a rock fall is *Beyond* [Doc 74597].", "latency_ms": 754.3129920959473, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Beyond,' matching the gold label exactly and restating the plot description without adding contradictory or extraneous information. It is both correct and faithful to the provided data.", "main_total_tokens": 2354, "main_prompt_tokens": 2310, "main_completion_tokens": 44, "judge_total_tokens": 519, "judge_prompt_tokens": 447, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman, fired from a financial corporation during the Asia crisis, returns home with no money is *6ixtynin9* [Doc 22478].", "latency_ms": 698.617696762085, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('6ixtynin9') and does not introduce any additional, unfounded information. It correctly identifies the movie and is faithful to the provided data.", "main_total_tokens": 2251, "main_prompt_tokens": 2211, "main_completion_tokens": 40, "judge_total_tokens": 510, "judge_prompt_tokens": 439, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie in which a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort—a hedonistic island paradise for swingers—and the club's staff must stop the violence or at least hide it, is **Club Dread** [Doc 32410].", "latency_ms": 997.6108074188232, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'Club Dread', matching the gold answer, so correctness is 1. However, it references '[Doc 32410]', an external citation not present in the user-provided context, which makes the justification unfaithful to the given information, so faithfulness is 0.", "main_total_tokens": 2496, "main_prompt_tokens": 2435, "main_completion_tokens": 61, "judge_total_tokens": 575, "judge_prompt_tokens": 479, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is *Toilet* [Doc 48873].", "latency_ms": 852.668285369873, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Toilet,' which matches the gold answer. It is fully consistent with the question and does not introduce any contradictions or unsupported claims aside from a harmless document reference, so it is both correct and faithful.", "main_total_tokens": 2981, "main_prompt_tokens": 2925, "main_completion_tokens": 56, "judge_total_tokens": 545, "judge_prompt_tokens": 465, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie described is **Electric Angel** [Doc 1024].", "latency_ms": 3422.9648113250732, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the given description. The gold answer is 'Electric Angel,' and the generated answer explicitly identifies the movie as 'Electric Angel.' This is correct and directly aligned with the gold, so both correctness and faithfulness are full.", "main_total_tokens": 3232, "main_prompt_tokens": 2544, "main_completion_tokens": 688, "judge_total_tokens": 511, "judge_prompt_tokens": 430, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 included in the context is *Doomsday Stories*, with a rating of 2.167 [Doc 108366].", "latency_ms": 3486.543893814087, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer specifies 'Mega Ape' as the lowest rated movie from 2023, but the generated answer states 'Doomsday Stories' with a rating of 2.167. This contradicts the gold label, so the answer is factually incorrect relative to the task. It also is unfaithful to the gold answer, providing a different movie title and rating than specified.", "main_total_tokens": 3106, "main_prompt_tokens": 2320, "main_completion_tokens": 786, "judge_total_tokens": 520, "judge_prompt_tokens": 415, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 1430.2635192871094, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific answer ('Tea House'), but the generated response is 'I don't know,' which is factually incorrect relative to the gold answer, so correctness is 0. However, the model is not hallucinating; it honestly admits not knowing, so it is fully faithful to its own uncertainty, giving faithfulness a 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 386, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 699.3143558502197, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct movie title 'The Uncut Story of Mewtwo's Origin,' so it is incorrect. However, the model does not hallucinate any incorrect facts and simply admits ignorance, so it is fully faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 486, "judge_prompt_tokens": 395, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 listed in the context is *Cataclysm*, with a rating of 3.889 [Doc 744]. The language of this movie is English (en) [Doc 744].", "latency_ms": 3073.582172393799, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980. The gold answer is 'English'. The generated answer identifies 'Cataclysm' as the lowest-rated 1980 movie from the context and correctly states that its language is English, matching the gold. The explanation explicitly references the supporting document and does not introduce contradictions or hallucinations beyond the given context.", "main_total_tokens": 2923, "main_prompt_tokens": 2292, "main_completion_tokens": 631, "judge_total_tokens": 538, "judge_prompt_tokens": 430, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "", "latency_ms": 4287.779808044434, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The model produced no answer to evaluate, so it did not correctly identify the show and provided no content to assess for faithfulness.", "main_total_tokens": 3712, "main_prompt_tokens": 2689, "main_completion_tokens": 1023, "judge_total_tokens": 448, "judge_prompt_tokens": 391, "judge_completion_tokens": 57}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 867.2714233398438, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and attributes it to the document, so it is both correct and faithful to the provided evidence.", "main_total_tokens": 2576, "main_prompt_tokens": 2555, "main_completion_tokens": 21, "judge_total_tokens": 463, "judge_prompt_tokens": 400, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in multiple years, according to the context:\n\n*   1999 [Doc 22481]\n*   2019 [Doc 88811]\n*   2025 [Doc 112077]", "latency_ms": 2319.6377754211426, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a single year of release, with the gold answer being 2019. The generated answer lists three different years (1999, 2019, 2025) instead of selecting the correct one, so it is not correct. However, it accurately reflects the (stated) context by reporting the years tied to document IDs as given, so it is faithful to the provided context description.", "main_total_tokens": 2793, "main_prompt_tokens": 2374, "main_completion_tokens": 419, "judge_total_tokens": 550, "judge_prompt_tokens": 436, "judge_completion_tokens": 114}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 175.24147033691406, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that 'My Dog Skip' received 279 votes, whereas the generated answer says it received 3 votes. This is numerically incorrect. However, the model did attempt to answer the question directly about how many votes the film received, so it is on-topic and faithful to the query, just factually wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 383, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "12", "latency_ms": 188.7185573577881, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (12 votes), so it is both correct and faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 434, "judge_prompt_tokens": 383, "judge_completion_tokens": 51}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 590.8865928649902, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the film was released in 2009, matching the gold answer. It is fully supported by the provided information, so it is both correct and faithful.", "main_total_tokens": 2487, "main_prompt_tokens": 2460, "main_completion_tokens": 27, "judge_total_tokens": 478, "judge_prompt_tokens": 410, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English (en) [Doc 48035].", "latency_ms": 2123.709201812744, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the original language of 'The King\\'s Speech' is English, matching the gold answer ('en'). It is fully consistent and faithful to the given information.", "main_total_tokens": 3094, "main_prompt_tokens": 2769, "main_completion_tokens": 325, "judge_total_tokens": 475, "judge_prompt_tokens": 405, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "1987 [Doc 6993]", "latency_ms": 4005.2947998046875, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer and staying faithful to the provided information.", "main_total_tokens": 3573, "main_prompt_tokens": 2990, "main_completion_tokens": 583, "judge_total_tokens": 445, "judge_prompt_tokens": 387, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Brain Fluid Explosion Girl", "latency_ms": 4966.808319091797, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which correctly identifies the movie as 'Brain Fluid Explosion Girl'. The response is both correct and fully consistent with the given reference.", "main_total_tokens": 3704, "main_prompt_tokens": 2988, "main_completion_tokens": 716, "judge_total_tokens": 457, "judge_prompt_tokens": 392, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 4851.883888244629, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.4306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.719741384391281, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is correct. It is fully faithful to the provided information.", "main_total_tokens": 3814, "main_prompt_tokens": 3071, "main_completion_tokens": 743, "judge_total_tokens": 447, "judge_prompt_tokens": 394, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 4908.787727355957, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'Nacht der Wölfe.'", "main_total_tokens": 3652, "main_prompt_tokens": 2897, "main_completion_tokens": 755, "judge_total_tokens": 456, "judge_prompt_tokens": 403, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of *Picnic at Hanging Rock* take place in 1900 [Doc 126191].", "latency_ms": 2999.5486736297607, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of 'Picnic at Hanging Rock' take place in 1900, matching the gold answer exactly and staying faithful to the provided information.", "main_total_tokens": 3201, "main_prompt_tokens": 2737, "main_completion_tokens": 464, "judge_total_tokens": 474, "judge_prompt_tokens": 407, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "Iris Blond", "latency_ms": 6116.380214691162, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a 2004 movie about a young violinist whose name is also a calendar month, and the gold answer is 'June.' The generated answer 'Iris Blond' is a different film and does not match the described plot or the specified year, so it is incorrect relative to the gold. It is also unfaithful to the question’s details (year, character name, and premise).", "main_total_tokens": 1090, "main_prompt_tokens": 116, "main_completion_tokens": 974, "judge_total_tokens": 548, "judge_prompt_tokens": 434, "judge_completion_tokens": 114}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 354.77614402770996, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a particular film, while the generated answer says 'I don't know,' which is incorrect given the gold. However, the generated answer does not hallucinate or contradict given information; it simply admits not knowing, so it is fully faithful to the model's stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 399, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 332.26680755615234, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the rating 7.6, while the generated answer says 'I don't know,' which does not provide the correct value. This makes the response incorrect. However, the model does not invent any incorrect facts and honestly states its lack of knowledge, so it is fully faithful to its own (claimed) uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 486, "judge_prompt_tokens": 389, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "12", "latency_ms": 4606.669902801514, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 0.9828920819566879, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3364045224324153, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (12) does not match the gold answer (4), so it is incorrect. The faithfulness score is 1 because the evaluation correctly compares the generated answer to the gold answer without introducing new, unsupported information.", "main_total_tokens": 2448, "main_prompt_tokens": 1567, "main_completion_tokens": 881, "judge_total_tokens": 471, "judge_prompt_tokens": 394, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 326.6472816467285, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 2 such TV shows, but the generated response claims there are none. This is factually incorrect with respect to the gold label, so correctness is 0. The model does not invent specific titles or data; it simply states an inability to find matches, which is faithful to the (assumed) lack of internal knowledge access in this context, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 518, "judge_prompt_tokens": 404, "judge_completion_tokens": 114}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 402.53758430480957, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not correctly identify 'Fantômas' as the answer, so it is incorrect (0 for correctness). However, it does not introduce any incorrect facts or hallucinations; it simply admits not knowing, which is faithful to the lack of knowledge, so faithfulness is high (1).", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 488, "judge_prompt_tokens": 389, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise", "latency_ms": 6092.702388763428, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the movie as 'Kill Cruise.'", "main_total_tokens": 4013, "main_prompt_tokens": 2899, "main_completion_tokens": 1114, "judge_total_tokens": 436, "judge_prompt_tokens": 387, "judge_completion_tokens": 49}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor? [Doc 84838]", "latency_ms": 4542.2022342681885, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, including punctuation, and does not introduce any contradictions or extraneous claims beyond a benign document tag.", "main_total_tokens": 3828, "main_prompt_tokens": 3138, "main_completion_tokens": 690, "judge_total_tokens": 473, "judge_prompt_tokens": 414, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The release years found in the context are:\n1981 [Doc 1423]\n2001 [Doc 26789]\n2005 [Doc 35916]\n2011 [Doc 52527, Doc 52474]\n2013 [Doc 61204]\n2014 [Doc 63036]\n2015 [Doc 71779, Doc 67606]\n2016 [Doc 76553, Doc 73921, Doc 72124]\n2018 [Doc 84838, Doc 86144]\n2019 [Doc 91273]", "latency_ms": 7804.77237701416, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a single year of release, with the gold answer being 2018. The generated answer lists multiple years and does not directly answer the question with a single year, so it is incorrect relative to the gold. It also appears to be describing some separate “context” with document IDs rather than answering the user’s question, so it is not a faithful direct answer to the query either.", "main_total_tokens": 4524, "main_prompt_tokens": 3079, "main_completion_tokens": 1445, "judge_total_tokens": 611, "judge_prompt_tokens": 491, "judge_completion_tokens": 120}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy [Doc 112591]", "latency_ms": 7867.427110671997, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies 'Astro Boy' as the TV series, so correctness is 1. However, it appends an extraneous reference tag '[Doc 112591]' that is not in the gold answer or the prompt context, making the response not fully faithful to the given information.", "main_total_tokens": 4755, "main_prompt_tokens": 3207, "main_completion_tokens": 1548, "judge_total_tokens": 486, "judge_prompt_tokens": 395, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "The woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 4413.191318511963, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer exactly and does not introduce any unsupported information.", "main_total_tokens": 3435, "main_prompt_tokens": 2653, "main_completion_tokens": 782, "judge_total_tokens": 476, "judge_prompt_tokens": 412, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m [Doc 133469].", "latency_ms": 1920.417308807373, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer repeats the correct amount (£26m) exactly as in the provided gold reference, with no additions or contradictions.", "main_total_tokens": 2540, "main_prompt_tokens": 2336, "main_completion_tokens": 204, "judge_total_tokens": 460, "judge_prompt_tokens": 404, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 618.3850765228271, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered that it did not know the answer, while the gold answer is a specific numeric popularity score (80.5772). Therefore the answer is incorrect. However, the response does not invent or misrepresent any facts; it simply states a lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 396, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 276.9176959991455, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that there was 1 Russian language TV show released in 1997, while the generated answer claims it cannot find any movies or shows matching those criteria. This directly contradicts the gold answer, so it is factually incorrect. It is also unfaithful to the gold answer because it does not reflect or approximate the correct count in any way.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 499, "judge_prompt_tokens": 394, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes [Doc 121928]", "latency_ms": 6383.851051330566, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, correctly identifying 'Angel Eyes' as the TV show. It is fully consistent with the given information and does not introduce any unsupported details.", "main_total_tokens": 4325, "main_prompt_tokens": 3173, "main_completion_tokens": 1152, "judge_total_tokens": 470, "judge_prompt_tokens": 403, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The languages of the movies and TV shows listed in the context are:\n\n*   ko (Korean) [Doc 121928]\n*   en (English) [Doc 117561, Doc 25900, Doc 14842, Doc 115156, Doc 11997]\n*   es (Spanish) [Doc 121204]\n*   fr (French) [Doc 132897", "latency_ms": 8736.261367797852, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the language of a specific movie, with the gold answer being simply \"Korean.\" The generated answer instead lists multiple languages for various movies/TV shows and never directly answers the question about the specific movie. This is incorrect with respect to the task and not faithful to the gold answer, which clearly specifies a single language.", "main_total_tokens": 4656, "main_prompt_tokens": 3135, "main_completion_tokens": 1521, "judge_total_tokens": 557, "judge_prompt_tokens": 458, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 659.4514846801758, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.5, "precision_at_5": 0.2, "ndcg_at_5": 0.6131471927654584, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.6131471927654584, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, while the generated answer is 1. Since these differ, the generated answer is incorrect. However, the model did faithfully provide the single numeric answer it computed without adding unsupported details, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 474, "judge_prompt_tokens": 394, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Based strictly on the context provided, there are no movies listed that were released in 1979.", "latency_ms": 5364.262104034424, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there are no movies listed that were released in 1979, which aligns with the gold answer. It does not introduce new information beyond the given context, so it is both correct and faithful.", "main_total_tokens": 4086, "main_prompt_tokens": 3065, "main_completion_tokens": 1021, "judge_total_tokens": 491, "judge_prompt_tokens": 415, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "Based strictly on the context provided, the movie \"Back to the Future part IV\" is not mentioned. The context refers to the series as a \"trilogy\" [Doc 47044, Doc 28792, Doc 113788], and *Back to the Future Part III* is described as \"The final installment\" [Doc 10621].", "latency_ms": 6146.494150161743, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a release year; the correct answer is that the movie does not exist in the database (and in reality, no Part IV exists). The generated answer instead discusses context about the trilogy and does not directly address the question or provide the gold answer, so it is incorrect. However, it stays consistent with the given context (that only three films exist and the third is the final installment), so it is faithful to the provided documents.", "main_total_tokens": 4516, "main_prompt_tokens": 3380, "main_completion_tokens": 1136, "judge_total_tokens": 594, "judge_prompt_tokens": 473, "judge_completion_tokens": 121}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer is factually correct in practice since there were no Chernobyl-disaster movies in 1980, so a popularity score cannot be given. However, relative to the provided gold answer, it is not fully faithful: the gold explicitly states that no such movies could be found, whereas the generated answer instead suggests checking whether the question refers to an event during that time period, implicitly questioning the premise rather than directly stating the unavailability of such movies. This diverges from the gold’s framing and content even though the overall conclusion (no matching movies) is similar.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 581, "judge_prompt_tokens": 434, "judge_completion_tokens": 147}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 305.0673007965088, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 4, but the generated response claims there are none, so it is factually incorrect. However, it does not add fabricated details; it simply states inability to find matches, so it remains faithful (no hallucinated specifics).", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 397, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 5488.177537918091, "retrieval_recall": 0.0, "retrieval_mrr": 0.125, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.19342640361727081, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer is a specific numerical value (2). The generated answer says it cannot find any movies or shows, which contradicts the gold, so it is incorrect (score 0). In terms of faithfulness to the user query and typical model behavior, the response is partially faithful because it honestly expresses inability to find matches rather than hallucinating a number, but it fails to answer the question and conflicts with the provided gold label, so only a partial score is warranted.", "main_total_tokens": 4729, "main_prompt_tokens": 3706, "main_completion_tokens": 1023, "judge_total_tokens": 518, "judge_prompt_tokens": 395, "judge_completion_tokens": 123}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 275.00152587890625, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there was 1 TV show, but the generated answer claims there were none. This is factually incorrect, so correctness is 0. However, the model does not invent specific false shows or details; it explicitly says it cannot find any, which is consistent with its stated uncertainty, so it is fully faithful to its own (albeit incomplete) information, giving faithfulness 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 511, "judge_prompt_tokens": 398, "judge_completion_tokens": 113}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 309.79394912719727, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0 TV shows. The generated answer says it cannot find any movies or shows matching the criteria, which implies there are no such shows. This matches the gold answer, so it is correct. It is also faithful to the question, directly addressing the criteria given (TV shows between 2000 and 2010 about Super Mario Bros).", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 503, "judge_prompt_tokens": 400, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 232.01632499694824, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto'. The generated answer says 'I don't know', which is incorrect given the reference, so correctness is 0. However, the model did not hallucinate; it admitted not knowing, so it is fully faithful to its own (stated) uncertainty, giving faithfulness 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 386, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "Based strictly on the context provided, there is no mention of a Jurassic Park/Power Ranger crossover movie or its release date.", "latency_ms": 5217.449188232422, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clarifies the movie does not exist, while the generated answer only states that the context has no mention of it, implicitly failing to state the crucial fact that it was not released. Thus it is not fully correct relative to the gold, though it is faithful to the provided context.", "main_total_tokens": 3909, "main_prompt_tokens": 3035, "main_completion_tokens": 874, "judge_total_tokens": 500, "judge_prompt_tokens": 410, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "", "latency_ms": 9749.236583709717, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer correctly states that no such movie exists/was released. The generated answer is empty, but we are only evaluating the gold answer relative to the question; it is correct and faithful to the question.", "main_total_tokens": 5012, "main_prompt_tokens": 3388, "main_completion_tokens": 1624, "judge_total_tokens": 456, "judge_prompt_tokens": 383, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "Based strictly on the context provided, there are no official Star Wars movies listed, so it is impossible to determine the 4th oldest one.", "latency_ms": 5469.346761703491, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question expects the specific title 'Star Wars: Droids - The Great Heep' as the 4th oldest Star Wars movie, but the generated answer claims it is impossible to determine and does not provide that title. This is incorrect. It is also unfaithful to the provided gold answer, as it contradicts it rather than attempting to align with or derive it.", "main_total_tokens": 4399, "main_prompt_tokens": 3427, "main_completion_tokens": 972, "judge_total_tokens": 525, "judge_prompt_tokens": 418, "judge_completion_tokens": 107}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "", "latency_ms": 8250.940322875977, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is empty, so it does not provide the correct year (1986) and does not address the question at all, resulting in zero for both correctness and faithfulness.", "main_total_tokens": 4616, "main_prompt_tokens": 3060, "main_completion_tokens": 1556, "judge_total_tokens": 446, "judge_prompt_tokens": 378, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 297.8050708770752, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct rating is 8.466, but the generated answer says it does not know, which is incorrect. However, the model is honest about its uncertainty and does not hallucinate a wrong rating, so it is fully faithful to its own stated knowledge limits.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 482, "judge_prompt_tokens": 398, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med [Doc 121125]", "latency_ms": 6199.661493301392, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med,' matching the gold answer exactly. However, it appends an extraneous reference tag '[Doc 121125]' that is not part of the requested answer. This slightly affects faithfulness to the expected plain answer format, though the core content is correct.", "main_total_tokens": 3926, "main_prompt_tokens": 2807, "main_completion_tokens": 1119, "judge_total_tokens": 497, "judge_prompt_tokens": 400, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina [Doc 53582].", "latency_ms": 3837.677478790283, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Argentina as the country where the prison is located, matching the gold answer exactly and staying fully grounded in the provided information.", "main_total_tokens": 3937, "main_prompt_tokens": 3409, "main_completion_tokens": 528, "judge_total_tokens": 458, "judge_prompt_tokens": 398, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The apprentice, Li Nai, falls into a trap designed by Jiang Jie [Doc 135231].", "latency_ms": 3417.020559310913, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.7, "justification": "The question asks: 'In Sweet Trap (2024), the apprentice falls into whose trap?' The gold answer is simply 'Jiang Jie.' The generated answer states: 'The apprentice, Li Nai, falls into a trap designed by Jiang Jie [Doc 135231].' The core factual claim— that the trap belongs to Jiang Jie—matches the gold, so the main answer is correct. However, the generated response adds extra detail ('the apprentice, Li Nai') and cites a specific document ID that are not present in the gold reference. While the added apprentice name may be correct in context, it is not supported by the given gold snippet, so there is some speculative content beyond the firmly supported part. Therefore, correctness is 1.0 because the main question is answered correctly, but faithfulness is slightly reduced to 0.7 due to the additional unsupported specifics.", "main_total_tokens": 3458, "main_prompt_tokens": 2936, "main_completion_tokens": 522, "judge_total_tokens": 614, "judge_prompt_tokens": 404, "judge_completion_tokens": 210}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 5056.231737136841, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.7095272044910244, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.7095272044910244, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a numeric count (4), but the generated answer claims there are none. This is factually incorrect relative to the gold. However, given the model has limited post-2023 data access, stating inability to find such movies is a faithful reflection of its capabilities rather than hallucinating a number, so faithfulness is high even though correctness is zero.", "main_total_tokens": 4788, "main_prompt_tokens": 3765, "main_completion_tokens": 1023, "judge_total_tokens": 499, "judge_prompt_tokens": 394, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 3416.9130325317383, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer state that no movies (or shows) meeting the criteria can be found. The question is ambiguous, but in context both correctly interpret it as asking for works featuring both Bruce Lee and Steven Seagal together. The generated answer is consistent with the gold answer and does not add incorrect information; it only slightly generalizes to movies or shows, which does not contradict the gold. Thus it is fully correct and faithful.", "main_total_tokens": 3093, "main_prompt_tokens": 2421, "main_completion_tokens": 672, "judge_total_tokens": 527, "judge_prompt_tokens": 406, "judge_completion_tokens": 121}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3 [Doc 42750]", "latency_ms": 724.4846820831299, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (6.3) and does not introduce any extraneous or conflicting information, so it is both correct and faithful.", "main_total_tokens": 2363, "main_prompt_tokens": 2351, "main_completion_tokens": 12, "judge_total_tokens": 456, "judge_prompt_tokens": 393, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The movie *Tokyo Mater* features a drift-style race [Doc 42750].", "latency_ms": 4414.451599121094, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the racing style as drift, matching the gold label, so correctness is high. However, it introduces a specific movie title and a document citation that are not present in the question or gold answer, making the response not fully faithful to the provided context.", "main_total_tokens": 3671, "main_prompt_tokens": 3079, "main_completion_tokens": 592, "judge_total_tokens": 481, "judge_prompt_tokens": 395, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer [Doc 131298]", "latency_ms": 5022.984027862549, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, 'Confronting a Serial Killer', and does not introduce any contradictions or extra claims beyond a harmless bracketed note, so it is both correct and faithful.", "main_total_tokens": 3839, "main_prompt_tokens": 2976, "main_completion_tokens": 863, "judge_total_tokens": 490, "judge_prompt_tokens": 418, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838 [Doc 20603]", "latency_ms": 2564.509153366089, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score value of 7.838 and does not introduce any contradictory or extra information, so it is both correct and faithful.", "main_total_tokens": 2931, "main_prompt_tokens": 2515, "main_completion_tokens": 416, "judge_total_tokens": 459, "judge_prompt_tokens": 395, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense [Doc 22098]", "latency_ms": 8627.941846847534, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'The Sixth Sense' as the 1999 psychological thriller that alludes to an extra sense. It is fully correct and faithful.", "main_total_tokens": 4510, "main_prompt_tokens": 2855, "main_completion_tokens": 1655, "judge_total_tokens": 469, "judge_prompt_tokens": 400, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in 2002 [Doc 27318].", "latency_ms": 2810.866594314575, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer, and it does not introduce any unsupported information beyond that.", "main_total_tokens": 3524, "main_prompt_tokens": 3144, "main_completion_tokens": 380, "judge_total_tokens": 458, "judge_prompt_tokens": 395, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled [Doc 67500].", "latency_ms": 2878.4730434417725, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, including the correct film title 'The Transporter Refueled'. The added bracketed reference tag does not change the correctness or contradict any given context, so the response is both correct and faithful.", "main_total_tokens": 3605, "main_prompt_tokens": 3193, "main_completion_tokens": 412, "judge_total_tokens": 482, "judge_prompt_tokens": 403, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015 [Doc 67500]", "latency_ms": 3632.765293121338, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, which matches the gold answer. The citation tag does not contradict the answer, so the response is both correct and faithful.", "main_total_tokens": 3618, "main_prompt_tokens": 3056, "main_completion_tokens": 562, "judge_total_tokens": 469, "judge_prompt_tokens": 393, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "Based strictly on the provided context, there is no movie identified as a romantic comedy in which Abraham Lincoln comes back from the dead.\n\nThe movies featuring Abraham Lincoln deal with zombies or vampires and are categorized as Horror, Action, Thriller, or Fantasy [Doc 57213, Doc 54943]. The movies categorized as Romantic Comedy do not feature Abraham Lincoln [Doc 47970, Doc 47042, Doc 3294, Doc 58857, Doc 33694].", "latency_ms": 8353.624105453491, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no identified romantic comedy where Abraham Lincoln comes back from the dead, which matches the gold answer. It also faithfully references and aligns with the provided document categories, noting that Lincoln-related films fall under horror/action/fantasy and that romantic comedies listed do not feature him. No unsupported claims are made.", "main_total_tokens": 4381, "main_prompt_tokens": 2855, "main_completion_tokens": 1526, "judge_total_tokens": 585, "judge_prompt_tokens": 486, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "", "latency_ms": 9250.060558319092, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "There is no generated answer to evaluate, so it cannot be considered correct or faithful relative to the gold answer 'Havana Darkness' or the question.", "main_total_tokens": 4573, "main_prompt_tokens": 2830, "main_completion_tokens": 1743, "judge_total_tokens": 450, "judge_prompt_tokens": 389, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show *Doll & Em*", "latency_ms": 3737.50901222229, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em' and stays faithful to the information in the gold answer without adding unsupported details.", "main_total_tokens": 3402, "main_prompt_tokens": 2887, "main_completion_tokens": 515, "judge_total_tokens": 472, "judge_prompt_tokens": 411, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam Foe [Doc 39389]", "latency_ms": 5185.345888137817, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2007 movie as 'Hallam Foe' and does not introduce any extraneous or contradictory information.", "main_total_tokens": 4081, "main_prompt_tokens": 3123, "main_completion_tokens": 958, "judge_total_tokens": 474, "judge_prompt_tokens": 409, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Based strictly on the provided context, there is no mention of a person whose last name is being analyzed to determine if it sounds like \"friend\" or \"foe\" [Doc 104792, Doc 97685, Doc 71781, Doc 61504].", "latency_ms": 7820.11342048645, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is an explicit label: \"A foe.\" The question is asking for a categorical judgment, and the generated answer instead claims the context lacks information and does not provide the required label. This is incorrect relative to the gold. However, the generated answer is faithful to the (stated) lack of context and does not introduce fabricated details, so it is faithful to the provided context while still being wrong compared to the gold.", "main_total_tokens": 4179, "main_prompt_tokens": 2895, "main_completion_tokens": 1284, "judge_total_tokens": 559, "judge_prompt_tokens": 441, "judge_completion_tokens": 118}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18 [Doc 124583]", "latency_ms": 3376.7383098602295, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and does not introduce any contradictions or extraneous claims. It is correct that Sam is 18 in the 2017 TV show Atypical, so the response is both correct and faithful.", "main_total_tokens": 3393, "main_prompt_tokens": 2819, "main_completion_tokens": 574, "judge_total_tokens": 473, "judge_prompt_tokens": 395, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "autism spectrum [Doc 124583]", "latency_ms": 3167.600154876709, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which matches the gold answer. It is faithful to the provided information.", "main_total_tokens": 3052, "main_prompt_tokens": 2656, "main_completion_tokens": 396, "judge_total_tokens": 453, "judge_prompt_tokens": 395, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix [Doc 59659]", "latency_ms": 4817.988872528076, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Chamonix'), and there is no hallucinated or conflicting information. It is both correct and faithful.", "main_total_tokens": 4137, "main_prompt_tokens": 3318, "main_completion_tokens": 819, "judge_total_tokens": 467, "judge_prompt_tokens": 406, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481]", "latency_ms": 4026.309013366699, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and is faithful to the provided information.", "main_total_tokens": 3867, "main_prompt_tokens": 3184, "main_completion_tokens": 683, "judge_total_tokens": 449, "judge_prompt_tokens": 398, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland [Doc 114810].", "latency_ms": 5265.738964080811, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the crown-prince location of Tabaluga, and the gold answer is Greenland. The generated answer is also 'Greenland', matching exactly. It is correct and faithful to the gold answer.", "main_total_tokens": 4012, "main_prompt_tokens": 3102, "main_completion_tokens": 910, "judge_total_tokens": 475, "judge_prompt_tokens": 401, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon [Doc 96838]", "latency_ms": 4452.786445617676, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('One Afternoon') and does not introduce any contradictions or unsupported claims. The additional bracketed document ID is metadata and does not affect the correctness of the movie title.", "main_total_tokens": 3380, "main_prompt_tokens": 2701, "main_completion_tokens": 679, "judge_total_tokens": 484, "judge_prompt_tokens": 412, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The Life Ahead [Doc 93260]", "latency_ms": 6211.413621902466, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('The Life Ahead') and correctly identifies the movie described. It is fully correct and faithful to the reference.", "main_total_tokens": 4223, "main_prompt_tokens": 2923, "main_completion_tokens": 1300, "judge_total_tokens": 468, "judge_prompt_tokens": 407, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary “McLaren” is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 3549.076795578003, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the subject of the 2016 documentary 'McLaren' as Bruce McLaren, matching the gold answer exactly and staying faithful to the given information.", "main_total_tokens": 3611, "main_prompt_tokens": 3042, "main_completion_tokens": 569, "judge_total_tokens": 483, "judge_prompt_tokens": 417, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The McLaren Motor Racing team [Doc 73859].", "latency_ms": 6417.358636856079, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer in content, only adding a citation marker. It is correct and faithful to the reference.", "main_total_tokens": 4101, "main_prompt_tokens": 3019, "main_completion_tokens": 1082, "judge_total_tokens": 452, "judge_prompt_tokens": 395, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "", "latency_ms": 7587.120294570923, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "No generated answer was provided to evaluate, so correctness and faithfulness cannot be confirmed.", "main_total_tokens": 4671, "main_prompt_tokens": 3326, "main_completion_tokens": 1345, "judge_total_tokens": 425, "judge_prompt_tokens": 377, "judge_completion_tokens": 48}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based strictly on the context provided, there is no mention of John McClane blowing up the Great Wall of China in any movie, including the 2013 film *A Good Day to Die Hard* [Doc 58791]. The plot for *A Good Day to Die Hard* states that John McClane travels to Moscow [Doc 58791].", "latency_ms": 6295.928478240967, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer correctly states that there is no such movie in the database, implying the premise is false. The generated answer correctly notes that no movie, including the 2013 film 'A Good Day to Die Hard,' features John McClane blowing up the Great Wall of China, and it aligns with the provided context. Both are consistent and accurate relative to the question and context.", "main_total_tokens": 4482, "main_prompt_tokens": 3269, "main_completion_tokens": 1213, "judge_total_tokens": 579, "judge_prompt_tokens": 471, "judge_completion_tokens": 108}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit [Doc 28589]", "latency_ms": 6290.947914123535, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2002 Japanese horror film as 'Tomie: Forbidden Fruit'. It is fully correct and faithful to the gold reference.", "main_total_tokens": 4431, "main_prompt_tokens": 3168, "main_completion_tokens": 1263, "judge_total_tokens": 496, "judge_prompt_tokens": 428, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 [Doc 98566]", "latency_ms": 3930.82332611084, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer (65) matches the gold answer (65) exactly and does not introduce any unsupported information.", "main_total_tokens": 4170, "main_prompt_tokens": 3486, "main_completion_tokens": 684, "judge_total_tokens": 455, "judge_prompt_tokens": 402, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King and Tala search for a legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 4304.87585067749, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that they search for The Book of Souls, which matches the gold answer and does not introduce any contradictory details. Calling it a legendary relic is consistent with the film’s premise and remains faithful to the source.", "main_total_tokens": 4013, "main_prompt_tokens": 3337, "main_completion_tokens": 676, "judge_total_tokens": 494, "judge_prompt_tokens": 417, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I am unable to answer this question as the title \"Zombies on the Moon\" is not mentioned in the provided context.", "latency_ms": 5445.02854347229, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database. The generated answer similarly states it cannot answer because the title is not in the provided context. Both convey inability to answer; no incorrect year is given, so correctness is preserved. The generated answer is faithful to the (implied) context limitation and does not fabricate information.", "main_total_tokens": 3781, "main_prompt_tokens": 2840, "main_completion_tokens": 941, "judge_total_tokens": 513, "judge_prompt_tokens": 416, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The requested title, \"The Galactic baker,\" is not present in the provided context.", "latency_ms": 4643.790483474731, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database and thus does not provide a release year. The generated answer similarly says the title is not present in the provided context and also does not invent a release year. This is correct with respect to the task and faithful to the given information.", "main_total_tokens": 3987, "main_prompt_tokens": 3183, "main_completion_tokens": 804, "judge_total_tokens": 495, "judge_prompt_tokens": 406, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I am unable to answer this question because the title \"Return of the Null Pointer\" is not mentioned in the provided context.", "latency_ms": 5039.285659790039, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states inability to answer, which aligns with the gold response that the movie is not in the database. It does not introduce unsupported facts and remains faithful to the given information and context.", "main_total_tokens": 3966, "main_prompt_tokens": 3088, "main_completion_tokens": 878, "judge_total_tokens": 487, "judge_prompt_tokens": 416, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "", "latency_ms": 6788.694858551025, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database, so the correct response is to state that limitation rather than guess a year. The generated answer is empty, but it does not contradict the gold answer or introduce incorrect information. Given the task is to compare generated vs. gold, an ideal generated answer would mirror the gold. Since no incorrect year or claim is made, it is effectively consistent and faithful to the gold intent (no hallucination).", "main_total_tokens": 4317, "main_prompt_tokens": 3066, "main_completion_tokens": 1251, "judge_total_tokens": 513, "judge_prompt_tokens": 392, "judge_completion_tokens": 121}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The context does not contain information about the release year of \"Python's Revenge 2\".", "latency_ms": 7308.582305908203, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer and the generated answer are equivalent in meaning: both correctly state that the information about the movie's release year is not available in the provided data/database. The generated answer does not introduce any unsupported claims and remains faithful to the context limitation mentioned in the gold.", "main_total_tokens": 4372, "main_prompt_tokens": 2985, "main_completion_tokens": 1387, "judge_total_tokens": 494, "judge_prompt_tokens": 409, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the movie titled \"The Last Pixel\" is not listed.", "latency_ms": 4680.005311965942, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database. The generated answer correctly reflects that the movie is not listed based on the provided context. It does not invent a release year or contradict the gold, so it is both correct and faithful.", "main_total_tokens": 3509, "main_prompt_tokens": 2765, "main_completion_tokens": 744, "judge_total_tokens": 489, "judge_prompt_tokens": 408, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I am unable to answer this question based strictly on the provided context, as the movie \"The man who ate the sun\" is not listed.", "latency_ms": 5041.959762573242, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the system cannot answer because the movie is not in the database/provided context. This is correct given the context and faithful to it.", "main_total_tokens": 4231, "main_prompt_tokens": 3366, "main_completion_tokens": 865, "judge_total_tokens": 487, "judge_prompt_tokens": 421, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not state who directed the movie \"The Last Days of Lehman Brothers\" [Doc 46069].", "latency_ms": 4285.189390182495, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the system cannot answer because the database lacks the director information. The generated answer correctly states that the provided context does not state who directed the movie, which is equivalent in meaning. It does not hallucinate a director and stays within the given context, so it is both correct and faithful.", "main_total_tokens": 4104, "main_prompt_tokens": 3383, "main_completion_tokens": 721, "judge_total_tokens": 514, "judge_prompt_tokens": 422, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not contain information about the movie titled \"A Love to Die For,\" nor does it list the director for any of the movies mentioned.", "latency_ms": 5289.953708648682, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided context/database. The generated answer is correct with respect to the task and remains faithful to the given information, not inventing any director name.", "main_total_tokens": 3875, "main_prompt_tokens": 2940, "main_completion_tokens": 935, "judge_total_tokens": 502, "judge_prompt_tokens": 426, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Based strictly on the context provided, the director of the movie \"Journey into Gaza\" is not mentioned [Doc 111360].", "latency_ms": 4616.893768310547, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director of the movie \"Journey into Gaza\" is not mentioned in the provided context, which matches the gold answer that the director information is not in the database. It is fully consistent and faithful to the given context and does not introduce unsupported information.", "main_total_tokens": 3859, "main_prompt_tokens": 3102, "main_completion_tokens": 757, "judge_total_tokens": 506, "judge_prompt_tokens": 419, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not contain information about who directed the movie \"The Last Night at Tremore Beach\" [Doc 135098].", "latency_ms": 4747.968673706055, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not provide information about the director of the movie, which aligns with the gold response that the director information is not in the database. It is both factually correct given the context and faithful to it.", "main_total_tokens": 3487, "main_prompt_tokens": 2673, "main_completion_tokens": 814, "judge_total_tokens": 504, "judge_prompt_tokens": 424, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 7639.094114303589, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy' and fully aligning with the provided reference.", "main_total_tokens": 4261, "main_prompt_tokens": 2867, "main_completion_tokens": 1394, "judge_total_tokens": 456, "judge_prompt_tokens": 396, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 7608.5944175720215, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Stranger in My Bed') and correctly identifies the movie based on the description, so it is both correct and faithful.", "main_total_tokens": 4213, "main_prompt_tokens": 2990, "main_completion_tokens": 1223, "judge_total_tokens": 464, "judge_prompt_tokens": 400, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 5752.351999282837, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie in which the described plot occurs. The gold answer is 'Game of Silence', and the generated answer exactly matches it: 'Game of Silence'. This is fully correct with no contradictions or hallucinations.", "main_total_tokens": 3549, "main_prompt_tokens": 2631, "main_completion_tokens": 918, "judge_total_tokens": 477, "judge_prompt_tokens": 400, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 6923.584699630737, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is correct for the described plot. Both correctness and faithfulness are maximal.", "main_total_tokens": 4055, "main_prompt_tokens": 2822, "main_completion_tokens": 1233, "judge_total_tokens": 451, "judge_prompt_tokens": 395, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job! [Doc 66094]", "latency_ms": 6708.207607269287, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as 'Wood Job!'. It is correct and fully consistent with the provided information.", "main_total_tokens": 4468, "main_prompt_tokens": 3225, "main_completion_tokens": 1243, "judge_total_tokens": 469, "judge_prompt_tokens": 409, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 5905.858278274536, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'Sesame Street: The Magical Wand Chase'. It is both correct and faithful to the given information.", "main_total_tokens": 4305, "main_prompt_tokens": 3333, "main_completion_tokens": 972, "judge_total_tokens": 483, "judge_prompt_tokens": 417, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond [Doc 74597]", "latency_ms": 3914.2990112304688, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold label exactly ('Beyond') and correctly identifies the movie described. It is fully consistent with the provided information.", "main_total_tokens": 3577, "main_prompt_tokens": 2923, "main_completion_tokens": 654, "judge_total_tokens": 473, "judge_prompt_tokens": 415, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9", "latency_ms": 7304.3248653411865, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('6ixtynin9'), correctly identifying the movie and staying fully faithful to the provided information.", "main_total_tokens": 3969, "main_prompt_tokens": 2857, "main_completion_tokens": 1112, "judge_total_tokens": 469, "judge_prompt_tokens": 409, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread [Doc 32410]", "latency_ms": 4554.490327835083, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer, 'Club Dread', matches the gold answer exactly and correctly identifies the movie based on the given plot description. It is fully correct and faithful.", "main_total_tokens": 3872, "main_prompt_tokens": 3086, "main_completion_tokens": 786, "judge_total_tokens": 497, "judge_prompt_tokens": 433, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet", "latency_ms": 6810.922384262085, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Toilet') and correctly identifies the movie based on the described plot, so it is both correct and faithful.", "main_total_tokens": 4962, "main_prompt_tokens": 3624, "main_completion_tokens": 1338, "judge_total_tokens": 477, "judge_prompt_tokens": 415, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel [Doc 1024]", "latency_ms": 6903.429985046387, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the given description. The gold answer is 'Electric Angel'. The generated answer is 'Electric Angel [Doc 1024]', which correctly identifies the same movie, with an extra document tag that does not change the correctness. It is therefore correct and faithful to the gold answer.", "main_total_tokens": 4492, "main_prompt_tokens": 3121, "main_completion_tokens": 1371, "judge_total_tokens": 516, "judge_prompt_tokens": 424, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 in the context is Doomsday Stories, with a rating of 2.167 [Doc 108366].", "latency_ms": 5375.741720199585, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'Mega Ape' as the lowest rated movie from 2023, while the generated answer claims it is 'Doomsday Stories' with a rating of 2.167. This directly contradicts the gold label, so the answer is incorrect. It also is not faithful to the provided gold reference, which explicitly states a different movie as the lowest rated, so faithfulness is also 0.", "main_total_tokens": 4160, "main_prompt_tokens": 2912, "main_completion_tokens": 1248, "judge_total_tokens": 523, "judge_prompt_tokens": 412, "judge_completion_tokens": 111}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 331.4650058746338, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Tea House', but the generated response says 'I don't know,' which is incorrect. However, it does not introduce any incorrect alternative answer and honestly admits lack of knowledge, so it remains faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 466, "judge_prompt_tokens": 386, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 416.3980484008789, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a known answer in the gold label, but the generated response says 'I don't know,' which is incorrect, so correctness is 0. However, the model does not hallucinate or provide fabricated information; it simply admits not knowing, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 395, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The language of the lowest-rated movie from 1980 (Rating: 3.889) is en [Doc 744].", "latency_ms": 5398.047208786011, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer is 'English', while the generated answer claims the language is 'en' with an added rating detail and document reference. 'en' is a code, not the spelled-out language; this does not exactly match the gold answer, so correctness is 0. The reference to a specific rating and document (\"Rating: 3.889\" and \"[Doc 744]\") is not grounded in the provided gold answer and may be hallucinated, so faithfulness is partial at 0.5.", "main_total_tokens": 3875, "main_prompt_tokens": 2877, "main_completion_tokens": 998, "judge_total_tokens": 547, "judge_prompt_tokens": 409, "judge_completion_tokens": 138}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The Sopranos [Doc 115146]", "latency_ms": 7250.879526138306, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'The Sopranos' is incorrect; the gold answer is 'Law & Order: Special Victims Unit'. There is no evidence it followed or was grounded in the provided gold answer, so it is also unfaithful.", "main_total_tokens": 4818, "main_prompt_tokens": 3304, "main_completion_tokens": 1514, "judge_total_tokens": 480, "judge_prompt_tokens": 400, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 702.646017074585, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and does not introduce any contradictions or unsupported claims. It restates the correct value and attributes it to a document, consistent with the task.", "main_total_tokens": 2589, "main_prompt_tokens": 2568, "main_completion_tokens": 21, "judge_total_tokens": 472, "judge_prompt_tokens": 400, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in 1999 [Doc 22481], 2025 [Doc 112077], and 2019 [Doc 88811].", "latency_ms": 4458.749294281006, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 2019, but the generated response lists three different years (1999, 2025, and 2019) without choosing or clarifying which is correct, so it is not correct as an answer. It is also unfaithful because it fabricates multiple conflicting release years instead of providing a single, accurate one.", "main_total_tokens": 3720, "main_prompt_tokens": 2961, "main_completion_tokens": 759, "judge_total_tokens": 524, "judge_prompt_tokens": 423, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 802.5777339935303, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 279 votes, but the generated answer is 3, which is numerically incorrect. The model’s answer directly contradicts the provided gold value, so correctness is 0. However, there is no evidence it contradicted its own reasoning or source (none provided), so faithfulness is scored as 1 by default, reflecting internal consistency rather than accuracy against the gold.", "main_total_tokens": 2434, "main_prompt_tokens": 2429, "main_completion_tokens": 5, "judge_total_tokens": 488, "judge_prompt_tokens": 383, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 5100.970268249512, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a specific numeric answer (12 votes). The generated response claims it cannot find any matching movies or shows, which is incorrect and does not provide the required number. It is therefore factually wrong and not grounded in the given gold answer or context.", "main_total_tokens": 3484, "main_prompt_tokens": 2461, "main_completion_tokens": 1023, "judge_total_tokens": 476, "judge_prompt_tokens": 392, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "2009 [Doc 46069]", "latency_ms": 4385.175704956055, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states the release year as 2009, which matches the gold answer, and it does not contradict the provided information.", "main_total_tokens": 3783, "main_prompt_tokens": 3057, "main_completion_tokens": 726, "judge_total_tokens": 455, "judge_prompt_tokens": 396, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-large_gemini-flash-latest", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English (en) [Doc 48035].", "latency_ms": 3057.9066276550293, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the original language is English (en). The generated answer correctly states that the original language is English and aligns with the gold. It references a document but does not contradict it, so it is both correct and faithful.", "main_total_tokens": 3847, "main_prompt_tokens": 3405, "main_completion_tokens": 442, "judge_total_tokens": 483, "judge_prompt_tokens": 405, "judge_completion_tokens": 78}
