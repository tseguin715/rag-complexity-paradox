{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The movie \"Wall Street\" was released in 1987 [Doc 6993].", "latency_ms": 5819.5600509643555, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'Wall Street' was released in 1987, matching the gold answer. It is consistent with the provided information and does not introduce any contradictions or extraneous claims.", "main_total_tokens": 2765, "main_prompt_tokens": 2747, "main_completion_tokens": 18, "judge_total_tokens": 469, "judge_prompt_tokens": 396, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "In the movie \"Brain Fluid Explosion Girl,\" classmates are forced to take part in a survival game using their cellphones [Doc 69929].", "latency_ms": 837.3141288757324, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Brain Fluid Explosion Girl' and restates the premise that classmates are forced into a survival game using their cell phones. This matches the gold answer exactly and does not introduce unsupported information, so it is both correct and faithful.", "main_total_tokens": 2484, "main_prompt_tokens": 2455, "main_completion_tokens": 29, "judge_total_tokens": 501, "judge_prompt_tokens": 417, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "In the TV show \"Dungeons & Dragons,\" six friends are transported into the Dungeons & Dragons realm and must try to find a way home with the help of their guide, the 'Dungeon Master' [Doc 112849].", "latency_ms": 1037.2679233551025, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' matching the gold answer. The added description about six friends being transported and guided by the Dungeon Master is accurate to the show's premise and remains faithful to the provided information and citation.", "main_total_tokens": 2543, "main_prompt_tokens": 2496, "main_completion_tokens": 47, "judge_total_tokens": 518, "judge_prompt_tokens": 437, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is \"Nacht der Wölfe\" [Doc 2590].", "latency_ms": 1009.9444389343262, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Nacht der Wölfe') and correctly reflects the information indicated, with no hallucinations or contradictions.", "main_total_tokens": 2543, "main_prompt_tokens": 2504, "main_completion_tokens": 39, "judge_total_tokens": 498, "judge_prompt_tokens": 437, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of \"Picnic at Hanging Rock\" take place on Valentine's Day, 1900 [Doc 126191].", "latency_ms": 759.0360641479492, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events occur in 1900, matching the gold answer, and it remains faithful to the given information without adding unsupported details.", "main_total_tokens": 2774, "main_prompt_tokens": 2749, "main_completion_tokens": 25, "judge_total_tokens": 473, "judge_prompt_tokens": 410, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie you are referring to is \"June.\" It is about a young, innocent, and quirky violinist named June, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man destined for her [Doc 32483].", "latency_ms": 1137.662649154663, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'June,' matching the gold answer, so correctness is 1. However, the answer fabricates a citation/reference '[Doc 32483]' that is not present in the provided context, so the faithfulness score is 0.", "main_total_tokens": 2887, "main_prompt_tokens": 2819, "main_completion_tokens": 68, "judge_total_tokens": 586, "judge_prompt_tokens": 500, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "The highest rated Russian-language movie released in 1980 is \"Moscow Does Not Believe in Tears,\" with a rating of 7.6 [Doc 61].", "latency_ms": 1431.1904907226562, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer identifies 'Moscow Does Not Believe in Tears' as the highest rated Russian-language movie released in 1980, but the gold answer is 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger.' Therefore, the answer is factually incorrect with respect to the gold label, so correctness is 0. However, it is faithful to the provided document citation [Doc 61], so faithfulness is 1.", "main_total_tokens": 3001, "main_prompt_tokens": 2967, "main_completion_tokens": 34, "judge_total_tokens": 549, "judge_prompt_tokens": 429, "judge_completion_tokens": 120}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest Russian-language movie rating in 1999 is for the TV show \"Kamenskaya,\" which has a rating of 8.1 [Doc 115256].", "latency_ms": 910.5396270751953, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 7.6, but the generated answer states 8.1, so it is factually incorrect. It also introduces a specific TV show and rating that are not supported by the provided gold answer, so it is not faithful to the reference.", "main_total_tokens": 2898, "main_prompt_tokens": 2863, "main_completion_tokens": 35, "judge_total_tokens": 500, "judge_prompt_tokens": 420, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "There are two movies released in 1987 that have a rating over 7 and are in Chinese:\n\n1. \"A Chinese Ghost Story\" with a rating of 7.3 [Doc 7064].\n2. \"Last Eunuch In China\" with a rating of 7.8 [Doc 7754].", "latency_ms": 1408.6179733276367, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 such movies, while the generated response asserts there are 2. This is factually incorrect relative to the gold. However, the generated answer is consistent with the evidence it cites (two specific movies with ratings over 7 in 1987 in Chinese), so it is faithful to its own provided sources but not correct against the gold standard.", "main_total_tokens": 2773, "main_prompt_tokens": 2707, "main_completion_tokens": 66, "judge_total_tokens": 565, "judge_prompt_tokens": 458, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "None of the TV shows released in 1987 have a rating over 7 and are in Chinese. The only TV shows listed from 1987 are in English or Polish [Doc 113363, Doc 113350, Doc 113253, Doc 113259, Doc 113344, Doc 113329, Doc 113324].", "latency_ms": 1325.4308700561523, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The correct answer is 2, but the generated answer claims there are none, so the correctness is 0. The answer appears grounded in the cited documents and does not hallucinate beyond them, but it incorrectly infers that no qualifying Chinese shows exist, suggesting a likely gap in retrieved evidence rather than pure fabrication, so faithfulness is relatively high but not perfect.", "main_total_tokens": 3015, "main_prompt_tokens": 2944, "main_completion_tokens": 71, "judge_total_tokens": 570, "judge_prompt_tokens": 464, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "The only French language TV show released in 1980 is \"Therese Raquin\" [Doc 112644].", "latency_ms": 1633.8419914245605, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the only French language TV show released in 1980 is 'Fantômas', but the generated answer claims it is 'Therese Raquin', which is incorrect. Since we are told the document ID citation, but not its contents, we must rely on the gold label; the generated answer thus contradicts the correct one. It is therefore factually incorrect and not faithful to the gold answer or the implied source information.", "main_total_tokens": 2883, "main_prompt_tokens": 2859, "main_completion_tokens": 24, "judge_total_tokens": 529, "judge_prompt_tokens": 409, "judge_completion_tokens": 120}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" [Doc 11205].", "latency_ms": 817.6300525665283, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' which matches the gold answer. It does not introduce any contradictions or extraneous claims and remains faithful to the provided information.", "main_total_tokens": 2712, "main_prompt_tokens": 2688, "main_completion_tokens": 24, "judge_total_tokens": 476, "judge_prompt_tokens": 409, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 657.367467880249, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, providing the correct movie title 'Won't You Be My Neighbor?'. It is fully supported by the given information and does not introduce any unsupported content.", "main_total_tokens": 2905, "main_prompt_tokens": 2885, "main_completion_tokens": 20, "judge_total_tokens": 491, "judge_prompt_tokens": 421, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The movie \"1969\" was released in the year 1988 [Doc 8389].", "latency_ms": 1113.8675212860107, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the year of release with gold answer 2018, but the generated answer gives 1988, which is incorrect. However, it appears consistent with its cited document, so it is faithful but not correct.", "main_total_tokens": 2938, "main_prompt_tokens": 2918, "main_completion_tokens": 20, "judge_total_tokens": 474, "judge_prompt_tokens": 397, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is \"Astro Boy\" [Doc 112591].", "latency_ms": 1303.0800819396973, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy,' matching the gold answer exactly. It is faithful to the provided information and does not introduce contradictions or unsupported claims.", "main_total_tokens": 2706, "main_prompt_tokens": 2677, "main_completion_tokens": 29, "judge_total_tokens": 483, "judge_prompt_tokens": 416, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 606.8339347839355, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer. It is fully consistent with the provided information and does not introduce any unsupported claims.", "main_total_tokens": 2510, "main_prompt_tokens": 2480, "main_completion_tokens": 30, "judge_total_tokens": 496, "judge_prompt_tokens": 424, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 843.9877033233643, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before the 26 November 1983 break-in, matching the provided gold value and staying faithful to the given information.", "main_total_tokens": 2826, "main_prompt_tokens": 2793, "main_completion_tokens": 33, "judge_total_tokens": 504, "judge_prompt_tokens": 428, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "The most popular English-speaking TV show released in 1997 is \"Recess,\" with a popularity score of 10.3614 [Doc 114720].", "latency_ms": 1164.6714210510254, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer gives a specific show (Recess) and a popularity score (10.3614) that do not match the gold popularity score of 80.5772, so it is factually incorrect. It also cites a document ID as evidence for this incorrect value, meaning it is not faithful to the ground truth evaluation target.", "main_total_tokens": 2976, "main_prompt_tokens": 2943, "main_completion_tokens": 33, "judge_total_tokens": 524, "judge_prompt_tokens": 425, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "There are no Russian language TV shows listed as being released in 1997 in the provided documents.", "latency_ms": 1473.6716747283936, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says there was 1 Russian language TV show released in 1997, while the generated answer claims there were none. This is factually incorrect. Additionally, the generated answer invents information about the documents (saying none are listed) that contradicts the gold label, so it is not faithful to the ground truth.", "main_total_tokens": 3033, "main_prompt_tokens": 3013, "main_completion_tokens": 20, "judge_total_tokens": 497, "judge_prompt_tokens": 403, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 768.8980102539062, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the TV show as 'Angel Eyes'. It is both correct and faithful to the provided information.", "main_total_tokens": 3231, "main_prompt_tokens": 3196, "main_completion_tokens": 35, "judge_total_tokens": 489, "judge_prompt_tokens": 429, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"I Dream in Another Language\" is Spanish (es) [Doc 78127].", "latency_ms": 783.6334705352783, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer claims the movie language is Spanish, but the gold label says Korean, so it is factually incorrect. It also contradicts the provided gold answer, so it is unfaithful to the reference.", "main_total_tokens": 2945, "main_prompt_tokens": 2922, "main_completion_tokens": 23, "judge_total_tokens": 473, "judge_prompt_tokens": 399, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "There are no English-speaking Horror movies released in 2009 with a popularity score greater than 7 in the provided documents.", "latency_ms": 835.3190422058105, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 2 such movies, but the generated answer claims there are none. This is factually incorrect relative to the gold label, so correctness is 0. The generated answer does not contradict its own stated reasoning about the provided documents, so faithfulness is 1.", "main_total_tokens": 2994, "main_prompt_tokens": 2969, "main_completion_tokens": 25, "judge_total_tokens": 507, "judge_prompt_tokens": 417, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The documents provided do not contain any information about movies released in 1979.", "latency_ms": 834.5742225646973, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that, within the provided documents/database, there is no information about movies released in 1979. The generated response is faithful to the (implied) context that only the documents can be used and makes no unsupported claims.", "main_total_tokens": 2878, "main_prompt_tokens": 2862, "main_completion_tokens": 16, "judge_total_tokens": 495, "judge_prompt_tokens": 411, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The context does not provide information about a movie titled \"Back to the Future Part IV.\"", "latency_ms": 857.4252128601074, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer is factually correct: there is no information about a movie titled 'Back to the Future Part IV' because such a movie has not been released. However, it is not faithful to the gold answer, which explicitly states that 'Back to the Future Part IV' does not exist in the database, rather than merely noting that the context lacks information about it.", "main_total_tokens": 3696, "main_prompt_tokens": 3678, "main_completion_tokens": 18, "judge_total_tokens": 524, "judge_prompt_tokens": 418, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "There is no information available about a movie from 1980 about the nuclear Chernobyl disaster in the provided documents.", "latency_ms": 1206.103801727295, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there is no information about a 1980 movie related to the Chernobyl disaster, which is factually accurate given Chernobyl occurred in 1986. The generated answer is faithful to the provided documents by explicitly referencing their lack of information.", "main_total_tokens": 2926, "main_prompt_tokens": 2902, "main_completion_tokens": 24, "judge_total_tokens": 520, "judge_prompt_tokens": 430, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are three TV shows or movies with 'Astro Boy' in the title:\n\n1. Astro Boy (TV, 1980) [Doc 112591]\n2. Astro Boy (Movie, 2009) [Doc 44947]\n3. Astro Boy (TV, 2003) [Doc 116265]", "latency_ms": 1449.5203495025635, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.8318724637288826, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.8318724637288826, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 4 such titles, but the generated answer says there are 3, so the numeric result is incorrect. However, the answer is faithful to the information it presents and does not contradict itself or add unsupported claims; it just omits one item compared to the gold.", "main_total_tokens": 2866, "main_prompt_tokens": 2798, "main_completion_tokens": 68, "judge_total_tokens": 545, "judge_prompt_tokens": 454, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "There are no movies from 2025 in the provided documents, so there are no movies from that year that allude to the Holocaust.", "latency_ms": 839.4796848297119, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there are 2 such movies, while the generated answer claims there are none. This directly contradicts the gold, so the answer is factually incorrect relative to the labeled data. It is also unfaithful because it asserts a negative based on 'provided documents' rather than aligning with the given gold information.", "main_total_tokens": 2830, "main_prompt_tokens": 2802, "main_completion_tokens": 28, "judge_total_tokens": 509, "judge_prompt_tokens": 412, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "There were three TV shows released in the 1990s about Super Mario Bros:\n\n1. \"The Adventures of Super Mario Bros. 3\" (1990) [Doc 113661].\n2. \"Super Mario World\" (1991) [Doc 113814].\n3. \"The Super Mario Bros. Super Show!\" (1989) is not from the 1990s, so it is not included in this count [Doc 113517].", "latency_ms": 1265.683650970459, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct answer per the gold standard is 1, but the generated answer states that there were three TV shows in the 1990s, which is incorrect. It miscounts by including shows from 1990 and 1991 (depending on definition, those may be valid 1990s shows) but still arrives at 3 instead of 1. Additionally, its own list only contains two 1990s shows while claiming three, which is internally inconsistent. Thus, correctness is 0. Regarding faithfulness, the model does reference specific shows and dates that plausibly align with documents, but it incorrectly summarizes the count and the reasoning about them, so it is only partially faithful to the presumed sources; hence a mid-level faithfulness score.", "main_total_tokens": 2879, "main_prompt_tokens": 2784, "main_completion_tokens": 95, "judge_total_tokens": 667, "judge_prompt_tokens": 482, "judge_completion_tokens": 185}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "There are no TV shows about Super Mario Bros released between 2000 and 2010 mentioned in the provided documents.", "latency_ms": 1150.9058475494385, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0, meaning there were no such TV shows in that period. The generated answer states that there are no TV shows about Super Mario Bros released between 2000 and 2010 mentioned in the provided documents, which effectively corresponds to 0 and does not contradict the gold. It is correct and faithful to the information scope (the provided documents).", "main_total_tokens": 2798, "main_prompt_tokens": 2774, "main_completion_tokens": 24, "judge_total_tokens": 518, "judge_prompt_tokens": 413, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular TV show or movie about ninjas, based on the provided data, is \"UNDER NINJA\" with a popularity score of 3.1982 [Doc 133621].", "latency_ms": 992.203950881958, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates 'Naruto' as the most popular ninja TV show, but the generated answer claims 'UNDER NINJA' based on a popularity score in the provided data. Relative to the gold, this is factually incorrect, so correctness is 0. However, the answer is faithful to the provided document snippet and does not introduce external hallucinations; it explicitly references the data and its score, so faithfulness to the given context is 1.", "main_total_tokens": 3136, "main_prompt_tokens": 3096, "main_completion_tokens": 40, "judge_total_tokens": 541, "judge_prompt_tokens": 422, "judge_completion_tokens": 119}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The context does not provide information about a Jurassic Park/Power Ranger crossover movie.", "latency_ms": 791.1267280578613, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the movie was not released, while the generated answer only says the context does not provide information, which does not correctly answer the question. However, the generated answer is faithful to the given context.", "main_total_tokens": 2928, "main_prompt_tokens": 2912, "main_completion_tokens": 16, "judge_total_tokens": 476, "judge_prompt_tokens": 401, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "The context provided does not mention any movie where Barbie fights the Predator.", "latency_ms": 801.1479377746582, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The gold answer clarifies that no such movie was released, while the generated answer only notes that the context does not mention such a movie, implying uncertainty rather than stating the correct fact. This makes the generated answer ultimately incorrect. It is mostly faithful to the given context, but it fails to address the underlying factual question directly, so faithfulness is high but not perfect.", "main_total_tokens": 3169, "main_prompt_tokens": 3155, "main_completion_tokens": 14, "judge_total_tokens": 503, "judge_prompt_tokens": 396, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The 4th oldest Star Wars movie is \"Star Wars: The Clone Wars,\" released in 2008 [Doc 42090].", "latency_ms": 959.0399265289307, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer names 'Star Wars: The Clone Wars' (2008) as the 4th oldest Star Wars movie, which contradicts the gold answer 'Star Wars: Droids - The Great Heep' and is therefore incorrect. Citing an unspecified document number without aligning with the provided gold label shows it is not faithful to the reference answer expected for this evaluation.", "main_total_tokens": 3226, "main_prompt_tokens": 3198, "main_completion_tokens": 28, "judge_total_tokens": 524, "judge_prompt_tokens": 417, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The movie \"1969\" was released in the year 1988 [Doc 8389].", "latency_ms": 696.3329315185547, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the release year is 1986, but the generated answer claims the movie was released in 1988, which is incorrect. Additionally, the generated answer refers to a different movie titled '1969' instead of the intended subject, so it is not faithful to the question context.", "main_total_tokens": 2938, "main_prompt_tokens": 2918, "main_completion_tokens": 20, "judge_total_tokens": 489, "judge_prompt_tokens": 397, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The rating of the movie \"Forrest Gump\" is 8.466 [Doc 15236].", "latency_ms": 1295.2485084533691, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating value (8.466) and clearly refers to the correct movie, \"Forrest Gump\". It also cites the document source, so it is faithful to the provided evidence.", "main_total_tokens": 2811, "main_prompt_tokens": 2789, "main_completion_tokens": 22, "judge_total_tokens": 492, "judge_prompt_tokens": 416, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show is \"Mighty Med\" [Doc 121125].", "latency_ms": 709.7961902618408, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med,' which matches the gold answer exactly. It is fully faithful to the provided information and does not introduce any contradictions or unsupported details.", "main_total_tokens": 2346, "main_prompt_tokens": 2328, "main_completion_tokens": 18, "judge_total_tokens": 480, "judge_prompt_tokens": 410, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In \"Túnel de los huesos\" (2011), seven prisoners escaped from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 790.1473045349121, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the country, and the generated answer correctly identifies Argentina as the country where the Villa Devoto prison in Buenos Aires is located. It matches the gold answer and does not introduce contradictory information, so it is both correct and faithful to the source.", "main_total_tokens": 2742, "main_prompt_tokens": 2708, "main_completion_tokens": 34, "judge_total_tokens": 508, "judge_prompt_tokens": 425, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice Li Nai falls into a trap designed by Jiang Jie, the \"Chef Assassin\" [Doc 135231].", "latency_ms": 1394.294261932373, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0.0, "faithfulness_score": 0.0, "justification": "The gold answer is simply 'Jiang Jie'. The generated answer invents the apprentice's name as 'Li Nai' and adds 'the \"Chef Assassin\" [Doc 135231]' which are unsupported details. While it correctly mentions Jiang Jie, the extra fabricated specifics make the answer unfaithful to the provided gold and to the question, which only asks whose trap it is. Therefore, it is neither fully correct nor faithful.", "main_total_tokens": 2897, "main_prompt_tokens": 2864, "main_completion_tokens": 33, "judge_total_tokens": 539, "judge_prompt_tokens": 417, "judge_completion_tokens": 122}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "There are no Korean-language movies mentioning assassins after 2023 in the provided documents.", "latency_ms": 659.6934795379639, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 such movies, but the generated answer claims there are none, which is factually incorrect relative to the gold. However, the generated answer is internally consistent with the information it references (\"the provided documents\"), so it is faithful to its own cited source context, just not correct compared to the gold.", "main_total_tokens": 2761, "main_prompt_tokens": 2743, "main_completion_tokens": 18, "judge_total_tokens": 501, "judge_prompt_tokens": 401, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "The documents provided do not contain any information about movies featuring both Steven Seagal and Bruce Lee.", "latency_ms": 756.2153339385986, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not contain information about movies featuring both Steven Seagal and Bruce Lee, which is equivalent in meaning to the gold answer that it cannot find any such movies. It does not introduce unsupported claims and stays faithful to the provided information.", "main_total_tokens": 3131, "main_prompt_tokens": 3112, "main_completion_tokens": 19, "judge_total_tokens": 497, "judge_prompt_tokens": 412, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 672.1909046173096, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the rating of Tokyo Mater (2008) is 6.3, which matches the gold answer. It also attributes the information to a document, staying faithful to the provided context.", "main_total_tokens": 2959, "main_prompt_tokens": 2939, "main_completion_tokens": 20, "judge_total_tokens": 477, "judge_prompt_tokens": 403, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The document does not specify the style of racing featured in the movie \"Fast Track: No Limits\" [Doc 41929].", "latency_ms": 1015.9547328948975, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the style of racing in the movie, and the correct answer is 'Drift'. The generated answer claims the document does not specify the style, which is incorrect since the gold answer clearly identifies it as drifting. However, given only the generated answer and no document text, the assistant is being faithful to its stated (albeit mistaken) reading of the document, so faithfulness to its own cited source is maintained even though the answer is factually wrong.", "main_total_tokens": 3129, "main_prompt_tokens": 3103, "main_completion_tokens": 26, "judge_total_tokens": 530, "judge_prompt_tokens": 404, "judge_completion_tokens": 126}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the TV show about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 906.0885906219482, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer gives the correct title, 'Confronting a Serial Killer', matching the gold. However, it calls it a TV show whereas the question asks for the title of the movie, so there is a minor mismatch in type/description, slightly reducing faithfulness.", "main_total_tokens": 2882, "main_prompt_tokens": 2835, "main_completion_tokens": 47, "judge_total_tokens": 541, "judge_prompt_tokens": 453, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 775.7477760314941, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score 7.838 and clearly attributes it. It is both correct and faithful to the provided information.", "main_total_tokens": 2965, "main_prompt_tokens": 2943, "main_completion_tokens": 22, "judge_total_tokens": 467, "judge_prompt_tokens": 407, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"Victim\" [Doc 22488].", "latency_ms": 834.8972797393799, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect: the correct title is 'The Sixth Sense', not 'Victim'. The cited document reference does not align with the gold answer, so the response is also unfaithful to the ground truth.", "main_total_tokens": 2746, "main_prompt_tokens": 2718, "main_completion_tokens": 28, "judge_total_tokens": 495, "judge_prompt_tokens": 419, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 1032.1309566497803, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2002) exactly and does not introduce any conflicting or extraneous claims, so it is both correct and faithful.", "main_total_tokens": 3066, "main_prompt_tokens": 3049, "main_completion_tokens": 17, "judge_total_tokens": 459, "judge_prompt_tokens": 397, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 1269.8683738708496, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2015 reboot title as 'The Transporter Refueled'. It does not add unsupported information; the bracketed citation is neutral regarding faithfulness. Therefore, it is both correct and faithful.", "main_total_tokens": 2886, "main_prompt_tokens": 2857, "main_completion_tokens": 29, "judge_total_tokens": 503, "judge_prompt_tokens": 420, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The movie 'The Transporter Refueled' was released in the year 2015 [Doc 67500].", "latency_ms": 916.132926940918, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It is also faithful to the cited document reference, with no hallucinated or contradictory information.", "main_total_tokens": 3174, "main_prompt_tokens": 3150, "main_completion_tokens": 24, "judge_total_tokens": 483, "judge_prompt_tokens": 408, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "There is no romantic comedy in the provided documents where Abraham Lincoln comes back from the dead.", "latency_ms": 787.1637344360352, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no such movie is found, and the generated answer states that there is no romantic comedy in the provided documents where Abraham Lincoln comes back from the dead. This is consistent and accurate given the context. Both correctness and faithfulness are satisfied.", "main_total_tokens": 2575, "main_prompt_tokens": 2557, "main_completion_tokens": 18, "judge_total_tokens": 486, "judge_prompt_tokens": 408, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of the movie that alludes to a manuscript written by Ernest Hemingway is \"Havana Darkness\" [Doc 90923].", "latency_ms": 821.4960098266602, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'Havana Darkness' as the movie that alludes to a manuscript written by Ernest Hemingway. It is fully consistent and faithful to the provided information.", "main_total_tokens": 2590, "main_prompt_tokens": 2563, "main_completion_tokens": 27, "judge_total_tokens": 487, "judge_prompt_tokens": 415, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "In the TV show \"Doll & Em,\" Doll heads to Hollywood to be with her childhood best friend Em, who is now a successful actress [Doc 122259].", "latency_ms": 1171.0069179534912, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Doll & Em,\" matching the gold answer exactly. It stays faithful to the provided information without adding unsupported claims; noting that Em is a successful actress is consistent with the source citation and typical show description.", "main_total_tokens": 2990, "main_prompt_tokens": 2955, "main_completion_tokens": 35, "judge_total_tokens": 506, "judge_prompt_tokens": 424, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in the 2007 movie \"Hallam Foe\" [Doc 39389].", "latency_ms": 843.3330059051514, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2007 movie as \"Hallam Foe,\" matching the gold answer exactly. It is fully supported by the provided information and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 3067, "main_prompt_tokens": 3029, "main_completion_tokens": 38, "judge_total_tokens": 509, "judge_prompt_tokens": 437, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The context provided does not contain any information about a character whose last name sounds like \"friend\" or \"foe.\" Therefore, the answer is not found.", "latency_ms": 956.2380313873291, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'A foe.' The generated answer claims the information is not in the context, which contradicts the gold; therefore it is incorrect. It also fails to follow the given QA task and ignores the labeled answer, so it is not faithful to the intended question-answering behavior.", "main_total_tokens": 3043, "main_prompt_tokens": 3011, "main_completion_tokens": 32, "judge_total_tokens": 508, "judge_prompt_tokens": 418, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "Sam is 18 years old in the 2017 TV show \"Atypical\" [Doc 124583].", "latency_ms": 901.2637138366699, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show 'Atypical', which matches the gold answer. It does not introduce contradictions or hallucinated content beyond a benign document reference, so it is both correct and faithful.", "main_total_tokens": 2648, "main_prompt_tokens": 2624, "main_completion_tokens": 24, "judge_total_tokens": 495, "judge_prompt_tokens": 411, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show \"Atypical,\" Sam is on the autism spectrum [Doc 124583].", "latency_ms": 861.5367412567139, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the 2017 TV show 'Atypical,' Sam is on the autism spectrum, which matches the gold answer exactly and does not introduce any contradictions or extraneous claims.", "main_total_tokens": 2581, "main_prompt_tokens": 2557, "main_completion_tokens": 24, "judge_total_tokens": 484, "judge_prompt_tokens": 411, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Sandra was born in Chamonix [Doc 59659].", "latency_ms": 889.7156715393066, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Chamonix as the place where Sandra was born, matching the gold answer exactly and staying faithful to the provided information.", "main_total_tokens": 3008, "main_prompt_tokens": 2995, "main_completion_tokens": 13, "judge_total_tokens": 470, "judge_prompt_tokens": 410, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481].", "latency_ms": 902.9543399810791, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and stays faithful to the provided information without adding extraneous content.", "main_total_tokens": 3323, "main_prompt_tokens": 3315, "main_completion_tokens": 8, "judge_total_tokens": 454, "judge_prompt_tokens": 398, "judge_completion_tokens": 56}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the German language animation \"Tabaluga,\" Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 780.1239490509033, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Greenland as the place of which Tabaluga is crown-prince, matching the gold answer exactly and remaining faithful to it.", "main_total_tokens": 3183, "main_prompt_tokens": 3151, "main_completion_tokens": 32, "judge_total_tokens": 487, "judge_prompt_tokens": 426, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers\" is \"One Afternoon\" [Doc 96838].", "latency_ms": 1226.6404628753662, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'One Afternoon' and does not introduce any contradictions or unsupported claims beyond restating the description and title. It is both correct and faithful.", "main_total_tokens": 2467, "main_prompt_tokens": 2423, "main_completion_tokens": 44, "judge_total_tokens": 515, "judge_prompt_tokens": 448, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" from 2020, with a rating of 6.746 [Doc 93260].", "latency_ms": 5797.5335121154785, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title of the Italian-language movie whose plot matches that of a Holocaust survivor taking care of a 12-year-old street kid who robbed her. Both the gold answer and the generated answer identify the film as \"The Life Ahead,\" which is correct. The added year, 2020, in the generated answer is accurate and does not contradict the gold. The reference to a rating and document ID is consistent with a retrieved-doc setting and does not conflict with the gold, so the response is fully faithful to the provided answer.", "main_total_tokens": 2915, "main_prompt_tokens": 2865, "main_completion_tokens": 50, "judge_total_tokens": 588, "judge_prompt_tokens": 448, "judge_completion_tokens": 140}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 846.6739654541016, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly states that the 2016 documentary 'McLaren' is about Bruce McLaren, matching the gold answer. However, it adds an unverified citation-style reference '[Doc 73859]' not present in the prompt, which is a hallucinated detail, so faithfulness is reduced.", "main_total_tokens": 2790, "main_prompt_tokens": 2757, "main_completion_tokens": 33, "judge_total_tokens": 509, "judge_prompt_tokens": 417, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The name of the racing team is not mentioned in the provided documents.", "latency_ms": 1205.7125568389893, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the team as 'The McLaren Motor Racing team'. The generated answer states that the name is not mentioned, which directly contradicts the gold and is therefore incorrect. However, assuming the model only had access to documents where the name truly was not given, the answer is internally consistent with that premise and does not fabricate information, so it is faithful to its own stated evidence.", "main_total_tokens": 2896, "main_prompt_tokens": 2882, "main_completion_tokens": 14, "judge_total_tokens": 507, "judge_prompt_tokens": 397, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context does not provide information about the country of origin for a specific individual referred to as \"he.\" If you have a specific document or character in mind, please provide more details.", "latency_ms": 1282.1154594421387, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the person is from New Zealand, but the generated answer claims the context does not provide this information and does not give the correct country. This is incorrect relative to the gold. However, given the model’s own stated interpretation of the (implied) context, it remains internally faithful to that interpretation and does not hallucinate a country, so faithfulness is high.", "main_total_tokens": 2528, "main_prompt_tokens": 2491, "main_completion_tokens": 37, "judge_total_tokens": 522, "judge_prompt_tokens": 413, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "There is no 2013 movie in the provided documents where John McClane blows up the Great Wall of China. \"A Good Day to Die Hard\" (2013) features John McClane, but the plot involves him traveling to Moscow to help his son, not anything related to the Great Wall of China [Doc 58791].", "latency_ms": 1183.3462715148926, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.8, "faithfulness_score": 0.8, "justification": "Gold says it has no such movie in its database; the generated answer adds external knowledge by naming 'A Good Day to Die Hard' (2013) and specifying its Moscow setting, which is not in the provided gold. The core claim that there is no 2013 movie where McClane blows up the Great Wall is logically consistent with the gold, but the reference to the specific film and plot is ungrounded. Thus, it's mostly correct in conclusion but not fully faithful to the given information.", "main_total_tokens": 3151, "main_prompt_tokens": 3081, "main_completion_tokens": 70, "judge_total_tokens": 605, "judge_prompt_tokens": 468, "judge_completion_tokens": 137}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with that description is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 1147.96781539917, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Tomie: Forbidden Fruit' and does not introduce any unsupported information. It is both correct and faithful to the given data.", "main_total_tokens": 3049, "main_prompt_tokens": 3024, "main_completion_tokens": 25, "judge_total_tokens": 507, "judge_prompt_tokens": 442, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie \"Lei mi parla ancora,\" Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 845.0460433959961, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (65 years) and directly addresses the question about the duration of Nino and Caterina's marriage in the 2021 movie 'Lei mi parla ancora'. There is no contradiction or hallucination relative to the provided gold statement.", "main_total_tokens": 3374, "main_prompt_tokens": 3344, "main_completion_tokens": 30, "judge_total_tokens": 509, "judge_prompt_tokens": 424, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In \"Scorpion King: Book of Souls\" (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls, which will allow them to put an end to an evil warlord [Doc 82582].", "latency_ms": 1021.3968753814697, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in 'Scorpion King: Book of Souls' (2018), The Scorpion King and Tala are searching for the Book of Souls, matching the gold answer. The added detail about it being a legendary relic that can end an evil warlord is consistent with the film and does not contradict the question or given gold answer, so the response is both correct and faithful.", "main_total_tokens": 3448, "main_prompt_tokens": 3397, "main_completion_tokens": 51, "judge_total_tokens": 556, "judge_prompt_tokens": 445, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Zombies on the Moon\" is not found in the provided documents.", "latency_ms": 938.9791488647461, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database; the generated answer states the release year is not found in the provided documents. Both convey that the information is unavailable, which is correct relative to the gold. The generated answer does not introduce contradictory or unsupported details, so it is faithful.", "main_total_tokens": 3140, "main_prompt_tokens": 3121, "main_completion_tokens": 19, "judge_total_tokens": 500, "judge_prompt_tokens": 410, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document does not contain any information about a movie or TV show titled \"The Galactic baker.\"", "latency_ms": 822.1123218536377, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database; the generated answer says the document does not contain information about that title. Both effectively acknowledge the lack of information about the movie and do not hallucinate a release year. This is correct and faithful to the gold.", "main_total_tokens": 2900, "main_prompt_tokens": 2881, "main_completion_tokens": 19, "judge_total_tokens": 493, "judge_prompt_tokens": 408, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Return of the Null Pointer\" is not found in the provided documents.", "latency_ms": 1554.4178485870361, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers convey that the release year (or the movie itself) is not available/known from the given information. The generated answer does not fabricate a date and stays within the scope of the provided documents, so it is both correct and faithful.", "main_total_tokens": 2591, "main_prompt_tokens": 2572, "main_completion_tokens": 19, "judge_total_tokens": 494, "judge_prompt_tokens": 410, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Data Science: The Movie\" is not found in the provided documents.", "latency_ms": 647.3419666290283, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about the release year of a movie. Both the gold and generated answers state that the information is not available, which is correct given the context. The generated answer is consistent with the (implied) source documents and does not hallucinate a year, so it is both correct and faithful.", "main_total_tokens": 3124, "main_prompt_tokens": 3105, "main_completion_tokens": 19, "judge_total_tokens": 502, "judge_prompt_tokens": 410, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document set does not contain any information about a movie titled \"Python's Revenge 2.\"", "latency_ms": 1152.6498794555664, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information in the document set about the movie, which aligns with the gold answer indicating it is not in the database. It makes no unsupported claims and remains faithful to the provided information.", "main_total_tokens": 2948, "main_prompt_tokens": 2929, "main_completion_tokens": 19, "judge_total_tokens": 486, "judge_prompt_tokens": 410, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Last Pixel\" is not found in the provided documents.", "latency_ms": 1512.627124786377, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available/found; the generated answer is correct and faithful to the given information.", "main_total_tokens": 2601, "main_prompt_tokens": 2584, "main_completion_tokens": 17, "judge_total_tokens": 466, "judge_prompt_tokens": 406, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document does not provide information about a movie titled \"The man who ate the sun.\"", "latency_ms": 1070.7614421844482, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the document does not contain information about the movie, which is consistent with the gold answer indicating lack of data. It does not invent a release year and remains faithful to the provided context.", "main_total_tokens": 3384, "main_prompt_tokens": 3366, "main_completion_tokens": 18, "judge_total_tokens": 484, "judge_prompt_tokens": 410, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Days of Lehman Brothers\" is not mentioned in the provided documents.", "latency_ms": 899.9302387237549, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the director information is not provided in the database. The generated answer states that the director is not mentioned in the provided documents, which is semantically equivalent. It correctly reflects the lack of information and does not hallucinate a director name, so it is both correct and faithful to the gold/reference.", "main_total_tokens": 3306, "main_prompt_tokens": 3284, "main_completion_tokens": 22, "judge_total_tokens": 513, "judge_prompt_tokens": 419, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"A Love to Die For\" is not mentioned in the provided documents.", "latency_ms": 1228.0070781707764, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned in the provided documents, which aligns with the gold response saying the director information is not provided in the database. It does not invent any unsupported details and remains faithful to the given context.", "main_total_tokens": 2943, "main_prompt_tokens": 2923, "main_completion_tokens": 20, "judge_total_tokens": 494, "judge_prompt_tokens": 415, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not mentioned in the provided documents. [Doc 111360]", "latency_ms": 1130.1162242889404, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided/database documents. The generated answer is correct and faithful to the given evidence, with only a minor wording difference that does not change the meaning.", "main_total_tokens": 2916, "main_prompt_tokens": 2892, "main_completion_tokens": 24, "judge_total_tokens": 493, "judge_prompt_tokens": 417, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Night at Tremore Beach\" is not mentioned in the provided documents. [Doc 135098]", "latency_ms": 1051.8100261688232, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned in the provided documents, which matches the gold answer that the director information is not in the database. It does not hallucinate a director and stays faithful to the given context.", "main_total_tokens": 2875, "main_prompt_tokens": 2847, "main_completion_tokens": 28, "judge_total_tokens": 502, "judge_prompt_tokens": 425, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 841.3577079772949, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer exactly matches the gold answer, so it is correct with respect to the question. However, it cites a specific document identifier ([Doc 98375]) that is not present in the provided context, so that part is hallucinated, reducing faithfulness.", "main_total_tokens": 2743, "main_prompt_tokens": 2716, "main_completion_tokens": 27, "judge_total_tokens": 502, "judge_prompt_tokens": 418, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse is \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 832.679271697998, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold reference title 'Stranger in My Bed' and stays consistent with the given information without adding unsupported details.", "main_total_tokens": 2758, "main_prompt_tokens": 2729, "main_completion_tokens": 29, "judge_total_tokens": 484, "judge_prompt_tokens": 425, "judge_completion_tokens": 59}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie you are referring to is not a movie, but a TV show titled \"Game of Silence\" [Doc 123938].", "latency_ms": 741.9400215148926, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question explicitly asks for a movie, and the gold answer is the TV show 'Game of Silence'. The generated answer correctly identifies that it is a TV show rather than a movie, so it is faithful to the available information and to the gold label. However, since the task is to name a movie and the response states it is not a movie but a TV show, it does not satisfy the question as posed, so correctness is 0.", "main_total_tokens": 2224, "main_prompt_tokens": 2197, "main_completion_tokens": 27, "judge_total_tokens": 545, "judge_prompt_tokens": 424, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie in which a true story about a killer from the site Craigslist is depicted is \"The Craigslist Killer\" [Doc 53039].", "latency_ms": 853.4281253814697, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer: it correctly identifies the movie as \"The Craigslist Killer.\" No unsupported information is introduced beyond the citation, so it is both correct and faithful to the prompt and gold.", "main_total_tokens": 2623, "main_prompt_tokens": 2595, "main_completion_tokens": 28, "judge_total_tokens": 493, "judge_prompt_tokens": 420, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which this happens is \"Wood Job!\" [Doc 66094].", "latency_ms": 1250.6613731384277, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Wood Job!', which matches the gold answer exactly. It does not introduce any unsupported information beyond restating the title, so it is fully faithful to the provided gold answer and context.", "main_total_tokens": 3075, "main_prompt_tokens": 3058, "main_completion_tokens": 17, "judge_total_tokens": 488, "judge_prompt_tokens": 417, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure is \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 901.40700340271, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase' and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 2786, "main_prompt_tokens": 2737, "main_completion_tokens": 49, "judge_total_tokens": 524, "judge_prompt_tokens": 459, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a rock fall traps a boy's parents inside a burning car, and a 7-year-old boy must find the hero within him to save their lives, is titled \"Beyond\" [Doc 74597].", "latency_ms": 1021.26145362854, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title and the gold answer is \"Beyond.\" The generated answer correctly identifies the movie as \"Beyond\" and does not introduce contradictions or unsupported details. It is both correct and faithful to the gold answer.", "main_total_tokens": 2330, "main_prompt_tokens": 2284, "main_completion_tokens": 46, "judge_total_tokens": 531, "judge_prompt_tokens": 454, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman is fired from a financial corporation during the Asia crisis and returns home with no money is \"6ixtynin9\" [Doc 22478].", "latency_ms": 775.7558822631836, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as \"6ixtynin9\". It does not introduce any contradictions or unsupported claims and is fully grounded in the provided gold reference.", "main_total_tokens": 2391, "main_prompt_tokens": 2354, "main_completion_tokens": 37, "judge_total_tokens": 511, "judge_prompt_tokens": 439, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie in which a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort is \"Club Dread\" [Doc 32410].", "latency_ms": 1079.5118808746338, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer exactly. It remains faithful to the question and does not introduce any contradictions or hallucinations beyond an irrelevant document citation.", "main_total_tokens": 2950, "main_prompt_tokens": 2918, "main_completion_tokens": 32, "judge_total_tokens": 528, "judge_prompt_tokens": 456, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is \"Toilet\" [Doc 48873].", "latency_ms": 1353.5428047180176, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Toilet,' which matches the gold answer. It stays faithful to the question and does not introduce unsupported information beyond restating the premise and title.", "main_total_tokens": 3126, "main_prompt_tokens": 3075, "main_completion_tokens": 51, "judge_total_tokens": 534, "judge_prompt_tokens": 465, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century is \"Electric Angel\" [Doc 1024].", "latency_ms": 1077.4121284484863, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Electric Angel,\" which matches the gold label. It stays faithful to the provided information by not introducing contradictions or unsupported claims beyond a minor citation-style note that does not affect factuality.", "main_total_tokens": 2739, "main_prompt_tokens": 2694, "main_completion_tokens": 45, "judge_total_tokens": 537, "judge_prompt_tokens": 461, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is \"Rare Objects\" with a rating of 4.3 [Doc 105190].", "latency_ms": 1016.7074203491211, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates that the lowest rated 2023 movie is 'Mega Ape', but the generated answer claims it is 'Rare Objects' with a rating of 4.3. This directly contradicts the gold, so the response is incorrect. However, the generated answer is internally consistent with its cited document and does not hallucinate beyond that, so it is faithful to its own source.", "main_total_tokens": 2805, "main_prompt_tokens": 2778, "main_completion_tokens": 27, "judge_total_tokens": 513, "judge_prompt_tokens": 408, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "The highest rated TV show from 2010 is \"Hawaii Five-0\" with a rating of 7.755 [Doc 119141].", "latency_ms": 893.3632373809814, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates that the correct response is 'Tea House', whereas the generated answer claims 'Hawaii Five-0' is the highest rated TV show from 2010. This is factually incorrect relative to the provided gold. Additionally, the generated answer introduces a specific rating and document reference that are not grounded in the given gold answer, so it is not faithful to the provided information.", "main_total_tokens": 3175, "main_prompt_tokens": 3144, "main_completion_tokens": 31, "judge_total_tokens": 523, "judge_prompt_tokens": 413, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest rated Japanese language movie from 1999 is \"Kikujiro,\" with a rating of 7.5 [Doc 22213].", "latency_ms": 881.7412853240967, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer names 'Kikujiro' as the highest-rated Japanese language movie from 1999, but the gold answer is 'The Uncut Story of Mewtwo's Origin.' Thus, the content is incorrect relative to the gold. However, it is faithful because the model explicitly cites a document and stays consistent with that source, even though that source conflicts with the gold label.", "main_total_tokens": 3101, "main_prompt_tokens": 3070, "main_completion_tokens": 31, "judge_total_tokens": 532, "judge_prompt_tokens": 422, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is \"Don't Answer the Phone!\" with a rating of 4.6, and its language is English [Doc 768].", "latency_ms": 786.6959571838379, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The question asks only for the language of the lowest-rated movie from 1980; the gold answer is 'English'. The generated answer correctly identifies the language as English, so it is correct with respect to the question. However, it adds an unsupported and unnecessary specific claim about the movie title and rating ('Don’t Answer the Phone!' with rating 4.6) that is not required by the question and may not be grounded, slightly reducing faithfulness.", "main_total_tokens": 2970, "main_prompt_tokens": 2936, "main_completion_tokens": 34, "judge_total_tokens": 542, "judge_prompt_tokens": 417, "judge_completion_tokens": 125}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was \"Freaks and Geeks,\" with a popularity score of 5.8043 [Doc 115187].", "latency_ms": 940.1450157165527, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect relative to the gold label; the correct show is 'Law & Order: Special Victims Unit,' not 'Freaks and Geeks.' It is also unfaithful to the provided gold answer, as it asserts a different TV show and provides a specific popularity score unsupported by the gold reference.", "main_total_tokens": 3371, "main_prompt_tokens": 3335, "main_completion_tokens": 36, "judge_total_tokens": 522, "judge_prompt_tokens": 426, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 776.3807773590088, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and correctly attributes it, so it is both correct and faithful to the provided information.", "main_total_tokens": 2900, "main_prompt_tokens": 2882, "main_completion_tokens": 18, "judge_total_tokens": 461, "judge_prompt_tokens": 400, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in 1999 [Doc 22481].", "latency_ms": 601.2918949127197, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states the film was released in 1999, but the gold label says 2019, so it is factually incorrect relative to the gold. However, the model clearly bases its answer on the cited document [Doc 22481], so it is faithful to its source despite being wrong versus the gold.", "main_total_tokens": 2885, "main_prompt_tokens": 2865, "main_completion_tokens": 20, "judge_total_tokens": 500, "judge_prompt_tokens": 404, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The number of votes \"My Dog Skip\" received is not provided in the document.", "latency_ms": 1321.8321800231934, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates that the correct number of votes is 279. The generated answer claims the number of votes is not provided, which is factually incorrect relative to the gold. However, the generated answer is internally consistent and does not invent a specific incorrect number; it simply states that the information is unavailable. Thus, it is fully faithful to what it asserts (no hallucinated value) but incorrect with respect to the gold answer.", "main_total_tokens": 2647, "main_prompt_tokens": 2630, "main_completion_tokens": 17, "judge_total_tokens": 511, "judge_prompt_tokens": 398, "judge_completion_tokens": 113}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The number of votes for \"Oh My God!\" is not provided in the documents.", "latency_ms": 658.9071750640869, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that \"Oh My God!\" received 12 votes. The generated answer claims the number of votes is not provided, which is incorrect relative to the gold. It also contradicts the given information, so it is neither correct nor faithful.", "main_total_tokens": 2933, "main_prompt_tokens": 2916, "main_completion_tokens": 17, "judge_total_tokens": 476, "judge_prompt_tokens": 398, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in the year 2009 [Doc 46069].", "latency_ms": 602.353572845459, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'The Last Days of Lehman Brothers' was released in 2009, matching the gold answer, and it is supported by the cited document. Both correctness and faithfulness are satisfied.", "main_total_tokens": 3286, "main_prompt_tokens": 3261, "main_completion_tokens": 25, "judge_total_tokens": 489, "judge_prompt_tokens": 412, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 696.3980197906494, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the original language is English (code 'en'), and the generated answer correctly states that the original language of 'The King's Speech' is English. The citation to a document does not contradict the question and is consistent with being grounded in the provided source, so the answer is both correct and faithful.", "main_total_tokens": 3047, "main_prompt_tokens": 3028, "main_completion_tokens": 19, "judge_total_tokens": 496, "judge_prompt_tokens": 402, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The movie \"Wall Street\" was released in the year 1987 [Doc 6993].", "latency_ms": 823.6358165740967, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'Wall Street' was released in 1987, which matches the gold answer. It is fully supported by the cited document, so it is both correct and faithful.", "main_total_tokens": 2193, "main_prompt_tokens": 2173, "main_completion_tokens": 20, "judge_total_tokens": 471, "judge_prompt_tokens": 398, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie in which classmates have to play a survival game using their cell phones is \"Brain Fluid Explosion Girl\" [Doc 69929].", "latency_ms": 680.0801753997803, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Brain Fluid Explosion Girl,' which matches the gold answer exactly. It is fully faithful to the provided information and does not introduce any unsupported details beyond citing the (hypothetical) document reference.", "main_total_tokens": 1860, "main_prompt_tokens": 1832, "main_completion_tokens": 28, "judge_total_tokens": 494, "judge_prompt_tokens": 416, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show in which six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" [Doc 112849].", "latency_ms": 705.7926654815674, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Dungeons & Dragons.' It is fully correct and faithful to the provided information.", "main_total_tokens": 1865, "main_prompt_tokens": 1835, "main_completion_tokens": 30, "judge_total_tokens": 483, "judge_prompt_tokens": 420, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is \"Nacht der Wölfe\" [Doc 2590].", "latency_ms": 679.2919635772705, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe', which matches the gold answer exactly. It is faithful to the provided information and does not introduce contradictions or unsupported details, only adding a citation-style reference [Doc 2590] that does not alter the core answer.", "main_total_tokens": 1889, "main_prompt_tokens": 1850, "main_completion_tokens": 39, "judge_total_tokens": 527, "judge_prompt_tokens": 437, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of \"Picnic at Hanging Rock\" take place in the year 1900 [Doc 126191].", "latency_ms": 733.7501049041748, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of 'Picnic at Hanging Rock' take place in 1900, matching the gold answer, and it does not introduce any unsupported information beyond the provided document reference.", "main_total_tokens": 2141, "main_prompt_tokens": 2117, "main_completion_tokens": 24, "judge_total_tokens": 482, "judge_prompt_tokens": 409, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie about a young violinist named June, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man destined for her, is titled \"June\" [Doc 32483].", "latency_ms": 1260.8726024627686, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title; the gold answer is 'June'. The generated answer identifies the film as 'June' and accurately paraphrases the description, so it is correct. It also stays faithful to the provided information and does not introduce contradictions or unsupported claims.", "main_total_tokens": 2162, "main_prompt_tokens": 2103, "main_completion_tokens": 59, "judge_total_tokens": 576, "judge_prompt_tokens": 491, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1418.8041687011719, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a particular movie, but the model said it did not know. This is incorrect relative to the gold, though the model did not hallucinate and honestly stated uncertainty, so it is fully faithful to its own stated knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 399, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 472.5773334503174, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 7.6, but the generated answer says 'I don't know,' which is incorrect. However, the model does not hallucinate an incorrect value and instead admits not knowing, so it is fully faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 470, "judge_prompt_tokens": 389, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1180.5155277252197, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.366123019050001, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is incorrect. The model directly answered the question without adding unrelated content, so it is faithful to the query but factually wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 468, "judge_prompt_tokens": 394, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 315.52624702453613, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is factually incorrect. It is, however, a direct answer to the question without inventing extra details, so it is faithful to the user query in form, just not in correctness.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 395, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1693.4971809387207, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific TV show, and the gold answer is 'Fantômas'. The generated answer says 'I don't know', which is incorrect. However, it does not introduce any incorrect facts or hallucinations; it simply admits lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 389, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" [Doc 11205].", "latency_ms": 1223.940372467041, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' which matches the gold answer exactly. It stays faithful to the provided information and does not introduce unsupported details beyond a harmless document citation.", "main_total_tokens": 2055, "main_prompt_tokens": 2031, "main_completion_tokens": 24, "judge_total_tokens": 478, "judge_prompt_tokens": 409, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 886.7642879486084, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as \"Won't You Be My Neighbor?\", which matches the gold answer exactly and stays faithful to the provided information without adding unsupported claims.", "main_total_tokens": 2354, "main_prompt_tokens": 2315, "main_completion_tokens": 39, "judge_total_tokens": 507, "judge_prompt_tokens": 440, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The movie \"1969\" was released in the year 1988 [Doc 8389].", "latency_ms": 1066.9968128204346, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 2018, but the generated answer gives 1988, which is incorrect. It also references a different movie (\"1969\") than implied by the gold answer, so it is not faithful to the intended question/answer pair.", "main_total_tokens": 2265, "main_prompt_tokens": 2245, "main_completion_tokens": 20, "judge_total_tokens": 480, "judge_prompt_tokens": 397, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is \"Astro Boy\" [Doc 112591].", "latency_ms": 782.9415798187256, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy', which matches the gold answer exactly. It is fully supported by the provided reference [Doc 112591], so both correctness and faithfulness are maximal.", "main_total_tokens": 2024, "main_prompt_tokens": 1995, "main_completion_tokens": 29, "judge_total_tokens": 491, "judge_prompt_tokens": 416, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 792.3173904418945, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer. It does not introduce any unsupported information, so it is both correct and faithful.", "main_total_tokens": 1776, "main_prompt_tokens": 1746, "main_completion_tokens": 30, "judge_total_tokens": 496, "judge_prompt_tokens": 424, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 713.7384414672852, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983, matching the provided gold answer exactly and not adding unsupported claims.", "main_total_tokens": 2039, "main_prompt_tokens": 2006, "main_completion_tokens": 33, "judge_total_tokens": 505, "judge_prompt_tokens": 428, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6339.542865753174, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model said it did not know the answer, which is incorrect because a specific numeric value is known (80.5772). However, the response did not fabricate any information and accurately reflected its uncertainty, so it is fully faithful to its own reasoning.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 396, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 336.883544921875, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 1 but the generated answer is 0, so it is factually incorrect. The evaluation itself correctly reflects the mismatch, so it is faithful to the given gold and generated answers.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 457, "judge_prompt_tokens": 385, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 774.6126651763916, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Angel Eyes' and remains faithful to the information provided without adding unsupported details.", "main_total_tokens": 2427, "main_prompt_tokens": 2392, "main_completion_tokens": 35, "judge_total_tokens": 491, "judge_prompt_tokens": 429, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French [Doc 110681].", "latency_ms": 687.5028610229492, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states the movie language is French, while the gold label says it is Korean, so the answer is factually incorrect. It also invents a movie title (\"Foreign Language\") and cites a document ID not provided in the prompt, so it is not faithful to the given information.", "main_total_tokens": 2108, "main_prompt_tokens": 2091, "main_completion_tokens": 17, "judge_total_tokens": 484, "judge_prompt_tokens": 393, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 707.2587013244629, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer is 0, so the answer is incorrect. The evaluation correctly compares the provided gold and generated values without adding unsupported information, so it is faithful to the given data.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 470, "judge_prompt_tokens": 394, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The context provided does not include information about movies released in 1979, so I cannot determine the best-rated movie from that year based on the given documents.", "latency_ms": 1194.4787502288818, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Given the gold answer that the database has no movies from 1979, the generated answer correctly states that the context does not include information about movies released in 1979 and therefore it cannot determine the best-rated movie. This is factually consistent with the provided gold answer and does not introduce unsupported information, so it is both correct and faithful.", "main_total_tokens": 2011, "main_prompt_tokens": 1979, "main_completion_tokens": 32, "judge_total_tokens": 527, "judge_prompt_tokens": 427, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The context provided does not mention the release of a movie titled \"Back to the Future Part IV.\" The documents only reference the original trilogy and related documentaries or series [Doc 10621], [Doc 4834], [Doc 9405], [Doc 113788], [Doc 68724], [Doc 50182], [Doc 28792], [Doc 103759], [Doc 68688]. Therefore, there is no information available about the release year of \"Back to the Future Part IV.\"", "latency_ms": 1703.4640312194824, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct answer is that 'Back to the Future Part IV' does not exist; there is no release year. The generated answer frames the issue as missing information in the provided context rather than stating that the movie does not exist. This could mislead a user into thinking the movie might exist but is just not documented, so it's factually incorrect. In terms of faithfulness to the gold answer, it partially overlaps (both say there is no info on a release year), but the gold explicitly denies the movie's existence, whereas the generated response stays agnostic and blames lack of context, so it is only partially faithful.", "main_total_tokens": 2934, "main_prompt_tokens": 2828, "main_completion_tokens": 106, "judge_total_tokens": 665, "judge_prompt_tokens": 506, "judge_completion_tokens": 159}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that no movies matching the criteria can be found, which is correct because the Chernobyl disaster occurred in 1986, so there are no 1980 movies about it. The generated answer is faithful to the gold answer’s core point, though phrased slightly differently and with an additional suggestion to check the event timing.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 537, "judge_prompt_tokens": 434, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 283.1852436065674, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer ('0') is factually incorrect given the gold answer ('4'), so correctness is 0. The answer does not contradict or hallucinate beyond the single numerical response and remains internally consistent with itself, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 388, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 297.8644371032715, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer ('0') is incorrect because the gold answer is '2'. The evaluation itself is faithful to the provided gold and generated answers.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 446, "judge_prompt_tokens": 386, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 291.98360443115234, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because there was 1 TV show about Super Mario Bros released in the 1990s, not 0. The evaluation is faithful to the provided gold answer and comparison.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 460, "judge_prompt_tokens": 389, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 142.32420921325684, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (0) and correctly states that there were no TV shows about Super Mario Bros released between 2000 and 2010. The reasoning aligns with the question and gold label, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 391, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 162.35780715942383, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto', a specific and well-known popular ninja TV show. The generated answer says 'I don't know,' which is factually incorrect relative to the gold and does not attempt to answer the question. However, it does not hallucinate or contradict given information; it simply expresses a lack of knowledge, so it is fully faithful to its own (claimed) uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 495, "judge_prompt_tokens": 386, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The documents provided do not mention a Jurassic Park/Power Ranger crossover movie or its release date.", "latency_ms": 1167.6201820373535, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie was not released. The generated answer says that the documents do not mention such a movie or its release date, which effectively avoids inventing a release date and does not contradict the fact that there is no such movie. It correctly refrains from fabricating information and is faithful to the (implied) absence of evidence in the documents.", "main_total_tokens": 2282, "main_prompt_tokens": 2263, "main_completion_tokens": 19, "judge_total_tokens": 509, "judge_prompt_tokens": 404, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "The context provided does not mention any movie where Barbie fights the Predator. None of the listed Barbie movies involve a plot that includes the Predator character.", "latency_ms": 1649.106502532959, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer clarifies that such a movie was never released, while the generated answer instead refers to missing context and specific listed Barbie movies, implying the answer is contingent on that context. It does not directly address the true status of the movie and is not faithful to the gold.", "main_total_tokens": 2335, "main_prompt_tokens": 2306, "main_completion_tokens": 29, "judge_total_tokens": 498, "judge_prompt_tokens": 411, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The 4th oldest Star Wars movie, based on the provided documents, is \"Star Wars: The Clone Wars\" from 2008 [Doc 42090]. The older Star Wars movies listed are \"Return of the Jedi\" (1983) [Doc 2858], \"The Story of Star Wars\" (2004) [Doc 32101], and \"Empire of Dreams: The Story of the Star Wars Trilogy\" (2004) [Doc 32312].", "latency_ms": 1362.725019454956, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says the 4th oldest Star Wars movie is 'Star Wars: Droids - The Great Heep', while the generated answer claims it is 'Star Wars: The Clone Wars' from 2008 and lists different earlier films. This directly contradicts the gold, so correctness is 0. The generated answer, however, is internally consistent with and explicitly references the provided documents, so it is faithful to its cited sources, giving faithfulness a 1.", "main_total_tokens": 2393, "main_prompt_tokens": 2295, "main_completion_tokens": 98, "judge_total_tokens": 614, "judge_prompt_tokens": 487, "judge_completion_tokens": 127}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The movie titled \"1969\" was released in the year 1988 [Doc 8389].", "latency_ms": 991.4283752441406, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The generated answer states the release year as 1988, while the correct (gold) answer is 1986, so it is factually incorrect. It does, however, appear to be consistent with the cited document, so it is mostly faithful to the provided source, just not to the gold answer.", "main_total_tokens": 2266, "main_prompt_tokens": 2245, "main_completion_tokens": 21, "judge_total_tokens": 493, "judge_prompt_tokens": 398, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 217.22865104675293, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct rating is 8.466, but the generated answer says 'I don't know', which is incorrect. However, the model does not fabricate an answer and honestly states lack of knowledge, so it is fully faithful to its own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 479, "judge_prompt_tokens": 398, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 758.0890655517578, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med,' which matches the gold answer exactly. It does not introduce any contradictory or extraneous information relative to the provided content, so it is both correct and faithful.", "main_total_tokens": 1807, "main_prompt_tokens": 1776, "main_completion_tokens": 31, "judge_total_tokens": 499, "judge_prompt_tokens": 423, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In \"Túnel de los huesos\" (2011), seven prisoners escaped from a prison in Argentina [Doc 53582].", "latency_ms": 860.3231906890869, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Argentina as the country and directly matches the gold answer without adding unsupported information; the bracketed document reference does not alter the factual content.", "main_total_tokens": 2021, "main_prompt_tokens": 1993, "main_completion_tokens": 28, "judge_total_tokens": 482, "judge_prompt_tokens": 419, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice Li Nai falls into a trap designed by Jiang Jie [Doc 135231].", "latency_ms": 1082.310438156128, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The gold answer indicates that the apprentice falls into Jiang Jie's trap, and the generated answer also states that the trap was designed by Jiang Jie. Thus, on the specific question asked, the generated answer is correct. However, the mention of the apprentice's name 'Li Nai' and the citation '[Doc 135231]' are not supported or verifiable from the provided gold information, so the response includes extraneous, unverifiable details, making it not fully faithful.", "main_total_tokens": 2213, "main_prompt_tokens": 2186, "main_completion_tokens": 27, "judge_total_tokens": 536, "judge_prompt_tokens": 411, "judge_completion_tokens": 125}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 641.6497230529785, "retrieval_recall": 0.0, "retrieval_mrr": 0.07692307692307693, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is numerically incorrect because it says 0 while the gold answer is 4. However, given the model likely lacks direct access to a precise count of Korean-language movies mentioning assassins after 2023, saying 0 could still be internally consistent with its own reasoning, so the response is faithful to its (albeit incomplete) knowledge, even though it is factually wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 496, "judge_prompt_tokens": 385, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "37", "latency_ms": 254.7135353088379, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '37' is incorrect because, according to the gold reference, there are no movies featuring both Bruce Lee and Steven Seagal together. The model output also contradicts the provided gold statement, so it is not faithful to the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 476, "judge_prompt_tokens": 395, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of \"Tokyo Mater\" (2008) is 6.3 [Doc 42750].", "latency_ms": 1277.3878574371338, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (6.3) and explicitly cites the document, indicating it is grounded in the provided source.", "main_total_tokens": 2244, "main_prompt_tokens": 2222, "main_completion_tokens": 22, "judge_total_tokens": 464, "judge_prompt_tokens": 405, "judge_completion_tokens": 59}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in the movie \"Fast Track: No Limits\" is street racing [Doc 41929].", "latency_ms": 951.1735439300537, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question expects the specific style of racing to be 'Drift'. The generated answer states 'street racing', which is not the same and is therefore incorrect. It partially aligns in that it still refers to a type of car racing, but it does not match the gold label, so correctness is 0. Since we only evaluate faithfulness to the provided gold answer and question, and the model introduces an unsupported movie title and document reference not present in the prompt, the answer is not fully grounded, leading to a partial faithfulness score.", "main_total_tokens": 2412, "main_prompt_tokens": 2389, "main_completion_tokens": 23, "judge_total_tokens": 542, "judge_prompt_tokens": 401, "judge_completion_tokens": 141}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 1917.008638381958, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the title as \"Confronting a Serial Killer\" and stays faithful to the provided information.", "main_total_tokens": 2234, "main_prompt_tokens": 2188, "main_completion_tokens": 46, "judge_total_tokens": 513, "judge_prompt_tokens": 452, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of \"Armageddon\" (1998) is 7.838 [Doc 20603].", "latency_ms": 871.3221549987793, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and is consistent with the provided document reference, so it is both correct and faithful to the given information.", "main_total_tokens": 2299, "main_prompt_tokens": 2275, "main_completion_tokens": 24, "judge_total_tokens": 475, "judge_prompt_tokens": 409, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is not explicitly mentioned in the provided documents.", "latency_ms": 664.1430854797363, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'The Sixth Sense.' The generated answer incorrectly states that the title is not mentioned, so it is factually incorrect. However, it remains faithful to the (imagined) constraint about provided documents and does not hallucinate an answer, so it is faithful to its own stated basis.", "main_total_tokens": 2193, "main_prompt_tokens": 2167, "main_completion_tokens": 26, "judge_total_tokens": 509, "judge_prompt_tokens": 417, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 801.0952472686768, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer, and it is faithful to the provided information.", "main_total_tokens": 2359, "main_prompt_tokens": 2342, "main_completion_tokens": 17, "judge_total_tokens": 458, "judge_prompt_tokens": 397, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 684.0577125549316, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly giving the title 'The Transporter Refueled' and does not introduce contradictions or unsupported content.", "main_total_tokens": 2360, "main_prompt_tokens": 2331, "main_completion_tokens": 29, "judge_total_tokens": 481, "judge_prompt_tokens": 420, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "\"The Transporter Refueled\" was released in the year 2015 [Doc 67500].", "latency_ms": 755.072832107544, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It does not add contradictory or unsupported information, and the brief citation-like marker does not detract from faithfulness to the provided gold.", "main_total_tokens": 2473, "main_prompt_tokens": 2452, "main_completion_tokens": 21, "judge_total_tokens": 490, "judge_prompt_tokens": 406, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "The context provided does not mention any romantic comedy in which Abraham Lincoln comes back from the dead. The documents listed focus on historical, dramatic, and fictional portrayals of Abraham Lincoln, but none are described as romantic comedies or involve him coming back from the dead in such a genre.", "latency_ms": 1104.999303817749, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a specific romantic comedy and the gold answer says no such movie description can be found. The generated answer correctly states that the provided context does not mention any such romantic comedy and that none of the documents fit that description. This matches the gold response in substance. It is both correct relative to the context and faithful to it, without inventing a movie.", "main_total_tokens": 1930, "main_prompt_tokens": 1873, "main_completion_tokens": 57, "judge_total_tokens": 552, "judge_prompt_tokens": 447, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of the movie that alludes to a manuscript written by Ernest Hemingway is \"Havana Darkness\" [Doc 90923].", "latency_ms": 603.8944721221924, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'Havana Darkness,' which matches the gold answer exactly. It is fully faithful to the information implied by the question and gold reference, with no extra fabricated details beyond a generic citation marker.", "main_total_tokens": 1970, "main_prompt_tokens": 1943, "main_completion_tokens": 27, "judge_total_tokens": 493, "judge_prompt_tokens": 415, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show \"Doll & Em\" [Doc 122259].", "latency_ms": 634.9959373474121, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em' and aligns fully with the gold answer without adding unsupported information. Reference to a document ID does not contradict anything.", "main_total_tokens": 2211, "main_prompt_tokens": 2182, "main_completion_tokens": 29, "judge_total_tokens": 485, "judge_prompt_tokens": 417, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The 2007 movie in which Hallam has a talent for spying on people and attempts to find the cause of his mother’s death is \"Hallam Foe\" [Doc 39389].", "latency_ms": 776.7407894134521, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2007 movie as \"Hallam Foe,\" which matches the gold answer exactly. It does not introduce any contradictions or unsupported details beyond a generic citation marker. The response is both correct and faithful.", "main_total_tokens": 2213, "main_prompt_tokens": 2173, "main_completion_tokens": 40, "judge_total_tokens": 517, "judge_prompt_tokens": 439, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The context provided does not include any specific last names or information about whether a last name sounds like a friend or a foe. Therefore, the answer to your question is not found in the documents provided.", "latency_ms": 1299.5779514312744, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is an explicit label: 'A foe'. The generated answer claims the information is not present in the context and declines to answer. That directly contradicts the gold, so correctness is 0. However, given only the conversation snippet, we do not see the original context; the model is honestly stating that it cannot find the answer in the provided documents, so with respect to the visible prompt it is faithful to its own stated reasoning, hence faithfulness is 1.", "main_total_tokens": 2335, "main_prompt_tokens": 2295, "main_completion_tokens": 40, "judge_total_tokens": 554, "judge_prompt_tokens": 426, "judge_completion_tokens": 128}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "In the 2017 TV show \"Atypical,\" Sam is 18 years old [Doc 124583].", "latency_ms": 817.9411888122559, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Sam's age as 18, matching the gold answer, and it stays faithful to the given information without adding unsupported claims.", "main_total_tokens": 2009, "main_prompt_tokens": 1985, "main_completion_tokens": 24, "judge_total_tokens": 472, "judge_prompt_tokens": 411, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show \"Atypical,\" Sam is on the autism spectrum [Doc 124583].", "latency_ms": 1176.4450073242188, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the TV show 'Atypical,' Sam is on the autism spectrum, which matches the gold answer exactly and does not introduce unsupported information.", "main_total_tokens": 1986, "main_prompt_tokens": 1962, "main_completion_tokens": 24, "judge_total_tokens": 477, "judge_prompt_tokens": 411, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "In the movie \"À corde tendue,\" Sandra was born in Chamonix [Doc 59659].", "latency_ms": 785.3891849517822, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that Sandra was born in Chamonix, which matches the gold answer exactly. It does not introduce contradictions or additional unsupported claims, so it is both correct and faithful.", "main_total_tokens": 2404, "main_prompt_tokens": 2382, "main_completion_tokens": 22, "judge_total_tokens": 488, "judge_prompt_tokens": 419, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In the Japanese language movie \"When the Rain Falls,\" Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 844.3529605865479, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly fills in the blank with 'rains,' matching the gold answer. However, it fabricates extra details: the supposed movie title 'When the Rain Falls' and a citation '[Doc 100481]' that are not supported by the provided information. Thus, correctness is 1.0 but faithfulness is reduced due to hallucinated specifics.", "main_total_tokens": 2531, "main_prompt_tokens": 2505, "main_completion_tokens": 26, "judge_total_tokens": 523, "judge_prompt_tokens": 417, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Tabaluga is the crown-prince of Greenland [Doc 114810].", "latency_ms": 734.3170642852783, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies that Tabaluga is the crown-prince of Greenland and aligns exactly with the gold answer, with no added or conflicting information.", "main_total_tokens": 2218, "main_prompt_tokens": 2202, "main_completion_tokens": 16, "judge_total_tokens": 472, "judge_prompt_tokens": 410, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers\" is \"One Afternoon\" [Doc 96838].", "latency_ms": 930.445671081543, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as 'One Afternoon'. It does not introduce contradictions or unsupported claims and remains grounded in the provided description and citation.", "main_total_tokens": 1822, "main_prompt_tokens": 1778, "main_completion_tokens": 44, "judge_total_tokens": 514, "judge_prompt_tokens": 448, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" [Doc 93260].", "latency_ms": 863.1165027618408, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title of a low-rated Italian language movie where a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her. The gold answer is \"The Life Ahead.\" The generated answer also identifies the movie as \"The Life Ahead\" and stays consistent with the provided information without adding contradictory details. Thus it is both correct and faithful to the gold/reference.", "main_total_tokens": 2107, "main_prompt_tokens": 2070, "main_completion_tokens": 37, "judge_total_tokens": 544, "judge_prompt_tokens": 435, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 937.4384880065918, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies that the 2016 documentary 'McLaren' is about Bruce McLaren and aligns with the gold answer. The additional details (that he is a New Zealander and founder of the McLaren Motor Racing team) are accurate and do not contradict the source, so the response is both correct and faithful.", "main_total_tokens": 2114, "main_prompt_tokens": 2081, "main_completion_tokens": 33, "judge_total_tokens": 515, "judge_prompt_tokens": 417, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The name of the racing team is not provided in the given documents.", "latency_ms": 783.8575839996338, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly provides the team name (The McLaren Motor Racing team). The generated answer claims the name is not provided, which is factually incorrect relative to the gold. However, it is faithful to the (incorrect) statement it makes about the documents, so faithfulness as an evaluation of hallucination vs. given context is high, but correctness relative to the gold is zero.", "main_total_tokens": 2216, "main_prompt_tokens": 2202, "main_completion_tokens": 14, "judge_total_tokens": 506, "judge_prompt_tokens": 397, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context provided does not specify the country he is from.", "latency_ms": 658.9839458465576, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates the correct country is New Zealand, but the generated answer claims the context does not specify the country, which is incorrect relative to the gold. However, the generated answer is internally consistent and does not fabricate a country, so it is faithful to its stated uncertainty, but factually wrong given the gold.", "main_total_tokens": 1923, "main_prompt_tokens": 1911, "main_completion_tokens": 12, "judge_total_tokens": 478, "judge_prompt_tokens": 388, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "The 2013 movie \"A Good Day to Die Hard\" does not involve John McClane blowing up the Great Wall of China. The plot is centered around John McClane traveling to Moscow to help his son, who is a CIA operative, stop a nuclear weapons heist [Doc 58791]. There is no mention of the Great Wall of China in the context provided.", "latency_ms": 1280.3332805633545, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.9, "faithfulness_score": 0.8, "justification": "The question is effectively a trick: there is no 2013 movie where John McClane blows up the Great Wall of China. The gold answer explicitly says such a movie is not in the database. The generated answer correctly states that in the actual 2013 McClane film, A Good Day to Die Hard, there is no Great Wall scene, so it correctly denies the premise, but it implicitly treats the question as referring to that real film instead of explicitly saying no such movie exists. This is mostly correct but not perfectly aligned with the gold. Faithfulness is high because the answer uses the provided document about A Good Day to Die Hard and does not hallucinate a Great Wall scene, but it adds plot details that are not in the gold reference answer, so it is slightly less than perfect relative to the specified gold behavior.", "main_total_tokens": 2489, "main_prompt_tokens": 2411, "main_completion_tokens": 78, "judge_total_tokens": 679, "judge_prompt_tokens": 476, "judge_completion_tokens": 203}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with the description \"When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental\" is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 1495.9988594055176, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as 'Tomie: Forbidden Fruit'. It is correct and fully supported by the provided description and citation, with no fabrication or contradiction.", "main_total_tokens": 2250, "main_prompt_tokens": 2192, "main_completion_tokens": 58, "judge_total_tokens": 545, "judge_prompt_tokens": 475, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years in the 2021 movie \"Lei mi parla ancora\" [Doc 98566].", "latency_ms": 873.9194869995117, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, matching the gold answer. It is faithful to the provided information and does not introduce unsupported details.", "main_total_tokens": 2613, "main_prompt_tokens": 2583, "main_completion_tokens": 30, "judge_total_tokens": 493, "judge_prompt_tokens": 424, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In \"Scorpion King: Book of Souls\" (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls, which will allow them to put an end to an evil warlord [Doc 82582].", "latency_ms": 906.6543579101562, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in 'Scorpion King: Book of Souls' (2018), the Scorpion King and Tala are searching for the Book of Souls, matching the gold answer. The mention of it being a legendary relic that can end an evil warlord is an accurate elaboration and does not contradict the core fact, so it is both correct and faithful.", "main_total_tokens": 2525, "main_prompt_tokens": 2474, "main_completion_tokens": 51, "judge_total_tokens": 551, "judge_prompt_tokens": 445, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Zombies on the Moon\" is not found in the provided documents.", "latency_ms": 862.4887466430664, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database; the generated answer states the release year is not found in the provided documents. Both convey that the information is unavailable, so the response is correct and faithful to the gold.", "main_total_tokens": 2258, "main_prompt_tokens": 2239, "main_completion_tokens": 19, "judge_total_tokens": 487, "judge_prompt_tokens": 410, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Galactic Baker\" is not provided in the given documents.", "latency_ms": 780.8268070220947, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, implying the release year is unknown. The generated answer similarly states that the release year is not provided in the given documents, which is effectively equivalent in context. It does not invent a year and remains consistent with the available information, so it is both correct and faithful to the gold.", "main_total_tokens": 2126, "main_prompt_tokens": 2109, "main_completion_tokens": 17, "judge_total_tokens": 506, "judge_prompt_tokens": 406, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Return of the Null Pointer\" is not found in the provided documents.", "latency_ms": 890.7322883605957, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available; the model answer is correct and consistent with the information given and does not hallucinate a date.", "main_total_tokens": 1893, "main_prompt_tokens": 1874, "main_completion_tokens": 19, "judge_total_tokens": 475, "judge_prompt_tokens": 410, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year of release for \"Data Science: The Movie\" is not provided in the given documents.", "latency_ms": 949.6657848358154, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the release year is not available, and the generated answer is faithful to the given information.", "main_total_tokens": 2420, "main_prompt_tokens": 2400, "main_completion_tokens": 20, "judge_total_tokens": 469, "judge_prompt_tokens": 411, "judge_completion_tokens": 58}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Python's Revenge 2\" is not found in the provided documents.", "latency_ms": 721.4107513427734, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available/found, which is correct given the information. The generated answer is faithful to the (implied) source documents, explicitly saying the year is not found in the provided documents, matching the gold response in substance.", "main_total_tokens": 2257, "main_prompt_tokens": 2238, "main_completion_tokens": 19, "judge_total_tokens": 498, "judge_prompt_tokens": 410, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Last Pixel\" is not found in the provided documents.", "latency_ms": 656.855583190918, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the release year is not available; the model does not hallucinate a date and stays faithful to the (absent) source information.", "main_total_tokens": 1935, "main_prompt_tokens": 1918, "main_completion_tokens": 17, "judge_total_tokens": 473, "judge_prompt_tokens": 406, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year of release for \"The man who ate the sun\" is not found in the provided documents.", "latency_ms": 930.3674697875977, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available/found. The generated answer correctly reflects that the information is not in the provided documents and does not fabricate a date, so it is both correct and faithful.", "main_total_tokens": 2666, "main_prompt_tokens": 2645, "main_completion_tokens": 21, "judge_total_tokens": 490, "judge_prompt_tokens": 413, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Days of Lehman Brothers\" is not mentioned in the provided documents.", "latency_ms": 749.1378784179688, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the director information is not available in the database. The generated answer similarly states that the director is not mentioned in the provided documents. This matches the limitation of the given information, so it is both correct (relative to the database constraint) and faithful to the provided context.", "main_total_tokens": 2586, "main_prompt_tokens": 2564, "main_completion_tokens": 22, "judge_total_tokens": 508, "judge_prompt_tokens": 419, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"A Love to Die For\" is not mentioned in the provided documents.", "latency_ms": 843.1265354156494, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned and aligns with the gold answer that the information is not in the database. It does not introduce unsupported facts and remains faithful to the given context.", "main_total_tokens": 2356, "main_prompt_tokens": 2336, "main_completion_tokens": 20, "judge_total_tokens": 486, "judge_prompt_tokens": 415, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not mentioned in the provided documents. [Doc 111360]", "latency_ms": 939.7695064544678, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director is not provided/mentioned, which matches the information constraint. The generated answer is consistent with the given documents and does not hallucinate a director, so it is both correct and faithful.", "main_total_tokens": 2232, "main_prompt_tokens": 2208, "main_completion_tokens": 24, "judge_total_tokens": 495, "judge_prompt_tokens": 417, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Night at Tremore Beach\" is not mentioned in the provided documents.", "latency_ms": 672.7311611175537, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information for 'The Last Night at Tremore Beach' is not provided in the available data. The generated answer is correct relative to the database and faithful to the given context and constraints.", "main_total_tokens": 1968, "main_prompt_tokens": 1946, "main_completion_tokens": 22, "judge_total_tokens": 497, "judge_prompt_tokens": 419, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 785.529375076294, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, 'Rise of the Mummy', and does not introduce unsupported information beyond a harmless document reference. It is both correct and faithful to the provided gold answer.", "main_total_tokens": 1965, "main_prompt_tokens": 1938, "main_completion_tokens": 27, "judge_total_tokens": 489, "judge_prompt_tokens": 418, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse is \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 783.5044860839844, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Stranger in My Bed,' which matches the gold answer exactly. It stays faithful to the provided information without adding unsupported details, only restating the plot element given in the question.", "main_total_tokens": 2117, "main_prompt_tokens": 2088, "main_completion_tokens": 29, "judge_total_tokens": 501, "judge_prompt_tokens": 425, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The scenario described, where a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years, occurs in the TV show \"Game of Silence\" [Doc 123938].", "latency_ms": 951.6193866729736, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question explicitly asks for a movie, but both the gold answer and the generated answer refer to 'Game of Silence', which is a TV show, not a movie. Therefore, the generated answer is incorrect with respect to the question. In terms of faithfulness, the task is to match the gold answer; however, since the gold itself does not match the question type (movie vs TV show), the generated answer repeating 'Game of Silence' cannot be considered faithful to the correct task intent. As an evaluator, we must rate based on correctness to the question, so both correctness and faithfulness are 0.", "main_total_tokens": 1703, "main_prompt_tokens": 1664, "main_completion_tokens": 39, "judge_total_tokens": 590, "judge_prompt_tokens": 436, "judge_completion_tokens": 154}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie in which the plot involves a killer from the site Craigslist is \"The Craigslist Killer\" [Doc 53039].", "latency_ms": 1274.6024131774902, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, 'The Craigslist Killer', and correctly identifies the movie. It does not introduce contradictions or fabricate details beyond reiterating the correct title, so it is fully correct and faithful.", "main_total_tokens": 2049, "main_prompt_tokens": 2024, "main_completion_tokens": 25, "judge_total_tokens": 491, "judge_prompt_tokens": 417, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which Yuki Hirano, after failing his university entrance exam, sets his eyes on the role of a forestry trainee is \"Wood Job!\" [Doc 66094].", "latency_ms": 830.4634094238281, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", which matches the gold answer exactly. It stays faithful to the given information without introducing contradictions or unsupported claims.", "main_total_tokens": 2363, "main_prompt_tokens": 2326, "main_completion_tokens": 37, "judge_total_tokens": 502, "judge_prompt_tokens": 437, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure is \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 806.1296939849854, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase' and does not introduce any contradictions or unsupported information.", "main_total_tokens": 2130, "main_prompt_tokens": 2081, "main_completion_tokens": 49, "judge_total_tokens": 524, "judge_prompt_tokens": 459, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a 7-year-old boy must find the hero within him to save his parents trapped inside a burning car due to a rock fall is \"Beyond\" [Doc 74597].", "latency_ms": 765.8195495605469, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title that matches the described plot. The gold answer is \"Beyond.\" The generated answer identifies the movie as \"Beyond\" and repeats the plot, matching the gold exactly. There is no contradiction or hallucination beyond referencing a document ID. Thus, the answer is both correct and faithful to the provided gold.", "main_total_tokens": 1769, "main_prompt_tokens": 1729, "main_completion_tokens": 40, "judge_total_tokens": 546, "judge_prompt_tokens": 448, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman, fired from a financial corporation during the Asia crisis, returns home with no money is \"6ixtynin9\" [Doc 22478].", "latency_ms": 772.9995250701904, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, naming the correct movie '6ixtynin9' and does not introduce any unsupported information. It is both correct and faithful.", "main_total_tokens": 1849, "main_prompt_tokens": 1812, "main_completion_tokens": 37, "judge_total_tokens": 506, "judge_prompt_tokens": 439, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie in which a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort, and it's up to the club's staff to stop the violence or at least hide it, is \"Club Dread\" [Doc 32410].", "latency_ms": 929.4564723968506, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread', which matches the gold answer exactly. It remains faithful to the given information and does not introduce contradictions or irrelevant details.", "main_total_tokens": 2190, "main_prompt_tokens": 2139, "main_completion_tokens": 51, "judge_total_tokens": 542, "judge_prompt_tokens": 475, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is \"Toilet\" [Doc 48873].", "latency_ms": 916.6314601898193, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" which matches the gold answer exactly. It stays fully grounded in the provided information without adding unsupported details, so both correctness and faithfulness are perfect.", "main_total_tokens": 2441, "main_prompt_tokens": 2390, "main_completion_tokens": 51, "judge_total_tokens": 537, "judge_prompt_tokens": 465, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century is \"Electric Angel\" [Doc 1024].", "latency_ms": 934.5176219940186, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot. The gold answer is 'Electric Angel'. The generated answer also identifies the movie as 'Electric Angel' and restates the description consistently. Thus, it is both correct and faithful to the provided information.", "main_total_tokens": 2111, "main_prompt_tokens": 2066, "main_completion_tokens": 45, "judge_total_tokens": 542, "judge_prompt_tokens": 461, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023, based on the provided context, is \"Down Low\" with a rating of 5.207 [Doc 106995].", "latency_ms": 773.1752395629883, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the lowest rated movie from 2023 is 'Mega Ape', while the generated answer claims it is 'Down Low' with a specific rating. This directly contradicts the gold answer, so the response is factually incorrect in this evaluation setting. Additionally, the model references a document and rating not present in the given QA pair, indicating it is not faithful to the provided information.", "main_total_tokens": 2019, "main_prompt_tokens": 1985, "main_completion_tokens": 34, "judge_total_tokens": 522, "judge_prompt_tokens": 415, "judge_completion_tokens": 107}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 1492.8126335144043, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Tea House', but the generated answer is 'I don't know', which is incorrect relative to the gold answer. However, it does not fabricate any incorrect specific content and honestly reflects uncertainty, so it is fully faithful to the model's stated knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 471, "judge_prompt_tokens": 386, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 756.3018798828125, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct movie ('The Uncut Story of Mewtwo's Origin'), so it is incorrect. However, it does not contradict the input or fabricate information, so it is fully faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 395, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The language of the lowest-rated movie from 1980 is English. The movie is \"Don't Answer the Phone!\" with a rating of 4.6 [Doc 768].", "latency_ms": 1080.2946090698242, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly states that the language is English, which matches the gold answer. However, it additionally specifies a particular movie title and rating that are not present in the gold reference, so those extra details may not be fully supported by the provided ground truth, slightly reducing faithfulness.", "main_total_tokens": 2394, "main_prompt_tokens": 2358, "main_completion_tokens": 36, "judge_total_tokens": 509, "judge_prompt_tokens": 419, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was \"Freaks and Geeks,\" with a popularity score of 5.8043 [Doc 115187].", "latency_ms": 1269.913673400879, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect relative to the gold label. The gold specifies 'Law & Order: Special Victims Unit' as the TV show with the highest popularity in 1999, but the generated answer claims it was 'Freaks and Geeks' with a specific popularity score. However, the generated answer is faithful to its cited document (Doc 115187), so faithfulness is high even though the final answer is wrong compared to the gold.", "main_total_tokens": 2350, "main_prompt_tokens": 2314, "main_completion_tokens": 36, "judge_total_tokens": 549, "judge_prompt_tokens": 426, "judge_completion_tokens": 123}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 819.8370933532715, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and does not introduce any unsupported information. It is both correct and faithful to the provided gold answer.", "main_total_tokens": 2221, "main_prompt_tokens": 2203, "main_completion_tokens": 18, "judge_total_tokens": 464, "judge_prompt_tokens": 400, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in 1999 [Doc 22481].", "latency_ms": 1120.3205585479736, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer gives the release year as 1999, but the gold label says 2019, so it is factually incorrect relative to the gold. However, the model explicitly cites a document as its source ([Doc 22481]) and does not contradict its own source, so it is faithful to the provided context.", "main_total_tokens": 2218, "main_prompt_tokens": 2198, "main_completion_tokens": 20, "judge_total_tokens": 501, "judge_prompt_tokens": 404, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "2", "latency_ms": 252.30908393859863, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 279 votes, but the generated answer is 2, which is incorrect. The model clearly outputs a specific numeric answer, so it is faithful to its own reasoning, but the answer does not match the gold standard, so correctness is 0.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 463, "judge_prompt_tokens": 383, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "11", "latency_ms": 109.86661911010742, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (11) does not match the gold answer (12), so it is incorrect. However, the evaluation is faithful because it correctly reflects the provided gold and generated answers.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 451, "judge_prompt_tokens": 383, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in the year 2009 [Doc 46069].", "latency_ms": 599.7076034545898, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Last Days of Lehman Brothers' was released in 2009, matching the gold answer, and it is supported by the cited document, so it is both correct and faithful.", "main_total_tokens": 2563, "main_prompt_tokens": 2538, "main_completion_tokens": 25, "judge_total_tokens": 487, "judge_prompt_tokens": 412, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 608.6173057556152, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'en' (English), and the generated answer correctly states that the original language of 'The King's Speech' is English. It matches the gold and does not introduce contradictions, so it is both correct and faithful to the provided information.", "main_total_tokens": 2241, "main_prompt_tokens": 2222, "main_completion_tokens": 19, "judge_total_tokens": 484, "judge_prompt_tokens": 402, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "1987 [Doc 6993].", "latency_ms": 1195.2214241027832, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, which matches the gold answer. The citation tag does not change the factual content, so the response is both correct and faithful to the question and gold reference.", "main_total_tokens": 2740, "main_prompt_tokens": 2731, "main_completion_tokens": 9, "judge_total_tokens": 464, "judge_prompt_tokens": 387, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Brain Fluid Explosion Girl", "latency_ms": 1666.4156913757324, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie. It is both correct and faithful.", "main_total_tokens": 2391, "main_prompt_tokens": 2386, "main_completion_tokens": 5, "judge_total_tokens": 444, "judge_prompt_tokens": 392, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 930.778980255127, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Dungeons & Dragons'.", "main_total_tokens": 2390, "main_prompt_tokens": 2385, "main_completion_tokens": 5, "judge_total_tokens": 447, "judge_prompt_tokens": 394, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 1076.155662536621, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'Nacht der Wölfe' as the movie about two rival gangs in Munich, one German and one Turkish. It is both correct and fully faithful to the reference.", "main_total_tokens": 2412, "main_prompt_tokens": 2405, "main_completion_tokens": 7, "judge_total_tokens": 480, "judge_prompt_tokens": 403, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900", "latency_ms": 1069.7264671325684, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer '1900' and correctly identifies the year in which the events of Picnic at Hanging Rock take place.", "main_total_tokens": 2695, "main_prompt_tokens": 2692, "main_completion_tokens": 3, "judge_total_tokens": 448, "judge_prompt_tokens": 388, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June", "latency_ms": 507.1101188659668, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('June'), correctly identifying the movie and staying fully faithful to the reference.", "main_total_tokens": 128, "main_prompt_tokens": 127, "main_completion_tokens": 1, "judge_total_tokens": 487, "judge_prompt_tokens": 433, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 240.28539657592773, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer admits not knowing the answer, while a correct answer exists in the gold. This makes the response incorrect. However, it does not invent or contradict any facts, so it is fully faithful to the model's stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 477, "judge_prompt_tokens": 399, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 333.5082530975342, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the rating as 7.6, while the generated answer says 'I don't know,' which does not match the correct value. Therefore, the correctness is 0. However, the model honestly admits not knowing and does not fabricate any information, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 389, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "12", "latency_ms": 1314.8248195648193, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.366123019050001, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (12) does not match the gold answer (4), so it is factually incorrect for this evaluation. However, there is no indication that the model contradicted or ignored the user’s question; it attempted to answer the exact query (count of movies from 1987, rating over 7, in Chinese). Thus, it is unfaithful to the ground truth but faithful to the user’s request.", "main_total_tokens": 1465, "main_prompt_tokens": 1442, "main_completion_tokens": 23, "judge_total_tokens": 511, "judge_prompt_tokens": 394, "judge_completion_tokens": 117}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 325.28185844421387, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The gold answer indicates there are 2 such TV shows, but the generated answer says none match the criteria, so it is factually incorrect. It is somewhat faithful in that it does not hallucinate a specific wrong number, instead expressing inability, but this still contradicts the known gold answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 496, "judge_prompt_tokens": 404, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 299.74889755249023, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer admits not knowing the answer, while the gold answer states a specific show (Fantômas). Since the question asks for a specific TV show, saying 'I don't know' is not correct. However, the response is faithful to the model's uncertainty and does not fabricate information, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 389, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise", "latency_ms": 883.5234642028809, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the movie as 'Kill Cruise.'", "main_total_tokens": 2579, "main_prompt_tokens": 2576, "main_completion_tokens": 3, "judge_total_tokens": 436, "judge_prompt_tokens": 387, "judge_completion_tokens": 49}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 769.9499130249023, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the documentary film about Fred Rogers and his TV series as 'Won't You Be My Neighbor?'", "main_total_tokens": 2939, "main_prompt_tokens": 2931, "main_completion_tokens": 8, "judge_total_tokens": 469, "judge_prompt_tokens": 408, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018", "latency_ms": 1787.0562076568604, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (2018), so it is both correct and faithful.", "main_total_tokens": 2882, "main_prompt_tokens": 2866, "main_completion_tokens": 16, "judge_total_tokens": 431, "judge_prompt_tokens": 380, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy", "latency_ms": 1904.458999633789, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'Astro Boy' as the animated TV series about a young boy robot in 2030 AD. It is both correct and faithful to the gold standard.", "main_total_tokens": 2594, "main_prompt_tokens": 2590, "main_completion_tokens": 4, "judge_total_tokens": 463, "judge_prompt_tokens": 389, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 [Doc 67513]", "latency_ms": 1308.8629245758057, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer '7' matches the gold answer '7 years' and correctly reflects the information without adding unsupported details.", "main_total_tokens": 2337, "main_prompt_tokens": 2329, "main_completion_tokens": 8, "judge_total_tokens": 457, "judge_prompt_tokens": 402, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26 million [Doc 133469].", "latency_ms": 783.1807136535645, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer in both value and meaning (£26m vs £26 million) and stays faithful to the provided document reference.", "main_total_tokens": 2028, "main_prompt_tokens": 2019, "main_completion_tokens": 9, "judge_total_tokens": 464, "judge_prompt_tokens": 404, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 594.534158706665, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score, and the gold answer provides that number (80.5772). The generated answer says only \"I don't know,\" which does not match the correct value, so correctness is 0. However, the model is honest about its lack of knowledge and does not fabricate a value, so it is fully faithful to its own uncertainty, yielding a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 511, "judge_prompt_tokens": 396, "judge_completion_tokens": 115}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 345.0908660888672, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there was 1 Russian language TV show released in 1997, while the generated answer claims none can be found. This directly contradicts the gold, so the answer is incorrect. It is also unfaithful to the gold reference because it denies the existence of the show instead of providing the correct count.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 486, "judge_prompt_tokens": 394, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes", "latency_ms": 1027.5814533233643, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Angel Eyes') and correctly identifies the TV show where Soo-wan regains her sight after an eye transplant. It is fully correct and faithful to the provided gold answer.", "main_total_tokens": 3003, "main_prompt_tokens": 3000, "main_completion_tokens": 3, "judge_total_tokens": 471, "judge_prompt_tokens": 397, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "en", "latency_ms": 1251.79123878479, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says the language is 'en' (English), while the gold label is Korean, so the answer is factually incorrect, giving correctness 0. The model is at least answering the question directly and not adding fabricated details, so faithfulness to the prompt itself is high, scored as 1.", "main_total_tokens": 2936, "main_prompt_tokens": 2924, "main_completion_tokens": 12, "judge_total_tokens": 471, "judge_prompt_tokens": 377, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 662.8987789154053, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there are 2 such movies, while the generated answer claims there are none. This is factually incorrect, so correctness is 0. However, given the model stated its inability to find matching movies rather than fabricating a specific number, it remains faithful to its own (albeit limited) retrieval, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 506, "judge_prompt_tokens": 403, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 394.9871063232422, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer explicitly states that there are no movies from 1979 in its database. The generated answer says only 'I don't know,' which does not convey the same specific limitation about the database contents. It is therefore not faithful to the gold. In addition, relative to the gold context (where the system knows its database), the correct response would be to state the database limitation, so the generated response is also considered incorrect with respect to that standard.", "main_total_tokens": 568, "main_prompt_tokens": 567, "main_completion_tokens": 1, "judge_total_tokens": 521, "judge_prompt_tokens": 399, "judge_completion_tokens": 122}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "I don't know.", "latency_ms": 454.54931259155273, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer correctly states that 'Back to the Future Part IV' does not exist. The generated answer, 'I don't know,' is not fully correct, because with the available information it should explicitly state that such a movie does not exist rather than expressing uncertainty. Therefore, correctness is 0. Faithfulness is also 0 because the generated answer does not convey the key information in the gold response that the film does not exist; it changes the meaning from a definitive statement to uncertainty.", "main_total_tokens": 589, "main_prompt_tokens": 588, "main_completion_tokens": 1, "judge_total_tokens": 533, "judge_prompt_tokens": 404, "judge_completion_tokens": 129}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly notes that it cannot find any movies or shows matching the criteria, which aligns with the gold answer that there are no movies from 1980 about the Chernobyl disaster. The Chernobyl disaster occurred in 1986, so it's accurate that such a 1980 movie does not exist. The reasoning and conclusion are faithful to the question and constraints.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 541, "judge_prompt_tokens": 434, "judge_completion_tokens": 107}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 293.1962013244629, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there are 4 such titles, but the generated answer claims there are none. This is factually incorrect relative to the gold, so correctness is 0. However, the model does not hallucinate a specific incorrect number and instead admits inability to find matches, which is faithful to its stated limitation, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 499, "judge_prompt_tokens": 397, "judge_completion_tokens": 102}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 852.9541492462158, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct (gold) answer is 2, but the generated answer states that there are no matching movies, which is factually incorrect. However, the model does faithfully acknowledge its inability to find information rather than fabricating a number, so it is faithful to its own (albeit limited) retrieval, just not correct relative to the gold.", "main_total_tokens": 3285, "main_prompt_tokens": 3284, "main_completion_tokens": 1, "judge_total_tokens": 495, "judge_prompt_tokens": 395, "judge_completion_tokens": 100}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 290.7285690307617, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 1, but the generated answer says there are none, so it is factually incorrect (score 0 for correctness). However, it does not invent any facts or contradict its own statements; it simply admits not finding any, so it is internally faithful to its (stated) inability (score 1 for faithfulness).", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 499, "judge_prompt_tokens": 398, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 229.85506057739258, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states there were 0 TV shows, while the generated answer says it cannot find any movies or shows matching the criteria. This effectively aligns with the gold: no such TV shows are found. The answer is correct given the gold label, and it does not contradict the question or introduce unsupported claims, so it is faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 498, "judge_prompt_tokens": 400, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 289.73960876464844, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto', but the generated answer says 'I don't know', which is incorrect. It is faithful in that it honestly reflects uncertainty rather than hallucinating an answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 455, "judge_prompt_tokens": 386, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 770.6742286682129, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that the movie was not released, but the generated answer said 'I don't know,' which is incorrect. However, it is faithful to the prompt since it does not introduce any fabricated information.", "main_total_tokens": 531, "main_prompt_tokens": 530, "main_completion_tokens": 1, "judge_total_tokens": 462, "judge_prompt_tokens": 389, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 793.764591217041, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the movie was not released, whereas the generated answer says 'I don't know,' which is incorrect relative to the gold. It also does not reflect the provided gold information, so it is unfaithful.", "main_total_tokens": 554, "main_prompt_tokens": 553, "main_completion_tokens": 1, "judge_total_tokens": 463, "judge_prompt_tokens": 386, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "I don't know.", "latency_ms": 573.8165378570557, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which is factually incorrect because the 4th oldest Star Wars movie is known (per the gold: 'Star Wars: Droids - The Great Heep'). However, the response does not introduce any incorrect information; it simply declines to answer, so it is fully faithful to the (lack of) reasoning or claims made.", "main_total_tokens": 530, "main_prompt_tokens": 529, "main_completion_tokens": 1, "judge_total_tokens": 499, "judge_prompt_tokens": 393, "judge_completion_tokens": 106}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "2004", "latency_ms": 2197.6842880249023, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer ('2004') does not match the gold answer ('1986'), so it is factually incorrect. However, there is no sign of ignoring or contradicting any provided context beyond giving the wrong year, so it is still fully faithful to the (minimal) prompt.", "main_total_tokens": 2816, "main_prompt_tokens": 2800, "main_completion_tokens": 16, "judge_total_tokens": 468, "judge_prompt_tokens": 380, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 303.9534091949463, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model response is factually incorrect because the gold rating is 8.466, but the answer given was 'I don't know.' However, it is faithful to the prompt in that it does not hallucinate a rating and honestly states lack of knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 398, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med", "latency_ms": 816.3292407989502, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Mighty Med' exactly matches the gold answer and correctly identifies the TV show, so it is both correct and faithful.", "main_total_tokens": 2360, "main_prompt_tokens": 2356, "main_completion_tokens": 4, "judge_total_tokens": 453, "judge_prompt_tokens": 394, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina [Doc 53582].", "latency_ms": 1101.9566059112549, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Argentina as the country, matching the gold answer without adding unsupported information.", "main_total_tokens": 2612, "main_prompt_tokens": 2604, "main_completion_tokens": 8, "judge_total_tokens": 448, "judge_prompt_tokens": 398, "judge_completion_tokens": 50}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "Jiang Jie [Doc 135231].", "latency_ms": 2408.853530883789, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, identifying Jiang Jie as the person whose trap the apprentice falls into. There is no contradiction or hallucinated information beyond the provided content, so it is both correct and faithful.", "main_total_tokens": 2764, "main_prompt_tokens": 2754, "main_completion_tokens": 10, "judge_total_tokens": 466, "judge_prompt_tokens": 392, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 816.0879611968994, "retrieval_recall": 0.0, "retrieval_mrr": 0.07692307692307693, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The correct answer is 4, but the generated answer claims there are none, so it is factually incorrect (correctness 0). The model stays within its stated limitations by saying it cannot find any, rather than hallucinating a specific incorrect number, so it is mostly faithful to its own capabilities and does not invent details (faithfulness 0.8).", "main_total_tokens": 3424, "main_prompt_tokens": 3423, "main_completion_tokens": 1, "judge_total_tokens": 495, "judge_prompt_tokens": 394, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 544.7967052459717, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer is correct: there are no known movies featuring both Bruce Lee and Steven Seagal together. It is also faithful to the gold answer, effectively conveying the same information that no such movies can be found, albeit with slightly different wording mentioning movies or shows and criteria.", "main_total_tokens": 2222, "main_prompt_tokens": 2221, "main_completion_tokens": 1, "judge_total_tokens": 492, "judge_prompt_tokens": 406, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3 [Doc 42750].", "latency_ms": 1029.8428535461426, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3 and does not introduce unsupported information, so it is both correct and faithful.", "main_total_tokens": 2244, "main_prompt_tokens": 2235, "main_completion_tokens": 9, "judge_total_tokens": 452, "judge_prompt_tokens": 393, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "Drift racing [Doc 33742].", "latency_ms": 1749.6724128723145, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The correct answer is 'Drift', and the generated answer 'Drift racing' is effectively correct in content. However, it introduces an unsupported citation/identifier '[Doc 33742]' that is not grounded in the provided context, so it is not fully faithful.", "main_total_tokens": 3133, "main_prompt_tokens": 3111, "main_completion_tokens": 22, "judge_total_tokens": 471, "judge_prompt_tokens": 386, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer", "latency_ms": 1505.8465003967285, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct title of the described movie. It is both correct and faithful to the provided reference.", "main_total_tokens": 2786, "main_prompt_tokens": 2779, "main_completion_tokens": 7, "judge_total_tokens": 473, "judge_prompt_tokens": 412, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838 [Doc 20603]", "latency_ms": 691.7662620544434, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and does not introduce unsupported information. It is both correct and faithful.", "main_total_tokens": 2297, "main_prompt_tokens": 2288, "main_completion_tokens": 9, "judge_total_tokens": 454, "judge_prompt_tokens": 395, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense", "latency_ms": 837.6901149749756, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 1999 psychological thriller as 'The Sixth Sense' and remaining fully faithful to the question.", "main_total_tokens": 2734, "main_prompt_tokens": 2730, "main_completion_tokens": 4, "judge_total_tokens": 456, "judge_prompt_tokens": 394, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002", "latency_ms": 1781.470775604248, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the release year as 2002.", "main_total_tokens": 2925, "main_prompt_tokens": 2922, "main_completion_tokens": 3, "judge_total_tokens": 433, "judge_prompt_tokens": 383, "judge_completion_tokens": 50}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled", "latency_ms": 939.2440319061279, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly gives the title of the 2015 reboot of 'The Transporter' as 'The Transporter Refueled'.", "main_total_tokens": 2925, "main_prompt_tokens": 2918, "main_completion_tokens": 7, "judge_total_tokens": 463, "judge_prompt_tokens": 397, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015 [Doc 67500].", "latency_ms": 1753.0109882354736, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer gives the correct year (2015) and does not introduce any contradictions or unsupported claims beyond a harmless document reference, so it is both correct and faithful to the gold answer.", "main_total_tokens": 3049, "main_prompt_tokens": 3040, "main_completion_tokens": 9, "judge_total_tokens": 462, "judge_prompt_tokens": 393, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "I don't know.", "latency_ms": 584.1073989868164, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no such movie description can be found, effectively conveying lack of knowledge about a specific romantic comedy where Abraham Lincoln comes back from the dead. The generated answer, 'I don't know,' is consistent with this and thus is correct relative to the gold and faithful to the prompt and provided information.", "main_total_tokens": 550, "main_prompt_tokens": 549, "main_completion_tokens": 1, "judge_total_tokens": 487, "judge_prompt_tokens": 394, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Havana Darkness", "latency_ms": 1034.6043109893799, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Havana Darkness'), correctly identifying the movie title that alludes to a manuscript written by Ernest Hemingway. It is fully correct and faithful to the provided reference.", "main_total_tokens": 2496, "main_prompt_tokens": 2492, "main_completion_tokens": 4, "judge_total_tokens": 461, "judge_prompt_tokens": 390, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 1701.4997005462646, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Doll & Em' and is fully faithful to the provided information.", "main_total_tokens": 2854, "main_prompt_tokens": 2820, "main_completion_tokens": 34, "judge_total_tokens": 453, "judge_prompt_tokens": 392, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam Foe", "latency_ms": 1277.895212173462, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2007 movie as 'Hallam Foe'. It is fully correct and faithful to the provided information.", "main_total_tokens": 2784, "main_prompt_tokens": 2779, "main_completion_tokens": 5, "judge_total_tokens": 468, "judge_prompt_tokens": 403, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Foe", "latency_ms": 1697.9424953460693, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Foe' matches the gold answer 'A foe' in meaning, so it is both correct and faithful.", "main_total_tokens": 2721, "main_prompt_tokens": 2698, "main_completion_tokens": 23, "judge_total_tokens": 445, "judge_prompt_tokens": 388, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18", "latency_ms": 866.0585880279541, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (18), so it is both correct with respect to the question and faithful to the provided gold reference.", "main_total_tokens": 2524, "main_prompt_tokens": 2522, "main_completion_tokens": 2, "judge_total_tokens": 449, "judge_prompt_tokens": 389, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Autism [Doc 124583].", "latency_ms": 911.4844799041748, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.5, "justification": "The generated answer 'Autism' is essentially correct because the character Sam is on the autism spectrum; however, it is less precise than the gold answer 'Autism spectrum'. The answer is mostly faithful to the question and implied context but omits the word 'spectrum', so it only partially matches the reference phrasing.", "main_total_tokens": 2544, "main_prompt_tokens": 2535, "main_completion_tokens": 9, "judge_total_tokens": 492, "judge_prompt_tokens": 394, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix [Doc 59659].", "latency_ms": 1040.1122570037842, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly states that Sandra was born in Chamonix, matching the gold answer. However, it appends a citation-like bracketed reference ([Doc 59659]) that is not supported by the user-provided context, so it is not fully faithful to the given information format.", "main_total_tokens": 2982, "main_prompt_tokens": 2972, "main_completion_tokens": 10, "judge_total_tokens": 496, "judge_prompt_tokens": 406, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481].", "latency_ms": 1046.4394092559814, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and the citation does not contradict it, so it is both correct and faithful.", "main_total_tokens": 3130, "main_prompt_tokens": 3121, "main_completion_tokens": 9, "judge_total_tokens": 457, "judge_prompt_tokens": 398, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland [Doc 114810].", "latency_ms": 976.1104583740234, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks where Tabaluga is crown-prince of. The gold answer is Greenland, and the generated answer is also Greenland, matching exactly. The answer is both correct and faithful to the provided gold answer.", "main_total_tokens": 2794, "main_prompt_tokens": 2785, "main_completion_tokens": 9, "judge_total_tokens": 475, "judge_prompt_tokens": 401, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon", "latency_ms": 1210.460901260376, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('One Afternoon'). The evaluation is faithful to the given question, gold, and generated outputs.", "main_total_tokens": 2364, "main_prompt_tokens": 2361, "main_completion_tokens": 3, "judge_total_tokens": 464, "judge_prompt_tokens": 406, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" [Doc 93260].", "latency_ms": 1215.8753871917725, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Life Ahead,\" which matches the gold answer exactly. It does not introduce contradictions or unsupported claims beyond a harmless document reference, so it is both correct and faithful.", "main_total_tokens": 2684, "main_prompt_tokens": 2646, "main_completion_tokens": 38, "judge_total_tokens": 508, "judge_prompt_tokens": 435, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "Bruce McLaren [Doc 73859].", "latency_ms": 1268.9623832702637, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 2646, "main_prompt_tokens": 2636, "main_completion_tokens": 10, "judge_total_tokens": 451, "judge_prompt_tokens": 393, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren Motor Racing team [Doc 73859].", "latency_ms": 2492.8879737854004, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer in content; the only difference is the omission of the leading article 'The', which does not change the meaning. The added citation tag '[Doc 73859]' is extraneous but does not contradict the gold answer, so the response remains correct and faithful.", "main_total_tokens": 2979, "main_prompt_tokens": 2955, "main_completion_tokens": 24, "judge_total_tokens": 484, "judge_prompt_tokens": 394, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "New Zealand [Doc 73859].", "latency_ms": 1435.8704090118408, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the country as New Zealand, matching the gold answer exactly. The added citation-like tag '[Doc 73859]' does not contradict or alter the claim, so the response remains fully faithful to the implied source content and context.", "main_total_tokens": 2736, "main_prompt_tokens": 2716, "main_completion_tokens": 20, "judge_total_tokens": 465, "judge_prompt_tokens": 384, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "None", "latency_ms": 1317.6636695861816, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific 2013 movie, but no such John McClane movie exists where he blows up the Great Wall of China. The gold answer correctly states that there is no such movie in the database. The generated answer 'None' is uninformative and does not correctly explain that the premise is incorrect, so it is not a correct answer. However, it does not contradict the gold answer or introduce new facts; it simply provides no answer, so it is still faithful to the given information.", "main_total_tokens": 3016, "main_prompt_tokens": 3014, "main_completion_tokens": 2, "judge_total_tokens": 534, "judge_prompt_tokens": 399, "judge_completion_tokens": 135}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit", "latency_ms": 986.0897064208984, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2002 Japanese horror film as 'Tomie: Forbidden Fruit'. The response is fully correct and faithful to the provided information.", "main_total_tokens": 2801, "main_prompt_tokens": 2795, "main_completion_tokens": 6, "judge_total_tokens": 491, "judge_prompt_tokens": 422, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 [Doc 98566].", "latency_ms": 1289.3576622009277, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer matches the gold answer (65), so it is correct. However, it adds an extraneous citation-like note '[Doc 98566]' that is not supported by the given information, which slightly reduces faithfulness to the instruction context.", "main_total_tokens": 3186, "main_prompt_tokens": 3178, "main_completion_tokens": 8, "judge_total_tokens": 485, "judge_prompt_tokens": 402, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King and Tala search for a legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 1067.946195602417, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer is correct: in 'Scorpion King: Book of Souls' they search for the Book of Souls, matching the gold answer. However, the reference to '[Doc 82582]' is hallucinated and not grounded in the provided information, reducing faithfulness.", "main_total_tokens": 3122, "main_prompt_tokens": 3098, "main_completion_tokens": 24, "judge_total_tokens": 503, "judge_prompt_tokens": 417, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 780.9834480285645, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database, which is equivalent in content to the generated answer saying 'I don't know.' Both convey a lack of information rather than fabricating a date, so the generated response is correct and faithful.", "main_total_tokens": 557, "main_prompt_tokens": 556, "main_completion_tokens": 1, "judge_total_tokens": 475, "judge_prompt_tokens": 395, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 518.0113315582275, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the database lacks information on this movie. The generated answer honestly states not knowing, which is effectively equivalent, so it is correct and faithful to the lack of information.", "main_total_tokens": 594, "main_prompt_tokens": 593, "main_completion_tokens": 1, "judge_total_tokens": 461, "judge_prompt_tokens": 393, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1243.269443511963, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly indicates lack of knowledge, which aligns with the gold response that the movie is not in the database. It does not fabricate any information and remains faithful to the source intent.", "main_total_tokens": 536, "main_prompt_tokens": 535, "main_completion_tokens": 1, "judge_total_tokens": 464, "judge_prompt_tokens": 395, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 524.0440368652344, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly indicate a lack of knowledge about the movie rather than inventing a year. The generated answer is fully consistent with the information in the gold answer and does not introduce any contradictory or additional factual content.", "main_total_tokens": 566, "main_prompt_tokens": 565, "main_completion_tokens": 1, "judge_total_tokens": 472, "judge_prompt_tokens": 395, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year of release for \"Python's Revenge 2\" is not provided in the given documents.", "latency_ms": 1141.5362358093262, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available; the generated answer attributes this to the given documents. This is correct given the context and does not introduce unsupported claims.", "main_total_tokens": 2839, "main_prompt_tokens": 2818, "main_completion_tokens": 21, "judge_total_tokens": 480, "judge_prompt_tokens": 411, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 618.7138557434082, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database, which is equivalent in content to the generated answer saying it doesn’t know. Both convey lack of knowledge about the release year, so the generated answer is correct and faithful to the gold.", "main_total_tokens": 533, "main_prompt_tokens": 532, "main_completion_tokens": 1, "judge_total_tokens": 473, "judge_prompt_tokens": 393, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 679.9147129058838, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer and the generated answer both correctly indicate lack of knowledge about the film. The generated answer is slightly less specific but semantically equivalent, so it is correct and faithful to the intended response.", "main_total_tokens": 560, "main_prompt_tokens": 559, "main_completion_tokens": 1, "judge_total_tokens": 467, "judge_prompt_tokens": 396, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 432.4166774749756, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the database does not provide the director information, so the correct response is to indicate not knowing. The generated answer 'I don't know.' accurately reflects that lack of information and does not hallucinate a director.", "main_total_tokens": 552, "main_prompt_tokens": 551, "main_completion_tokens": 1, "judge_total_tokens": 477, "judge_prompt_tokens": 401, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 657.2389602661133, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not available in the database, and the generated answer says 'I don't know,' which is equivalent in meaning. It does not introduce any unsupported claims and correctly reflects the lack of information.", "main_total_tokens": 573, "main_prompt_tokens": 572, "main_completion_tokens": 1, "judge_total_tokens": 476, "judge_prompt_tokens": 399, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 536.9420051574707, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer explicitly states that the director information is not available, effectively meaning it is unknown. The generated answer, 'I don't know,' aligns with this and does not introduce any unsupported claims. Therefore, it is both correct and faithful to the gold answer.", "main_total_tokens": 558, "main_prompt_tokens": 557, "main_completion_tokens": 1, "judge_total_tokens": 480, "judge_prompt_tokens": 397, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 783.9934825897217, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly indicates lack of knowledge, which matches the gold response stating the database does not provide the director information. It does not fabricate any director name and stays faithful to the given data limitations.", "main_total_tokens": 575, "main_prompt_tokens": 574, "main_completion_tokens": 1, "judge_total_tokens": 472, "judge_prompt_tokens": 401, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 979.1066646575928, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the correct movie title 'Rise of the Mummy'. It is fully correct and faithful to the reference.", "main_total_tokens": 2552, "main_prompt_tokens": 2546, "main_completion_tokens": 6, "judge_total_tokens": 458, "judge_prompt_tokens": 396, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 1095.6988334655762, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Stranger in My Bed') and correctly identifies the movie. It is fully correct and faithful to the provided reference.", "main_total_tokens": 2687, "main_prompt_tokens": 2681, "main_completion_tokens": 6, "judge_total_tokens": 463, "judge_prompt_tokens": 400, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "None.", "latency_ms": 1071.580171585083, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer identifies 'Game of Silence', which is a TV series, not a movie. Therefore the generated answer 'None.' is not correct, since a closely matching work exists, but it is faithful to the question and does not introduce fabricated details.", "main_total_tokens": 2235, "main_prompt_tokens": 2232, "main_completion_tokens": 3, "judge_total_tokens": 481, "judge_prompt_tokens": 399, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 2288.3269786834717, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct movie title for the described plot. It is both correct and faithful to the reference.", "main_total_tokens": 2676, "main_prompt_tokens": 2641, "main_completion_tokens": 35, "judge_total_tokens": 456, "judge_prompt_tokens": 395, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 966.1314487457275, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Wood Job!') and correctly identifies the movie described. It is both factually correct and fully consistent with the provided reference.", "main_total_tokens": 2942, "main_prompt_tokens": 2938, "main_completion_tokens": 4, "judge_total_tokens": 468, "judge_prompt_tokens": 403, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 1726.9866466522217, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'Sesame Street: The Magical Wand Chase.'", "main_total_tokens": 2729, "main_prompt_tokens": 2712, "main_completion_tokens": 17, "judge_total_tokens": 472, "judge_prompt_tokens": 417, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond", "latency_ms": 864.8271560668945, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Beyond'). The plot description given corresponds to the film 'Beyond', so the response is both correct and fully faithful to the provided reference.", "main_total_tokens": 2301, "main_prompt_tokens": 2299, "main_completion_tokens": 2, "judge_total_tokens": 476, "judge_prompt_tokens": 409, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9", "latency_ms": 1629.8489570617676, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the correct movie '6ixtynin9'. The response is both correct and fully faithful to the expected solution.", "main_total_tokens": 2382, "main_prompt_tokens": 2375, "main_completion_tokens": 7, "judge_total_tokens": 474, "judge_prompt_tokens": 409, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread [Doc 32410].", "latency_ms": 1567.5034523010254, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer, 'Club Dread', exactly matches the gold answer and correctly identifies the movie described. It is fully correct and faithful to the question and gold reference.", "main_total_tokens": 2746, "main_prompt_tokens": 2736, "main_completion_tokens": 10, "judge_total_tokens": 498, "judge_prompt_tokens": 433, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet", "latency_ms": 1507.7085494995117, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot. The gold answer is 'Toilet', and the generated answer is also 'Toilet'. This matches the known film 'Toilet' (2010, directed by Naoko Ogigami) whose plot involves a 30-something engineer and Gundam fan moving in with his siblings to care for their Japanese grandmother after their mother's death. Therefore, the generated answer is both correct and fully faithful to the gold.", "main_total_tokens": 2994, "main_prompt_tokens": 2991, "main_completion_tokens": 3, "judge_total_tokens": 539, "judge_prompt_tokens": 415, "judge_completion_tokens": 124}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel", "latency_ms": 854.9737930297852, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Electric Angel'). It is correct and fully faithful to the provided information.", "main_total_tokens": 2634, "main_prompt_tokens": 2631, "main_completion_tokens": 3, "judge_total_tokens": 472, "judge_prompt_tokens": 418, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "Down Low [Doc 106995].", "latency_ms": 1613.529920578003, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer does not match the gold answer; therefore it is factually incorrect relative to the provided reference. There is no indication that the model was instructed to deviate from the gold answer, so it is also unfaithful to the specified ground truth.", "main_total_tokens": 2540, "main_prompt_tokens": 2531, "main_completion_tokens": 9, "judge_total_tokens": 472, "judge_prompt_tokens": 389, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 359.00068283081055, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific TV show title, which is 'Tea House' according to the gold answer. The generated response 'I don't know.' is incorrect because it fails to provide the correct show title. However, it is faithful in that it does not fabricate an answer or contradict the prompt; it honestly expresses a lack of knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 386, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 249.800443649292, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the correct answer is 'The Uncut Story of Mewtwo's Origin' rather than 'I don't know.' However, it is faithful in that it does not hallucinate an incorrect movie title; it simply admits not knowing.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 395, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "fr [Doc 768]", "latency_ms": 973.7076759338379, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer says the language is French, while the gold answer is English. Therefore it is factually incorrect. Since the model had access to the correct gold answer but produced a different one, it is also unfaithful to the source.", "main_total_tokens": 2933, "main_prompt_tokens": 2926, "main_completion_tokens": 7, "judge_total_tokens": 469, "judge_prompt_tokens": 389, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "Freaks and Geeks [Doc 115187].", "latency_ms": 1048.1455326080322, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect; the gold label states that the TV show is 'Law & Order: Special Victims Unit', but the model answered 'Freaks and Geeks'. It is also unfaithful to the provided gold answer, as it contradicts it.", "main_total_tokens": 2874, "main_prompt_tokens": 2861, "main_completion_tokens": 13, "judge_total_tokens": 487, "judge_prompt_tokens": 401, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2 [Doc 79355].", "latency_ms": 770.0362205505371, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and does not introduce any conflicting information. The citation tag does not affect factuality, so the response is both correct and faithful.", "main_total_tokens": 2225, "main_prompt_tokens": 2216, "main_completion_tokens": 9, "judge_total_tokens": 462, "judge_prompt_tokens": 392, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "1999 [Doc 22481].", "latency_ms": 1172.1382141113281, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states the movie was released in 1999, while the gold answer says 2019. Therefore, the generated answer is incorrect. However, it is faithful to the cited document [Doc 22481], so faithfulness is high even though the content is wrong relative to the gold.", "main_total_tokens": 2783, "main_prompt_tokens": 2774, "main_completion_tokens": 9, "judge_total_tokens": 485, "judge_prompt_tokens": 393, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "2", "latency_ms": 628.3218860626221, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 279 votes, but the generated answer is 2, which is incorrect. The model clearly used the question information (a vote count) but produced the wrong number, so it is faithful to the task but factually wrong.", "main_total_tokens": 2156, "main_prompt_tokens": 2153, "main_completion_tokens": 3, "judge_total_tokens": 459, "judge_prompt_tokens": 383, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "1", "latency_ms": 653.4204483032227, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1) does not match the gold answer (12), so it is factually incorrect. However, it directly answers the user’s question without adding unsupported details, so it is faithful to the question asked.", "main_total_tokens": 2234, "main_prompt_tokens": 2233, "main_completion_tokens": 1, "judge_total_tokens": 459, "judge_prompt_tokens": 383, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "2009 [Doc 46069].", "latency_ms": 1713.9983177185059, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold year (2009) and does not introduce unsupported information; the bracketed citation does not affect correctness or faithfulness.", "main_total_tokens": 3127, "main_prompt_tokens": 3118, "main_completion_tokens": 9, "judge_total_tokens": 457, "judge_prompt_tokens": 396, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en", "latency_ms": 1548.9017963409424, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The movie 'The King’s Speech' is originally in English (en), and the generated answer matches the gold label exactly and is faithful to the question.", "main_total_tokens": 2798, "main_prompt_tokens": 2796, "main_completion_tokens": 2, "judge_total_tokens": 449, "judge_prompt_tokens": 384, "judge_completion_tokens": 65}
