{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "Wall Street was released in 1987 [Doc 6993].", "latency_ms": 7614.450931549072, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer, and does not introduce any contradictory or extraneous information. Reference to a document ID does not affect correctness or faithfulness.", "main_total_tokens": 2842, "main_prompt_tokens": 2828, "main_completion_tokens": 14, "judge_total_tokens": 467, "judge_prompt_tokens": 392, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "In the movie \"47 Hours to Live,\" two socially awkward teenage girls find a game on a creepy website that claims you can use your phone to summon the supernatural, and if you lose the game, you lose your life [Doc 90746].", "latency_ms": 1636.6643905639648, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question refers to the movie 'Brain Fluid Explosion Girl,' where classmates must play a survival game using their cell phones. The generated answer instead describes '47 Hours to Live,' which is a different film and thus factually incorrect. However, the answer appears consistent with its cited document, so it is faithful to the provided source but not correct with respect to the question and gold answer.", "main_total_tokens": 2701, "main_prompt_tokens": 2651, "main_completion_tokens": 50, "judge_total_tokens": 546, "judge_prompt_tokens": 438, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show in which six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons,\" which aired in 1983 [Doc 112849].", "latency_ms": 2132.301092147827, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' matching the gold answer exactly. The added detail about it airing in 1983 is accurate and does not contradict the source, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2748, "main_prompt_tokens": 2712, "main_completion_tokens": 36, "judge_total_tokens": 507, "judge_prompt_tokens": 426, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is titled \"Nacht der Wölfe,\" released in 1982 [Doc 2590].", "latency_ms": 1290.8689975738525, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe,' which matches the gold answer exactly. The additional details (location Munich, rival German and Turkish teen gangs, year 1982, and citation to a document) are consistent with the question and do not contradict any provided information. Thus, the response is both correct and faithful to the given data.", "main_total_tokens": 3093, "main_prompt_tokens": 3048, "main_completion_tokens": 45, "judge_total_tokens": 549, "judge_prompt_tokens": 443, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of \"Picnic at Hanging Rock\" take place in the year 1900 [Doc 126191].", "latency_ms": 1229.1133403778076, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of 'Picnic at Hanging Rock' take place in 1900, matching the gold answer. It is faithful to the provided information and does not introduce unsupported claims.", "main_total_tokens": 2814, "main_prompt_tokens": 2790, "main_completion_tokens": 24, "judge_total_tokens": 482, "judge_prompt_tokens": 409, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The movie you are referring to is \"June,\" which was released in 2004. It features a young violinist named June who works in a coffee shop and dreams of a professional music career while navigating her love life [Doc 32483].", "latency_ms": 1559.4031810760498, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The question asks for the title of the 2004 movie; the gold answer is simply \"June.\" The generated answer correctly identifies the movie as \"June\" and restates the plot, so it is correct. However, it adds a specific reference to a document number (\"[Doc 32483]\") that is not grounded in the user-provided context, slightly reducing faithfulness.", "main_total_tokens": 3088, "main_prompt_tokens": 3038, "main_completion_tokens": 50, "judge_total_tokens": 594, "judge_prompt_tokens": 482, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "The highest rated Russian-language movie released in 1980 is \"Moscow Does Not Believe in Tears,\" which has a rating of 7.6 [Doc 61].", "latency_ms": 1505.5921077728271, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer lists 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger' as the highest rated, while the generated answer claims it is 'Moscow Does Not Believe in Tears.' This directly contradicts the gold, so the answer is incorrect. However, the generated answer is internally consistent with its cited document, so it is faithful to the provided evidence.", "main_total_tokens": 3071, "main_prompt_tokens": 3036, "main_completion_tokens": 35, "judge_total_tokens": 536, "judge_prompt_tokens": 430, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest Russian-language movie rating in 1999 is 4.571 for the movie \"Features of the Russian Bath\" [Doc 22144].", "latency_ms": 1036.6981029510498, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the highest Russian-language movie rating in 1999 is 7.6, while the generated answer claims it is 4.571 for a specific movie. This is numerically incorrect. Additionally, the generated response references a specific document and movie not supported by the provided gold answer, so it is not faithful to the given information.", "main_total_tokens": 2514, "main_prompt_tokens": 2483, "main_completion_tokens": 31, "judge_total_tokens": 518, "judge_prompt_tokens": 416, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "There are two movies released in 1987 that have a rating over 7 and are in Chinese:\n\n1. A Chinese Ghost Story - Rating: 7.3 [Doc 7064]\n2. Last Eunuch In China - Rating: 7.8 [Doc 7754]", "latency_ms": 1738.1854057312012, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer says there are 4 such movies, while the generated answer states there are 2, so the correctness is 0. The reasoning appears consistent with the two documents it cites, but it omits two additional qualifying movies that must exist in the provided corpus, so it is only partially faithful to the underlying data.", "main_total_tokens": 2981, "main_prompt_tokens": 2921, "main_completion_tokens": 60, "judge_total_tokens": 551, "judge_prompt_tokens": 452, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "There are no TV shows released in 1987 that have a rating over 7 and are in Chinese based on the provided context.", "latency_ms": 872.2527027130127, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "Gold answer says 2 shows; generated answer says 0, so it is factually incorrect. However, the answer is internally consistent with the (albeit unspecified) provided context and does not invent details beyond that, so it is faithful to the context as stated in the response.", "main_total_tokens": 2669, "main_prompt_tokens": 2642, "main_completion_tokens": 27, "judge_total_tokens": 508, "judge_prompt_tokens": 420, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "The only French language TV show released in 1980 is \"Secrets of a French Maid\" [Doc 207].", "latency_ms": 2039.306402206421, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is factually incorrect: 'Secrets of a French Maid' is not the correct title and is not the gold answer 'Fantômas'. It also appears to be fabricated and not supported by the provided gold information, so it is unfaithful to the source.", "main_total_tokens": 3043, "main_prompt_tokens": 3019, "main_completion_tokens": 24, "judge_total_tokens": 496, "judge_prompt_tokens": 409, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is \"Kill Cruise,\" released in 1990 [Doc 11205].", "latency_ms": 1467.6437377929688, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' matching the gold reference exactly and directly answering the question without adding unsupported details.", "main_total_tokens": 2916, "main_prompt_tokens": 2887, "main_completion_tokens": 29, "judge_total_tokens": 472, "judge_prompt_tokens": 414, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 1220.5352783203125, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as \"Won't You Be My Neighbor?\", matching the gold answer exactly and staying faithful to the provided information.", "main_total_tokens": 2992, "main_prompt_tokens": 2953, "main_completion_tokens": 39, "judge_total_tokens": 502, "judge_prompt_tokens": 440, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The movie \"YEAR\" was released in 2023 [Doc 107925].", "latency_ms": 771.594762802124, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer says the movie was released in 2023, while the gold answer is 2018, so the year is incorrect. It also fabricates a movie name placeholder (\"YEAR\") and a document reference without evidence from the given information, so it is not faithful to the provided context.", "main_total_tokens": 3045, "main_prompt_tokens": 3028, "main_completion_tokens": 17, "judge_total_tokens": 486, "judge_prompt_tokens": 394, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is \"Astro Boy,\" which was released in 1980. The plot involves a young boy robot who is rescued by an Android Advocate, Dr. Elefun, after being abandoned by his creator [Doc 112591].", "latency_ms": 2863.828420639038, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy', matching the gold answer. However, it adds specific plot details and a reference ([Doc 112591]) not supported or verifiable from the given question and gold answer alone, so there is slight ungrounded elaboration affecting faithfulness.", "main_total_tokens": 3228, "main_prompt_tokens": 3165, "main_completion_tokens": 63, "judge_total_tokens": 545, "judge_prompt_tokens": 450, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 950.3271579742432, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9463946303571863, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (7 years) and does not introduce any extraneous or contradictory information. It is therefore both correct and faithful to the reference.", "main_total_tokens": 2525, "main_prompt_tokens": 2495, "main_completion_tokens": 30, "judge_total_tokens": 489, "judge_prompt_tokens": 424, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 1042.123794555664, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983, which matches the provided gold answer. It is fully faithful to the given information and does not introduce unsupported details.", "main_total_tokens": 2764, "main_prompt_tokens": 2731, "main_completion_tokens": 33, "judge_total_tokens": 515, "judge_prompt_tokens": 428, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "The most popular English-speaking TV show released in 1997 is \"South Park,\" with a popularity score of 61.7375 [Doc 114707].", "latency_ms": 1304.4497966766357, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold popularity score is 80.5772, but the generated answer gives 61.7375 for South Park, so the numeric answer is incorrect. However, the model’s statement is consistent with its cited document, so it is faithful to the provided source.", "main_total_tokens": 3579, "main_prompt_tokens": 3546, "main_completion_tokens": 33, "judge_total_tokens": 505, "judge_prompt_tokens": 425, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "Based on the provided context, there are two Russian language TV shows released in 1997:\n\n1. **13 posterunek** - Rating: 7.0 [Doc 114791]\n2. **Kamenskaya** - Rating: 8.1 [Doc 115256]\n\nTherefore, the answer is two Russian language TV shows.", "latency_ms": 1659.3353748321533, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "Gold answer says 1 show; generated says 2, so correctness is 0. The model invents counts and specific titles without evidence from the (omitted) context, so it is not faithful to the provided information.", "main_total_tokens": 2377, "main_prompt_tokens": 2307, "main_completion_tokens": 70, "judge_total_tokens": 530, "judge_prompt_tokens": 453, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 1722.534418106079, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Angel Eyes' and does not introduce any unsupported information beyond a document reference.", "main_total_tokens": 2828, "main_prompt_tokens": 2793, "main_completion_tokens": 35, "judge_total_tokens": 491, "judge_prompt_tokens": 429, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French (fr) [Doc 110681].", "latency_ms": 1003.8785934448242, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states the movie language is French, while the gold label says Korean, so the content is incorrect. However, it is faithful to the cited document (Doc 110681) and does not contradict its own source, so faithfulness is high.", "main_total_tokens": 2765, "main_prompt_tokens": 2745, "main_completion_tokens": 20, "judge_total_tokens": 479, "judge_prompt_tokens": 396, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "There are no English-speaking Horror movies released in 2009 with a popularity score greater than 7 based on the provided context. The highest popularity score for a 2009 Horror movie listed is 2.9925 for \"My Ex\" [Doc 45056].", "latency_ms": 1690.598964691162, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer says there are no such movies, so it is factually incorrect. However, the answer is consistent with and grounded in the provided context (it explicitly references the listed data and its highest popularity score), so it is faithful to the context even though the context itself appears incomplete relative to the gold answer.", "main_total_tokens": 2628, "main_prompt_tokens": 2573, "main_completion_tokens": 55, "judge_total_tokens": 549, "judge_prompt_tokens": 447, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The context provided does not include any movies released in 1979, so I cannot provide an answer.", "latency_ms": 1590.973138809204, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot provide an answer about movies released in 1979 given the context, which matches the gold response that there are no such movies in the database. It does not introduce external information and remains faithful to the given context.", "main_total_tokens": 2832, "main_prompt_tokens": 2811, "main_completion_tokens": 21, "judge_total_tokens": 498, "judge_prompt_tokens": 416, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The movie \"Back to the Future Part IV\" is not mentioned in the provided context, so I cannot provide a release year for it.", "latency_ms": 1151.7431735992432, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.9, "faithfulness_score": 0.5, "justification": "The generated answer is essentially correct that it cannot provide a release year, since there is no such movie and thus no valid year. However, it frames the limitation as due to the provided context rather than acknowledging that the movie does not exist, which is less accurate and less faithful to the gold answer. The gold explicitly states the movie does not exist in its database; the generated answer instead implies the information might exist but is just missing from the context, so it is only partially faithful.", "main_total_tokens": 3105, "main_prompt_tokens": 3077, "main_completion_tokens": 28, "judge_total_tokens": 560, "judge_prompt_tokens": 428, "judge_completion_tokens": 132}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "The least popular movie in 1980 about the nuclear Chernobyl disaster is \"Nuclear Shelter,\" which has a popularity score of 0.2861 [Doc 1818].", "latency_ms": 1483.309268951416, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer invents a specific movie (“Nuclear Shelter”) and a precise popularity score, while the gold answer correctly states that no such 1980 Chernobyl-disaster movies could be found. This is factually incorrect given the constraint and is not supported by the provided information, so it is neither correct nor faithful to the evidence.", "main_total_tokens": 3106, "main_prompt_tokens": 3068, "main_completion_tokens": 38, "judge_total_tokens": 539, "judge_prompt_tokens": 444, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are three entries with 'Astro Boy' in the title:\n\n1. Astro Boy (Movie, 2009) - [Doc 44947]\n2. Astro Boy (TV, 1980) - [Doc 112591]\n3. Astro Boy (TV, 2003) - [Doc 116265]", "latency_ms": 1907.2134494781494, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 1.1510196182278052, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.5330476224182168, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 titles, but the generated answer lists only 3 and explicitly claims there are three entries, which is numerically incorrect. However, the assistant’s response is internally consistent with its own enumeration (it does not contradict itself or the documents it cites), so it is faithful to its own reasoning but factually incorrect relative to the gold.", "main_total_tokens": 2985, "main_prompt_tokens": 2917, "main_completion_tokens": 68, "judge_total_tokens": 554, "judge_prompt_tokens": 454, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "Based on the provided context, there are two movies in 2025 that allude to the Holocaust:\n\n1. \"The Future Awaits\" - This film tells the story of a young Jewish girl and her family during the Vel’ d’Hiv’ Roundup in Paris, highlighting their struggle for survival during the Holocaust [Doc 112009].\n\n2. \"Mazel Tov\" - While primarily focused on family dynamics during a funeral and a Bat Mitzvah, it is set within a Jewish context, which may allude to the historical experiences of Jewish families, including the Holocaust [Doc 112087].\n\nHowever, the direct reference to the Holocaust is more explicit in \"The Future Awaits.\"", "latency_ms": 3375.753879547119, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0.0, "faithfulness_score": 0.0, "justification": "The question asks only for the number of movies, with the gold answer being 2. The generated answer provides a narrative explanation, lists movie titles, and adds contextual details not present in the question or gold answer, rather than simply stating the number. This makes it incorrect relative to the evaluation target. It is also unfaithful to the gold standard because it fabricates specific film titles and plot details that are not given in the prompt and are unnecessary for answering the numeric question.", "main_total_tokens": 3343, "main_prompt_tokens": 3201, "main_completion_tokens": 142, "judge_total_tokens": 657, "judge_prompt_tokens": 526, "judge_completion_tokens": 131}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "There were three TV shows released in the 1990s about Super Mario Bros:\n\n1. The Super Mario Bros. Super Show! (1989) [Doc 113517]\n2. The Adventures of Super Mario Bros. 3 (1990) [Doc 113661]\n3. Super Mario World (1991) [Doc 113814]", "latency_ms": 1937.8488063812256, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.8010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer states there was 1 TV show released in the 1990s about Super Mario Bros. The generated answer claims there were three, and even lists one from 1989, which is outside the 1990s, so the numerical answer is incorrect. It does reference specific titles and documents, so it is partially grounded in the provided information, but still contradicts the gold. Hence low correctness and partial faithfulness.", "main_total_tokens": 3140, "main_prompt_tokens": 3067, "main_completion_tokens": 73, "judge_total_tokens": 581, "judge_prompt_tokens": 460, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "Based on the provided context, there is one TV show about Super Mario Bros released between 2000 and 2010:\n\n1. **Mario** (2013) - Although it is listed, it is outside the specified range.\n2. **The Super Mario Bros. Super Show!** (1989) - Outside the specified range.\n3. **The Adventures of Super Mario Bros. 3** (1990) - Outside the specified range.\n4. **Super Mario World** (1991) - Outside the specified range.\n\nNo TV shows specifically about Super Mario Bros were released between 2000 and 2010 according to the provided documents.", "latency_ms": 2641.641855239868, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The question asks for how many TV shows between 2000 and 2010; the gold answer is 0. The generated answer concludes that no TV shows were released in that time frame, so its final numeric answer effectively matches the gold (0), despite the confusing enumeration that mentions shows outside the range (including an incorrect 2013 example). So correctness of the final answer is 1. Faithfulness is slightly reduced because it lists a 2013 show as if drawn from the provided context and mixes in details that are extraneous or possibly not fully supported. However, the core reasoning about the 2000–2010 range and its conclusion are aligned, so faithfulness is mostly but not perfectly preserved.", "main_total_tokens": 2885, "main_prompt_tokens": 2752, "main_completion_tokens": 133, "judge_total_tokens": 699, "judge_prompt_tokens": 522, "judge_completion_tokens": 177}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular TV show about ninjas is \"House of Ninjas,\" which has a popularity rating of 4.4317 [Doc 135161]. Among movies, \"Ninja Scroll\" is the most popular with a popularity rating of 3.0712 [Doc 14164].", "latency_ms": 1728.0676364898682, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question likely refers to overall cultural popularity; the gold answer is 'Naruto', a globally famous ninja-themed anime. The generated answer instead names 'House of Ninjas' and 'Ninja Scroll' based on some internal popularity ratings, which are not more popular than Naruto in general. This makes the factual content incorrect relative to the task. It is also unfaithful to the gold label, as it does not mention Naruto or align with the intended answer.", "main_total_tokens": 3220, "main_prompt_tokens": 3160, "main_completion_tokens": 60, "judge_total_tokens": 566, "judge_prompt_tokens": 442, "judge_completion_tokens": 124}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "There is no information available about a Jurassic Park/Power Ranger crossover movie in the provided context.", "latency_ms": 1019.4551944732666, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer clarifies that the movie was not released, while the generated answer only says there is no information in the provided context, which does not directly and correctly address the question. Thus, the generated answer is not correct relative to the gold. It is partially faithful since it avoids fabricating a release date but fails to convey the essential fact that the movie was not released.", "main_total_tokens": 2989, "main_prompt_tokens": 2970, "main_completion_tokens": 19, "judge_total_tokens": 513, "judge_prompt_tokens": 404, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "There is no movie in the provided context where Barbie fights the Predator. The context includes separate movies for Barbie and the Predator, but they do not intersect in any plot. [Doc 68094], [Doc 82000].", "latency_ms": 1458.9545726776123, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer is essentially correct: there is no such movie where Barbie fights the Predator, matching the gold statement that the movie was not released. However, it references specific document IDs and a notion of a 'provided context' that are not present in the user prompt, which slightly reduces faithfulness to the given information. Overall, the core factual claim aligns with the gold answer.", "main_total_tokens": 2972, "main_prompt_tokens": 2926, "main_completion_tokens": 46, "judge_total_tokens": 537, "judge_prompt_tokens": 428, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The 4th oldest Star Wars movie is \"Return of the Jedi,\" which was released in 1983 [Doc 2858].", "latency_ms": 1134.6843242645264, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the 4th oldest Star Wars movie, and the gold answer is 'Star Wars: Droids - The Great Heep.' The generated answer instead gives 'Return of the Jedi' (1983), which does not match the gold answer and is therefore incorrect. However, the generated answer appears internally consistent and cites a document, so it is faithful to its own retrieval, even though that retrieval is wrong with respect to the gold.", "main_total_tokens": 3032, "main_prompt_tokens": 3004, "main_completion_tokens": 28, "judge_total_tokens": 539, "judge_prompt_tokens": 417, "judge_completion_tokens": 122}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The movie \"YEAR\" was released in 2023 [Doc 107925].", "latency_ms": 1728.7914752960205, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 1986, but the generated answer states 2023, so it is factually incorrect. It also hallucinates a movie titled \"YEAR\" and a document reference without basis in the given gold answer, so it is unfaithful to the provided information.", "main_total_tokens": 3045, "main_prompt_tokens": 3028, "main_completion_tokens": 17, "judge_total_tokens": 483, "judge_prompt_tokens": 394, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The rating of the movie Forrest Gump is 8.466 [Doc 15236].", "latency_ms": 749.4220733642578, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating value 8.466 and does not introduce unsupported information. It is both correct and faithful to the provided reference.", "main_total_tokens": 2664, "main_prompt_tokens": 2645, "main_completion_tokens": 19, "judge_total_tokens": 470, "judge_prompt_tokens": 413, "judge_completion_tokens": 57}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 1019.7329521179199, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med,' which matches the gold answer exactly. It remains faithful to the provided information without adding unsupported details.", "main_total_tokens": 2841, "main_prompt_tokens": 2810, "main_completion_tokens": 31, "judge_total_tokens": 487, "judge_prompt_tokens": 423, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in Argentina, specifically from the Villa Devoto prison in Buenos Aires [Doc 53582].", "latency_ms": 1320.375680923462, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies Argentina as the country, matching the gold answer. However, it adds extra details about 'Villa Devoto prison in Buenos Aires [Doc 53582]' that are not present in the question or gold, so while likely true, this additional specificity slightly reduces faithfulness to the provided information.", "main_total_tokens": 3141, "main_prompt_tokens": 3104, "main_completion_tokens": 37, "judge_total_tokens": 524, "judge_prompt_tokens": 428, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice Li Nai falls into a trap designed by the \"Chef Assassin\" Jiang Jie [Doc 135231].", "latency_ms": 1088.2890224456787, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks whose trap the apprentice falls into, and the gold answer is 'Jiang Jie.' The generated answer states that the apprentice falls into a trap designed by Jiang Jie, which matches the gold. The extra descriptive details (\"Li Nai\", \"Chef Assassin\", document reference) do not contradict the gold answer, so the response is both correct and faithful to the provided gold.", "main_total_tokens": 2838, "main_prompt_tokens": 2806, "main_completion_tokens": 32, "judge_total_tokens": 524, "judge_prompt_tokens": 416, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "There are two Korean-language movies that mention assassins and were released after 2023:\n\n1. \"The Assassin\" (2023) - An action historical drama depicting the story of Joseon's best swordsman Yi Nan in front of an irreversible fate [Doc 106219].\n2. \"Assassination\" (2015) - Although this movie is not after 2023, it is included here for context. It is set in Japanese-occupied Korea and involves freedom fighters assigned to assassinate a military leader [Doc 67739].\n\nHowever, only \"The Assassin\" (2023) fits the criteria of being a Korean-language movie released after 2023. Therefore, the answer is one.", "latency_ms": 2757.2407722473145, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there are 4 such movies, while the generated answer concludes there is 1, so the correctness is 0. The assistant also includes a 2015 movie and then dismisses it but still ends with the wrong count; it appears to be inventing a rationale inconsistent with the gold. Thus the reasoning is not faithful to the gold answer and earns 0.", "main_total_tokens": 2860, "main_prompt_tokens": 2717, "main_completion_tokens": 143, "judge_total_tokens": 635, "judge_prompt_tokens": 526, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "The context provided does not contain any information about Steven Seagal or the number of movies he has. It only includes details about various movies related to Bruce Lee. Therefore, I cannot provide an answer to your question.", "latency_ms": 1220.8118438720703, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "Gold answer states explicitly that there are no movies featuring both Bruce Lee and Steven Seagal. The generated answer instead claims it cannot provide an answer because the context lacks information, which contradicts the gold conclusion and fails to answer the question. Thus it is factually incorrect relative to the gold. It is partially faithful to the context in that it correctly observes the context only has Bruce Lee-related movies, but it ignores the key implication that no joint movies are found, so faithfulness is limited.", "main_total_tokens": 3302, "main_prompt_tokens": 3259, "main_completion_tokens": 43, "judge_total_tokens": 567, "judge_prompt_tokens": 436, "judge_completion_tokens": 131}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 1035.0911617279053, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (6.3) and attributes it to a document, so it is both correct and faithful to the provided information.", "main_total_tokens": 3031, "main_prompt_tokens": 3011, "main_completion_tokens": 20, "judge_total_tokens": 466, "judge_prompt_tokens": 403, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The movie \"Fast Track: No Limits\" features street racing, as it is centered around a group of street racers [Doc 41929].", "latency_ms": 2306.020736694336, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer specifies the style of racing as 'Drift', but the generated answer claims the movie features 'street racing' without mentioning drifting. This is factually incorrect relative to the gold. It also appears to rely on an external document rather than the provided gold answer, so it is not faithful to the given information.", "main_total_tokens": 3082, "main_prompt_tokens": 3053, "main_completion_tokens": 29, "judge_total_tokens": 503, "judge_prompt_tokens": 407, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 1729.0458679199219, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Confronting a Serial Killer,' and it stays faithful to the provided information without adding unsupported claims.", "main_total_tokens": 3034, "main_prompt_tokens": 2988, "main_completion_tokens": 46, "judge_total_tokens": 519, "judge_prompt_tokens": 452, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 1071.394443511963, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and attributes it to the provided document, so it is both correct and faithful.", "main_total_tokens": 3057, "main_prompt_tokens": 3035, "main_completion_tokens": 22, "judge_total_tokens": 468, "judge_prompt_tokens": 407, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 1166.3410663604736, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6895405204413555, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Sixth Sense,\" which matches the gold answer. It stays faithful to the given information and does not introduce contradictions or unsupported claims, aside from a benign citation-style reference that does not alter the content.", "main_total_tokens": 2672, "main_prompt_tokens": 2643, "main_completion_tokens": 29, "judge_total_tokens": 500, "judge_prompt_tokens": 420, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 972.4655151367188, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer, and it does not introduce any unsupported information beyond that.", "main_total_tokens": 2978, "main_prompt_tokens": 2961, "main_completion_tokens": 17, "judge_total_tokens": 460, "judge_prompt_tokens": 397, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is 'The Transporter Refueled' [Doc 67500].", "latency_ms": 1436.739444732666, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2015 reboot title as 'The Transporter Refueled' and does not introduce unsupported information.", "main_total_tokens": 2909, "main_prompt_tokens": 2880, "main_completion_tokens": 29, "judge_total_tokens": 484, "judge_prompt_tokens": 420, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in 2015 [Doc 67500].", "latency_ms": 888.1075382232666, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, which matches the gold answer. It also stays faithful to the given information, only adding a bracketed document reference without contradicting or fabricating other details.", "main_total_tokens": 2970, "main_prompt_tokens": 2950, "main_completion_tokens": 20, "judge_total_tokens": 487, "judge_prompt_tokens": 404, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "There is no mention of a romantic comedy in the provided context that features Abraham Lincoln coming back from the dead.", "latency_ms": 1003.0601024627686, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no movie description with this premise can be found. The generated answer correctly echoes that the provided context contains no such romantic comedy. This is factually correct relative to the context and fully faithful to it.", "main_total_tokens": 2688, "main_prompt_tokens": 2666, "main_completion_tokens": 22, "judge_total_tokens": 488, "judge_prompt_tokens": 412, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of the movie that alludes to a manuscript written by Ernest Hemingway is \"Havana Darkness\" [Doc 90923].", "latency_ms": 1051.2661933898926, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Havana Darkness'. It is fully correct and aligns with the given reference, demonstrating complete faithfulness to the source.", "main_total_tokens": 2769, "main_prompt_tokens": 2742, "main_completion_tokens": 27, "judge_total_tokens": 485, "judge_prompt_tokens": 415, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "In the TV show \"Doll & Em,\" Doll heads to Hollywood to be with her childhood best friend Em, who is now a successful actress [Doc 122259].", "latency_ms": 1182.9769611358643, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em' and accurately reflects the information in the question. It remains faithful to the given content and does not introduce contradictions; the added detail that Em is a successful actress is consistent with known context and does not conflict with the prompt.", "main_total_tokens": 2868, "main_prompt_tokens": 2833, "main_completion_tokens": 35, "judge_total_tokens": 514, "judge_prompt_tokens": 424, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The movie is \"Hallam Foe,\" released in 2007 [Doc 39389].", "latency_ms": 1483.2193851470947, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as 'Hallam Foe.' It is correct and consistent with the provided information, with no hallucinations or deviations.", "main_total_tokens": 2859, "main_prompt_tokens": 2839, "main_completion_tokens": 20, "judge_total_tokens": 486, "judge_prompt_tokens": 419, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The context does not provide information about a specific character's last name or whether it sounds like a friend or a foe. Therefore, I cannot answer the question based on the provided context.", "latency_ms": 1457.3051929473877, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is an explicit label: 'A foe'. The generated answer claims the question cannot be answered from context, which directly contradicts the gold. Thus, it is incorrect. However, given the model's stated limitation to the provided context, its answer is internally consistent and not hallucinated, so it is faithful to its own reasoning process.", "main_total_tokens": 2737, "main_prompt_tokens": 2700, "main_completion_tokens": 37, "judge_total_tokens": 524, "judge_prompt_tokens": 423, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "In the 2017 TV show \"Atypical,\" Sam is 18 years old [Doc 124583].", "latency_ms": 1353.8727760314941, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: Sam is 18 years old in the 2017 TV show Atypical. It is fully correct and consistent with the provided information.", "main_total_tokens": 2686, "main_prompt_tokens": 2662, "main_completion_tokens": 24, "judge_total_tokens": 479, "judge_prompt_tokens": 411, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 1019.8109149932861, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the 2017 TV show Atypical, Sam is on the autism spectrum, which matches the gold answer exactly. It is fully faithful to the provided information.", "main_total_tokens": 2856, "main_prompt_tokens": 2833, "main_completion_tokens": 23, "judge_total_tokens": 481, "judge_prompt_tokens": 410, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "The information about the movie \"À corde tendue\" and the name of the place where Sandra was born is not found in the provided context.", "latency_ms": 1258.1253051757812, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that Sandra was born in Chamonix. The generated answer claims the information is not found, which contradicts the gold answer. This makes the generated answer factually incorrect relative to the gold. However, the model is faithful to its stated limitation (it says the info is not in the provided context and does not fabricate an answer), so it is fully faithful to its own reasoning and available context.", "main_total_tokens": 3116, "main_prompt_tokens": 3087, "main_completion_tokens": 29, "judge_total_tokens": 541, "judge_prompt_tokens": 426, "judge_completion_tokens": 115}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In this Japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 887.6488208770752, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly fills the blank with the correct word 'rains' and fully matches the provided gold answer without adding incorrect information. It is both correct and faithful.", "main_total_tokens": 2900, "main_prompt_tokens": 2879, "main_completion_tokens": 21, "judge_total_tokens": 477, "judge_prompt_tokens": 412, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the low-rated German language animation \"Tabaluga,\" the character Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 1907.4478149414062, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which matches the gold answer exactly and does not introduce contradictions or unsupported claims.", "main_total_tokens": 3057, "main_prompt_tokens": 3021, "main_completion_tokens": 36, "judge_total_tokens": 493, "judge_prompt_tokens": 430, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is titled \"One Afternoon,\" released in 2021 [Doc 96838].", "latency_ms": 1542.9761409759521, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'One Afternoon,' matching the gold answer. The added detail about it being released in 2021 and referencing a specific document (Doc 96838) is not supported by the given prompt, so that part is unverifiable, but it does not contradict anything explicit either. Thus correctness is 1, and faithfulness is slightly reduced for including extra, source-claimed metadata.", "main_total_tokens": 2777, "main_prompt_tokens": 2727, "main_completion_tokens": 50, "judge_total_tokens": 571, "judge_prompt_tokens": 454, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead,\" which has a rating of 6.746 [Doc 93260].", "latency_ms": 6770.42818069458, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title of the low-rated Italian language movie where a Holocaust survivor cares for a 12-year-old street kid who robbed her. The gold answer is \"The Life Ahead.\" The generated answer correctly identifies the movie as \"The Life Ahead\" and adds consistent supporting detail about the plot and rating. There are no contradictions with the gold answer, and the added information does not conflict with it, so the response is both correct and faithful.", "main_total_tokens": 3431, "main_prompt_tokens": 3385, "main_completion_tokens": 46, "judge_total_tokens": 566, "judge_prompt_tokens": 444, "judge_completion_tokens": 122}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team, showcasing how a man of humble beginnings could take on the elite of motor racing and win [Doc 73859].", "latency_ms": 1668.7209606170654, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the subject of the 2016 documentary as Bruce McLaren, matching the gold answer. However, it introduces extra descriptive details and even a citation-like reference ([Doc 73859]) that are not grounded in the provided gold answer. While the elaboration about his background and achievements is likely true, it goes beyond the given information, slightly reducing faithfulness relative to the reference.", "main_total_tokens": 3117, "main_prompt_tokens": 3066, "main_completion_tokens": 51, "judge_total_tokens": 549, "judge_prompt_tokens": 435, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The context does not provide information about the name of Danny Krueger's racing team. Therefore, I cannot answer that question.", "latency_ms": 1104.2442321777344, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the racing team is the McLaren Motor Racing team. The generated answer says the context does not provide the information and that it cannot answer, which is incorrect given the gold. However, the generated answer is faithful to the (claimed) context, since it does not hallucinate another team name and instead admits lack of information.", "main_total_tokens": 3144, "main_prompt_tokens": 3118, "main_completion_tokens": 26, "judge_total_tokens": 505, "judge_prompt_tokens": 409, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context does not provide information about a specific person or their country of origin. Therefore, I cannot answer your question.", "latency_ms": 1342.4592018127441, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the person is from New Zealand. The generated answer claims the information is not provided and refuses to answer. This is incorrect relative to the gold, so correctness is 0. However, given only the user question and no context about who 'he' is, the model's statement that it cannot determine the country is faithful to the (missing) context, so faithfulness is 1.", "main_total_tokens": 2928, "main_prompt_tokens": 2904, "main_completion_tokens": 24, "judge_total_tokens": 513, "judge_prompt_tokens": 400, "judge_completion_tokens": 113}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "There is no mention of John McClane blowing up the Great Wall of China in any 2013 movie. The movie \"A Good Day to Die Hard\" features John McClane, but it does not involve the Great Wall of China in its plot [Doc 58791].", "latency_ms": 1659.9881649017334, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that no 2013 movie shows John McClane blowing up the Great Wall of China and notes that while 'A Good Day to Die Hard' (2013) features John McClane, it does not involve the Great Wall. This is consistent with the gold answer, which says there is no such movie in the database. The response stays within the information provided and does not introduce contradictions, so it is both correct and faithful.", "main_total_tokens": 2932, "main_prompt_tokens": 2874, "main_completion_tokens": 58, "judge_total_tokens": 580, "judge_prompt_tokens": 456, "judge_completion_tokens": 124}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with that description is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 1056.9968223571777, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Tomie: Forbidden Fruit'. It is fully correct and consistent with the given information.", "main_total_tokens": 2945, "main_prompt_tokens": 2920, "main_completion_tokens": 25, "judge_total_tokens": 505, "judge_prompt_tokens": 442, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie \"Lei mi parla ancora,\" Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 1404.8442840576172, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, stating that Nino and Caterina have been married for 65 years in the 2021 movie 'Lei mi parla ancora'. It is fully correct and consistent with the provided gold answer.", "main_total_tokens": 3170, "main_prompt_tokens": 3140, "main_completion_tokens": 30, "judge_total_tokens": 502, "judge_prompt_tokens": 424, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In \"Scorpion King: Book of Souls\" (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls, which will allow them to put an end to an evil warlord [Doc 82582].", "latency_ms": 2293.3273315429688, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that they search for The Book of Souls. The generated answer explicitly says they search for a legendary relic known as The Book of Souls. This is fully correct and aligned with the gold answer; the added detail about ending an evil warlord is extra but not contradictory or misleading relative to the question, so correctness and faithfulness are both 1.", "main_total_tokens": 3558, "main_prompt_tokens": 3507, "main_completion_tokens": 51, "judge_total_tokens": 549, "judge_prompt_tokens": 445, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"Zombies on the Moon\" was released is not found in the provided context.", "latency_ms": 1019.8330879211426, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold answer says the movie is not in the database; generated answer says the release year is not found in the provided context. Both convey that the release year is unknown/unavailable and do not invent a date, so the content is correct and faithful to the given information.", "main_total_tokens": 2623, "main_prompt_tokens": 2604, "main_completion_tokens": 19, "judge_total_tokens": 495, "judge_prompt_tokens": 410, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year of release for \"The Galactic Baker\" is not found in the provided context.", "latency_ms": 886.9822025299072, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available/found. The wording differs slightly but the meaning is equivalent: the system cannot provide the year of release. This is correct and faithful to the gold.", "main_total_tokens": 3226, "main_prompt_tokens": 3208, "main_completion_tokens": 18, "judge_total_tokens": 483, "judge_prompt_tokens": 407, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Return of the Null Pointer\" is not found in the provided context.", "latency_ms": 1004.2860507965088, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both gold and generated answers correctly state that the release year is not available / not found. The generated answer is consistent with the given context and does not hallucinate a date, matching the gold answer in substance.", "main_total_tokens": 2929, "main_prompt_tokens": 2910, "main_completion_tokens": 19, "judge_total_tokens": 483, "judge_prompt_tokens": 410, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The information about \"Data Science: The Movie\" is not found in the provided context.", "latency_ms": 1135.1087093353271, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers indicate that the information about the movie is not available, which is effectively correct given the setup. The generated answer stays faithful to the (implied) context limitation, matching the intent of the gold response.", "main_total_tokens": 2836, "main_prompt_tokens": 2818, "main_completion_tokens": 18, "judge_total_tokens": 487, "judge_prompt_tokens": 409, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The context does not provide any information about a movie titled \"Python's Revenge 2.\" Therefore, I cannot answer your question.", "latency_ms": 1082.0927619934082, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold answer: the movie is not in the database, so the system cannot provide the year. Generated answer: says the context does not provide any information about that movie and therefore cannot answer. This is effectively the same epistemic position and does not hallucinate a year, so it is correct. It is also fully faithful to the given information (no extra unsupported claims).", "main_total_tokens": 3238, "main_prompt_tokens": 3212, "main_completion_tokens": 26, "judge_total_tokens": 522, "judge_prompt_tokens": 417, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The title \"The Last Pixel\" does not appear in the provided context. Therefore, I cannot provide a release year for it.", "latency_ms": 1189.2240047454834, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year cannot be provided because the movie/title is not in the known information or context. This is correct given the lack of data, and the generated answer remains faithful to the gold by not fabricating a release year or extra details.", "main_total_tokens": 2561, "main_prompt_tokens": 2535, "main_completion_tokens": 26, "judge_total_tokens": 502, "judge_prompt_tokens": 415, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The information about \"The man who ate the sun\" is not found in the provided context.", "latency_ms": 1004.0178298950195, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the information about the movie is not available. The question asks for a release year, and the generated response correctly indicates that this information is not found in the context without hallucinating an answer. It is fully faithful to the (implicit) context limitations and matches the gold answer in substance.", "main_total_tokens": 3437, "main_prompt_tokens": 3418, "main_completion_tokens": 19, "judge_total_tokens": 507, "judge_prompt_tokens": 411, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Days of Lehman Brothers\" is not mentioned in the provided context. Therefore, I cannot provide that information.", "latency_ms": 1267.1232223510742, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not in the database and thus cannot be answered. The generated answer says the director is not mentioned in the provided context and therefore cannot be provided. Both convey the same limitation and do not fabricate an answer, so the response is correct and faithful to the gold.", "main_total_tokens": 3204, "main_prompt_tokens": 3174, "main_completion_tokens": 30, "judge_total_tokens": 519, "judge_prompt_tokens": 427, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The information regarding the director of the movie \"A Love to Die For\" is not provided in the context.", "latency_ms": 1050.2996444702148, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director information for the movie is not provided and aligns with the gold response. It does not invent a director and remains faithful to the given database/context limitation.", "main_total_tokens": 3022, "main_prompt_tokens": 3000, "main_completion_tokens": 22, "judge_total_tokens": 486, "judge_prompt_tokens": 417, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not mentioned in the provided context.", "latency_ms": 987.7302646636963, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director is not provided/mentioned in the available information, which correctly reflects the limitation of the context. The generated answer is consistent with the gold answer and does not introduce unsupported details.", "main_total_tokens": 3122, "main_prompt_tokens": 3104, "main_completion_tokens": 18, "judge_total_tokens": 487, "judge_prompt_tokens": 411, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of \"The Last Night at Tremore Beach\" is not mentioned in the provided context.", "latency_ms": 934.7553253173828, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director is not provided in the available information/context. This matches the limitation described and does not hallucinate a director name, so it is both correct relative to the task and faithful to the given context.", "main_total_tokens": 2665, "main_prompt_tokens": 2645, "main_completion_tokens": 20, "judge_total_tokens": 497, "judge_prompt_tokens": 417, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy,\" released in 2021 [Doc 98375].", "latency_ms": 1342.3802852630615, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'Rise of the Mummy,' matching the gold answer, so correctness is 1. However, it cites a specific release year and a document reference [Doc 98375] that are not present in the user-provided information, making those details hallucinated; thus faithfulness is 0.", "main_total_tokens": 2799, "main_prompt_tokens": 2767, "main_completion_tokens": 32, "judge_total_tokens": 522, "judge_prompt_tokens": 423, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse is \"Stranger in My Bed\" (2005) [Doc 35237].", "latency_ms": 1119.54927444458, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Stranger in My Bed' and does not introduce contradictions or unsupported details; the added year is plausible and does not affect correctness regarding the movie identification.", "main_total_tokens": 2822, "main_prompt_tokens": 2789, "main_completion_tokens": 33, "judge_total_tokens": 499, "judge_prompt_tokens": 429, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie you are referring to is \"Game of Silence\" from 2016, where a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years [Doc 123938].", "latency_ms": 1552.4444580078125, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question explicitly asks for a movie, but the correct answer and the generated answer both identify \"Game of Silence,\" which is actually a TV series, not a movie. Thus the generated answer is not correct in the context of the question. It is partially faithful to the gold output because it repeats the same title, but it adds an incorrect description (calling it a movie and giving a specific year) and an irrelevant citation placeholder, so faithfulness is only partial.", "main_total_tokens": 2455, "main_prompt_tokens": 2414, "main_completion_tokens": 41, "judge_total_tokens": 564, "judge_prompt_tokens": 438, "judge_completion_tokens": 126}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie you are referring to is \"The Craigslist Killer,\" which is based on the true story of Philip Markoff, a med student who leads a double life as a brutal sexual deviant abusing prostitutes he finds via Craigslist [Doc 53039].", "latency_ms": 1875.8747577667236, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the description; the gold answer is 'The Craigslist Killer.' The generated answer also identifies the movie as 'The Craigslist Killer' and adds accurate context about Philip Markoff. There are no contradictions or hallucinated claims relative to the provided information, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2687, "main_prompt_tokens": 2637, "main_completion_tokens": 50, "judge_total_tokens": 537, "judge_prompt_tokens": 442, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie you are referring to is \"Wood Job!\" [Doc 66094].", "latency_ms": 1427.8554916381836, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", which matches the gold reference exactly and does not add any unsupported information.", "main_total_tokens": 4075, "main_prompt_tokens": 4058, "main_completion_tokens": 17, "judge_total_tokens": 475, "judge_prompt_tokens": 417, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure, is titled \"Sesame Street: The Magical Wand Chase\" from the year 2017 [Doc 78373].", "latency_ms": 1420.9332466125488, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'Sesame Street: The Magical Wand Chase,' matching the gold answer exactly on the main required information. It adds the year 2017 and a document reference [Doc 78373], which are not present in the gold and appear hallucinated given only the Q/A pair, so a small faithfulness deduction is warranted. Overall, the core answer is fully correct.", "main_total_tokens": 3183, "main_prompt_tokens": 3126, "main_completion_tokens": 57, "judge_total_tokens": 582, "judge_prompt_tokens": 467, "judge_completion_tokens": 115}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a 7-year-old boy must find the hero within him to save his parents trapped inside a burning car due to a rock fall is titled \"Beyond,\" released in 2016 [Doc 74597].", "latency_ms": 1652.7397632598877, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Beyond,\" which matches the gold answer. It stays faithful to the given information, only adding a plausible release year reference, without contradicting the source description.", "main_total_tokens": 2653, "main_prompt_tokens": 2607, "main_completion_tokens": 46, "judge_total_tokens": 525, "judge_prompt_tokens": 454, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie you are referring to is \"6ixtynin9\" from the year 1999. In this film, a woman is fired from a financial corporation during the Asia crisis and returns home with no money [Doc 22478].", "latency_ms": 1737.654447555542, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"6ixtynin9,\" matching the gold answer exactly. It does not introduce contradictions or extraneous claims beyond confirming the plot detail given in the question. The reference tag is generic but does not affect factuality, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2769, "main_prompt_tokens": 2719, "main_completion_tokens": 50, "judge_total_tokens": 546, "judge_prompt_tokens": 452, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie you are referring to is \"Club Dread,\" which was released in 2004. [Doc 32410]", "latency_ms": 1468.1198596954346, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' matching the gold answer exactly. It does not introduce contradictions or unsupported claims; adding the release year is accurate and does not conflict with the provided information, so it is fully faithful.", "main_total_tokens": 3053, "main_prompt_tokens": 3027, "main_completion_tokens": 26, "judge_total_tokens": 530, "judge_prompt_tokens": 450, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie you are referring to is \"Toilet,\" which was released in 2010. The plot involves a 30-something Gundam fan and engineer who is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral [Doc 48873].", "latency_ms": 1838.0181789398193, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" matching the gold answer exactly and accurately describing the plot. It remains faithful to the provided information without adding unsupported claims, and the reference to a document is consistent with typical citation style in such contexts.", "main_total_tokens": 3434, "main_prompt_tokens": 3369, "main_completion_tokens": 65, "judge_total_tokens": 561, "judge_prompt_tokens": 479, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie you are referring to is \"Electric Angel,\" which is a typical experimental film that uses a variety of audiovisual techniques to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century [Doc 1024].", "latency_ms": 1321.0694789886475, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Electric Angel,' matching the gold answer exactly. It stays faithful to the provided information and does not introduce contradictions or unsupported claims relevant to the evaluation context.", "main_total_tokens": 3259, "main_prompt_tokens": 3206, "main_completion_tokens": 53, "judge_total_tokens": 539, "judge_prompt_tokens": 469, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is \"Down Low,\" which has a rating of 5.207 [Doc 106995].", "latency_ms": 884.5822811126709, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold label states the lowest rated movie from 2023 is 'Mega Ape', but the model answered 'Down Low'. It is also unfaithful to the provided gold answer, directly contradicting it.", "main_total_tokens": 2612, "main_prompt_tokens": 2584, "main_completion_tokens": 28, "judge_total_tokens": 488, "judge_prompt_tokens": 409, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "The highest rated TV show from 2010 is \"Sherlock,\" which has a rating of 8.511 [Doc 119150].", "latency_ms": 1003.0653476715088, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer claims that the highest rated TV show from 2010 is 'Sherlock' with a specific rating, but the gold answer says 'Tea House.' This contradicts the gold, so the answer is incorrect. However, within the context of the provided document citation [Doc 119150], the model appears to be faithfully reflecting that source, so faithfulness is high.", "main_total_tokens": 2947, "main_prompt_tokens": 2918, "main_completion_tokens": 29, "judge_total_tokens": 519, "judge_prompt_tokens": 411, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest rated Japanese language movie from 1999 is \"Kikujiro,\" which has a rating of 7.5 [Doc 22213].", "latency_ms": 1351.1042594909668, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies \"The Uncut Story of Mewtwo's Origin\" as the correct highest-rated Japanese language movie from 1999, while the generated answer claims it is \"Kikujiro.\" This directly contradicts the gold and is therefore incorrect. Additionally, the generated answer invents a specific rating (7.5) and relies on a cited document instead of the provided gold answer, so it is not faithful to the given reference.", "main_total_tokens": 3113, "main_prompt_tokens": 3081, "main_completion_tokens": 32, "judge_total_tokens": 545, "judge_prompt_tokens": 423, "judge_completion_tokens": 122}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is \"Read Lips,\" which has a rating of 4.5. The language of this movie is Chinese (cn) [Doc 354].", "latency_ms": 1690.258264541626, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the language is English, while the generated answer claims it is Chinese, so the content is factually incorrect. The generated answer also invents a specific movie title and rating not supported by the gold, so it is not faithful to the reference.", "main_total_tokens": 2662, "main_prompt_tokens": 2624, "main_completion_tokens": 38, "judge_total_tokens": 505, "judge_prompt_tokens": 421, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was \"Teen Choice Awards,\" which had a popularity rating of 0.4589 [Doc 115397].", "latency_ms": 1628.0784606933594, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold label identifies 'Law & Order: Special Victims Unit' as the TV show in 1999 with the highest popularity, while the model outputs 'Teen Choice Awards' instead. It is also unfaithful to the provided gold answer, citing a different show and a specific popularity rating that contradicts the gold standard.", "main_total_tokens": 3859, "main_prompt_tokens": 3825, "main_completion_tokens": 34, "judge_total_tokens": 527, "judge_prompt_tokens": 424, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 1120.103120803833, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (7.2) and is consistent with the cited document. Both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2870, "main_prompt_tokens": 2852, "main_completion_tokens": 18, "judge_total_tokens": 461, "judge_prompt_tokens": 400, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "There are multiple movies titled \"The Story of Us\" released in different years:\n\n1. The Story of Us (2025) [Doc 112077]\n2. The Story of Us (2019) [Doc 88811]\n3. The Story of Us (1999) [Doc 22481]\n\nPlease specify which one you are referring to for more detailed information.", "latency_ms": 2223.4089374542236, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer clearly expects the 2019 movie, but the generated answer refuses to commit to a single year and instead lists multiple possibilities, so it does not provide the correct required year. It also invents specific document IDs and a 2025 title that are not grounded in the given QA pair, so it is not faithful to the intended context.", "main_total_tokens": 2854, "main_prompt_tokens": 2777, "main_completion_tokens": 77, "judge_total_tokens": 563, "judge_prompt_tokens": 461, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The context does not provide information about the number of votes \"My Dog Skip\" received. Therefore, I cannot answer that question.", "latency_ms": 1654.106855392456, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 279 votes, but the generated answer claims the information is not provided and does not give the correct number, so it is factually incorrect (correctness 0). However, given only the premise that the model had access to some unspecified context and that context may not have contained the votes count, its statement about the context not providing the information is consistent with its own described limitations, so it is faithful to its reasoning (faithfulness 1).", "main_total_tokens": 3058, "main_prompt_tokens": 3032, "main_completion_tokens": 26, "judge_total_tokens": 527, "judge_prompt_tokens": 407, "judge_completion_tokens": 120}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The context provided does not include information about the number of votes received for \"Oh My God!\" or any of the other movies listed. Therefore, I cannot provide that information.", "latency_ms": 1336.2767696380615, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that 'Oh My God!' received 12 votes. The generated answer claims that the context does not include this information and therefore does not provide the number, which is incorrect relative to the gold. However, given the assistant only used the provided context and did not hallucinate a number, the answer is faithful to the stated lack of information but factually incorrect per the gold.", "main_total_tokens": 2913, "main_prompt_tokens": 2878, "main_completion_tokens": 35, "judge_total_tokens": 526, "judge_prompt_tokens": 416, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 941.2326812744141, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2009) exactly and is fully supported by the given information.", "main_total_tokens": 3190, "main_prompt_tokens": 3167, "main_completion_tokens": 23, "judge_total_tokens": 462, "judge_prompt_tokens": 410, "judge_completion_tokens": 52}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o-mini", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 871.9251155853271, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the original language is English (\"en\"), and the generated answer states the same. This is correct and matches the gold exactly, and the added citation does not contradict anything.", "main_total_tokens": 3165, "main_prompt_tokens": 3146, "main_completion_tokens": 19, "judge_total_tokens": 473, "judge_prompt_tokens": 402, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "Wall Street was released in the year 1987 [Doc 6993].", "latency_ms": 881.0532093048096, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, which matches the gold answer. It is accurate and aligns with the referenced document, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2268, "main_prompt_tokens": 2252, "main_completion_tokens": 16, "judge_total_tokens": 466, "judge_prompt_tokens": 394, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie in which classmates have to play a survival game using their cell phones is \"Perfect Strangers\" (2022). In this film, seven friends decide to play a game with their cell phones during a dinner, which leads to disastrous revelations of their dark secrets [Doc 103513].", "latency_ms": 718.3961868286133, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect: the question refers to the movie 'Brain Fluid Explosion Girl', not 'Perfect Strangers (2022)'. Additionally, the explanation about friends at a dinner party revealing secrets via a phone game does not match the described survival game among classmates, indicating the response is not faithful to the question or gold answer.", "main_total_tokens": 2046, "main_prompt_tokens": 1987, "main_completion_tokens": 59, "judge_total_tokens": 545, "judge_prompt_tokens": 447, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show in which six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons,\" which aired in 1983. In this show, the friends are transported into the Dungeons & Dragons realm and must try to find a way home with the help of their guide, the Dungeon Master [Doc 112849].", "latency_ms": 1768.968105316162, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' matching the gold answer exactly. The additional detail about the premise and year is accurate and consistent with known information, and does not contradict the given gold answer, so the response is both correct and faithful.", "main_total_tokens": 2105, "main_prompt_tokens": 2035, "main_completion_tokens": 70, "judge_total_tokens": 546, "judge_prompt_tokens": 460, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is titled **Nacht der Wölfe**. It was released in 1982 and is described as the German version of \"West Side Story\" [Doc 2590].", "latency_ms": 1559.4284534454346, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe,' matching the gold answer exactly. The additional details (year, comparison to West Side Story, and citation) are consistent with the referenced document, so the response remains faithful to the provided context.", "main_total_tokens": 2281, "main_prompt_tokens": 2220, "main_completion_tokens": 61, "judge_total_tokens": 544, "judge_prompt_tokens": 459, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of \"Picnic at Hanging Rock\" take place on Valentine's Day, 1900 [Doc 126191].", "latency_ms": 865.8194541931152, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the year as 1900 and directly matches the gold answer, indicating both factual correctness and faithful use of the provided information.", "main_total_tokens": 2069, "main_prompt_tokens": 2044, "main_completion_tokens": 25, "judge_total_tokens": 471, "judge_prompt_tokens": 410, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie you are referring to is \"June.\" It features a young violinist named June, who spends her days working in a quirky coffee shop in Venice, rehearses for a professional music career at night, and daydreams about a beautiful man destined for her. The plot also includes her discovering that this man, Jack, is engaged to someone else, leading to romantic conflicts [Doc 32483].", "latency_ms": 1791.2850379943848, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The question asks for the title of the 2004 movie; the correct answer is indeed \"June,\" which the generated answer provides, so correctness is 1. The description of the plot (young violinist named June, Venice coffee shop, romance complications) matches the question and appears plausible, but the citation [Doc 32483] is fabricated and not verifiable, slightly reducing faithfulness.", "main_total_tokens": 2421, "main_prompt_tokens": 2336, "main_completion_tokens": 85, "judge_total_tokens": 630, "judge_prompt_tokens": 517, "judge_completion_tokens": 113}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1318.413257598877, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a concrete movie title, but the generated answer says 'I don't know,' which is factually incorrect given the gold. However, the model does not hallucinate and is faithful to its own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 399, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 579.843282699585, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric rating (7.6). The generated answer says 'I don't know,' which does not provide the requested value, so it is incorrect. However, given the model's uncertainty, saying 'I don't know' is honest and does not introduce fabricated information, so it is faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 389, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1153.2325744628906, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.5143371794949736, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.013881288441812, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is incorrect. The assistant clearly outputs a specific numerical answer without hedging or contradicting itself, so it is internally consistent and thus faithful to its own reasoning, even though the content is wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 394, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 253.74436378479004, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is factually incorrect. However, there is no indication that the model contradicted its own reasoning or invented constraints beyond the question; it directly produced a (wrong) numeric answer, so it remains faithful to its (implicit) reasoning.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 490, "judge_prompt_tokens": 395, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1736.0832691192627, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not correctly identify 'Fantômas' as the answer, so it is incorrect. However, it does not introduce any false information or contradictions relative to the question; it simply expresses a lack of knowledge, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 389, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is titled **Kill Cruise**. It was released in 1990 and features a plot where the yacht captain does not know what he's in for with the two British beauties [Doc 11205].", "latency_ms": 1257.1017742156982, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' matching the gold answer exactly. It adds a brief, accurate description and year that do not conflict with the given information. The response is fully correct and faithful to the question and gold answer.", "main_total_tokens": 2304, "main_prompt_tokens": 2251, "main_completion_tokens": 53, "judge_total_tokens": 519, "judge_prompt_tokens": 438, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 1557.5335025787354, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, including punctuation, and does not introduce unsupported information. It is both correct and faithful to the reference.", "main_total_tokens": 2311, "main_prompt_tokens": 2272, "main_completion_tokens": 39, "judge_total_tokens": 495, "judge_prompt_tokens": 440, "judge_completion_tokens": 55}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The movie \"YEAR\" was released in 2023 [Doc 107925].", "latency_ms": 1019.4554328918457, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 2018, but the generated answer states 2023, so it is factually incorrect. It also appears to invent a movie name \"YEAR\" and a citation [Doc 107925] that do not correspond to the gold answer, so it is not faithful to the provided information.", "main_total_tokens": 2243, "main_prompt_tokens": 2226, "main_completion_tokens": 17, "judge_total_tokens": 488, "judge_prompt_tokens": 394, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is **Astro Boy**. In this series, after being abandoned by his creator, a young boy robot is rescued by an Android Advocate, Dr. Elefun, who adopts him and guides him in using his special powers to aid and protect mankind and robot-kind [Doc 112591].", "latency_ms": 2494.581937789917, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy,' matching the gold answer, so correctness is 1. The additional plot details are broadly consistent with common descriptions of Astro Boy, but they introduce specific claims and a document citation ('[Doc 112591]') that we cannot directly verify from the given prompt, so faithfulness is slightly reduced.", "main_total_tokens": 2387, "main_prompt_tokens": 2311, "main_completion_tokens": 76, "judge_total_tokens": 569, "judge_prompt_tokens": 463, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room,\" released in 2015, the woman and her son were held captive for 7 years in an enclosed space before they gained their freedom, allowing the boy to experience the outside world for the first time [Doc 67513].", "latency_ms": 1427.253007888794, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9463946303571863, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer. It is faithful to the provided information and does not introduce any contradictions; the extra descriptive detail is consistent with the movie’s plot.", "main_total_tokens": 1917, "main_prompt_tokens": 1865, "main_completion_tokens": 52, "judge_total_tokens": 528, "judge_prompt_tokens": 446, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 865.6651973724365, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before the 26 November 1983 break-in, which matches the provided gold value. It faithfully reflects the source information without adding unsupported claims.", "main_total_tokens": 2131, "main_prompt_tokens": 2098, "main_completion_tokens": 33, "judge_total_tokens": 509, "judge_prompt_tokens": 428, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6246.6371059417725, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score. The gold answer is 80.5772, but the model responded with 'I don't know,' which does not provide the correct score, so correctness is 0. However, the model did not fabricate any answer and honestly stated its lack of knowledge, so the response is faithful to its uncertainty, giving a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 506, "judge_prompt_tokens": 396, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 335.57963371276855, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there was 1 Russian language TV show released in 1997, but the generated answer was 0, which is incorrect. However, the evaluation here is strictly about comparing the generated answer to the provided gold answer, so it is fully faithful to that comparison while being factually incorrect relative to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 482, "judge_prompt_tokens": 385, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 965.5637741088867, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Angel Eyes' and does not introduce unsupported information beyond a document reference.", "main_total_tokens": 2096, "main_prompt_tokens": 2061, "main_completion_tokens": 35, "judge_total_tokens": 490, "judge_prompt_tokens": 429, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French (fr) [Doc 110681].", "latency_ms": 1003.8537979125977, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says the movie language is Korean, but the generated answer says it is French. This is factually incorrect relative to the gold, so correctness is 0. The generated answer is consistent with itself and does not contradict its own context, so faithfulness is 1.", "main_total_tokens": 1985, "main_prompt_tokens": 1965, "main_completion_tokens": 20, "judge_total_tokens": 484, "judge_prompt_tokens": 396, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 623.3949661254883, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. However, there is no indication that the model contradicted its own reasoning or used unsupported assumptions in arriving at that incorrect numeric answer, so faithfulness to its (unstated) reasoning process cannot be judged as violated based on the given information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 493, "judge_prompt_tokens": 394, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The context provided does not contain any information about movies released in 1979, including their ratings. Therefore, I cannot answer the question regarding the best-rated movie released in that year.", "latency_ms": 1119.7535991668701, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Given the gold answer that the database has no movies from 1979, the generated response correctly states that it cannot answer because the context lacks information on 1979 movies, which is consistent with the provided context. Thus it is both correct and faithful to the context and gold.", "main_total_tokens": 2194, "main_prompt_tokens": 2157, "main_completion_tokens": 37, "judge_total_tokens": 519, "judge_prompt_tokens": 432, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The movie \"Back to the Future Part IV\" was not mentioned in the provided context, and therefore, I cannot provide a release year for it.", "latency_ms": 1103.8224697113037, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct answer is that 'Back to the Future Part IV' does not exist, so there is no release year. The generated answer instead claims it cannot answer because it was not mentioned in the provided context, which is incorrect as a response to the user’s question and fails to convey that the movie does not exist. It is partially faithful in admitting it cannot provide a year, but the stated reason contradicts the gold answer’s basis (non-existence vs. missing from context).", "main_total_tokens": 2459, "main_prompt_tokens": 2429, "main_completion_tokens": 30, "judge_total_tokens": 560, "judge_prompt_tokens": 430, "judge_completion_tokens": 130}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that no movies matching the criteria can be found. This is factually correct because the Chernobyl disaster occurred in 1986, so a 1980 movie about it would not exist. The generated answer is faithful to the question: it correctly interprets the constraints and explains that nothing matches, also hinting at the temporal inconsistency. There is no hallucinated movie or incorrect popularity score.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 552, "judge_prompt_tokens": 434, "judge_completion_tokens": 118}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 253.60369682312012, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer ('0') is factually incorrect because the correct number is 4. However, the model faithfully provided a single numeric answer to the question asked, without adding any extraneous or conflicting information beyond that incorrect number.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 388, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 269.4087028503418, "retrieval_recall": 0.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.17723928678404774, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2 while the generated answer is 1, so the generated answer is incorrect. The explanation of correctness vs. gold is consistent with the provided values, so faithfulness is fine, but the actual answer does not match the gold label.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 386, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 169.07882690429688, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the number of TV shows released in the 1990s about Super Mario Bros. The gold answer is 1, but the generated answer is 0, which is numerically incorrect. However, the model directly answers the question without adding unsupported details; it is simply wrong about the factual number. Thus, correctness is 0, but faithfulness to its own (albeit incorrect) output is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 507, "judge_prompt_tokens": 389, "judge_completion_tokens": 118}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 184.01241302490234, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0 and the generated answer is also 0, so the generated response exactly matches the gold. The question is factual and the provided answer aligns perfectly, so it is both correct and faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 391, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 178.51710319519043, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the most popular TV show or movie about ninjas, and the gold answer is 'Naruto'. The generated answer says 'I don't know', which is incorrect because a specific correct answer exists. However, the model did not fabricate any incorrect information; it admitted not knowing, so it is fully faithful to its own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 386, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The context provided does not mention a Jurassic Park/Power Ranger crossover movie, nor does it provide a release date for such a film. Therefore, I cannot answer your question based on the available information.", "latency_ms": 1552.6671409606934, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the movie was not released. The generated answer only claims it cannot answer based on the provided context, implying uncertainty rather than correctly stating that no such movie exists or was released. Thus it is incorrect relative to the gold. However, it is faithful, as it accurately reflects the lack of information in the context without fabricating details.", "main_total_tokens": 2232, "main_prompt_tokens": 2192, "main_completion_tokens": 40, "judge_total_tokens": 527, "judge_prompt_tokens": 425, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "There is no movie in which Barbie fights the Predator. The context provided includes details about various Barbie movies and a separate movie titled \"The Predator,\" but there is no mention of a crossover or any scenario where Barbie fights the Predator.", "latency_ms": 1358.5712909698486, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that such a movie does not exist, and the generated answer remains consistent with the provided context without introducing contradictions or hallucinations.", "main_total_tokens": 2252, "main_prompt_tokens": 2206, "main_completion_tokens": 46, "judge_total_tokens": 492, "judge_prompt_tokens": 428, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The 4th oldest Star Wars movie is \"Return of the Jedi,\" which was released in 1983 [Doc 2858]. The oldest Star Wars movies in chronological order are:\n\n1. \"Star Wars: Episode IV - A New Hope\" (1977)\n2. \"Star Wars: Episode V - The Empire Strikes Back\" (1980)\n3. \"Star Wars: Episode VI - Return of the Jedi\" (1983)\n4. \"Star Wars: Episode I - The Phantom Menace\" (1999)\n\nSince the context does not provide the release years of the first three episodes, I can only confirm that \"Return of the Jedi\" is the 4th oldest based on the information available.", "latency_ms": 5002.923965454102, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct answer per the gold label is 'Star Wars: Droids - The Great Heep,' but the generated answer claims 'Return of the Jedi' is the 4th oldest, which is factually incorrect. The reasoning and list given are also wrong about which film is 4th oldest by release. However, the answer does attempt to base its conclusion on the provided (though incomplete or misleading) context and explicitly notes limitations, so it is partially faithful to the cited document but not to the actual ground truth.", "main_total_tokens": 2527, "main_prompt_tokens": 2379, "main_completion_tokens": 148, "judge_total_tokens": 675, "judge_prompt_tokens": 537, "judge_completion_tokens": 138}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The movie \"YEAR\" was released in 2023 [Doc 107925].", "latency_ms": 1189.462661743164, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer gives the release year as 2023, while the correct year is 1986, so it is factually incorrect. It also invents a movie title \"YEAR\" not grounded in the gold answer or any provided context, so it is unfaithful to the reference.", "main_total_tokens": 2243, "main_prompt_tokens": 2226, "main_completion_tokens": 17, "judge_total_tokens": 484, "judge_prompt_tokens": 394, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 332.216739654541, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The user asked for the movie rating, and the gold answer is a specific numerical value (8.466). The generated answer says 'I don't know,' which is incorrect because it does not provide the correct rating. However, it is faithful in that it does not hallucinate or fabricate an answer; it admits lack of knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 495, "judge_prompt_tokens": 398, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is **Mighty Med** [Doc 121125].", "latency_ms": 1157.5329303741455, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med', which matches the gold answer exactly. It does not introduce any contradictory or extraneous information beyond a document citation, so it is both correct and faithful to the reference answer.", "main_total_tokens": 2036, "main_prompt_tokens": 2005, "main_completion_tokens": 31, "judge_total_tokens": 502, "judge_prompt_tokens": 423, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In \"Túnel de los huesos\" (2011), seven prisoners escaped from a prison in Argentina, specifically from the Villa Devoto prison in Buenos Aires [Doc 53582].", "latency_ms": 1305.0453662872314, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies Argentina as the country, matching the gold answer. However, it adds the specific detail 'Villa Devoto prison in Buenos Aires' which is not present in the provided gold answer and may not be supported by the referenced document snippet, so there is a small faithfulness concern due to this extra, potentially unverifiable detail.", "main_total_tokens": 2460, "main_prompt_tokens": 2421, "main_completion_tokens": 39, "judge_total_tokens": 532, "judge_prompt_tokens": 430, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice Li Nai falls into a trap designed by the \"Chef Assassin\" Jiang Jie [Doc 135231].", "latency_ms": 1118.4697151184082, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The question asks whose trap the apprentice falls into; the gold answer is simply 'Jiang Jie.' The generated answer states that the apprentice falls into a trap designed by Jiang Jie, which correctly identifies the person and thus answers the question. However, it adds extra details such as the apprentice being 'Li Nai' and describing Jiang Jie as a 'Chef Assassin' with a document citation, none of which are supported or required by the gold answer. This makes the core answer correct but introduces unverifiable specifics, so correctness is 1 while faithfulness is slightly reduced.", "main_total_tokens": 2159, "main_prompt_tokens": 2127, "main_completion_tokens": 32, "judge_total_tokens": 562, "judge_prompt_tokens": 416, "judge_completion_tokens": 146}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 633.1503391265869, "retrieval_recall": 0.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.19519002499605084, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.2, "ndcg_at_10": 0.3080351663449861, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 4, but the generated answer is 0, which is incorrect. The evaluation correctly identifies the mismatch between the gold and generated answers, so the faithfulness of the evaluation is high even though the generated answer itself is wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 466, "judge_prompt_tokens": 385, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "46", "latency_ms": 131.6697597503662, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer \"46\" is incorrect because the gold answer states that there are no movies with both Bruce Lee and Steven Seagal together. The model’s response is not supported by the information and contradicts the gold, so it is neither correct nor faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 395, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of \"Tokyo Mater\" (2008) is 6.3 [Doc 42750].", "latency_ms": 803.3590316772461, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3 and correctly states it as the rating of 'Tokyo Mater' (2008). It is fully correct and faithful to the gold.", "main_total_tokens": 2299, "main_prompt_tokens": 2277, "main_completion_tokens": 22, "judge_total_tokens": 475, "judge_prompt_tokens": 405, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in the movie \"Fast Track: No Limits\" is centered around street racing, as it focuses on a group of street racers [Doc 41929]. Similarly, the movie \"Street Racer\" also features street racing, highlighting the competitive and dangerous nature of illegal street races [Doc 42313].", "latency_ms": 1772.4087238311768, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer specifies the style of racing as 'Drift', while the generated answer describes 'street racing' and related movies, which is incorrect for this question. Thus correctness is 0. The answer also introduces specific movie titles and document references not supported by the provided gold label, so it is partly hallucinated and not faithful; assign 0.5 for faithfulness.", "main_total_tokens": 2349, "main_prompt_tokens": 2285, "main_completion_tokens": 64, "judge_total_tokens": 551, "judge_prompt_tokens": 442, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is **\"Confronting a Serial Killer\"** [Doc 131298].", "latency_ms": 1304.5971393585205, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer: 'Confronting a Serial Killer.' It correctly identifies the movie title and does not introduce any contradictions or unsupported claims. Therefore, it is both correct and faithful to the reference.", "main_total_tokens": 2301, "main_prompt_tokens": 2253, "main_completion_tokens": 48, "judge_total_tokens": 530, "judge_prompt_tokens": 454, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of \"Armageddon\" (1998) is 7.838 [Doc 20603].", "latency_ms": 950.6826400756836, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) for Armageddon (1998) and does not introduce any contradictions or extraneous claims. It is both correct and faithful to the provided gold answer.", "main_total_tokens": 2317, "main_prompt_tokens": 2293, "main_completion_tokens": 24, "judge_total_tokens": 485, "judge_prompt_tokens": 409, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 834.4578742980957, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6895405204413555, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'The Sixth Sense' and stays faithful to the question without adding unsupported information.", "main_total_tokens": 1960, "main_prompt_tokens": 1931, "main_completion_tokens": 29, "judge_total_tokens": 482, "judge_prompt_tokens": 420, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 965.4061794281006, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer. It is fully consistent with the provided information and does not introduce unsupported claims.", "main_total_tokens": 2259, "main_prompt_tokens": 2242, "main_completion_tokens": 17, "judge_total_tokens": 464, "judge_prompt_tokens": 397, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is 'The Transporter Refueled' [Doc 67500].", "latency_ms": 1352.2584438323975, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the title as 'The Transporter Refueled' and does not introduce any unsupported information.", "main_total_tokens": 2281, "main_prompt_tokens": 2252, "main_completion_tokens": 29, "judge_total_tokens": 481, "judge_prompt_tokens": 420, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in the year 2015 [Doc 67500].", "latency_ms": 740.123987197876, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer, and it remains faithful to the given information without adding unsupported details.", "main_total_tokens": 2303, "main_prompt_tokens": 2281, "main_completion_tokens": 22, "judge_total_tokens": 476, "judge_prompt_tokens": 406, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "The context provided does not mention any romantic comedy in which Abraham Lincoln comes back from the dead. The movies listed include titles like \"Abraham Lincoln: Vampire Hunter\" and \"Abraham Lincoln vs. Zombies,\" but these are not categorized as romantic comedies. Therefore, I cannot provide an answer based on the given context.", "latency_ms": 1659.5995426177979, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no such movie description can be found, effectively saying there is no known romantic comedy where Abraham Lincoln comes back from the dead. The generated answer similarly concludes that the context does not mention such a romantic comedy and therefore cannot provide an answer. This matches the gold’s stance. It correctly interprets the context and does not hallucinate a movie title, thus it is both correct and faithful to the provided information.", "main_total_tokens": 2100, "main_prompt_tokens": 2035, "main_completion_tokens": 65, "judge_total_tokens": 572, "judge_prompt_tokens": 455, "judge_completion_tokens": 117}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by Ernest Hemingway is \"Havana Darkness\" (2019). The plot involves three friends traveling to Havana, Cuba, to investigate the origins of a mysterious manuscript supposedly written by Hemingway, leading them into a terrifying situation [Doc 90923].", "latency_ms": 1537.0988845825195, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'Havana Darkness,' which matches the gold answer exactly. The additional plot description is consistent with the known premise of the film and does not contradict the question or add unsupported claims. Therefore, both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2086, "main_prompt_tokens": 2025, "main_completion_tokens": 61, "judge_total_tokens": 536, "judge_prompt_tokens": 449, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "The TV show in which Doll heads to Hollywood to be with her childhood best friend Em is titled \"Doll & Em\" [Doc 122259].", "latency_ms": 871.7679977416992, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Doll & Em,\" which matches the gold answer exactly. It is fully supported by the cited document reference and does not introduce any unsupported information, so it is both correct and faithful.", "main_total_tokens": 2186, "main_prompt_tokens": 2155, "main_completion_tokens": 31, "judge_total_tokens": 498, "judge_prompt_tokens": 420, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in the 2007 movie titled \"Hallam Foe\" [Doc 39389].", "latency_ms": 1389.6064758300781, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Hallam Foe,\" matching the gold answer exactly, and does not introduce any contradictions or unsupported claims. It is therefore both correct and faithful.", "main_total_tokens": 2107, "main_prompt_tokens": 2068, "main_completion_tokens": 39, "judge_total_tokens": 507, "judge_prompt_tokens": 438, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The context does not provide specific information about a last name that sounds like a friend or a foe. Therefore, I cannot answer the question based on the provided documents.", "latency_ms": 988.2135391235352, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'A foe,' indicating the question is answerable and has a specific expected response. The generated answer claims the question cannot be answered from context and does not provide 'A foe,' so it is incorrect. However, the model does not hallucinate any alternative content; it simply expresses inability to answer, which is faithful to its stated reasoning, so faithfulness is high.", "main_total_tokens": 2016, "main_prompt_tokens": 1983, "main_completion_tokens": 33, "judge_total_tokens": 528, "judge_prompt_tokens": 419, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "Sam is 18 years old in the 2017 TV show *Atypical* [Doc 124583].", "latency_ms": 825.4461288452148, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that Sam is 18 years old in the 2017 TV show 'Atypical', which matches the gold answer of 18. It is correct and aligns exactly with the provided gold reference, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 1980, "main_prompt_tokens": 1956, "main_completion_tokens": 24, "judge_total_tokens": 496, "judge_prompt_tokens": 411, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show *Atypical*, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 1722.4996089935303, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the 2017 TV show 'Atypical', Sam is on the autism spectrum, which matches the gold answer exactly. It is faithful to the provided information and does not introduce unsupported details.", "main_total_tokens": 2201, "main_prompt_tokens": 2177, "main_completion_tokens": 24, "judge_total_tokens": 488, "judge_prompt_tokens": 411, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "The context provided does not contain any information about the movie \"À corde tendue\" or the name of the place where Sandra was born. Therefore, I cannot answer your question.", "latency_ms": 1041.696548461914, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is Chamonix, but the generated answer claims the information is not available and refuses to answer. This is incorrect. However, the model is faithful to the (stated) context, since it honestly reports that the context does not contain the information, so faithfulness is high even though the answer is wrong relative to the gold.", "main_total_tokens": 2313, "main_prompt_tokens": 2277, "main_completion_tokens": 36, "judge_total_tokens": 534, "judge_prompt_tokens": 433, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In the Japanese language movie \"When the Rain Falls,\" Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 1035.1049900054932, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly fills in the blank with 'rains', matching the gold answer. However, it adds an unsupported specific movie title ('When the Rain Falls') and a document reference, which are not grounded in the provided information, slightly reducing faithfulness.", "main_total_tokens": 2165, "main_prompt_tokens": 2139, "main_completion_tokens": 26, "judge_total_tokens": 502, "judge_prompt_tokens": 417, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the low-rated German language animation \"Tabaluga,\" the character Tabaluga is the last of the dragons and the crown-prince of Greenland, a magical place inhabited by talking animals of many different species [Doc 114810].", "latency_ms": 1543.6608791351318, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Greenland as the place where Tabaluga is crown-prince and matches the gold answer. It does not introduce contradictions and remains faithful to the given information, with the extra descriptive detail not conflicting with the core fact.", "main_total_tokens": 2432, "main_prompt_tokens": 2384, "main_completion_tokens": 48, "judge_total_tokens": 521, "judge_prompt_tokens": 442, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie that matches the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers\" is **One Afternoon**. It was released in 2021 and has a rating of 6.8 [Doc 96838].", "latency_ms": 1405.1706790924072, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies the movie as 'One Afternoon', which matches the gold answer, so correctness is 1. However, it adds specific details about release year (2021), rating (6.8), and a citation [Doc 96838] that are not present in the provided prompt and may or may not be supported by the source, so faithfulness is slightly reduced.", "main_total_tokens": 2229, "main_prompt_tokens": 2167, "main_completion_tokens": 62, "judge_total_tokens": 577, "judge_prompt_tokens": 466, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is titled \"The Life Ahead,\" released in 2020. It has a rating of 6.746 [Doc 93260].", "latency_ms": 1511.7855072021484, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as \"The Life Ahead,\" matching the gold answer, so correctness is full. However, it adds an unverified and oddly specific rating value (6.746) and a citation [Doc 93260] that are not present in the gold answer and may not be grounded in the provided context, so faithfulness is slightly reduced.", "main_total_tokens": 2623, "main_prompt_tokens": 2570, "main_completion_tokens": 53, "judge_total_tokens": 558, "judge_prompt_tokens": 451, "judge_completion_tokens": 107}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team. The film illustrates how he, coming from humble beginnings, was able to compete against and win against the elite of motor racing [Doc 73859].", "latency_ms": 1374.4618892669678, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies that the 2016 documentary \"McLaren\" is about Bruce McLaren, matching the gold answer, so correctness is 1. However, it introduces an external citation/reference ([Doc 73859]) that is not present in the prompt and cannot be verified, making the answer not fully faithful to the provided context, so faithfulness is 0.", "main_total_tokens": 2133, "main_prompt_tokens": 2075, "main_completion_tokens": 58, "judge_total_tokens": 549, "judge_prompt_tokens": 442, "judge_completion_tokens": 107}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The context provided does not specify the name of any racing team associated with the characters or plots mentioned in the movies. Therefore, I cannot provide an answer to your question based on the available information.", "latency_ms": 1467.7536487579346, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the racing team is the McLaren Motor Racing team, so the correct answer is known. The generated answer incorrectly claims the context does not specify any racing team and refuses to answer, making it factually incorrect with respect to the gold. However, it remains faithful to the (claimed) context by not inventing a team name, so faithfulness is high even though the ultimate answer is wrong relative to the gold.", "main_total_tokens": 2355, "main_prompt_tokens": 2316, "main_completion_tokens": 39, "judge_total_tokens": 541, "judge_prompt_tokens": 422, "judge_completion_tokens": 119}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context does not provide specific information about the country of origin for \"he.\" Therefore, I cannot answer the question based on the provided context.", "latency_ms": 1052.001714706421, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'New Zealand', but the generated answer says the country cannot be determined, so it is incorrect. However, given only the user question and no explicit context mentioning New Zealand, the model's claim that it cannot answer from context is consistent with the visible information, so it is faithful to the provided context.", "main_total_tokens": 2261, "main_prompt_tokens": 2232, "main_completion_tokens": 29, "judge_total_tokens": 501, "judge_prompt_tokens": 405, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "The context does not provide any information about a 2013 movie in which John McClane blows up the Great Wall of China. The only relevant movie mentioned is \"A Good Day to Die Hard,\" which features John McClane but does not indicate any scene involving the Great Wall of China [Doc 58791].", "latency_ms": 1642.8539752960205, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says there is no such movie in the database. The generated answer correctly notes that the provided context does not mention any 2013 movie where John McClane blows up the Great Wall of China, and specifically clarifies that 'A Good Day to Die Hard' does not include such a scene. This aligns with the gold response and does not introduce unsupported claims, so it is both correct and faithful to the given context.", "main_total_tokens": 2220, "main_prompt_tokens": 2155, "main_completion_tokens": 65, "judge_total_tokens": 582, "judge_prompt_tokens": 463, "judge_completion_tokens": 119}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie that matches the description is **\"Tomie: Forbidden Fruit.\"** The plot involves a lonely high school girl who befriends a strange, mysterious, and beautiful girl, and it becomes clear that their meeting was not purely accidental [Doc 28589].", "latency_ms": 2710.245132446289, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Tomie: Forbidden Fruit,' which matches the gold answer exactly. It restates the description and does not introduce any contradictions or unsupported claims beyond referencing a document ID, so it is both correct and faithful to the given information.", "main_total_tokens": 2352, "main_prompt_tokens": 2293, "main_completion_tokens": 59, "judge_total_tokens": 561, "judge_prompt_tokens": 476, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie \"Lei mi parla ancora,\" Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 1003.892183303833, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years), so it is correct. It directly uses the information given without introducing contradictions or unsupported details, so it is faithful.", "main_total_tokens": 2338, "main_prompt_tokens": 2308, "main_completion_tokens": 30, "judge_total_tokens": 490, "judge_prompt_tokens": 424, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In \"Scorpion King: Book of Souls\" (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls, which will allow them to put an end to an evil warlord [Doc 82582].", "latency_ms": 1242.7129745483398, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly regarding what they search for: The Book of Souls. The added description about it being a legendary relic that can end an evil warlord is consistent with the film and does not contradict the gold answer, so the response is both correct and faithful.", "main_total_tokens": 2615, "main_prompt_tokens": 2564, "main_completion_tokens": 51, "judge_total_tokens": 533, "judge_prompt_tokens": 445, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Zombies on the Moon\" is not found in the provided context.", "latency_ms": 1182.5759410858154, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database; the generated answer states the release year is not found in the provided context. Both effectively convey that the information is unavailable, so the answer is correct relative to the gold and faithful to the given context.", "main_total_tokens": 1964, "main_prompt_tokens": 1945, "main_completion_tokens": 19, "judge_total_tokens": 493, "judge_prompt_tokens": 410, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"The Galactic Baker\" was released is not found in the provided context.", "latency_ms": 919.1544055938721, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available/not found. This is correct given the context and faithful to the information shown.", "main_total_tokens": 2396, "main_prompt_tokens": 2379, "main_completion_tokens": 17, "judge_total_tokens": 467, "judge_prompt_tokens": 406, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"Return of the Null Pointer\" was released is not found in the provided context.", "latency_ms": 935.0442886352539, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the release year is not available / not found, and the generated answer does not invent any information beyond the prompt, so it is both correct and faithful to the context.", "main_total_tokens": 2084, "main_prompt_tokens": 2065, "main_completion_tokens": 19, "judge_total_tokens": 484, "judge_prompt_tokens": 410, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year of \"Data Science: The Movie\" is not found in the provided context.", "latency_ms": 850.334644317627, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers convey that the release year is unknown/not available. The generated answer is correct relative to the question and faithful to the information in the gold response, which also indicates that the movie is not in the database (hence no release year).", "main_total_tokens": 2165, "main_prompt_tokens": 2146, "main_completion_tokens": 19, "judge_total_tokens": 494, "judge_prompt_tokens": 410, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year of \"Python's Revenge 2\" is not found in the provided context.", "latency_ms": 794.0216064453125, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers indicate that the release year/information is not available, which is correct given the context, and the model does not invent a year. The generated answer is fully consistent with the provided context and does not hallucinate any details.", "main_total_tokens": 2421, "main_prompt_tokens": 2402, "main_completion_tokens": 19, "judge_total_tokens": 492, "judge_prompt_tokens": 410, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year of \"The Last Pixel\" is not found in the provided context.", "latency_ms": 934.3521595001221, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available / not found, which is effectively the same information. The generated answer stays within the information given in the context and does not hallucinate details.", "main_total_tokens": 1824, "main_prompt_tokens": 1807, "main_completion_tokens": 17, "judge_total_tokens": 480, "judge_prompt_tokens": 406, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"The Man Who Ate the Sun\" was released is not found in the provided context.", "latency_ms": 854.7825813293457, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the release year is not available; the generated answer is consistent with the given context and does not hallucinate a date.", "main_total_tokens": 2578, "main_prompt_tokens": 2558, "main_completion_tokens": 20, "judge_total_tokens": 476, "judge_prompt_tokens": 412, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was directed by Adrian Sitaru [Doc 46069].", "latency_ms": 617.9180145263672, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the director information is not available in the database, so the system should abstain. The generated answer confidently names a director (Adrian Sitaru) and cites a document, which contradicts the gold and appears fabricated relative to the database. Therefore it is both incorrect with respect to the task and unfaithful to the provided evidence/limitations.", "main_total_tokens": 2582, "main_prompt_tokens": 2558, "main_completion_tokens": 24, "judge_total_tokens": 527, "judge_prompt_tokens": 421, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not contain any information about a movie titled \"A Love to Die For,\" including its director. Therefore, I cannot provide an answer to your question.", "latency_ms": 624.4516372680664, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the director information is not available in the provided context/database and decline to answer. The generated answer is faithful to the lack of information in the context and does not hallucinate a director.", "main_total_tokens": 2418, "main_prompt_tokens": 2384, "main_completion_tokens": 34, "judge_total_tokens": 506, "judge_prompt_tokens": 429, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not mentioned in the provided context. Therefore, I cannot provide that information.", "latency_ms": 1468.2667255401611, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned and aligns with the gold answer that the database does not provide this information. It does not introduce any unsupported details and is fully faithful to the given context.", "main_total_tokens": 2440, "main_prompt_tokens": 2414, "main_completion_tokens": 26, "judge_total_tokens": 492, "judge_prompt_tokens": 419, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not specify who directed the movie \"The Last Night at Tremore Beach.\" Therefore, I cannot provide that information.", "latency_ms": 903.4452438354492, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided context/database. The generated response correctly refrains from guessing and aligns with the limitation described by the gold answer. Therefore, it is both correct and faithful to the given information.", "main_total_tokens": 2077, "main_prompt_tokens": 2050, "main_completion_tokens": 27, "judge_total_tokens": 507, "judge_prompt_tokens": 424, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is titled **\"Rise of the Mummy,\"** released in 2021. In this film, the students become trapped in a time loop and must defeat the mummy to escape, as it seeks to collect the souls of those who awakened it to walk the earth permanently [Doc 98375].", "latency_ms": 2108.1197261810303, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies the movie as 'Rise of the Mummy,' matching the gold answer, so correctness is 1. However, it adds specific plot details (time loop, soul collection, release year 2021, and a citation) that are not supported by the given gold answer and may not be grounded in the provided context, slightly reducing faithfulness.", "main_total_tokens": 2202, "main_prompt_tokens": 2129, "main_completion_tokens": 73, "judge_total_tokens": 571, "judge_prompt_tokens": 464, "judge_completion_tokens": 107}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse is titled **\"Stranger in My Bed,\"** released in 2005. In this film, after faking her death, Sarah's estranged husband quickly discovers the truth and begins to hunt her down, leading to a deadly confrontation between them [Doc 35237].", "latency_ms": 1518.8848972320557, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'Stranger in My Bed,' which matches the gold answer, so correctness is 1. The added plot details about the husband discovering the truth and hunting her down are plausible but not directly supported by the provided gold snippet, so faithfulness is slightly reduced to 0.9.", "main_total_tokens": 2196, "main_prompt_tokens": 2125, "main_completion_tokens": 71, "judge_total_tokens": 565, "judge_prompt_tokens": 467, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie where a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years is titled \"Game of Silence\" [Doc 123938].", "latency_ms": 944.6413516998291, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Game of Silence') and does not introduce any contradictory or extraneous claims, so it is both correct and faithful.", "main_total_tokens": 1869, "main_prompt_tokens": 1836, "main_completion_tokens": 33, "judge_total_tokens": 493, "judge_prompt_tokens": 430, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie you are referring to is \"The Craigslist Killer,\" which was released in 2011. It is based on the true story of Philip Markoff, a med student who led a double life as a brutal sexual deviant, abusing prostitutes he found via Craigslist [Doc 53039].", "latency_ms": 1421.1857318878174, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer,' matching the gold answer, so correctness is 1. However, it introduces an external citation '[Doc 53039]' and additional specific characterization details that are not present in the original question or gold answer, indicating hallucinated sourcing/content relative to the provided context, so faithfulness is 0.", "main_total_tokens": 2128, "main_prompt_tokens": 2069, "main_completion_tokens": 59, "judge_total_tokens": 554, "judge_prompt_tokens": 451, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which Yuki Hirano sets his eyes on the role of a forestry trainee after failing his university entrance exam is titled **Wood Job!**. It was released in 2014 and falls under the genres of drama and comedy. The plot follows Yuki as he embarks on a training course and eventually works in Kamusari Village, where he learns to appreciate nature and the local community [Doc 66094].", "latency_ms": 2270.8938121795654, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Wood Job!' matching the gold answer. The extra details about the film (year, genre, plot elements) are consistent with known information about the movie and do not contradict the question or the gold answer, so the response is both correct and faithful.", "main_total_tokens": 2962, "main_prompt_tokens": 2875, "main_completion_tokens": 87, "judge_total_tokens": 577, "judge_prompt_tokens": 487, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure, is **\"Sesame Street: The Magical Wand Chase\"** released in 2017 [Doc 78373].", "latency_ms": 1760.0479125976562, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as \"Sesame Street: The Magical Wand Chase,\" which matches the gold answer. This makes the answer factually correct. However, it introduces an additional detail about being released in 2017 and cites a specific document ID [Doc 78373], which may not be grounded in the provided context, slightly reducing faithfulness.", "main_total_tokens": 2400, "main_prompt_tokens": 2343, "main_completion_tokens": 57, "judge_total_tokens": 573, "judge_prompt_tokens": 467, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a 7-year-old boy must find the hero within him to save his parents trapped inside a burning car due to a rock fall is titled **Beyond**, released in **2016** [Doc 74597].", "latency_ms": 1377.051591873169, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Beyond,\" matching the gold answer exactly. It also stays faithful to the given information and does not introduce contradictions or unsupported claims beyond specifying the title and year, which aligns with the prompt context.", "main_total_tokens": 2001, "main_prompt_tokens": 1954, "main_completion_tokens": 47, "judge_total_tokens": 533, "judge_prompt_tokens": 455, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman is fired from a financial corporation during the Asia crisis and returns home with no money is titled **6ixtynin9**. In this film, after losing her job, she finds a box with a fortune in front of her door and decides to keep it, leading to complications when the original owners want it back [Doc 22478].", "latency_ms": 1635.7758045196533, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', which matches the gold answer, so correctness is 1. The additional plot detail about finding a box of money and complications is consistent with the actual film, but it is not supported or referenced by the given gold snippet itself, so faithfulness to the provided information is slightly less than perfect.", "main_total_tokens": 2152, "main_prompt_tokens": 2076, "main_completion_tokens": 76, "judge_total_tokens": 585, "judge_prompt_tokens": 478, "judge_completion_tokens": 107}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie you are referring to is **Club Dread** (2004). In this film, a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort, a hedonistic island paradise for swingers, and it's up to the club's staff to stop the violence or at least hide it [Doc 32410].", "latency_ms": 1775.7601737976074, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread' and restates the plot description from the prompt without adding contradictions. It is both correct and faithful to the given information.", "main_total_tokens": 2458, "main_prompt_tokens": 2389, "main_completion_tokens": 69, "judge_total_tokens": 561, "judge_prompt_tokens": 493, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie you are referring to is titled \"Toilet,\" which was released in 2010. The plot involves a 30-something Gundam fan and engineer who is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral [Doc 48873].", "latency_ms": 2387.9785537719727, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" matching the gold answer exactly. It does not introduce any contradictions with the provided information and remains faithful to the source document reference given.", "main_total_tokens": 2654, "main_prompt_tokens": 2588, "main_completion_tokens": 66, "judge_total_tokens": 549, "judge_prompt_tokens": 480, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie you are referring to is \"Electric Angel,\" released in 1981. It is described as a typical experimental film that employs a variety of audiovisual techniques to evoke a sense of polymorphic eroticism, drawing from European and Mediterranean cinematography of the 20th century. The film combines animation and live action, featuring a loose structure that enhances its open-ended character, with rhythm being a key element [Doc 1024].", "latency_ms": 2739.1180992126465, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the given description. The gold answer is 'Electric Angel.' The generated answer correctly identifies the movie as 'Electric Angel' and stays consistent with the given description, without contradicting the provided information. Therefore, it is both correct and faithful to the source.", "main_total_tokens": 2474, "main_prompt_tokens": 2387, "main_completion_tokens": 87, "judge_total_tokens": 591, "judge_prompt_tokens": 503, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is \"Bottom of the Water,\" which has a rating of 4.2 [Doc 104557].", "latency_ms": 821.7620849609375, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the lowest rated 2023 movie per the gold label is 'Mega Ape,' not 'Bottom of the Water.' It is also unfaithful to the provided gold answer, as it introduces a different movie and rating that do not match the gold reference.", "main_total_tokens": 2010, "main_prompt_tokens": 1980, "main_completion_tokens": 30, "judge_total_tokens": 499, "judge_prompt_tokens": 411, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 275.60925483703613, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Tea House', but the generated response says 'I don't know', which is incorrect with respect to the gold. However, the model does not fabricate an incorrect specific answer; it honestly admits not knowing, so it is fully faithful to its own (lack of) knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 476, "judge_prompt_tokens": 386, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 664.2041206359863, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which is factually incorrect because the gold answer specifies a particular movie. However, the model does not fabricate an alternative answer and simply admits lack of knowledge, so it remains fully faithful to its own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 477, "judge_prompt_tokens": 395, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is \"Read Lips,\" which has a rating of 4.5. The language of \"Read Lips\" is Chinese (cn) [Doc 354].", "latency_ms": 1722.9194641113281, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the language is English but the generated answer claims it is Chinese, so it is factually incorrect relative to the gold. Additionally, the generated answer introduces a specific movie title and language that contradict the gold, indicating it is not faithful to the reference answer or intended task.", "main_total_tokens": 2075, "main_prompt_tokens": 2035, "main_completion_tokens": 40, "judge_total_tokens": 512, "judge_prompt_tokens": 423, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was \"The Amanda Show,\" which had a popularity rating of 3.4197 and a rating of 7.1 [Doc 115219].", "latency_ms": 1315.138578414917, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the gold label indicates that the TV show with the highest popularity in 1999 was 'Law & Order: Special Victims Unit,' while the model instead answers 'The Amanda Show.' However, the model grounds its answer in a specific cited document and does not hallucinate beyond that source, so it is faithful to its referenced document but factually wrong with respect to the gold answer.", "main_total_tokens": 2606, "main_prompt_tokens": 2564, "main_completion_tokens": 42, "judge_total_tokens": 546, "judge_prompt_tokens": 432, "judge_completion_tokens": 114}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 872.3921775817871, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 and attributes it to a document, indicating it used the provided context. It is both correct and faithful.", "main_total_tokens": 2149, "main_prompt_tokens": 2131, "main_completion_tokens": 18, "judge_total_tokens": 465, "judge_prompt_tokens": 400, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in three different years:\n\n1. In 1999, featuring Ben and Katie Jordan, a married couple facing challenges in their relationship [Doc 22481].\n2. In 2019, which tells the story of a small-town bookstore owner, Jamie Vaughn, trying to save her business [Doc 88811].\n3. In 2025, focusing on a couple named Angela and Antonio as they navigate the complexities of their relationship [Doc 112077]. \n\nIf you are looking for a specific version, please clarify which one you mean.", "latency_ms": 1691.0920143127441, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question refers to a specific movie with a known correct answer year of 2019, but the generated answer lists three different years (1999, 2019, 2025) and does not provide a single clear answer. This makes it incorrect relative to the gold label. It is also unfaithful because it invents a 2025 film and multiple plot descriptions and document references that are not grounded in the given information, rather than directly answering with the gold year.", "main_total_tokens": 2248, "main_prompt_tokens": 2127, "main_completion_tokens": 121, "judge_total_tokens": 633, "judge_prompt_tokens": 505, "judge_completion_tokens": 128}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 169.17753219604492, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct number of votes is 279, but the generated answer was 3. This is factually incorrect, so correctness is 0. The answer does at least attempt to directly answer the question without adding unrelated content, so faithfulness is marked as 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 468, "judge_prompt_tokens": 383, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "12", "latency_ms": 178.60651016235352, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (12), so it is both correct and faithful to the provided reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 437, "judge_prompt_tokens": 383, "judge_completion_tokens": 54}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in the year 2009 [Doc 46069].", "latency_ms": 872.27463722229, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie was released in 2009, matching the gold answer. It is fully consistent and grounded in the provided information.", "main_total_tokens": 2246, "main_prompt_tokens": 2221, "main_completion_tokens": 25, "judge_total_tokens": 474, "judge_prompt_tokens": 412, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 872.1053600311279, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the original language of 'The King's Speech' is English, matching the gold answer 'en'. It is fully faithful to the provided information and does not introduce unsupported claims.", "main_total_tokens": 2448, "main_prompt_tokens": 2429, "main_completion_tokens": 19, "judge_total_tokens": 473, "judge_prompt_tokens": 402, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "1987", "latency_ms": 868.3762550354004, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct release year for the film 'Wall Street'.", "main_total_tokens": 2806, "main_prompt_tokens": 2803, "main_completion_tokens": 3, "judge_total_tokens": 435, "judge_prompt_tokens": 381, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Perfect Strangers", "latency_ms": 1589.2295837402344, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer 'Perfect Strangers' is incorrect because the movie in which classmates must play a survival game using their cell phones is 'Brain Fluid Explosion Girl.' However, the answer is fully faithful to the provided generated output; it evaluates that exact response without inventing or altering it.", "main_total_tokens": 2532, "main_prompt_tokens": 2528, "main_completion_tokens": 4, "judge_total_tokens": 479, "judge_prompt_tokens": 391, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 1628.8394927978516, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Dungeons & Dragons') and correctly identifies the TV show described in the question, so it is both correct and fully faithful.", "main_total_tokens": 2625, "main_prompt_tokens": 2620, "main_completion_tokens": 5, "judge_total_tokens": 459, "judge_prompt_tokens": 394, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 1190.093755722046, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Nacht der Wölfe', which is the correct movie for the described plot about rival German and Turkish teen gangs in Munich.", "main_total_tokens": 2756, "main_prompt_tokens": 2749, "main_completion_tokens": 7, "judge_total_tokens": 469, "judge_prompt_tokens": 403, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900", "latency_ms": 965.8768177032471, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (1900) and correctly identifies the year in which the events of Picnic at Hanging Rock take place. It is fully faithful to the provided gold answer.", "main_total_tokens": 2586, "main_prompt_tokens": 2583, "main_completion_tokens": 3, "judge_total_tokens": 458, "judge_prompt_tokens": 388, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June", "latency_ms": 518.7101364135742, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold standard ('June') and correctly identifies the movie based on the description, with no added or conflicting information.", "main_total_tokens": 138, "main_prompt_tokens": 137, "main_completion_tokens": 1, "judge_total_tokens": 492, "judge_prompt_tokens": 433, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 201.88283920288086, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which is incorrect because there is a known correct answer provided in the gold. However, it does not fabricate any information and accurately reflects lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 399, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 470.2939987182617, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 7.6, but the generated response said 'I don't know', which is incorrect. However, the model did not fabricate any incorrect specific value and instead admitted uncertainty, so it is fully faithful to its own stated knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 470, "judge_prompt_tokens": 389, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "9", "latency_ms": 1094.3291187286377, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.5143371794949736, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.013881288441812, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (9) does not match the gold answer (4), so it is incorrect. However, the model clearly provided a numeric count as requested and did not fabricate reasoning steps or extra claims beyond that single number, so it is faithful to its own (though incorrect) computation or retrieval.", "main_total_tokens": 1459, "main_prompt_tokens": 1442, "main_completion_tokens": 17, "judge_total_tokens": 485, "judge_prompt_tokens": 394, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 438.9610290527344, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there are 2 such TV shows, while the generated answer claims there are none. This is factually incorrect. The model also ignores the provided gold answer and instead asserts an inability to find matching titles, so it is not faithful to the given ground truth.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 404, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 300.74548721313477, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific TV show and the gold answer is Fantômas, but the generated answer says 'I don't know,' which is incorrect. However, the model does not hallucinate or contradict given information; it simply admits not knowing, so it is fully faithful to its own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 389, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise", "latency_ms": 1167.1233177185059, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Kill Cruise') and accurately responds to the question, so it is both correct and faithful.", "main_total_tokens": 2809, "main_prompt_tokens": 2806, "main_completion_tokens": 3, "judge_total_tokens": 445, "judge_prompt_tokens": 387, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 1411.1695289611816, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct title of the documentary film about Fred Rogers and his TV series.", "main_total_tokens": 2881, "main_prompt_tokens": 2873, "main_completion_tokens": 8, "judge_total_tokens": 465, "judge_prompt_tokens": 408, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018", "latency_ms": 1435.2574348449707, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (2018), so it is both correct and faithful to the reference.", "main_total_tokens": 2782, "main_prompt_tokens": 2766, "main_completion_tokens": 16, "judge_total_tokens": 434, "judge_prompt_tokens": 380, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy", "latency_ms": 1051.1977672576904, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Astro Boy', which correctly identifies the animated TV series about a young boy robot in 2030 AD. The answer is both correct and fully faithful to the reference.", "main_total_tokens": 2899, "main_prompt_tokens": 2895, "main_completion_tokens": 4, "judge_total_tokens": 463, "judge_prompt_tokens": 389, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 years [Doc 67513].", "latency_ms": 1003.899335861206, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9463946303571863, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer ('7 years') is factually correct and matches the gold answer. However, it introduces an unverified citation marker ('[Doc 67513]') that is not grounded in the provided information, so the response is not fully faithful to the given context.", "main_total_tokens": 2473, "main_prompt_tokens": 2464, "main_completion_tokens": 9, "judge_total_tokens": 489, "judge_prompt_tokens": 403, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m", "latency_ms": 570.8985328674316, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The Brink's-Mat security depot was holding gold worth about £26 million before the 26 November 1983 robbery, so the generated answer matches the gold answer and is both correct and faithful.", "main_total_tokens": 2114, "main_prompt_tokens": 2111, "main_completion_tokens": 3, "judge_total_tokens": 469, "judge_prompt_tokens": 398, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 528.6061763763428, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score. The gold answer provides that value. The generated answer says 'I don't know,' which does not provide the correct score, so correctness is 0. However, it does not invent any information and honestly expresses uncertainty, so it is fully faithful to its own (lack of) knowledge, giving faithfulness 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 500, "judge_prompt_tokens": 396, "judge_completion_tokens": 104}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 285.16364097595215, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there was 1 Russian language TV show released in 1997, but the generated answer says it cannot find any such shows, which is factually incorrect. However, the model does not invent or contradict specific source content; it simply expresses inability to find information, so it is fully faithful to its (claimed) retrieval limitations.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 495, "judge_prompt_tokens": 394, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes", "latency_ms": 840.8160209655762, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Angel Eyes' exactly matches the gold answer and correctly identifies the TV show in which Soo-wan, who had been blind, regains her sight after an eye transplant surgery. The response is both correct and fully faithful to the provided gold answer.", "main_total_tokens": 2671, "main_prompt_tokens": 2668, "main_completion_tokens": 3, "judge_total_tokens": 481, "judge_prompt_tokens": 397, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "en", "latency_ms": 1906.9325923919678, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'en' is incorrect because the gold language is 'Korean'. There is no basis in the given information to justify 'en', so the response is both factually wrong and unfaithful to the gold label.", "main_total_tokens": 2909, "main_prompt_tokens": 2899, "main_completion_tokens": 10, "judge_total_tokens": 455, "judge_prompt_tokens": 377, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 531.9051742553711, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the model claimed there were no matching movies, so the answer is factually incorrect. However, the model did not hallucinate specific movies or data; it simply stated an inability to find any matching entries, which is faithful to its stated limitation.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 403, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 454.4808864593506, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.0, "faithfulness_score": 1.0, "justification": "Relative to the gold answer, the generated answer is incorrect because the system *does* have a position (that there are no movies from 1979 in its database), whereas the generated answer says only 'I don’t know.' This contradicts the gold response, so correctness is 0. However, the generated answer does not introduce any new unsupported facts and simply expresses uncertainty. It remains faithful to the (lack of) information, so faithfulness is 1.", "main_total_tokens": 564, "main_prompt_tokens": 563, "main_completion_tokens": 1, "judge_total_tokens": 528, "judge_prompt_tokens": 399, "judge_completion_tokens": 129}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The movie Back to the Future Part IV was not released in any year, as there is no record of such a film existing in the provided context. The last installment of the original trilogy is Back to the Future Part III, released in 1990 [Doc 10621].", "latency_ms": 1643.9971923828125, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Back to the Future Part IV was never released and references Part III in 1990, aligning with the gold answer that Part IV does not exist in the database. It stays within the provided context and does not introduce unsupported claims, so both correctness and faithfulness are high.", "main_total_tokens": 3091, "main_prompt_tokens": 3034, "main_completion_tokens": 57, "judge_total_tokens": 548, "judge_prompt_tokens": 456, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says there were no movies from 1980 related to the Chernobyl disaster. The generated answer similarly states it cannot find any movies or shows matching the criteria and hints at a temporal mismatch, which is factually correct since the Chernobyl disaster occurred in 1986. It does not hallucinate a movie or a score and stays aligned with the information in the gold, so both correctness and faithfulness are high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 553, "judge_prompt_tokens": 434, "judge_completion_tokens": 119}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 264.6675109863281, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is factually incorrect because there are four TV shows or movies with 'Astro Boy' in the title, while it claims there are none. It is unfaithful to the gold answer as it directly contradicts the known correct count and does not reflect the reference information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 397, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "5", "latency_ms": 1219.58589553833, "retrieval_recall": 0.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.17723928678404774, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered '5' while the gold answer is '2', so the content is incorrect. There is no sign of the model contradicting its own reasoning or the prompt, so it is faithful but wrong.", "main_total_tokens": 3371, "main_prompt_tokens": 3362, "main_completion_tokens": 9, "judge_total_tokens": 460, "judge_prompt_tokens": 386, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 184.722900390625, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show, but the generated answer says there are none, which is factually incorrect. However, the model does not hallucinate a specific incorrect number or show; it honestly states it cannot find any, so it remains faithful to its own (albeit limited) retrieval.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 398, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 200.33502578735352, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "Gold answer says 0 shows. Generated answer: 'I cannot find any movies or shows matching those criteria.' This implies the model did not identify any such shows, effectively aligning with 0, so the final numeric fact matches the gold, hence correctness 1. However, wording suggests uncertainty and refers to both movies and shows and to inability to find rather than clearly stating the answer, so the faithfulness to the gold answer is slightly weaker, not an exact explicit 0, so 0.8.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 535, "judge_prompt_tokens": 400, "judge_completion_tokens": 135}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 184.73362922668457, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer given the gold label is 'Naruto'. The generated answer says 'I don't know,' which does not provide the correct information, so correctness is 0. However, the model is honest about its uncertainty and does not fabricate an answer, so faithfulness to its stated uncertainty is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 474, "judge_prompt_tokens": 386, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "There is no record of a Jurassic Park/Power Ranger crossover movie in the provided context.", "latency_ms": 1838.3073806762695, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie was not released. The generated answer correctly states that there is no record of such a crossover movie, which is equivalent in meaning. It is both factually correct relative to the gold and faithful to the provided information.", "main_total_tokens": 2752, "main_prompt_tokens": 2733, "main_completion_tokens": 19, "judge_total_tokens": 484, "judge_prompt_tokens": 403, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 470.1387882232666, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is that such a movie was never released. The generated answer 'I don't know.' is not correct, so correctness is 0. The model also fails to provide the known gold answer and instead pleads ignorance, so faithfulness to the provided gold reference is 0.", "main_total_tokens": 561, "main_prompt_tokens": 560, "main_completion_tokens": 1, "judge_total_tokens": 475, "judge_prompt_tokens": 386, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The 4th oldest Star Wars movie is \"Return of the Jedi\" from 1983 [Doc 2858].", "latency_ms": 1288.8448238372803, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer incorrectly identifies 'Return of the Jedi' (1983) as the 4th oldest Star Wars movie, while the gold answer is 'Star Wars: Droids - The Great Heep'. Thus, it is factually incorrect with respect to the gold. However, the answer appears to be derived from the cited document [Doc 2858], so it is faithful to its stated source even though that source-based reasoning leads to a wrong final answer versus the gold.", "main_total_tokens": 2980, "main_prompt_tokens": 2954, "main_completion_tokens": 26, "judge_total_tokens": 542, "judge_prompt_tokens": 414, "judge_completion_tokens": 128}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "1983", "latency_ms": 1304.8362731933594, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer (1983) does not match the gold answer (1986), so it is factually incorrect. There is no alignment with the correct information, so it is not faithful to the gold reference.", "main_total_tokens": 3030, "main_prompt_tokens": 3016, "main_completion_tokens": 14, "judge_total_tokens": 454, "judge_prompt_tokens": 380, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 199.03326034545898, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct rating is 8.466, but the generated answer says it does not know the rating. This makes the answer incorrect. However, the model does not fabricate any information and honestly expresses a lack of knowledge, so it is fully faithful to its own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 484, "judge_prompt_tokens": 398, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med", "latency_ms": 1242.0291900634766, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which correctly identifies the TV show as 'Mighty Med'. It is both correct and faithful.", "main_total_tokens": 2575, "main_prompt_tokens": 2571, "main_completion_tokens": 4, "judge_total_tokens": 453, "judge_prompt_tokens": 394, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina", "latency_ms": 1283.212423324585, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (Argentina), so it is both correct and fully faithful to the provided reference.", "main_total_tokens": 3041, "main_prompt_tokens": 3039, "main_completion_tokens": 2, "judge_total_tokens": 447, "judge_prompt_tokens": 392, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "I don't know.", "latency_ms": 771.7654705047607, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Jiang Jie'. The generated answer is 'I don't know', which does not match the correct answer, so it is incorrect. However, it does not introduce any incorrect specific claim; it simply expresses a lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 536, "main_prompt_tokens": 535, "main_completion_tokens": 1, "judge_total_tokens": 480, "judge_prompt_tokens": 388, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "5", "latency_ms": 786.4129543304443, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.19519002499605084, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.2, "ndcg_at_10": 0.3080351663449861, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (5) does not match the gold answer (4), so it is incorrect. The explanation is consistent with the provided gold and generated values, so faithfulness is high, but correctness is zero because the numeric answer is wrong.", "main_total_tokens": 3489, "main_prompt_tokens": 3480, "main_completion_tokens": 9, "judge_total_tokens": 465, "judge_prompt_tokens": 385, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 586.2040519714355, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly conveys that no movies (or shows) are found featuring both Bruce Lee and Steven Seagal together, which aligns with the gold answer. It is fully faithful to the intent of the gold response and does not introduce any contradictory or unfounded information.", "main_total_tokens": 2209, "main_prompt_tokens": 2208, "main_completion_tokens": 1, "judge_total_tokens": 490, "judge_prompt_tokens": 406, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3", "latency_ms": 803.1516075134277, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3, so it is both correct and faithful to the provided reference.", "main_total_tokens": 2293, "main_prompt_tokens": 2290, "main_completion_tokens": 3, "judge_total_tokens": 444, "judge_prompt_tokens": 387, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "Drift-style racing [Doc 42750].", "latency_ms": 1942.0063495635986, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The gold answer is simply 'Drift', and the generated answer 'Drift-style racing [Doc 42750]' is semantically correct because it identifies the style as drift racing. However, the mention of '[Doc 42750]' appears to be an unsupported citation artifact not grounded in the provided context, so while the core answer is correct, the response is not fully faithful to the source, reducing the faithfulness score.", "main_total_tokens": 2998, "main_prompt_tokens": 2975, "main_completion_tokens": 23, "judge_total_tokens": 504, "judge_prompt_tokens": 387, "judge_completion_tokens": 117}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer", "latency_ms": 1674.9565601348877, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the title of the movie as 'Confronting a Serial Killer', so it is both correct and faithful.", "main_total_tokens": 2855, "main_prompt_tokens": 2848, "main_completion_tokens": 7, "judge_total_tokens": 476, "judge_prompt_tokens": 412, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838", "latency_ms": 1405.073881149292, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838), so it is both correct and faithful to the provided reference.", "main_total_tokens": 2309, "main_prompt_tokens": 2306, "main_completion_tokens": 3, "judge_total_tokens": 446, "judge_prompt_tokens": 389, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is The Sixth Sense. [Doc 22098]", "latency_ms": 1385.4787349700928, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6895405204413555, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Sixth Sense' and matches the gold answer exactly. It is fully supported by the provided information and does not introduce any unsupported claims.", "main_total_tokens": 2524, "main_prompt_tokens": 2495, "main_completion_tokens": 29, "judge_total_tokens": 486, "judge_prompt_tokens": 419, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002", "latency_ms": 941.0345554351807, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (2002), so it is both correct and faithful to the reference.", "main_total_tokens": 2824, "main_prompt_tokens": 2821, "main_completion_tokens": 3, "judge_total_tokens": 437, "judge_prompt_tokens": 383, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled", "latency_ms": 919.1489219665527, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly states the title of the 2015 reboot of 'The Transporter' as 'The Transporter Refueled'.", "main_total_tokens": 2851, "main_prompt_tokens": 2844, "main_completion_tokens": 7, "judge_total_tokens": 463, "judge_prompt_tokens": 397, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015", "latency_ms": 856.5411567687988, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (2015), so it is both correct and faithful.", "main_total_tokens": 2867, "main_prompt_tokens": 2864, "main_completion_tokens": 3, "judge_total_tokens": 438, "judge_prompt_tokens": 387, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "There is no romantic comedy in the provided context that features Abraham Lincoln coming back from the dead.", "latency_ms": 2161.3495349884033, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no such movie description can be found. The generated answer correctly mirrors this by saying there is no romantic comedy in the provided context that features Abraham Lincoln coming back from the dead. It does not introduce unsupported details and stays aligned with the given information, so it is both correct and faithful.", "main_total_tokens": 2634, "main_prompt_tokens": 2614, "main_completion_tokens": 20, "judge_total_tokens": 502, "judge_prompt_tokens": 409, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Havana Darkness", "latency_ms": 2239.8734092712402, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Havana Darkness' and correctly identifies the movie title that alludes to a manuscript written by Ernest Hemingway. It is both correct and fully faithful to the gold response.", "main_total_tokens": 2581, "main_prompt_tokens": 2577, "main_completion_tokens": 4, "judge_total_tokens": 463, "judge_prompt_tokens": 390, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 1653.5754203796387, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is correct for the question. It is both correct and faithful.", "main_total_tokens": 2830, "main_prompt_tokens": 2796, "main_completion_tokens": 34, "judge_total_tokens": 446, "judge_prompt_tokens": 392, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in the movie \"Hallam Foe\" [Doc 39389].", "latency_ms": 1976.3336181640625, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2007 movie as \"Hallam Foe,\" which matches the gold answer. It stays faithful to the question and does not introduce incorrect or unsupported information, only restating the plot premise from the query with the correct title.", "main_total_tokens": 2707, "main_prompt_tokens": 2671, "main_completion_tokens": 36, "judge_total_tokens": 517, "judge_prompt_tokens": 434, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "His last name sounds at first like he’s a friend. [Doc 39389]", "latency_ms": 2656.2206745147705, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'A foe' but the generated answer says 'a friend', which is the opposite. This is factually incorrect relative to the gold. However, the evaluation is faithful to the provided generated text and gold; there is no hallucination about their contents.", "main_total_tokens": 3003, "main_prompt_tokens": 2966, "main_completion_tokens": 37, "judge_total_tokens": 490, "judge_prompt_tokens": 404, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18", "latency_ms": 988.44313621521, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (18), so it is both correct and faithful.", "main_total_tokens": 2545, "main_prompt_tokens": 2543, "main_completion_tokens": 2, "judge_total_tokens": 439, "judge_prompt_tokens": 389, "judge_completion_tokens": 50}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam is on the autism spectrum [Doc 124583].", "latency_ms": 865.9799098968506, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the TV show 'Atypical', Sam is on the autism spectrum, which matches the gold answer exactly and is fully supported by the given information.", "main_total_tokens": 2778, "main_prompt_tokens": 2765, "main_completion_tokens": 13, "judge_total_tokens": 467, "judge_prompt_tokens": 399, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "I don't know.", "latency_ms": 455.04164695739746, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is Chamonix, while the generated answer says it does not know, so the correctness is 0. The model does not invent any incorrect details and honestly admits not knowing, so its response is fully faithful to its own uncertainty, giving faithfulness a score of 1.", "main_total_tokens": 569, "main_prompt_tokens": 568, "main_completion_tokens": 1, "judge_total_tokens": 486, "judge_prompt_tokens": 401, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In this Japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 1035.1684093475342, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly fills in the blank with 'rains', matching the gold answer exactly. It is faithful to the provided information and does not introduce any unsupported details beyond referencing a document ID, which does not conflict with the gold standard.", "main_total_tokens": 2723, "main_prompt_tokens": 2701, "main_completion_tokens": 22, "judge_total_tokens": 491, "judge_prompt_tokens": 412, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland [Doc 114810]", "latency_ms": 1025.7480144500732, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the place Tabaluga is crown-prince of; the gold answer is Greenland. The generated answer is 'Greenland [Doc 114810]', which correctly includes 'Greenland' and matches the gold. The added bracketed reference does not change the factual content, so it is correct and faithful.", "main_total_tokens": 3002, "main_prompt_tokens": 2993, "main_completion_tokens": 9, "judge_total_tokens": 497, "judge_prompt_tokens": 401, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon", "latency_ms": 787.7357006072998, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('One Afternoon') and correctly identifies the movie based on the description, so it is both correct and faithful.", "main_total_tokens": 2749, "main_prompt_tokens": 2746, "main_completion_tokens": 3, "judge_total_tokens": 467, "judge_prompt_tokens": 406, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" [Doc 93260].", "latency_ms": 1412.21022605896, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Life Ahead,' matching the gold answer exactly. It remains faithful to the provided information without adding incorrect or speculative details.", "main_total_tokens": 3195, "main_prompt_tokens": 3157, "main_completion_tokens": 38, "judge_total_tokens": 499, "judge_prompt_tokens": 435, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary “McLaren” is about Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 1892.160177230835, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies that the 2016 documentary 'McLaren' is about Bruce McLaren, matching the gold answer. The extra detail that he founded the McLaren Motor Racing team is true but appears to cite an external document ([Doc 73859]) that is not provided in the prompt, so a small deduction in faithfulness.", "main_total_tokens": 2688, "main_prompt_tokens": 2658, "main_completion_tokens": 30, "judge_total_tokens": 515, "judge_prompt_tokens": 413, "judge_completion_tokens": 102}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren", "latency_ms": 1490.3309345245361, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0.7, "faithfulness_score": 1, "justification": "The generated answer 'McLaren' is a shortened form of the correct team name 'The McLaren Motor Racing team.' It identifies the correct entity but omits part of the official name, so it is partially correct. It is faithful to the gold answer in substance, but not exact in wording.", "main_total_tokens": 3069, "main_prompt_tokens": 3054, "main_completion_tokens": 15, "judge_total_tokens": 478, "judge_prompt_tokens": 385, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "I don't know.", "latency_ms": 934.7732067108154, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'New Zealand', but the generated answer was 'I don't know', which is factually incorrect. However, the model did not fabricate an incorrect country and instead admitted uncertainty, so it is fully faithful to its own stated uncertainty.", "main_total_tokens": 649, "main_prompt_tokens": 640, "main_completion_tokens": 9, "judge_total_tokens": 461, "judge_prompt_tokens": 380, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "I don't know.", "latency_ms": 354.63619232177734, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that there is no such movie in the database, effectively expressing lack of knowledge. The generated answer says 'I don't know,' which is semantically equivalent in this context. It does not introduce incorrect facts and stays within the bounds of the given information, so it is both correct and faithful to the gold response.", "main_total_tokens": 547, "main_prompt_tokens": 546, "main_completion_tokens": 1, "judge_total_tokens": 499, "judge_prompt_tokens": 402, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit", "latency_ms": 1157.2518348693848, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2002 Japanese horror movie as 'Tomie: Forbidden Fruit'. It is fully correct and faithful to the provided information.", "main_total_tokens": 2929, "main_prompt_tokens": 2923, "main_completion_tokens": 6, "judge_total_tokens": 490, "judge_prompt_tokens": 422, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65", "latency_ms": 887.9811763763428, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, indicating it is both correct and faithful to the reference.", "main_total_tokens": 2905, "main_prompt_tokens": 2903, "main_completion_tokens": 2, "judge_total_tokens": 447, "judge_prompt_tokens": 396, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King and Tala search for The Book of Souls [Doc 82582].", "latency_ms": 2108.232021331787, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Scorpion King and Tala search for The Book of Souls, which matches the gold answer exactly. It is fully correct and faithful to the given information.", "main_total_tokens": 3203, "main_prompt_tokens": 3184, "main_completion_tokens": 19, "judge_total_tokens": 480, "judge_prompt_tokens": 412, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 533.4279537200928, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database. The generated answer says 'I don't know,' which is effectively equivalent in this context. It does not introduce incorrect information and is consistent with the lack of knowledge, so it is both correct and faithful to the gold.", "main_total_tokens": 558, "main_prompt_tokens": 557, "main_completion_tokens": 1, "judge_total_tokens": 482, "judge_prompt_tokens": 395, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 417.3448085784912, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is not in the database, which is equivalent in content to the generated answer saying 'I don't know.' Both correctly express lack of knowledge and the generated answer does not introduce any unsupported information.", "main_total_tokens": 571, "main_prompt_tokens": 570, "main_completion_tokens": 1, "judge_total_tokens": 468, "judge_prompt_tokens": 393, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 564.5439624786377, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, effectively expressing a lack of knowledge. The generated answer, \"I don't know,\" is equivalent in meaning. It is correct with respect to the gold and does not invent any information, so it is fully faithful.", "main_total_tokens": 535, "main_prompt_tokens": 534, "main_completion_tokens": 1, "judge_total_tokens": 481, "judge_prompt_tokens": 395, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 539.3626689910889, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly indicate a lack of knowledge about the movie, and the generated answer does not introduce any unsupported details. It is fully consistent with the gold response.", "main_total_tokens": 552, "main_prompt_tokens": 551, "main_completion_tokens": 1, "judge_total_tokens": 462, "judge_prompt_tokens": 395, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"Python's Revenge 2\" was released is not specified in the provided context.", "latency_ms": 1326.7929553985596, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "Gold answer says the movie is not in the database. The generated answer says the year is not specified in the provided context. Both effectively convey that the release year is unknown, so correctness is fine. However, the gold explicitly refers to the database limitation, while the generated answer references only the provided context, slightly diverging from the gold rationale, so faithfulness is slightly reduced.", "main_total_tokens": 3003, "main_prompt_tokens": 2983, "main_completion_tokens": 20, "judge_total_tokens": 519, "judge_prompt_tokens": 410, "judge_completion_tokens": 109}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 483.6313724517822, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, meaning the correct response is to admit lack of knowledge. The generated answer, 'I don't know,' matches this and is both correct and faithful to the gold response.", "main_total_tokens": 504, "main_prompt_tokens": 503, "main_completion_tokens": 1, "judge_total_tokens": 470, "judge_prompt_tokens": 393, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 517.1537399291992, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, which is equivalent in practice to saying the system does not know. The generated answer 'I don't know.' conveys the same lack of knowledge and does not introduce any incorrect facts, so it is both correct relative to the gold and faithful to the prompt.", "main_total_tokens": 576, "main_prompt_tokens": 575, "main_completion_tokens": 1, "judge_total_tokens": 490, "judge_prompt_tokens": 396, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was directed by Adrian Sitaru [Doc 46069].", "latency_ms": 1404.653549194336, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says the database does not provide the director information and thus it cannot be answered. The generated answer confidently names Adrian Sitaru as the director, which contradicts the gold and invents unsupported information. Therefore it is both incorrect and unfaithful to the provided evidence.", "main_total_tokens": 3165, "main_prompt_tokens": 3140, "main_completion_tokens": 25, "judge_total_tokens": 509, "judge_prompt_tokens": 421, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context does not provide information about the director of the movie \"A Love to Die For.\"", "latency_ms": 1435.5645179748535, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not provide information about the director, aligning with the gold response that the director information is not available in the database. It does not introduce any unsupported claims and remains faithful to the given context.", "main_total_tokens": 2990, "main_prompt_tokens": 2970, "main_completion_tokens": 20, "judge_total_tokens": 492, "judge_prompt_tokens": 414, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not specified in the provided context.", "latency_ms": 988.6252880096436, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not specified in the provided context, which aligns with the gold answer that the database does not provide the director information. It is both correct and faithful to the given information.", "main_total_tokens": 2990, "main_prompt_tokens": 2971, "main_completion_tokens": 19, "judge_total_tokens": 485, "judge_prompt_tokens": 411, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of \"The Last Night at Tremore Beach\" is not specified in the provided context.", "latency_ms": 1537.4326705932617, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director is not specified or available in the provided information. This matches the limitation of the context/database, so the answer is correct and faithful to the given data.", "main_total_tokens": 2649, "main_prompt_tokens": 2628, "main_completion_tokens": 21, "judge_total_tokens": 489, "judge_prompt_tokens": 417, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 1351.975679397583, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, indicating it is correct and fully faithful to the reference.", "main_total_tokens": 2725, "main_prompt_tokens": 2719, "main_completion_tokens": 6, "judge_total_tokens": 447, "judge_prompt_tokens": 396, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 1289.1552448272705, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Stranger in My Bed') and correctly identifies the movie based on the description. It is fully correct and faithful.", "main_total_tokens": 2732, "main_prompt_tokens": 2726, "main_completion_tokens": 6, "judge_total_tokens": 463, "judge_prompt_tokens": 400, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 951.3161182403564, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question actually describes the premise of a TV series, not a movie, but both the gold and generated answers give the same title, 'Game of Silence,' which matches the described plot. Therefore the generated answer is correct relative to the gold and fully faithful to the given information.", "main_total_tokens": 2414, "main_prompt_tokens": 2410, "main_completion_tokens": 4, "judge_total_tokens": 487, "judge_prompt_tokens": 400, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 1252.1061897277832, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'The Craigslist Killer', so it is both correct and faithful.", "main_total_tokens": 2718, "main_prompt_tokens": 2699, "main_completion_tokens": 19, "judge_total_tokens": 453, "judge_prompt_tokens": 395, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 919.3844795227051, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Wood Job!') and correctly identifies the movie from the description. It is both correct and faithful.", "main_total_tokens": 3494, "main_prompt_tokens": 3490, "main_completion_tokens": 4, "judge_total_tokens": 463, "judge_prompt_tokens": 403, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 1806.6179752349854, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase.'", "main_total_tokens": 3027, "main_prompt_tokens": 3010, "main_completion_tokens": 17, "judge_total_tokens": 472, "judge_prompt_tokens": 417, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond", "latency_ms": 825.1893520355225, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Beyond'), and the movie title is correct for the described plot. The response is both correct and fully faithful to the reference.", "main_total_tokens": 2542, "main_prompt_tokens": 2540, "main_completion_tokens": 2, "judge_total_tokens": 474, "judge_prompt_tokens": 409, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9", "latency_ms": 1034.538984298706, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('6ixtynin9'), correctly identifying the movie and faithfully reflecting the reference.", "main_total_tokens": 2670, "main_prompt_tokens": 2663, "main_completion_tokens": 7, "judge_total_tokens": 466, "judge_prompt_tokens": 409, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread", "latency_ms": 903.8004875183105, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Club Dread') and correctly identifies the movie described, demonstrating both correctness and faithfulness.", "main_total_tokens": 3030, "main_prompt_tokens": 3026, "main_completion_tokens": 4, "judge_total_tokens": 485, "judge_prompt_tokens": 427, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet", "latency_ms": 818.5250759124756, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Toilet'), which is the correct movie title. It is therefore both correct and faithful.", "main_total_tokens": 3206, "main_prompt_tokens": 3203, "main_completion_tokens": 3, "judge_total_tokens": 473, "judge_prompt_tokens": 415, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel", "latency_ms": 902.7557373046875, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Electric Angel'). The evaluation description is accurately reflected by the scores given.", "main_total_tokens": 2970, "main_prompt_tokens": 2967, "main_completion_tokens": 3, "judge_total_tokens": 472, "judge_prompt_tokens": 418, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "4.2", "latency_ms": 803.7598133087158, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the title of the lowest rated movie from 2023, and the gold answer is the movie name 'Mega Ape'. The generated answer is '4.2', which is a rating number, not a movie title, so it is factually incorrect relative to the gold. It also does not follow from or align with the gold answer, so it is unfaithful.", "main_total_tokens": 2501, "main_prompt_tokens": 2497, "main_completion_tokens": 4, "judge_total_tokens": 495, "judge_prompt_tokens": 385, "judge_completion_tokens": 110}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 265.3026580810547, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Tea House', but the generated answer says 'I don't know', which is incorrect. However, it does not invent any conflicting facts; it simply expresses a lack of knowledge, so it is fully faithful to the (missing) reasoning even though it is wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 474, "judge_prompt_tokens": 386, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 302.8230667114258, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a concrete movie title, while the generated answer claims not to know. This is incorrect relative to the gold. However, saying 'I don't know' does not invent any incorrect facts, so it is fully faithful to the model's own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 395, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The language of the lowest-rated movie from 1980 is cn [Doc 354].", "latency_ms": 1423.9461421966553, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect: the gold answer says the language is English, while the generated answer claims it is 'cn'. It also fabricates a document reference [Doc 354] supporting 'cn', which contradicts the provided gold answer, so it is not faithful to the evidence.", "main_total_tokens": 2580, "main_prompt_tokens": 2561, "main_completion_tokens": 19, "judge_total_tokens": 490, "judge_prompt_tokens": 401, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was \"The Amanda Show\" with a popularity score of 3.4197 [Doc 115219].", "latency_ms": 1274.3234634399414, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is factually incorrect relative to the gold label. The correct show is 'Law & Order: Special Victims Unit', but the model outputs 'The Amanda Show.' It also fabricates a specific popularity score and citation, which are not grounded in the provided gold answer, so it is unfaithful.", "main_total_tokens": 3169, "main_prompt_tokens": 3135, "main_completion_tokens": 34, "judge_total_tokens": 518, "judge_prompt_tokens": 423, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2", "latency_ms": 1083.0957889556885, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (7.2). It is correct and faithful.", "main_total_tokens": 2147, "main_prompt_tokens": 2144, "main_completion_tokens": 3, "judge_total_tokens": 436, "judge_prompt_tokens": 386, "judge_completion_tokens": 50}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "1999", "latency_ms": 833.8048458099365, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1999) does not match the gold answer (2019), so it is factually incorrect. However, the model did provide a specific year as requested and did not hallucinate unrelated information, so it is faithful to the task format and question intent.", "main_total_tokens": 2687, "main_prompt_tokens": 2684, "main_completion_tokens": 3, "judge_total_tokens": 473, "judge_prompt_tokens": 387, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 517.5621509552002, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer (3) does not match the gold answer (279), so it is incorrect. There is no indication it was derived from or grounded in the provided gold information, so it is also unfaithful.", "main_total_tokens": 2183, "main_prompt_tokens": 2178, "main_completion_tokens": 5, "judge_total_tokens": 458, "judge_prompt_tokens": 383, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 649.5277881622314, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question has a clear numeric answer: 12 votes. The generated answer incorrectly states it cannot find any matching movies or shows, which is factually wrong given the gold answer. It is neither correct nor faithful to the reference.", "main_total_tokens": 2217, "main_prompt_tokens": 2216, "main_completion_tokens": 1, "judge_total_tokens": 469, "judge_prompt_tokens": 392, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "2009", "latency_ms": 786.9260311126709, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (2009), correctly identifying the movie's release year and remaining faithful to the question.", "main_total_tokens": 2804, "main_prompt_tokens": 2801, "main_completion_tokens": 3, "judge_total_tokens": 447, "judge_prompt_tokens": 390, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o-mini", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en", "latency_ms": 956.9647312164307, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The original language of the movie \"The King's Speech\" is English (en). The generated answer matches the gold answer and is correct and faithful.", "main_total_tokens": 3003, "main_prompt_tokens": 3001, "main_completion_tokens": 2, "judge_total_tokens": 444, "judge_prompt_tokens": 384, "judge_completion_tokens": 60}
