{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "None of the documents provided contain information about a title \"Wall Street.\" Therefore, based on the given context, the release year for \"Wall Street\" cannot be determined.", "latency_ms": 7519.995927810669, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 1987, but the generated answer claims the year cannot be determined, which is factually incorrect, so correctness is 0. However, given the stated constraint of using only the provided documents and that they indeed contain no information about the film, the answer is faithful to the context, so faithfulness is 1.", "main_total_tokens": 3254, "main_prompt_tokens": 3219, "main_completion_tokens": 35, "judge_total_tokens": 512, "judge_prompt_tokens": 412, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The context does not provide information about a specific movie where classmates have to play a survival game exclusively using their cell phones. However, there are several movies that involve students in some form of dangerous or mysterious game, often involving technology like mobile phones:\n\n1. \"Tokyo Gore School\" [Doc 45096] describes a violent game spreading via the students' mobile phones where victims become senseless killing machines.\n2. \"Naraka 19\" [Doc 41254] involves a terrifying mobile phone game from hell that traps the main character, Rain.\n3. \"Future Diary Another World\" [Doc 120683] revolves around the \"Future Diary,\" a cell phone diary containing entries about future events, which leads to a survival game among seven people.\n\nNone of these exactly match the description of classmates playing a survival game using their cell phones, but they are close examples involving students and mobile phones in dangerous or mysterious games.", "latency_ms": 6105.856418609619, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The question asks for a specific movie, and the gold answer is 'Brain Fluid Explosion Girl'. The generated answer claims the context does not provide such a movie and instead lists other films; it never identifies 'Brain Fluid Explosion Girl' or gives a correct direct answer, so correctness is 0. In terms of faithfulness, the answer appears to be grounded in the provided documents (Tokyo Gore School, Naraka 19, Future Diary Another World) and does not invent arbitrary titles beyond those; however, it asserts that none match the description when in fact the gold indicates that one does, suggesting some mismatch or omission relative to the underlying data. Still, it mostly stays within referenced material, so faithfulness is relatively high but not perfect.", "main_total_tokens": 3090, "main_prompt_tokens": 2890, "main_completion_tokens": 200, "judge_total_tokens": 755, "judge_prompt_tokens": 574, "judge_completion_tokens": 181}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show where six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" [Doc 112849].", "latency_ms": 2923.929214477539, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' which matches the gold answer exactly. It remains faithful to the provided information and does not introduce any contradictory or extraneous claims beyond a generic citation marker.", "main_total_tokens": 2725, "main_prompt_tokens": 2691, "main_completion_tokens": 34, "judge_total_tokens": 495, "judge_prompt_tokens": 419, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Based on the provided documents, there is no movie that specifically describes the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones. The closest context provided is about individual movies with different themes and settings but none matching the described scenario exactly.", "latency_ms": 3661.9327068328857, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Nacht der Wölfe', but the generated answer claims no such movie exists, so it is factually incorrect. However, given that the model was instructed to base its answer only on the provided documents and it explicitly states that within those documents no such movie is mentioned, it remains faithful to its stated evidence and limitations.", "main_total_tokens": 3283, "main_prompt_tokens": 3227, "main_completion_tokens": 56, "judge_total_tokens": 554, "judge_prompt_tokens": 453, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of \"Picnic at Hanging Rock\" take place in the year 1900 [Doc 126191].", "latency_ms": 2911.0612869262695, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of 'Picnic at Hanging Rock' take place in 1900, which matches the gold answer. It is fully consistent and faithful to the provided information.", "main_total_tokens": 2952, "main_prompt_tokens": 2921, "main_completion_tokens": 31, "judge_total_tokens": 480, "judge_prompt_tokens": 409, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The provided context does not contain information regarding a 2004 movie that matches the description given. There is no mention of a young violinist working in a Venice coffee shop, nor any character whose name is that of a calendar month. The closest reference to Venice and a violinist is from \"The Bohemian,\" but it does not fit the specified criteria or year.", "latency_ms": 4053.2610416412354, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question is asking for the movie title, which is 'June', but the generated answer claims the context does not contain the information and does not provide the correct title. This makes the answer incorrect. However, it remains faithful to the (stated) context by not hallucinating a movie and explicitly saying it cannot be found there, so the faithfulness is high.", "main_total_tokens": 3297, "main_prompt_tokens": 3219, "main_completion_tokens": 78, "judge_total_tokens": 611, "judge_prompt_tokens": 506, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "The highest-rated Russian-language movie released in 1980 based on the provided documents is \"Moscow Does Not Believe in Tears\" with a rating of 7.6 [Doc 61].", "latency_ms": 3137.8166675567627, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger,' but the generated answer states 'Moscow Does Not Believe in Tears.' This is factually incorrect relative to the gold. It is also unfaithful to the provided supervision signal (the gold answer) because it uses a different movie title, so both correctness and faithfulness are 0.", "main_total_tokens": 2930, "main_prompt_tokens": 2887, "main_completion_tokens": 43, "judge_total_tokens": 537, "judge_prompt_tokens": 434, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest Russian-language movie rating in 1999, based on the provided documents, is 7.117 for \"The Rifleman of the Voroshilov Regiment\" [Doc 15410].", "latency_ms": 3300.226926803589, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the highest rating is 7.6, but the generated answer claims 7.117 for a specific film, which is numerically incorrect. There is no evidence in the given context that supports 7.117 being the highest rating, so the response is both factually wrong relative to the gold and not faithful to the correct information.", "main_total_tokens": 3052, "main_prompt_tokens": 3003, "main_completion_tokens": 49, "judge_total_tokens": 524, "judge_prompt_tokens": 426, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "Based on the provided documents, there are two movies that meet your criteria of being released in 1987, having a rating over 7, and being in Chinese (cn):\n\n1. \"A Chinese Ghost Story\" with a rating of 7.3 [Doc 7064].\n2. \"Last Eunuch In China\" with a rating of 7.8 [Doc 7754].", "latency_ms": 3980.4141521453857, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there are 4 movies, while the generated answer claims there are 2, so it is numerically incorrect. The system prompt indicates that the answer must be based solely on the provided documents, but we are not shown those documents here, so we cannot verify whether only two movies actually qualified; however, because the gold label is authoritative, the model’s reasoning is treated as unfaithful to the task outcome. Therefore both correctness and faithfulness are scored 0.", "main_total_tokens": 2921, "main_prompt_tokens": 2832, "main_completion_tokens": 89, "judge_total_tokens": 602, "judge_prompt_tokens": 473, "judge_completion_tokens": 129}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "Based on the provided documents, there is only one TV show that meets all three criteria: it was released in 1987, has a rating over 7, and is in Chinese. That would be \"A Chinese Ghost Story,\" which actually appears to be categorized as a Movie rather than a TV show [Doc 7064]. However, if we strictly follow the documents provided, there are no TV shows that meet all three criteria.\n\nIt's important to note that \"A Chinese Ghost Story\" is listed under Type: Movie, not TV. Therefore, based on the information given, there are no TV shows in Chinese from 1987 with a rating over 7 [Doc 7064].", "latency_ms": 5326.246500015259, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 2 such TV shows, but the generated answer concludes there are none. This is factually incorrect relative to the gold label, so correctness is 0. However, the assistant carefully reasons from the documents it references and does not hallucinate extra items; it explicitly notes that the only candidate is actually a movie, not a TV show. Thus, within the context of the provided documents, the reasoning is faithful, yielding a faithfulness score of 1.", "main_total_tokens": 3419, "main_prompt_tokens": 3269, "main_completion_tokens": 150, "judge_total_tokens": 662, "judge_prompt_tokens": 533, "judge_completion_tokens": 129}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "Based on the provided documents, there isn't a clear indication of a sole French language TV show from 1980. Multiple entries are listed as French movies or TV shows but none specify being the only one. Documents [Doc 112637], [Doc 112641], and [Doc 112653] list French-language productions, but they do not claim exclusivity. Therefore, it's not possible to definitively state which is the \"only\" French language TV show from 1980 based on this context alone.", "latency_ms": 4681.0057163238525, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is Fantômas, but the generated answer claims the information cannot be determined from the documents, which is factually incorrect given the gold answer. However, the response is faithful to the (stated) contents of the documents as it describes its own limitations and does not invent a specific show name, so faithfulness is high while correctness relative to the gold is zero.", "main_total_tokens": 2879, "main_prompt_tokens": 2757, "main_completion_tokens": 122, "judge_total_tokens": 596, "judge_prompt_tokens": 488, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie that features two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" [Doc 11205].", "latency_ms": 2968.390941619873, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' which matches the gold answer. It does not introduce contradictions or unsupported information beyond referencing a document, so it is both correct and faithful.", "main_total_tokens": 2918, "main_prompt_tokens": 2887, "main_completion_tokens": 31, "judge_total_tokens": 480, "judge_prompt_tokens": 410, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie that describes Fred Rogers speaking to children about some of life's most important issues through his television program is \"Won't You Be My Neighbor?\" [Doc 84838]", "latency_ms": 3336.557149887085, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, providing the correct movie title 'Won't You Be My Neighbor?' and stays faithful to the question without adding incorrect information.", "main_total_tokens": 3225, "main_prompt_tokens": 3182, "main_completion_tokens": 43, "judge_total_tokens": 503, "judge_prompt_tokens": 439, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The document does not specify a particular \"it\" to refer to, but if we consider the first movie mentioned in the context, which has no specific title provided (only \"Title: \"), it was released in 1989 [Doc 9460].", "latency_ms": 3478.8973331451416, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question, with its gold answer, clearly refers to something released in 2018, while the generated answer claims 1989, so it is factually incorrect. It also fabricates contextual assumptions about an unspecified movie and a document [Doc 9460] that are not supported by the given gold answer or prompt, so it is unfaithful to the intended reference.", "main_total_tokens": 3009, "main_prompt_tokens": 2953, "main_completion_tokens": 56, "judge_total_tokens": 535, "judge_prompt_tokens": 428, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "None of the provided documents describe a TV series that matches all the criteria: being an animated series about a young boy robot set in 2030 AD. The closest match is \"Victor_Robot\" which describes an animation about a little voiceless robot Victor and an 8-year-old girl Victoria, but it does not specify the year 2030 AD [Doc 96535].", "latency_ms": 3888.439655303955, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer to the question is 'Astro Boy,' but the generated answer claims that none of the documents describe such a series and does not provide the correct title. This makes it factually incorrect with respect to the gold answer. However, the answer is faithful to the provided document snippet, accurately reflecting that the closest described series in the documents is 'Victor_Robot' and noting the missing 2030 AD detail, so it is faithful to the cited context.", "main_total_tokens": 2855, "main_prompt_tokens": 2769, "main_completion_tokens": 86, "judge_total_tokens": 590, "judge_prompt_tokens": 465, "judge_completion_tokens": 125}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 2774.8537063598633, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states they were held captive for 7 years, which matches the gold answer. It is fully consistent and faithful to the provided information.", "main_total_tokens": 2585, "main_prompt_tokens": 2549, "main_completion_tokens": 36, "judge_total_tokens": 486, "judge_prompt_tokens": 424, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26m before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 3503.903865814209, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983, matching the given gold value. It faithfully reflects the provided information without adding unsupported claims.", "main_total_tokens": 3369, "main_prompt_tokens": 3325, "main_completion_tokens": 44, "judge_total_tokens": 510, "judge_prompt_tokens": 428, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "The most popular English-speaking TV show released in 1997 based on the provided documents is \"Oz\" with a popularity score of 14.7527 [Doc 114714].", "latency_ms": 3198.4152793884277, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says the popularity score is 80.5772, but the generated answer gives 14.7527 and identifies a specific show, so the numeric result is incorrect. However, the generated answer appears to be grounded in the referenced document [Doc 114714], so it is faithful to its cited source, just not to the gold label.", "main_total_tokens": 2857, "main_prompt_tokens": 2809, "main_completion_tokens": 48, "judge_total_tokens": 532, "judge_prompt_tokens": 429, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "None of the documents provided contain information about any Russian language (ru) TV shows from 1997. The languages mentioned are de, sv, en, pl, fi, el, it, et, and fr, but not ru (Russian). Therefore, based on the given context, there is no evidence to support the existence of Russian language TV shows released in 1997.", "latency_ms": 3871.8714714050293, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates the correct number of Russian language TV shows released in 1997 is 1. The generated answer instead claims there is no evidence and does not provide the correct count, so it is incorrect. However, it faithfully describes what is (not) present in the provided documents and does not introduce hallucinated information, so it is faithful to the given context.", "main_total_tokens": 2941, "main_prompt_tokens": 2859, "main_completion_tokens": 82, "judge_total_tokens": 566, "judge_prompt_tokens": 460, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 3690.8981800079346, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, naming the TV show 'Angel Eyes' and does not introduce any contradictions or unsupported claims beyond a harmless document citation.", "main_total_tokens": 3752, "main_prompt_tokens": 3712, "main_completion_tokens": 40, "judge_total_tokens": 492, "judge_prompt_tokens": 429, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"What Is This Film Called Love?\" is English [Doc 58692].", "latency_ms": 2735.487222671509, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states that the movie is in English, while the gold answer says Korean. This is factually incorrect relative to the gold. It also contradicts the provided gold label, so it is not faithful to the given reference.", "main_total_tokens": 2743, "main_prompt_tokens": 2718, "main_completion_tokens": 25, "judge_total_tokens": 475, "judge_prompt_tokens": 397, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "Based on the provided documents, there are no English-speaking (language: en) horror movies from 2009 with a popularity score greater than 7. The highest popularity scores for English-speaking horror movies from 2009 listed here are:\n\n- \"Friday the 13th\" with a popularity of 4.9425 [Doc 44859]\n- \"Halloween II\" with a popularity of 4.0125 [Doc 44901]\n- \"Triangle\" with a popularity of 4.9214 [Doc 47869]\n\nNone of these scores exceed 7.", "latency_ms": 5016.935110092163, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer claims there are 0 such movies, so it is numerically incorrect. However, the reasoning is consistent with the documents it cites and does not introduce external information or contradictions, so it is faithful to the provided context.", "main_total_tokens": 3141, "main_prompt_tokens": 3001, "main_completion_tokens": 140, "judge_total_tokens": 596, "judge_prompt_tokens": 510, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Based on the provided documents, there is no movie from 1979 that has the highest rating. All movies listed are from 1980 or have different years. Therefore, with the given context, we cannot determine the best-rated movie released in 1979.", "latency_ms": 3631.38747215271, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.8, "faithfulness_score": 0, "justification": "The gold answer says explicitly there are no movies from 1979 in the database. The generated answer instead talks about there being no 1979 movie among the provided documents (mentioning 1980 and other years), which contradicts the gold context and invents details about the documents. It partially matches the conclusion that we cannot determine the best-rated 1979 movie, so it's somewhat correct on outcome but not aligned with the stated database limitation, and it is unfaithful to the gold answer's reasoning and constraints.", "main_total_tokens": 3106, "main_prompt_tokens": 3046, "main_completion_tokens": 60, "judge_total_tokens": 586, "judge_prompt_tokens": 448, "judge_completion_tokens": 138}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "Based on the provided documents, there is no information about a movie titled \"Back to the Future Part IV.\" The documents mention only up to \"Back to the Future Part III,\" which was released in 1990 [Doc 10621]. Therefore, there is no answer found for a \"Back to the Future part IV\" release year.", "latency_ms": 4268.2178020477295, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that 'Back to the Future Part IV' does not exist in the database, effectively indicating no such movie/year. The generated answer correctly notes that only up to Part III (1990) is mentioned and that there is no information or answer for a Part IV release year. This is factually correct (no Part IV film exists) and remains faithful to the given documents. The extra mention of Doc 10621 is consistent with the claim that only Part III is documented, and it does not introduce hallucinated specifics about Part IV, so both correctness and faithfulness are high.", "main_total_tokens": 3653, "main_prompt_tokens": 3578, "main_completion_tokens": 75, "judge_total_tokens": 620, "judge_prompt_tokens": 469, "judge_completion_tokens": 151}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "The provided documents do not contain information on any movie from 1980 that is related to the nuclear Chernobyl disaster. Therefore, it is not possible to determine a popularity score for such a movie based on the given context.", "latency_ms": 3469.31529045105, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is unanswerable because Chernobyl occurred in 1986, so there are no 1980 movies about the Chernobyl disaster. The gold answer states such movies could not be found, and the generated answer correctly states that the documents have no such information and that a popularity score cannot be determined. This matches the gold in substance, is factually correct, and is faithful to the given context.", "main_total_tokens": 3175, "main_prompt_tokens": 3126, "main_completion_tokens": 49, "judge_total_tokens": 562, "judge_prompt_tokens": 452, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are three entries with 'Astro Boy' in the title:\n\n1. Astro Boy (TV, Year: 2003) [Doc 116265]\n2. Astro Boy (TV, Year: 1980) [Doc 112591]\n3. Astro Boy (Movie, Year: 2009) [Doc 44947]", "latency_ms": 4142.57550239563, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.8318724637288826, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.8318724637288826, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 4 titles with 'Astro Boy', but the generated answer lists only 3 and explicitly claims there are three entries, so it is factually incorrect relative to the gold. However, the generated answer is internally consistent and accurately reflects what it reports from the documents it lists, so it is faithful to its own cited content.", "main_total_tokens": 3102, "main_prompt_tokens": 3013, "main_completion_tokens": 89, "judge_total_tokens": 560, "judge_prompt_tokens": 457, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "None of the provided documents mention or allude to the Holocaust.", "latency_ms": 2651.505708694458, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 2 such movies, but the generated answer claims there are none. This directly contradicts the correct answer, so correctness is 0. However, the generated answer is consistent with its stated basis (that none of the provided documents mention or allude to the Holocaust), so it is faithful to the given context, yielding a faithfulness score of 1.", "main_total_tokens": 2946, "main_prompt_tokens": 2932, "main_completion_tokens": 14, "judge_total_tokens": 506, "judge_prompt_tokens": 397, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "Based on the provided documents, there are three TV shows related to Super Mario Bros. that were released in the 1990s:\n\n1. **The Super Mario Bros. Super Show!** - Released in 1989 [Doc 113517].\n2. **The Adventures of Super Mario Bros. 3** - Released in 1990 [Doc 113661].\n3. **Super Mario World** - Released in 1991 [Doc 113814].\n\nTherefore, three TV shows about Super Mario Bros were released in the 1990s.", "latency_ms": 5440.473794937134, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks specifically for TV shows released in the 1990s. Among the three shows listed, only two were released in the 1990s: The Adventures of Super Mario Bros. 3 (1990) and Super Mario World (1991). The Super Mario Bros. Super Show! (1989) was released in the 1980s and should not be counted. The generated answer concludes that three TV shows were released in the 1990s, which is incorrect. However, it is faithful to the cited documents and does not introduce information beyond them.", "main_total_tokens": 3714, "main_prompt_tokens": 3578, "main_completion_tokens": 136, "judge_total_tokens": 648, "judge_prompt_tokens": 500, "judge_completion_tokens": 148}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "None of the documents provided contain information on any TV shows about Super Mario Bros that were released between 2000 and 2010.", "latency_ms": 3196.9354152679443, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the number of TV shows about Super Mario Bros released between 2000 and 2010, with gold answer 0. The generated answer states that none of the provided documents mention any such shows, which supports the conclusion that there are none in that period, matching the gold. It correctly refrains from hallucinating a number and stays grounded in the documents, so it is both correct and faithful.", "main_total_tokens": 3260, "main_prompt_tokens": 3228, "main_completion_tokens": 32, "judge_total_tokens": 531, "judge_prompt_tokens": 416, "judge_completion_tokens": 115}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular TV show or movie about ninjas, based on the provided documents, is \"NARUTO Spin-Off: Rock Lee & His Ninja Pals\" with a popularity rating of 10.7994 [Doc 120321].", "latency_ms": 3719.1967964172363, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question expects 'Naruto' as the answer, but the generated response instead answers 'NARUTO Spin-Off: Rock Lee & His Ninja Pals,' which is a different show, so it is incorrect. It also explicitly relies on a cited document and its popularity rating, which is outside the provided gold answer context, so it is not faithful to the gold-standard reasoning.", "main_total_tokens": 3222, "main_prompt_tokens": 3164, "main_completion_tokens": 58, "judge_total_tokens": 538, "judge_prompt_tokens": 431, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The provided documents do not mention a Jurassic Park/Power Rangers crossover movie. Therefore, based on the given context, there is no information regarding when such a crossover movie was released [None of the Docs].", "latency_ms": 3293.8551902770996, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer states clearly that the movie was not released. The generated answer instead claims there is no information in the documents about the release date, implying the issue is lack of evidence rather than that the movie does not exist or was not released. This is factually incorrect relative to the gold answer and does not address the question as intended, so correctness is 0. In terms of faithfulness to the provided context (which the rubric suggests mentioned no such movie), the answer is partially faithful because it honestly reports that the documents contain no information, but it fails to adopt the correct conclusion that the movie was not released. Hence, faithfulness is partial.", "main_total_tokens": 3105, "main_prompt_tokens": 3063, "main_completion_tokens": 42, "judge_total_tokens": 590, "judge_prompt_tokens": 426, "judge_completion_tokens": 164}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "None of the documents provided contain information about Barbie fighting a Predator. The closest reference is to \"Killer Barbys vs. Dracula,\" where Killer Barbies are mentioned, but there is no connection to a Predator. [Doc 28703]", "latency_ms": 3665.3852462768555, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is effectively a trick; such a movie does not exist. The generated answer correctly notes there is no information about Barbie fighting a Predator and points to the closest available reference (\"Killer Barbys vs. Dracula\"), which aligns with the gold answer that the movie was not released. It does not fabricate a movie title and stays grounded in the provided document, so both correctness and faithfulness are high.", "main_total_tokens": 3368, "main_prompt_tokens": 3314, "main_completion_tokens": 54, "judge_total_tokens": 545, "judge_prompt_tokens": 431, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The information provided does not directly list the chronological order of all Star Wars movies, but it does mention several Star Wars-related documentaries and features from the year 2004. To determine the fourth oldest Star Wars movie, we would need a comprehensive list of all Star Wars films in chronological order by release date.\n\nHowever, based on the documents provided, there is no direct answer to what the 4th oldest Star Wars movie is without additional context or information about the entire Star Wars filmography. The documents focus on documentaries and features rather than the main series of movies. [Doc X] does not provide this specific information.", "latency_ms": 5042.050838470459, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the 4th oldest Star Wars movie is 'Star Wars: Droids - The Great Heep'. The generated answer claims the documents do not provide enough information and does not give this title or any definitive answer. Thus, it is incorrect relative to the gold. However, it accurately reflects the (stated) limitations of the provided documents and does not fabricate specific movie information, so it is faithful to its cited context.", "main_total_tokens": 3468, "main_prompt_tokens": 3341, "main_completion_tokens": 127, "judge_total_tokens": 634, "judge_prompt_tokens": 513, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The document does not specify a particular \"it\" to refer to, but if we consider the first movie mentioned in the context, which has no specific title provided (only \"Title: \"), it was released in 1989 [Doc 9460].", "latency_ms": 3503.814697265625, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer clearly states the release year as 1986, while the generated answer claims 1989, so it is factually incorrect. It also invents contextual details about an unspecified movie and a document reference (Doc 9460) that are not supported by the given gold answer, making it unfaithful to the reference.", "main_total_tokens": 3009, "main_prompt_tokens": 2953, "main_completion_tokens": 56, "judge_total_tokens": 527, "judge_prompt_tokens": 428, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The provided context does not contain information about the rating for the movie \"Forrest Gump\". It only provides details on a documentary titled \"Through the Eyes of Forrest Gump\" [Doc 15881]. Therefore, I cannot determine the rating for \"Forrest Gump\" based solely on this information.", "latency_ms": 3790.8928394317627, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for Forrest Gump's rating; the gold label gives 8.466, but the model only has context about a different documentary and says it cannot determine the rating from the provided context. Given typical constraints, this is an appropriate and factually correct response with respect to the given context, and it does not hallucinate beyond it, so it is both correct and faithful.", "main_total_tokens": 3200, "main_prompt_tokens": 3134, "main_completion_tokens": 66, "judge_total_tokens": 565, "judge_prompt_tokens": 456, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 2716.737985610962, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Mighty Med') and correctly addresses the question. It does not introduce unsupported information beyond the source.", "main_total_tokens": 2465, "main_prompt_tokens": 2428, "main_completion_tokens": 37, "judge_total_tokens": 483, "judge_prompt_tokens": 423, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In \"Túnel de los huesos\" (2011), the seven prisoners escaped from a prison in Argentina [Doc 53582].", "latency_ms": 3105.1759719848633, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Argentina as the country where the prison break occurred in 'Túnel de los huesos' (2011), matching the gold answer exactly and staying faithful to the provided information.", "main_total_tokens": 3082, "main_prompt_tokens": 3047, "main_completion_tokens": 35, "judge_total_tokens": 492, "judge_prompt_tokens": 420, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The provided documents do not contain information about a movie titled \"Sweet Trap\" from 2024, nor is there any mention of an apprentice falling into someone's trap in that context. Therefore, based on the given sources, it is not possible to answer the question.", "latency_ms": 3458.488941192627, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a clear answer (Jiang Jie), but the generated response claims the information is not available, so it is factually incorrect. However, given the assumption that the model must rely only on the provided documents, the answer is faithful to those documents, as it does not invent unsupported details.", "main_total_tokens": 3003, "main_prompt_tokens": 2946, "main_completion_tokens": 57, "judge_total_tokens": 529, "judge_prompt_tokens": 437, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "Based on the provided context, there are two Korean-language movies that mention assassins after 2023:\n\n1. \"The Assassin\" [Doc 106219] which is described as an action historical drama depicting the story of Joseon's best swordsman Yi Nan.\n2. \"Kill Boksoon\" [Doc 105198] which describes a renowned assassin who is also a single mom to a teenage daughter.\n\nThese are the only Korean-language movies (language: ko) that mention assassins or assassination-related themes in their descriptions.", "latency_ms": 4566.441059112549, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 4, but the generated answer states there are 2, so the numerical result is incorrect. It is also unfaithful to the gold answer, which clearly specifies 4 as the correct count, while the generated response asserts a different number and a different set of movies.", "main_total_tokens": 2918, "main_prompt_tokens": 2799, "main_completion_tokens": 119, "judge_total_tokens": 580, "judge_prompt_tokens": 489, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "The provided documents do not contain any information about movies that feature both Steven Seagal and Bruce Lee. Therefore, based on the given context, there is no evidence to suggest that there are any movies with both Steven Seagal and Bruce Lee.", "latency_ms": 3686.6531372070312, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no evidence of any movies featuring both Steven Seagal and Bruce Lee, which matches the gold answer. It does not introduce unsupported claims and remains grounded in the provided context, so it is both correct and faithful.", "main_total_tokens": 3580, "main_prompt_tokens": 3531, "main_completion_tokens": 49, "judge_total_tokens": 522, "judge_prompt_tokens": 441, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 2888.2343769073486, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3 and does not introduce any contradictions or unsupported information. It is both correct and faithful.", "main_total_tokens": 3040, "main_prompt_tokens": 3014, "main_completion_tokens": 26, "judge_total_tokens": 464, "judge_prompt_tokens": 403, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The provided context does not specify a particular style of racing for the movie \"Oram Po.\" However, it describes that Chandru is an expert auto driver & racer, and Bigilu customizes autos to run at high speeds. The plot revolves around a challenge race between Chandru and Son of Gun. Therefore, we can infer that the racing in this movie involves street or illegal racing, as it mentions challenging rivals and settling dues through races [Doc 39729].", "latency_ms": 4420.318841934204, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks specifically for the style of racing and the gold answer is 'Drift'. The generated answer instead infers 'street or illegal racing' from the context and explicitly states there is no specified style in the movie. This is factually incorrect relative to the gold answer. It is also unfaithful because it adds an inferred racing style that contradicts the labeled correct style and claims the context does not specify a style. Therefore, both correctness and faithfulness are 0.", "main_total_tokens": 3328, "main_prompt_tokens": 3229, "main_completion_tokens": 99, "judge_total_tokens": 600, "judge_prompt_tokens": 472, "judge_completion_tokens": 128}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and her relationship with the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 3366.689682006836, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, 'Confronting a Serial Killer,' and correctly identifies it as the movie about Jillian Lauren and Samuel Little. It remains faithful to the provided information without adding contradictions or unsupported claims.", "main_total_tokens": 2947, "main_prompt_tokens": 2893, "main_completion_tokens": 54, "judge_total_tokens": 532, "judge_prompt_tokens": 455, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 3129.1298866271973, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score value of 7.838 for Armageddon (1998) and correctly attributes it to the cited document, so it is both correct and faithful.", "main_total_tokens": 3308, "main_prompt_tokens": 3278, "main_completion_tokens": 30, "judge_total_tokens": 478, "judge_prompt_tokens": 407, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 3686.687231063843, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'The Sixth Sense'. It is fully consistent with the provided information and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 3030, "main_prompt_tokens": 2995, "main_completion_tokens": 35, "judge_total_tokens": 489, "judge_prompt_tokens": 420, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 4126.925706863403, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer, and it remains faithful to the provided information.", "main_total_tokens": 3224, "main_prompt_tokens": 3201, "main_completion_tokens": 23, "judge_total_tokens": 458, "judge_prompt_tokens": 397, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 4607.569217681885, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly naming the 2015 reboot as 'The Transporter Refueled' and stays faithful to the information provided.", "main_total_tokens": 3298, "main_prompt_tokens": 3263, "main_completion_tokens": 35, "judge_total_tokens": 484, "judge_prompt_tokens": 420, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in the year 2015 [Doc 67500].", "latency_ms": 4224.433898925781, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It is also faithful to the cited document reference without adding unsupported details.", "main_total_tokens": 3263, "main_prompt_tokens": 3235, "main_completion_tokens": 28, "judge_total_tokens": 477, "judge_prompt_tokens": 406, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "None of the provided documents mention a plot involving Abraham Lincoln coming back from the dead in any romantic comedy.", "latency_ms": 4119.522333145142, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there is no movie description in the provided documents matching a romantic comedy where Abraham Lincoln comes back from the dead. The generated answer is faithful to the given information and does not introduce unsupported claims.", "main_total_tokens": 3177, "main_prompt_tokens": 3155, "main_completion_tokens": 22, "judge_total_tokens": 489, "judge_prompt_tokens": 411, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The provided documents do not contain information about a movie that alludes to a manuscript written by Ernest Hemingway. Therefore, based on the given context, I cannot provide a specific title for such a movie.", "latency_ms": 4485.685110092163, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the title of a movie that alludes to a manuscript written by Ernest Hemingway, and the correct answer is 'Havana Darkness.' The generated answer claims the documents do not contain this information and does not provide the correct title, so it is factually incorrect. However, it remains faithful to the (stated) limitation of the provided context, not hallucinating a title, so it is faithful to the given context and reasoning process.", "main_total_tokens": 2983, "main_prompt_tokens": 2940, "main_completion_tokens": 43, "judge_total_tokens": 550, "judge_prompt_tokens": 428, "judge_completion_tokens": 122}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show \"Doll & Em\" [Doc 122259].", "latency_ms": 3726.3007164001465, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em,' which matches the gold answer exactly. It remains faithful to the provided information and does not introduce unsupported content, aside from a harmless citation-style note that does not contradict anything.", "main_total_tokens": 2925, "main_prompt_tokens": 2891, "main_completion_tokens": 34, "judge_total_tokens": 497, "judge_prompt_tokens": 417, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Based on the provided context, there is no information about a character named Hallam with a talent for spying or attempting to find the cause of his mother's death. None of the movies listed contain this specific plotline. Therefore, I cannot provide an answer based strictly on the given documents. [Doc X]", "latency_ms": 3493.0973052978516, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question clearly refers to the 2007 film 'Hallam Foe,' which matches the gold answer. The generated answer incorrectly claims the answer cannot be determined from context and does not provide 'Hallam Foe,' so it is factually incorrect. However, it remains faithful to the (assumed) constraint of using only the provided documents, so faithfulness is high.", "main_total_tokens": 2966, "main_prompt_tokens": 2903, "main_completion_tokens": 63, "judge_total_tokens": 567, "judge_prompt_tokens": 460, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The provided context does not contain any information about a character whose last name could initially be mistaken for that of a friend or a foe. Therefore, based strictly on the given documents, there is no evidence to support this query.", "latency_ms": 3006.721258163452, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is explicit: the last name sounds like that of a foe. The generated answer claims there is no information and refuses to answer, so it is factually incorrect relative to the gold. However, it is faithful to the (unstated) context by not hallucinating an answer and explicitly stating that the information is not present, so faithfulness is high.", "main_total_tokens": 2623, "main_prompt_tokens": 2577, "main_completion_tokens": 46, "judge_total_tokens": 531, "judge_prompt_tokens": 431, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "The age of Sam in the 2017 TV show \"Atypical\" is 18 [Doc 124583].", "latency_ms": 3165.5049324035645, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, matching the gold answer exactly. It does not introduce any contradictions or extraneous information that would conflict with the source, so it is both correct and faithful.", "main_total_tokens": 3168, "main_prompt_tokens": 3136, "main_completion_tokens": 32, "judge_total_tokens": 497, "judge_prompt_tokens": 412, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 2966.712474822998, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which matches the gold answer exactly. It is fully supported by the provided information and does not introduce any unsupported details.", "main_total_tokens": 3043, "main_prompt_tokens": 3014, "main_completion_tokens": 29, "judge_total_tokens": 477, "judge_prompt_tokens": 410, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Sandra was born in Chamonix according to the plot description for \"À corde tendue\" [Doc 59659].", "latency_ms": 3311.4328384399414, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that Sandra was born in Chamonix, which matches the gold answer exactly. It does not introduce contradictions or unsupported claims beyond referencing the plot description, so it is both correct and faithful.", "main_total_tokens": 3519, "main_prompt_tokens": 3488, "main_completion_tokens": 31, "judge_total_tokens": 495, "judge_prompt_tokens": 422, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "in this japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 3635.551691055298, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly fills in the blank with 'rains' and matches the gold answer exactly. It is fully consistent with the provided information.", "main_total_tokens": 4088, "main_prompt_tokens": 4062, "main_completion_tokens": 26, "judge_total_tokens": 472, "judge_prompt_tokens": 412, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the animated series \"Tabaluga,\" which is produced in German (de), Tabaluga is the last of the dragons and the crown-prince of Greenland. [Doc 114810]", "latency_ms": 3453.46999168396, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the place where Tabaluga is crown-prince. The gold answer is Greenland. The generated answer correctly states that Tabaluga is the crown-prince of Greenland and matches the gold exactly, without adding contradictory information. Therefore it is fully correct and faithful to the given gold answer.", "main_total_tokens": 3375, "main_prompt_tokens": 3330, "main_completion_tokens": 45, "judge_total_tokens": 526, "judge_prompt_tokens": 434, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is titled **One Afternoon** [Doc 96838].", "latency_ms": 3154.787540435791, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'One Afternoon' and does not introduce unsupported information beyond echoing the description and title, so it is both correct and faithful.", "main_total_tokens": 2712, "main_prompt_tokens": 2661, "main_completion_tokens": 51, "judge_total_tokens": 514, "judge_prompt_tokens": 449, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "Based on the provided documents, there is no mention of an Italian language movie where a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her. None of the described movies match this specific plot detail.", "latency_ms": 6780.206203460693, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "recall_at_10": 0.0, "precision_at_5": 0.0, "precision_at_10": 0.0, "ndcg_at_5": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_5": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'The Life Ahead'. The generated answer incorrectly states that no such movie is mentioned, so it is factually incorrect. However, it remains faithful to the provided documents by not inventing information beyond them, so faithfulness is high even though the final answer is wrong relative to the gold.", "main_total_tokens": 3032, "main_prompt_tokens": 2985, "main_completion_tokens": 47, "judge_total_tokens": 536, "judge_prompt_tokens": 443, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about Bruce McLaren, a New Zealander who founded the McLaren Motor Racing team. [Doc 73859] describes how the film shows the world that a man of humble beginnings could take on the elite of motor racing and win.", "latency_ms": 3741.950511932373, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and adds accurate contextual detail that does not contradict the gold answer. It remains faithful to the source description and stays on-topic.", "main_total_tokens": 3209, "main_prompt_tokens": 3145, "main_completion_tokens": 64, "judge_total_tokens": 516, "judge_prompt_tokens": 443, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The documentary \"Super Speedway\" follows the Newman-Haas (Andretti) racing team [Doc 20456].", "latency_ms": 2887.2122764587402, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question asks for the name of \"his\" racing team, and the gold answer is \"The McLaren Motor Racing team.\" The generated answer instead gives \"the Newman-Haas (Andretti) racing team,\" which is incorrect relative to the gold. Thus correctness is 0. Regarding faithfulness, the answer cites a specific document and presents the team name as taken from that source; there is no obvious hallucination beyond being factually different from the gold. Since we cannot see the document, we cannot verify if the citation is accurate, so we assign a mid-level faithfulness score.", "main_total_tokens": 2868, "main_prompt_tokens": 2839, "main_completion_tokens": 29, "judge_total_tokens": 558, "judge_prompt_tokens": 406, "judge_completion_tokens": 152}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The question \"What country is he from?\" does not have a clear referent in the provided documents. Each document describes different movies with characters or plots that do not specify a particular individual's origin unless directly asked about a character from one of these contexts. For example, there are no specific individuals mentioned as \"he\" across the documents that would allow us to determine his country of origin without additional context.", "latency_ms": 3807.8694343566895, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the person is from New Zealand, so the correct answer is explicitly known. The generated answer claims the referent is unclear and does not provide the known correct country, making it factually incorrect relative to the gold. However, the answer remains faithful to its own stated reasoning, asserting a lack of context rather than inventing a country or facts unsupported by the prompt, so it is faithful but incorrect.", "main_total_tokens": 2863, "main_prompt_tokens": 2781, "main_completion_tokens": 82, "judge_total_tokens": 572, "judge_prompt_tokens": 456, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "None of the movies listed in the provided documents feature John McClane blowing up the Great Wall of China. The described plot elements do not match any of the summaries given for 2013 movies, nor is there a specific reference to such an event in any document. [Doc X] does not apply as no document contains this information.", "latency_ms": 3640.6948566436768, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer is correct that there is no 2013 movie in which John McClane blows up the Great Wall of China; this matches the gold answer’s substance that such a movie is not in the database. However, the gold answer is a simple database statement, while the generated answer adds references to documents ([Doc X]) and claims about their contents that are not grounded in the gold reference, so faithfulness is slightly reduced.", "main_total_tokens": 2915, "main_prompt_tokens": 2845, "main_completion_tokens": 70, "judge_total_tokens": 587, "judge_prompt_tokens": 466, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with the described plot is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 3031.3122272491455, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Tomie: Forbidden Fruit,' which matches the gold answer. It remains faithful to the given information and does not add unsupported details, aside from a neutral citation token that does not affect factuality.", "main_total_tokens": 3053, "main_prompt_tokens": 3021, "main_completion_tokens": 32, "judge_total_tokens": 522, "judge_prompt_tokens": 443, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 3211.181402206421, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, which matches the gold answer, and it remains grounded in the provided document citation.", "main_total_tokens": 3522, "main_prompt_tokens": 3499, "main_completion_tokens": 23, "judge_total_tokens": 478, "judge_prompt_tokens": 412, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The provided documents do not contain any information about \"Scorpion King: Book of Souls\" or details regarding a quest undertaken by The Scorpion King and Tala. Therefore, based on the given context, I cannot answer the question.", "latency_ms": 3157.8304767608643, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks what The Scorpion King and Tala search for in 'Scorpion King: Book of Souls (2018)'. The correct answer is 'The Book of Souls,' but the generated answer claims it cannot answer based on the documents. This is incorrect relative to the gold answer, so correctness is 0. However, the response is faithful to the (stated) lack of context and does not hallucinate, so faithfulness is 1.", "main_total_tokens": 2799, "main_prompt_tokens": 2751, "main_completion_tokens": 48, "judge_total_tokens": 563, "judge_prompt_tokens": 440, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "None of the documents provided contain information about a movie titled \"Zombies on the Moon\". Therefore, I cannot specify in which year this particular movie was released based on the given context.", "latency_ms": 3309.6766471862793, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database. The generated answer similarly says none of the documents provide information about that movie and thus it cannot specify the release year. This is correct (no fabricated date) and faithful to the lack of information in the provided context.", "main_total_tokens": 3356, "main_prompt_tokens": 3318, "main_completion_tokens": 38, "judge_total_tokens": 514, "judge_prompt_tokens": 428, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "None of the documents provided contain information about a movie or show titled \"The Galactic Baker\". Therefore, based on the given context, the release year for \"The Galactic Baker\" is not found.", "latency_ms": 3606.1348915100098, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available / not found. This is correct given the context and faithful to the provided documents, which contain no information about a movie titled 'The Galactic Baker'.", "main_total_tokens": 3666, "main_prompt_tokens": 3626, "main_completion_tokens": 40, "judge_total_tokens": 504, "judge_prompt_tokens": 428, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document provided does not contain information about a movie titled \"Return of the Null Pointer\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 3089.754104614258, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question expects the assistant to say the movie is not in its database, without hedging about a document or context. The generated answer instead refers to an unspecified 'document provided' and limits its inability to answer to that context, which does not match the gold response pattern. While it does not hallucinate a year, it is not the requested behavior, so correctness is 0. The answer is faithful to the (implied) context in that it does not invent information and accurately states that no such movie is found there, so faithfulness is 1.", "main_total_tokens": 2877, "main_prompt_tokens": 2840, "main_completion_tokens": 37, "judge_total_tokens": 571, "judge_prompt_tokens": 427, "judge_completion_tokens": 144}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document provided does not contain any information about a movie titled \"Data Science: The Movie\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 3141.3843631744385, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.8, "faithfulness_score": 0.8, "justification": "The correct answer is that the assistant does not know / movie not in database. The generated answer instead says it cannot answer based on the provided context, suggesting that with other information it might, which is slightly different from the gold response that clearly states lack of database entry. Still, it does not fabricate a year and correctly declines to provide one, so it is mostly correct and mostly faithful but not perfectly aligned with the gold reasoning.", "main_total_tokens": 3015, "main_prompt_tokens": 2977, "main_completion_tokens": 38, "judge_total_tokens": 549, "judge_prompt_tokens": 428, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "None of the documents provided contain information about a movie titled \"Python's Revenge 2\". The closest matches are \"Python 2\", which was released in 2002 [Doc 28408], and another document that also mentions \"Python 2\" with the same release year [Doc 28408]. There is no \"Python's Revenge 2\" mentioned.", "latency_ms": 4165.249824523926, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information about a movie titled \"Python's Revenge 2\" in the documents, which aligns with the gold response that it is not in the database. It does not hallucinate a release year and clearly distinguishes the different, similarly named movie \"Python 2\". The answer remains grounded in the provided documents and is faithful to them.", "main_total_tokens": 3191, "main_prompt_tokens": 3108, "main_completion_tokens": 83, "judge_total_tokens": 572, "judge_prompt_tokens": 465, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "None of the documents provided contain information about a movie titled \"The Last Pixel\". Therefore, the release year for \"The Last Pixel\" cannot be determined from the given context.", "latency_ms": 2820.8303451538086, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, effectively that the release year cannot be given. The generated answer says that the provided documents do not contain information about the movie and therefore the release year cannot be determined from the context. Both convey that the release year cannot be provided, which is correct given the information, and the generated answer stays within the given context. Hence both correctness and faithfulness are high.", "main_total_tokens": 2633, "main_prompt_tokens": 2597, "main_completion_tokens": 36, "judge_total_tokens": 540, "judge_prompt_tokens": 424, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no document provided that contains information on a movie titled \"The Man Who Ate The Sun\". Therefore, I cannot provide a release year for this specific title based on the given documents.", "latency_ms": 3197.1826553344727, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie that the gold answer says is not in the database. The generated answer correctly states that it cannot provide the release year due to lack of information in the provided documents. This is consistent with the gold answer and does not hallucinate a date, so it is both correct with respect to the task and faithful to the stated context limitations.", "main_total_tokens": 3042, "main_prompt_tokens": 3002, "main_completion_tokens": 40, "judge_total_tokens": 537, "judge_prompt_tokens": 430, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The document provided does not contain information about who directed the movie \"The Last Days of Lehman Brothers\".", "latency_ms": 2794.0313816070557, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the document does not contain the director information, matching the gold answer that the information is not provided in the database. It does not hallucinate a director and is fully consistent with the given context.", "main_total_tokens": 2940, "main_prompt_tokens": 2918, "main_completion_tokens": 22, "judge_total_tokens": 494, "judge_prompt_tokens": 418, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about a movie titled \"A Love to Die For\" or its director. Therefore, I cannot provide the name of the director based on the given context. [Doc 117179], [Doc 58692], [Doc 29899], [Doc 14779], [Doc 131220], [Doc 4390], [Doc 24763], [Doc 88425], [Doc 73313], [Doc 43840], [Doc 16243], [Doc 9280], [Doc 16739], [Doc 82231], [Doc 41562], [Doc 89014], [Doc 98077], [Doc 122273], [Doc 15670], and [Doc 68848] do not mention the movie \"A Love to Die For\" or its director.", "latency_ms": 7079.036235809326, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director cannot be identified from the provided information. The generated answer correctly notes that the documents do not mention the movie or its director, which is consistent with the gold answer and does not introduce unsupported claims. Therefore it is both correct and faithful to the given context.", "main_total_tokens": 3434, "main_prompt_tokens": 3197, "main_completion_tokens": 237, "judge_total_tokens": 662, "judge_prompt_tokens": 570, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The document does not provide information about who directed the movie \"Journey into Gaza\".", "latency_ms": 3192.610740661621, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer accurately states that the document does not provide information about the director, which is equivalent in meaning to the gold answer indicating the database lacks this information. It is both correct and faithful to the given information.", "main_total_tokens": 3712, "main_prompt_tokens": 3694, "main_completion_tokens": 18, "judge_total_tokens": 483, "judge_prompt_tokens": 409, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The document provided does not contain information about who directed the TV show \"The Last Night at Tremore Beach\". [Doc 135098] only includes basic details such as its type, year, rating, popularity, language, genres, and a brief description but does not mention the director.", "latency_ms": 3907.38844871521, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says \"movie\" while the generated answer says \"TV show,\" which is factually different, so correctness relative to the gold is 0. Additionally, faithfulness is 0 because the instruction is to match the gold answer, including wording that it is a movie, and the generated answer contradicts it by calling it a TV show.", "main_total_tokens": 2691, "main_prompt_tokens": 2628, "main_completion_tokens": 63, "judge_total_tokens": 557, "judge_prompt_tokens": 455, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The scenario where a group of archaeology students awaken an ancient mummy occurs in multiple movies described here, but one specific example is:\n\n- **Title:** \"Rise of the Mummy\" [Doc 98375]\n- **Year:** 2021\n\nIn this movie, a group of archaeology students awaken an ancient mummy and find themselves trapped in a time loop, with the only way to escape being by defeating the mummy.", "latency_ms": 5707.607984542847, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The model correctly identifies the movie as 'Rise of the Mummy,' matching the gold answer. However, it fabricates extra details such as the year (2021), a document reference, and a time-loop plot element that are not supported by the given information, so faithfulness is low.", "main_total_tokens": 2905, "main_prompt_tokens": 2809, "main_completion_tokens": 96, "judge_total_tokens": 565, "judge_prompt_tokens": 475, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The plot described where Sarah fakes her own death to escape her abusive spouse happens in the movie \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 3411.4229679107666, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as \"Stranger in My Bed.\" It is fully faithful to the provided information and does not introduce contradictions or unsupported details.", "main_total_tokens": 2868, "main_prompt_tokens": 2832, "main_completion_tokens": 36, "judge_total_tokens": 497, "judge_prompt_tokens": 428, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The plot described, where a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years, is from the TV show titled \"Game of Silence\" [Doc 123938].", "latency_ms": 2957.0765495300293, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The question asks for a movie, but both the gold and the generated answer identify 'Game of Silence', which is correct for the described plot, though it is actually a TV series, not a movie. The generated answer is factually consistent with the gold label and correctly matches the description, but it adds a fabricated citation '[Doc 123938]', which is not grounded in the provided content. Thus correctness is 1, while faithfulness is slightly reduced due to the invented reference.", "main_total_tokens": 2495, "main_prompt_tokens": 2449, "main_completion_tokens": 46, "judge_total_tokens": 567, "judge_prompt_tokens": 437, "judge_completion_tokens": 130}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie that fits the description of being based on a true story about a killer who uses Craigslist is \"The Craigslist Killer\" [Doc 53039]. This film details the story of Philip Markoff, a charismatic medical student at Boston University who leads a double life as a brutal sexual deviant, targeting prostitutes he finds via Craigslist.", "latency_ms": 3935.3649616241455, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Craigslist Killer,\" matching the gold answer. The additional description of the plot and real-life basis aligns with the known details of the film and does not contradict the question or gold answer, so it is both correct and faithful.", "main_total_tokens": 2911, "main_prompt_tokens": 2840, "main_completion_tokens": 71, "judge_total_tokens": 544, "judge_prompt_tokens": 459, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The scenario where Yuki Hirano fails his university entrance exam and then decides to become a forestry trainee is from the movie \"Wood Job!\" [Doc 66094].", "latency_ms": 3878.894567489624, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", which matches the gold answer exactly. It stays fully faithful to the provided information and does not introduce contradictions or irrelevant content.", "main_total_tokens": 4015, "main_prompt_tokens": 3976, "main_completion_tokens": 39, "judge_total_tokens": 502, "judge_prompt_tokens": 434, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The scenario where a playful bird snatches Abby's magic wand during a hot-air balloon ride, leading Elmo, Abby, and their friends on a colorful adventure, occurs in the movie \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 5575.668573379517, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Sesame Street: The Magical Wand Chase,' which matches the gold answer exactly. The response is fully supported by the provided information and does not introduce any unsupported claims, so it is both correct and faithful.", "main_total_tokens": 3712, "main_prompt_tokens": 3654, "main_completion_tokens": 58, "judge_total_tokens": 544, "judge_prompt_tokens": 463, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The plot described happens in the movie titled \"Beyond\" [Doc 74597].", "latency_ms": 3663.1104946136475, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot. The gold answer is \"Beyond\" and the generated answer states that the plot happens in the movie titled \"Beyond\". This exactly matches the gold, so the response is correct and fully faithful to the provided information.", "main_total_tokens": 2708, "main_prompt_tokens": 2687, "main_completion_tokens": 21, "judge_total_tokens": 509, "judge_prompt_tokens": 425, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The plot described happens in the movie \"6ixtynin9\" [Doc 22478].", "latency_ms": 3761.4376544952393, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', which matches the gold answer. It does not introduce unsupported details beyond a generic citation, so it is both correct and faithful.", "main_total_tokens": 2700, "main_prompt_tokens": 2675, "main_completion_tokens": 25, "judge_total_tokens": 495, "judge_prompt_tokens": 423, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The description provided matches the plot of the movie \"Club Dread\" [Doc 32410].", "latency_ms": 4050.963878631592, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer, and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 3083, "main_prompt_tokens": 3060, "main_completion_tokens": 23, "judge_total_tokens": 504, "judge_prompt_tokens": 444, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The described scenario occurs in the movie titled \"Toilet\" [Doc 48873].", "latency_ms": 4255.2170753479, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" which matches the gold answer exactly. It stays faithful to the prompt without adding unsupported details, only restating the title and lightly referencing a document identifier, which does not contradict the given information.", "main_total_tokens": 3299, "main_prompt_tokens": 3277, "main_completion_tokens": 22, "judge_total_tokens": 513, "judge_prompt_tokens": 432, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The description provided matches the plot details for the movie \"Electric Angel\" [Doc 1024].", "latency_ms": 3735.4373931884766, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the given description, and the gold answer is \"Electric Angel.\" The generated answer explicitly identifies the movie as \"Electric Angel\" and does not introduce any contradictory or extraneous claims beyond a neutral reference marker. Therefore it is both correct and faithful to the gold.", "main_total_tokens": 3273, "main_prompt_tokens": 3250, "main_completion_tokens": 23, "judge_total_tokens": 525, "judge_prompt_tokens": 436, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 based on the provided documents is \"Rare Objects\" with a rating of 4.3 [Doc 105190].", "latency_ms": 2450.4144191741943, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'Mega Ape' as the lowest-rated 2023 movie, but the generated answer claims it is 'Rare Objects' with a 4.3 rating. This directly contradicts the gold answer, so it is factually incorrect relative to the task. Additionally, the model asserts this is based on provided documents without evidence here, and it does not align with the specified correct output, so it is not faithful to the gold/reference answer.", "main_total_tokens": 3152, "main_prompt_tokens": 3113, "main_completion_tokens": 39, "judge_total_tokens": 536, "judge_prompt_tokens": 413, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "The highest rated TV show from 2010 based on the provided documents is \"Tea House\" with a rating of 9.2 [Doc 119448].", "latency_ms": 2638.6313438415527, "retrieval_recall": 1.0, "retrieval_mrr": 0.08333333333333333, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the highest rated TV show from 2010, and the gold answer is 'Tea House'. The generated answer correctly identifies 'Tea House' as the highest rated TV show from 2010 and adds a plausible rating and document reference that are consistent with the gold. There are no contradictions or hallucinations relative to the given gold answer, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3531, "main_prompt_tokens": 3490, "main_completion_tokens": 41, "judge_total_tokens": 527, "judge_prompt_tokens": 415, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest-rated Japanese language movie from 1999 based on the provided documents is \"Yu☆Gi☆Oh!\" with a rating of 7.0 [Doc 22216].", "latency_ms": 3284.454584121704, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the highest rated Japanese language movie from 1999 in the gold label is 'The Uncut Story of Mewtwo's Origin', not 'Yu☆Gi☆Oh!'. It is therefore not faithful to the gold answer and provides a different film title as the result.", "main_total_tokens": 3043, "main_prompt_tokens": 3000, "main_completion_tokens": 43, "judge_total_tokens": 520, "judge_prompt_tokens": 428, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movies from 1980 based on the provided documents are \"Cataclysm\" and \"Action,\" both with a rating of 3.889 and 4.3 respectively. However, since we need to identify the language of the lowest-rated movie, we will consider \"Cataclysm\" which has a rating of 3.889. The language for \"Cataclysm\" is English [Doc 705].", "latency_ms": 4618.969440460205, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The final answer, English, matches the gold answer and is correct. However, the reasoning is not faithful to the prompt because it introduces specific movies, ratings, and a document reference (Doc 705) that are not mentioned in the given QA pair; these details are hallucinated relative to the provided context.", "main_total_tokens": 3605, "main_prompt_tokens": 3506, "main_completion_tokens": 99, "judge_total_tokens": 566, "judge_prompt_tokens": 473, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show with the highest popularity in 1999 based on the provided documents is \"The Amanda Show\" with a popularity of 3.4197 [Doc 115219].", "latency_ms": 3736.2871170043945, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect relative to the gold label. The correct show is \"Law & Order: Special Victims Unit,\" but the model answered \"The Amanda Show.\" However, the answer is faithful to the cited document, as it explicitly attributes the popularity ranking to Doc 115219, so faithfulness is high even though the final result is wrong.", "main_total_tokens": 3733, "main_prompt_tokens": 3687, "main_completion_tokens": 46, "judge_total_tokens": 529, "judge_prompt_tokens": 427, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 2898.36049079895, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states the rating of \"Bluefin\" as 7.2, matching the gold answer exactly. It is faithful to the provided gold reference and does not introduce any contradictions or unsupported information.", "main_total_tokens": 3092, "main_prompt_tokens": 3070, "main_completion_tokens": 22, "judge_total_tokens": 473, "judge_prompt_tokens": 400, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "There are two movies titled \"The Story of Us\" in the provided documents, and they were released in different years:\n\n1. One was released in [Year: 1999][Doc 22481].\n2. The other was released in [Year: 2019][Doc 88811].", "latency_ms": 3843.8286781311035, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer expects a single year, 2019, while the generated answer lists two different release years (1999 and 2019) and does not clearly identify which one addresses the question, so it is incorrect relative to the gold. However, given the described documents, it accurately reflects the content of those docs (two films with those years), so it is faithful to the provided context.", "main_total_tokens": 3151, "main_prompt_tokens": 3079, "main_completion_tokens": 72, "judge_total_tokens": 556, "judge_prompt_tokens": 445, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The document provided does not contain the number of votes for \"My Dog Skip.\" The popularity score is given as 1.7749, but this is not the same as the number of votes. [Doc 24114] does not specify the number of votes received by the movie.", "latency_ms": 3791.428804397583, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 279 votes, but the generated response claims the document does not contain the number of votes. This is factually incorrect relative to the gold answer, so correctness is 0. The answer also misrepresents what is in the source document by asserting the absence of vote data, so faithfulness is 0.", "main_total_tokens": 3161, "main_prompt_tokens": 3097, "main_completion_tokens": 64, "judge_total_tokens": 532, "judge_prompt_tokens": 439, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The provided documents do not contain information about the number of votes for \"Oh My God!\". Therefore, I cannot cite a specific document for this information. The context only provides rating and popularity scores but does not detail the number of votes received by any of the titles listed.", "latency_ms": 3539.828300476074, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that 'Oh My God!' received 12 votes. The generated answer says the documents do not contain information about the number of votes and thus does not provide the correct number, so it is incorrect. However, given the described context (only rating and popularity, no votes), the assistant accurately reflects that context and does not hallucinate a number, so it is faithful to the provided documents.", "main_total_tokens": 3202, "main_prompt_tokens": 3147, "main_completion_tokens": 55, "judge_total_tokens": 548, "judge_prompt_tokens": 435, "judge_completion_tokens": 113}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "None of the documents provided contain information about the movie \"The Last Days of Lehman Brothers\". Therefore, I cannot determine the release year based on the given context.", "latency_ms": 2957.59916305542, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 2009, but the model said it could not determine the release year, so the answer is incorrect. However, given the constraint that it relied only on the provided documents, the statement about not finding the information in the context is faithful to its sources.", "main_total_tokens": 2856, "main_prompt_tokens": 2822, "main_completion_tokens": 34, "judge_total_tokens": 507, "judge_prompt_tokens": 420, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 2971.2910652160645, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the original language of 'The King's Speech' is English, matching the gold answer 'en'. It does not introduce contradictory or extraneous claims beyond a harmless document reference, so it is fully faithful.", "main_total_tokens": 3185, "main_prompt_tokens": 3161, "main_completion_tokens": 24, "judge_total_tokens": 479, "judge_prompt_tokens": 402, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The provided documents do not contain information about the release year of \"Wall Street.\" Therefore, based solely on the given context, the answer is not found.", "latency_ms": 2492.0897483825684, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 1987, but the model explicitly states that the answer is not found in the provided documents and does not hallucinate a year. Given the constraint to use only the provided context, this is the correct behavior, so correctness is 1. The answer is fully faithful to the given context, so faithfulness is also 1.", "main_total_tokens": 2326, "main_prompt_tokens": 2294, "main_completion_tokens": 32, "judge_total_tokens": 511, "judge_prompt_tokens": 409, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "None of the provided documents directly mention classmates playing a survival game using their cell phones, except for [Doc 45096] which describes \"Tokyo Gore School\" where students are involved in a violent game spread via mobile phones, but it does not specifically describe a survival game played by classmates. Therefore, based strictly on the provided context, there is no exact match to the question asked.", "latency_ms": 3353.363037109375, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific movie, and the correct answer is 'Brain Fluid Explosion Girl.' The generated answer claims there is no exact match in the provided context and does not identify the movie title, so it is factually incorrect. However, within the constraint of using only the provided documents, the assistant faithfully reports that it cannot find an exact match and refrains from guessing, so its reasoning is faithful to the given context.", "main_total_tokens": 2200, "main_prompt_tokens": 2118, "main_completion_tokens": 82, "judge_total_tokens": 582, "judge_prompt_tokens": 465, "judge_completion_tokens": 117}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show where six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" [Doc 112849].", "latency_ms": 2252.4020671844482, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' which matches the gold answer exactly. It remains faithful to the given information and does not introduce contradictory or unsupported content.", "main_total_tokens": 1898, "main_prompt_tokens": 1864, "main_completion_tokens": 34, "judge_total_tokens": 488, "judge_prompt_tokens": 419, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "None of the provided documents describe a movie that matches the description given in the question about two rival gangs in Munich, one composed of German teens and the other of Turkish ones. Therefore, based strictly on the context provided, there is no answer found.", "latency_ms": 2835.350751876831, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the movie title, which is known (from the gold answer) to be 'Nacht der Wölfe'. The generated answer claims no answer can be found. This is incorrect given the gold answer, so correctness is 0. However, the generated answer is faithful to the stated constraint of relying strictly on the provided documents/context and not hallucinating a title it does not see there, so faithfulness is high (1).", "main_total_tokens": 2276, "main_prompt_tokens": 2225, "main_completion_tokens": 51, "judge_total_tokens": 569, "judge_prompt_tokens": 448, "judge_completion_tokens": 121}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of Picnic at Hanging Rock take place in the year 1900 [Doc 126191].", "latency_ms": 2357.447624206543, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (year 1900) and does not add unsupported information. It is both correct and faithful to the provided reference.", "main_total_tokens": 2145, "main_prompt_tokens": 2116, "main_completion_tokens": 29, "judge_total_tokens": 469, "judge_prompt_tokens": 406, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "None of the provided documents describe a movie from 2004 that matches the specific details given in the question about a young violinist named after a calendar month working in an oddball Venice coffee shop and daydreaming about a beautiful man. Therefore, based on the context provided, there is no matching document for this query.", "latency_ms": 3329.010009765625, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the title of a 2004 movie, and the correct answer is 'June'. The generated answer does not provide the movie title and incorrectly claims there is no matching document, so it is factually incorrect. However, given only the provided (but limited) document context mentioned in the response, it faithfully describes that those documents do not contain the answer, so it is faithful to the stated context, albeit incomplete relative to external knowledge.", "main_total_tokens": 2561, "main_prompt_tokens": 2493, "main_completion_tokens": 68, "judge_total_tokens": 619, "judge_prompt_tokens": 497, "judge_completion_tokens": 122}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1171.4677810668945, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which is incorrect because the gold answer specifies a particular movie. However, the model does not fabricate information and honestly admits not knowing, so it is fully faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 399, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 271.69132232666016, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answer contradicts the gold answer, so it is incorrect. However, it honestly admits not knowing and does not fabricate any information, so it is fully faithful to its own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 459, "judge_prompt_tokens": 389, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1069.6940422058105, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3535124404757273, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model’s numeric answer (18) does not match the gold answer (4), so it is incorrect. However, the model did attempt to answer the question as asked (a count of movies meeting the specified year, rating, and language criteria), so it is faithful to the user’s query even though the value is wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 486, "judge_prompt_tokens": 394, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 188.21120262145996, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. However, the evaluation itself is faithful to the provided gold and generated values.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 461, "judge_prompt_tokens": 395, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1522.7787494659424, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answer is factually incorrect because it fails to provide the correct show ('Fantômas') and instead responds with ignorance. However, it is faithful to its own stated uncertainty and does not introduce fabricated information, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 389, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie described as being about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" [Doc 11205].", "latency_ms": 2485.2652549743652, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' which matches the gold answer. It is directly grounded in the provided information and does not introduce unsupported details, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2123, "main_prompt_tokens": 2090, "main_completion_tokens": 33, "judge_total_tokens": 486, "judge_prompt_tokens": 412, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838]. However, it's worth noting that this documentary focuses on Fred Rogers and his television program but isn't specifically described as him speaking directly about life's issues. The plot description for \"Mister Rogers: It's You I Like\" [Doc 86144] also aligns closely with the question, highlighting a celebration of \"Mister Rogers’ Neighborhood,\" where he addresses important issues with children.", "latency_ms": 4289.75510597229, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.5, "justification": "The generated answer correctly identifies the movie title as \"Won't You Be My Neighbor?\", matching the gold answer, so correctness is 1. However, it adds an unnecessary and somewhat conflicting commentary suggesting the documentary is not specifically described as him speaking directly about life's issues and brings in another film (\"Mister Rogers: It's You I Like\") as also aligning with the question. This extra speculation is not grounded in the given QA pair and partially undermines the straightforward answer, reducing faithfulness.", "main_total_tokens": 2443, "main_prompt_tokens": 2318, "main_completion_tokens": 125, "judge_total_tokens": 646, "judge_prompt_tokens": 514, "judge_completion_tokens": 132}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The question does not specify what \"it\" refers to, so I cannot provide a year without more context about which item or title you are asking about. The document contains multiple titles with different release years. Please clarify which specific movie is being referred to as \"it.\" [None of the documents contain a comprehensive answer directly applicable to an unspecified \"it\"].", "latency_ms": 3203.0375003814697, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates the correct year is 2018, so the model should have answered 2018. Instead, it claimed lack of context and did not provide the year, making the answer incorrect (score 0). However, given the model’s own stated understanding and constraints (that it did not know what 'it' referred to), its response is internally consistent and not hallucinated, so it is faithful to its own reasoning and to the provided information (score 1).", "main_total_tokens": 2225, "main_prompt_tokens": 2153, "main_completion_tokens": 72, "judge_total_tokens": 576, "judge_prompt_tokens": 448, "judge_completion_tokens": 128}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "None of the provided documents describe an animation about a young boy robot in 2030 AD. Therefore, based on the given context, there is no direct answer to the question.", "latency_ms": 2626.0123252868652, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer to the question is 'Astro Boy', but the generated answer claims there is no direct answer, so it is factually incorrect. However, within the constraint of relying only on the provided documents, the response is faithful because it correctly states that the documents do not contain the answer.", "main_total_tokens": 2191, "main_prompt_tokens": 2152, "main_completion_tokens": 39, "judge_total_tokens": 514, "judge_prompt_tokens": 423, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 2270.0695991516113, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: both state that the woman and her son were held captive for 7 years. The response is correct and directly derived from the provided information, so it is fully faithful.", "main_total_tokens": 1883, "main_prompt_tokens": 1847, "main_completion_tokens": 36, "judge_total_tokens": 498, "judge_prompt_tokens": 424, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26m before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 2741.720199584961, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26m before the 26 November 1983 break-in, matching the provided gold answer and not adding unsupported claims.", "main_total_tokens": 2357, "main_prompt_tokens": 2313, "main_completion_tokens": 44, "judge_total_tokens": 502, "judge_prompt_tokens": 428, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6263.749122619629, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model said it did not know the answer, but the gold answer is a specific numeric value (80.5772). Therefore the response is incorrect. However, the model did not hallucinate a wrong value; it acknowledged not knowing, which is faithful to its uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 482, "judge_prompt_tokens": 396, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 186.00010871887207, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there was 1 Russian language TV show released in 1997, while the generated answer says 0. This is factually incorrect, so correctness is 0. The model does not invent extra details or contradict any given reasoning beyond the numeric answer itself, so within the limited context it is faithful to its (albeit incorrect) conclusion, giving a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 498, "judge_prompt_tokens": 385, "judge_completion_tokens": 113}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 3069.138765335083, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the TV show as 'Angel Eyes.' There are no contradictions or unsupported claims, and the citation does not introduce any errors.", "main_total_tokens": 2739, "main_prompt_tokens": 2699, "main_completion_tokens": 40, "judge_total_tokens": 495, "judge_prompt_tokens": 429, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"What Is This Film Called Love?\" is English [Doc 58692].", "latency_ms": 2165.942430496216, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the language is Korean, while the generated answer claims it is English, which is factually incorrect relative to the gold. The generated answer also fabricates a document reference [Doc 58692] that is not grounded in the provided context, so it is not faithful.", "main_total_tokens": 2027, "main_prompt_tokens": 2002, "main_completion_tokens": 25, "judge_total_tokens": 486, "judge_prompt_tokens": 397, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 516.58034324646, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is factually incorrect. The model directly answered the question with a specific numeric value and did not hallucinate extra details beyond that incorrect number, so it is faithful to its own (though wrong) reasoning/assumptions.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 394, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Based on the provided documents, none of these movies were released in 1979. All the movies listed are from 1980 or later. Therefore, with the information given, we cannot determine the best-rated movie released specifically in 1979. [Doc 365] - [Doc 127]", "latency_ms": 3371.098518371582, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that there are no movies from 1979 in the database. The generated answer instead claims there are movies, but all are from 1980 or later, and refers to specific documents. This contradicts the gold answer and introduces unsupported document references. Therefore it is both incorrect relative to the task/gold and unfaithful to the provided information.", "main_total_tokens": 2401, "main_prompt_tokens": 2329, "main_completion_tokens": 72, "judge_total_tokens": 561, "judge_prompt_tokens": 456, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "Based on the provided context, there is no information about a Back to the Future Part IV movie being released in any year. The documents list details for parts I through III and other related content but do not mention a fourth part. [Doc 10621], [Doc 9405], [Doc 4834] and [Doc 107962] provide information about the first three movies in the series, with no reference to a Part IV.", "latency_ms": 4265.963077545166, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer correctly states that Back to the Future Part IV does not exist. The generated answer also correctly explains that there is no information or mention of a Part IV in the referenced documents, implying it was never released. It does not contradict the gold and remains grounded in the given context, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2999, "main_prompt_tokens": 2897, "main_completion_tokens": 102, "judge_total_tokens": 589, "judge_prompt_tokens": 490, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.4303455352783203, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that no movies from 1980 about the Chernobyl disaster can be found, which is factually correct because the disaster occurred in 1986. The generated answer does not invent any movies or scores and remains aligned with the information in the gold answer, so it is both correct and faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 532, "judge_prompt_tokens": 434, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 72.74746894836426, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (4), so it is factually incorrect for this QA pair. However, as an evaluator, I have no indication the model contradicted its own prior reasoning or context in producing the answer, so faithfulness to its (unknown) internal reasoning is assumed.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 388, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 167.07968711853027, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there are 2 such movies, while the generated answer says 0. This is factually incorrect relative to the provided gold, so correctness is 0. The generated answer is at least directly responsive to the question and does not hallucinate extra details, so faithfulness to the user query itself is acceptable, hence 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 386, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 136.0001564025879, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show, while the generated answer says 0. This directly contradicts the gold, so the generated answer is factually incorrect relative to the labeled truth, but it is internally consistent with the question (no hallucinated reasoning or extra unsupported details).", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 389, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 134.99188423156738, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer and the generated answer both state that the number of TV shows released between 2000 and 2010 about Super Mario Bros is 0, so the generated answer is fully correct and fully faithful to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 468, "judge_prompt_tokens": 391, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 132.98726081848145, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because a well-known and widely recognized popular ninja-related TV show is 'Naruto', which the gold answer provides. However, the generated answer does not hallucinate any incorrect facts; it simply states lack of knowledge, so it is faithful to its own (claimed) knowledge state.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 386, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The provided documents do not mention any crossover movie between \"Jurassic Park\" and \"Power Rangers\". Therefore, based on the given context, there is no information about when such a crossover movie was released [None of the Documents].", "latency_ms": 2907.2482585906982, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that this movie was not released. The generated answer instead says there is no information about when such a crossover movie was released, which implies uncertainty rather than affirming that it does not exist, so it is incorrect. However, it is faithful to the provided documents by not fabricating a release date and explicitly stating that the documents do not mention such a movie.", "main_total_tokens": 2306, "main_prompt_tokens": 2259, "main_completion_tokens": 47, "judge_total_tokens": 538, "judge_prompt_tokens": 431, "judge_completion_tokens": 107}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Based on the provided context, there is no movie where Barbie fights the Predator. The documents mention movies with titles like \"The Predator\" but none involve Barbie fighting against a Predator. Therefore, the answer to the question is that it's not found in the given context.", "latency_ms": 2987.518787384033, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer is factually correct: there is no released movie in which Barbie fights the Predator. However, the gold answer is more specific, stating that such a movie was not released, while the generated answer frames it as not found in the given context and focuses on the documents. This slightly diverges from the gold rationale, so faithfulness is high but not perfect.", "main_total_tokens": 2475, "main_prompt_tokens": 2420, "main_completion_tokens": 55, "judge_total_tokens": 543, "judge_prompt_tokens": 435, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "None of the documents provided contain information about the chronological order of Star Wars movies or explicitly state which one is the 4th oldest. They mostly detail documentaries and other films related to or inspired by the Star Wars franchise rather than the actual series itself.\n\nTherefore, based strictly on the context given, it's not possible to determine what the 4th oldest Star Wars movie is.", "latency_ms": 3455.228805541992, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer gives a specific title, while the generated answer claims it cannot be determined from the documents. Given the evaluation context, the assistant was expected to provide the gold answer; thus the generated answer is incorrect. However, the generated answer is faithful to the provided documents, as it accurately notes that they do not contain the needed information and refrains from hallucinating an answer.", "main_total_tokens": 2613, "main_prompt_tokens": 2536, "main_completion_tokens": 77, "judge_total_tokens": 571, "judge_prompt_tokens": 464, "judge_completion_tokens": 107}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The question does not specify what \"it\" refers to, so I cannot provide a year without more context about which item or title you are asking about. The document contains multiple titles with different release years. Please clarify which specific movie is being referred to as \"it.\" [None of the documents contain a comprehensive answer directly applicable to an unspecified \"it\"].", "latency_ms": 3201.028823852539, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly indicates the intended referent was released in 1986, so the correct response should have been '1986.' The generated answer claims the referent is unspecified and declines to answer, which is incorrect given the gold. However, the model does remain faithful to the (stated) lack of context from its own perspective and does not hallucinate; it accurately reflects its reasoning about ambiguity, so faithfulness is high even though it is wrong relative to the gold.", "main_total_tokens": 2225, "main_prompt_tokens": 2153, "main_completion_tokens": 72, "judge_total_tokens": 576, "judge_prompt_tokens": 448, "judge_completion_tokens": 128}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 66.01142883300781, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model says it does not know the rating, while the gold answer is a specific numeric rating (8.466). This is incorrect because the model fails to provide the correct answer, but it is faithful since it does not hallucinate a wrong rating and accurately reflects its uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 398, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 2287.937641143799, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med,' which matches the gold answer exactly. It stays faithful to the question and does not introduce contradictions or unrelated information, aside from a benign reference tag.", "main_total_tokens": 1911, "main_prompt_tokens": 1874, "main_completion_tokens": 37, "judge_total_tokens": 497, "judge_prompt_tokens": 423, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In Túnel de los huesos (2011), seven prisoners escaped from the Villa Devoto prison in Argentina [Doc 53582].", "latency_ms": 2558.830976486206, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the country; the generated answer states that the prison was in Argentina, which matches the gold answer. The mention of Villa Devoto prison is an extra but plausible detail and does not contradict the gold.", "main_total_tokens": 2318, "main_prompt_tokens": 2283, "main_completion_tokens": 35, "judge_total_tokens": 495, "judge_prompt_tokens": 420, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "None of the provided documents contain information about the movie \"Sweet Trap\" (2024) or any apprentice falling into someone's trap. Therefore, based on the given context, it is not possible to answer the question.", "latency_ms": 2795.691728591919, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific factual answer (whose trap the apprentice falls into), which is Jiang Jie. The generated answer instead claims the question cannot be answered from the context and does not provide the correct entity, so it is factually incorrect. However, it accurately reflects the stated limitation that the information is not in the provided documents, so with respect to the given context it is faithful, even though it conflicts with the external gold answer.", "main_total_tokens": 2357, "main_prompt_tokens": 2310, "main_completion_tokens": 47, "judge_total_tokens": 546, "judge_prompt_tokens": 427, "judge_completion_tokens": 119}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 399.62100982666016, "retrieval_recall": 0.0, "retrieval_mrr": 0.25, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.16812753627111746, "hit_rate_at_5": 1.0, "recall_at_10": 0.25, "precision_at_10": 0.1, "ndcg_at_10": 0.16812753627111746, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (4), so it is incorrect. However, the answer is a simple numeric response directly addressing the question without adding unsupported details, so it is fully faithful to the (incorrect) content it provides.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 468, "judge_prompt_tokens": 385, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "35", "latency_ms": 32.70149230957031, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer says there are 35 movies, which directly contradicts the gold answer that there are no movies featuring both Bruce Lee and Steven Seagal. This is factually incorrect and not faithful to the provided gold response.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 471, "judge_prompt_tokens": 395, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 2448.0366706848145, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (6.3) and explicitly attributes it to a document, so it is both correct and faithful to the provided evidence.", "main_total_tokens": 2226, "main_prompt_tokens": 2200, "main_completion_tokens": 26, "judge_total_tokens": 467, "judge_prompt_tokens": 403, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in \"Fast Track: No Limits\" is street racing [Doc 41929]. This can be inferred from the plot description which mentions it is centered around a group of street racers.", "latency_ms": 2877.383232116699, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'Drift', but the generated answer states 'street racing', which is incorrect. It also fabricates a specific movie title and citation not present in the prompt, so it is not faithful to the given information.", "main_total_tokens": 2494, "main_prompt_tokens": 2448, "main_completion_tokens": 46, "judge_total_tokens": 492, "judge_prompt_tokens": 419, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 2814.6629333496094, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the title as \"Confronting a Serial Killer,\" which matches the gold answer exactly. It also stays faithful to the provided information without adding unsupported claims, just citing the document reference.", "main_total_tokens": 2261, "main_prompt_tokens": 2211, "main_completion_tokens": 50, "judge_total_tokens": 524, "judge_prompt_tokens": 451, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 2617.9146766662598, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and correctly cites the referenced document, so it is both correct and faithful to the source.", "main_total_tokens": 2550, "main_prompt_tokens": 2520, "main_completion_tokens": 30, "judge_total_tokens": 470, "judge_prompt_tokens": 407, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 2557.152271270752, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('The Sixth Sense') and stays fully grounded in the provided information without adding extraneous or conflicting content.", "main_total_tokens": 2316, "main_prompt_tokens": 2281, "main_completion_tokens": 35, "judge_total_tokens": 480, "judge_prompt_tokens": 420, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 2399.3942737579346, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer and aligning with the cited document.", "main_total_tokens": 2430, "main_prompt_tokens": 2407, "main_completion_tokens": 23, "judge_total_tokens": 455, "judge_prompt_tokens": 397, "judge_completion_tokens": 58}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 2664.3683910369873, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the title as 'The Transporter Refueled'. It stays faithful to the question and does not introduce incorrect or extraneous information beyond a harmless document reference.", "main_total_tokens": 2505, "main_prompt_tokens": 2470, "main_completion_tokens": 35, "judge_total_tokens": 494, "judge_prompt_tokens": 420, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The year 'The Transporter Refueled' was released is 2015 [Doc 67500].", "latency_ms": 2482.9819202423096, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It does not add any unsupported information beyond a generic document citation, so it is both correct and faithful.", "main_total_tokens": 2422, "main_prompt_tokens": 2394, "main_completion_tokens": 28, "judge_total_tokens": 484, "judge_prompt_tokens": 406, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "None of the provided documents mention Abraham Lincoln coming back from the dead in any romantic comedy.", "latency_ms": 2287.6102924346924, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a specific romantic comedy in which Abraham Lincoln comes back from the dead. Both the gold answer and the generated answer state that such a movie cannot be found in the provided information. The generated answer correctly reflects the absence of such a movie in the documents and aligns with the gold response. It does not introduce any unsupported claim or hallucinated title, so it is both correct and faithful to the given context.", "main_total_tokens": 2362, "main_prompt_tokens": 2343, "main_completion_tokens": 19, "judge_total_tokens": 518, "judge_prompt_tokens": 408, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "None of the documents provided contain information about a movie that alludes to a manuscript written by Ernest Hemingway. Therefore, based on the given context, there is no answer found.", "latency_ms": 2618.5286045074463, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the title of a movie that alludes to a manuscript written by Ernest Hemingway, and the gold answer is 'Havana Darkness'. The generated answer says no answer is found, which is incorrect with respect to the gold label, so correctness is 0. However, the generated answer accurately reflects the stated limitation that none of the documents provided contain the required information, so it is faithful to the context, yielding a faithfulness score of 1.", "main_total_tokens": 2312, "main_prompt_tokens": 2274, "main_completion_tokens": 38, "judge_total_tokens": 542, "judge_prompt_tokens": 423, "judge_completion_tokens": 119}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "The TV show where Doll heads to Hollywood to be with her childhood best friend Em is \"Doll & Em\" [Doc 12259].", "latency_ms": 2430.962562561035, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Doll & Em'. There is no added or conflicting information, so it is both correct and faithful.", "main_total_tokens": 2156, "main_prompt_tokens": 2123, "main_completion_tokens": 33, "judge_total_tokens": 486, "judge_prompt_tokens": 418, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "None of the provided documents contain information about a movie from 2007 where a character named Hallam with spying talents attempts to uncover the cause of his mother's death. Therefore, based on the given context, there is no answer to this question.", "latency_ms": 2856.471300125122, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the name of the 2007 movie, which is 'Hallam Foe'. The generated answer claims there is no answer based on the context, which is factually incorrect, so correctness is 0. However, the assistant faithfully followed the (incorrectly limited) premise of using only the provided documents and did not fabricate an answer, so faithfulness to the given context is 1.", "main_total_tokens": 2262, "main_prompt_tokens": 2209, "main_completion_tokens": 53, "judge_total_tokens": 562, "judge_prompt_tokens": 448, "judge_completion_tokens": 114}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Based on the provided context, there is no specific mention of a character's last name that initially sounds like he’s either a friend or foe. The closest reference to a last name causing confusion is in [Doc 96347], where two men with the same last name \"Garcia\" get mixed up during a business trip, but this does not directly answer whether his last name sounds at first like he's a friend or a foe. Therefore, based strictly on the context provided, there isn't enough information to conclude that any character’s last name initially suggests they are either friendly or hostile.", "latency_ms": 4136.748313903809, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "Gold answer is a factual label: the last name sounds like a foe. The generated answer claims there is not enough information and does not provide the correct label, so it is incorrect. However, it consistently reasons from the (assumed) provided context and does not hallucinate extra details, so it is faithful to that context.", "main_total_tokens": 2092, "main_prompt_tokens": 1968, "main_completion_tokens": 124, "judge_total_tokens": 600, "judge_prompt_tokens": 503, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "According to the context provided, in the 2017 TV show Atypical, Sam is 18 years old [Doc 124583].", "latency_ms": 2667.7634716033936, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, matching the gold answer exactly. It also attributes this to the provided context, so it is faithful to the given information.", "main_total_tokens": 2439, "main_prompt_tokens": 2403, "main_completion_tokens": 36, "judge_total_tokens": 494, "judge_prompt_tokens": 416, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 2504.8253536224365, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the 2017 TV show Atypical, Sam is on the autism spectrum, which matches the gold answer exactly. It is fully faithful to the provided information and does not introduce unsupported claims.", "main_total_tokens": 2349, "main_prompt_tokens": 2320, "main_completion_tokens": 29, "judge_total_tokens": 487, "judge_prompt_tokens": 410, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "In the movie \"À corde tendue,\" which was released in 2013, Sandra was born in Chamonix [Doc 59659].", "latency_ms": 2862.6115322113037, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, which matches the gold answer exactly. It remains faithful to the given information without adding unsupported details.", "main_total_tokens": 2667, "main_prompt_tokens": 2630, "main_completion_tokens": 37, "judge_total_tokens": 491, "judge_prompt_tokens": 427, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In the Japanese language movie \"When the Rain Falls,\" Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 2965.8243656158447, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.5, "justification": "The key factual content needed to fill the blank is 'rains', and the generated answer correctly supplies this, so correctness is 1. However, the generated answer fabricates extra details not supported by the prompt or gold (the specific movie title 'When the Rain Falls' and a document citation), so faithfulness is reduced.", "main_total_tokens": 3058, "main_prompt_tokens": 3027, "main_completion_tokens": 31, "judge_total_tokens": 516, "judge_prompt_tokens": 417, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 2766.0324573516846, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Greenland as the place where Tabaluga is crown-prince, matching the gold answer exactly. It is also fully supported by the referenced document, so it is both correct and faithful.", "main_total_tokens": 2608, "main_prompt_tokens": 2573, "main_completion_tokens": 35, "judge_total_tokens": 497, "judge_prompt_tokens": 424, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie that matches the given description is \"One Afternoon\" [Doc 96838]. The plot describes a strict father and his two boys living a calm life in an old home until one summer moment changes everything for the brothers, which directly corresponds to the provided details.", "latency_ms": 2797.193765640259, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'One Afternoon,' which matches the gold answer. The explanation is grounded in the given description and aligns with the query, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2008, "main_prompt_tokens": 1949, "main_completion_tokens": 59, "judge_total_tokens": 531, "judge_prompt_tokens": 458, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "None of the provided documents contain information about a low-rated Italian language movie where a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her. Therefore, based on the given context, there is no answer to this question.", "latency_ms": 2289.273262023926, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "recall_at_10": 0.0, "precision_at_5": 0.0, "precision_at_10": 0.0, "ndcg_at_5": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_5": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'The Life Ahead'. The generated answer claims there is no answer based on the context, which is factually incorrect, so correctness is 0. However, it accurately reflects the absence of the information in the provided documents, so it is faithful to the given context, hence faithfulness is 1.", "main_total_tokens": 2435, "main_prompt_tokens": 2384, "main_completion_tokens": 51, "judge_total_tokens": 543, "judge_prompt_tokens": 447, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859]. It tells the story of how a man with humble beginnings could take on the elite of motor racing and win.", "latency_ms": 3139.169216156006, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and elaborates in a way that is consistent with the gold answer, without adding conflicting or unfounded details.", "main_total_tokens": 2494, "main_prompt_tokens": 2434, "main_completion_tokens": 60, "judge_total_tokens": 509, "judge_prompt_tokens": 439, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "None of the documents provided contain information about a racing team associated with an individual. Therefore, based on the given context, there is no name for his racing team that can be extracted.", "latency_ms": 2796.5645790100098, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the racing team is 'The McLaren Motor Racing team.' The generated answer claims that no such information is available and thus provides no team name, which is factually incorrect relative to the gold. However, the generated answer is faithful to its own stated limitation (it bases its response on the absence of information in the provided documents), so it does not hallucinate a team name or contradict its described context.", "main_total_tokens": 2140, "main_prompt_tokens": 2102, "main_completion_tokens": 38, "judge_total_tokens": 537, "judge_prompt_tokens": 420, "judge_completion_tokens": 117}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The question \"What country is he from?\" does not specify which 'he' is being referred to, making it impossible to provide a precise answer based on the provided documents. None of the documents directly state a specific country for an individual referred to as 'he'. Therefore, without additional context about who 'he' refers to, I cannot determine what country he is from.", "latency_ms": 3129.3530464172363, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the correct country is New Zealand, so the generated answer is factually incorrect and gets a correctness score of 0. However, given only the ambiguous question and the assistant’s access to information as described in the generated answer, it correctly states that it cannot determine the country; it does not hallucinate or contradict its own evidence, so it is faithful to its stated context, yielding a faithfulness score of 1.", "main_total_tokens": 2087, "main_prompt_tokens": 2011, "main_completion_tokens": 76, "judge_total_tokens": 571, "judge_prompt_tokens": 451, "judge_completion_tokens": 120}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "None of the provided documents mention John McClane blowing up the great wall of China in any 2013 movie. [Doc 72056] mentions \"The Great Wall\" but it is from 2016, not 2013 and does not involve John McClane.", "latency_ms": 2921.6129779815674, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly indicates that there is no 2013 movie in which John McClane blows up the Great Wall of China, so the correctness relative to the question is high. However, compared to the gold answer, which explicitly says the database lacks such a movie, the generated answer introduces references to specific documents and a 2016 film that are not present in the gold answer. This adds ungrounded details beyond the provided gold response, so faithfulness to the gold is low.", "main_total_tokens": 2135, "main_prompt_tokens": 2070, "main_completion_tokens": 65, "judge_total_tokens": 585, "judge_prompt_tokens": 455, "judge_completion_tokens": 130}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie that matches the given description is \"Tomie: Forbidden Fruit\" [Doc 28589]. This film's plot aligns with the provided details, where a lonely high school girl befriends another mysterious and beautiful girl, hinting at a non-accidental meeting.", "latency_ms": 3220.470666885376, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Tomie: Forbidden Fruit,' which matches the gold answer exactly. The explanation remains consistent with the question and does not introduce contradictions or unsupported claims, so it is fully faithful to the information provided.", "main_total_tokens": 2432, "main_prompt_tokens": 2364, "main_completion_tokens": 68, "judge_total_tokens": 557, "judge_prompt_tokens": 478, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie \"Lei mi parla ancora,\" Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 2853.510618209839, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years) and directly addresses the question without adding unsupported information. It is both correct and faithful to the gold reference.", "main_total_tokens": 2697, "main_prompt_tokens": 2658, "main_completion_tokens": 39, "judge_total_tokens": 488, "judge_prompt_tokens": 424, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The provided documents do not contain information about \"Scorpion King: Book of Souls (2018)\" or any search conducted by The Scorpion King and Tala. Therefore, based on the given context, it is not possible to determine what The Scorpion King and Tala are searching for [None of the docs provide this information].", "latency_ms": 3092.6783084869385, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks specifically what The Scorpion King and Tala search for in the movie, and the correct answer is 'The Book of Souls.' The generated answer states that it is not possible to determine what they are searching for from the provided documents, which is incorrect with respect to the gold answer. However, the generated answer is faithful to the given context, as it explicitly notes the lack of information in the documents and does not fabricate an answer.", "main_total_tokens": 2167, "main_prompt_tokens": 2097, "main_completion_tokens": 70, "judge_total_tokens": 579, "judge_prompt_tokens": 459, "judge_completion_tokens": 120}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"Zombies on the Moon\". Therefore, based strictly on the given context, the answer to the question is that the release year for \"Zombies on the Moon\" is not found.", "latency_ms": 2967.6284790039062, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer state that the release year is not available / not found. The generated response correctly and faithfully reflects the lack of information in the provided documents without hallucinating a year, so it is both correct and faithful.", "main_total_tokens": 2498, "main_prompt_tokens": 2448, "main_completion_tokens": 50, "judge_total_tokens": 520, "judge_prompt_tokens": 440, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document does not provide information about the release year of \"The Galactic baker\". [Doc 17000] [Doc 9787] [Doc 3431] [Doc 116276] [Doc 116547] [Doc 113129] [Doc 7967] [Doc 8286] [Doc 10391] [Doc 8296] [Doc 113129] [Doc 113528] [Doc 40] [Doc 217] [Doc 7438] [Doc 113129] [Doc 17000] [Doc 116276] [Doc 116547] [Doc 113528] [Doc 8296] [Doc 113129] [Doc 113129] [Doc 113129] [Doc 113129] [Doc 113129] [Doc 1", "latency_ms": 6835.997819900513, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the movie is not in the database, while the generated answer incorrectly asserts that a specific document does not provide the release year, referencing document IDs that are not in the given context. This is both incorrect relative to the gold and unfaithful to the provided information, as it fabricates document references and claims about their contents.", "main_total_tokens": 2940, "main_prompt_tokens": 2684, "main_completion_tokens": 256, "judge_total_tokens": 663, "judge_prompt_tokens": 563, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain any information regarding \"Return of the Null Pointer.\" Therefore, based on the given context, the release year for \"Return of the Null Pointer\" is not found.", "latency_ms": 2592.790365219116, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie not in the database. The gold answer says it is not in the database. The generated answer similarly states that the documents do not contain any information and that the release year is not found, without hallucinating a year. This is both correct and faithful to the absence of information.", "main_total_tokens": 2278, "main_prompt_tokens": 2238, "main_completion_tokens": 40, "judge_total_tokens": 527, "judge_prompt_tokens": 430, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"Data Science: The Movie\". Therefore, based on the given context, it is not possible to determine the year when \"Data Science: The Movie\" was released [None Found].", "latency_ms": 2774.9485969543457, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year cannot be determined / the movie is not in the available information. The question cannot be answered from the context, and the generated response correctly and faithfully reflects that limitation without hallucinating a year.", "main_total_tokens": 2249, "main_prompt_tokens": 2200, "main_completion_tokens": 49, "judge_total_tokens": 519, "judge_prompt_tokens": 439, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain any information about a movie titled \"Python's Revenge 2\". Therefore, based on the given context, the answer to the question is that the release year for \"Python's Revenge 2\" is not found.", "latency_ms": 2814.4757747650146, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the information about the movie or its release year is not available in the given context/database. The generated answer correctly reflects the lack of information in the provided documents and does not hallucinate a release year, so it is both correct and faithful.", "main_total_tokens": 2284, "main_prompt_tokens": 2234, "main_completion_tokens": 50, "judge_total_tokens": 527, "judge_prompt_tokens": 440, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about the release year of \"The Last Pixel.\" [None]", "latency_ms": 2075.505018234253, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates that the movie is not in the database, and the generated answer correctly states that the provided documents do not contain the release year information. Both convey that the information is unavailable, so the generated answer is both correct and faithful to the gold.", "main_total_tokens": 1989, "main_prompt_tokens": 1968, "main_completion_tokens": 21, "judge_total_tokens": 492, "judge_prompt_tokens": 409, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a title called \"The man who ate the sun\". Therefore, based on the given context, it is not possible to determine in what year this particular work was released [None of the documents provide information about \"The man who ate the sun\"].", "latency_ms": 2913.3658409118652, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year cannot be determined, which aligns with the gold answer indicating the movie is not in the database. It faithfully reflects the lack of information in the provided documents without fabricating a year or details.", "main_total_tokens": 2215, "main_prompt_tokens": 2157, "main_completion_tokens": 58, "judge_total_tokens": 527, "judge_prompt_tokens": 449, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about who directed the movie \"The Last Days of Lehman Brothers\". [Doc 46069] provides details about the plot, rating, and other information but does not mention the director. Therefore, based on the given context, I cannot determine who directed this movie.", "latency_ms": 3120.9146976470947, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the provided documents do not include the director information and therefore the question cannot be answered from the given context. This matches the gold answer and does not introduce any external or hallucinated information, so it is both correct and fully faithful to the provided evidence.", "main_total_tokens": 2394, "main_prompt_tokens": 2329, "main_completion_tokens": 65, "judge_total_tokens": 545, "judge_prompt_tokens": 458, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "None of the documents provided contain information about a movie titled \"A Love to Die For,\" nor is there any mention of its director. Therefore, based strictly on the context given, it's not possible to answer the question.", "latency_ms": 2874.831438064575, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director cannot be determined from the provided documents and does not hallucinate a director name. This matches the gold response and is fully grounded in the given context.", "main_total_tokens": 2491, "main_prompt_tokens": 2445, "main_completion_tokens": 46, "judge_total_tokens": 508, "judge_prompt_tokens": 439, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about who directed the movie \"Journey into Gaza\". [Doc 111360] provides details about the plot and other metadata but does not mention a director. Therefore, based strictly on the context given, I cannot determine who directed the movie \"Journey into Gaza\".", "latency_ms": 3378.546953201294, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director of 'Journey into Gaza' cannot be determined from the provided documents and explicitly notes that the context lacks director information, matching the gold response. It is both factually correct and fully faithful to the given context.", "main_total_tokens": 2703, "main_prompt_tokens": 2637, "main_completion_tokens": 66, "judge_total_tokens": 533, "judge_prompt_tokens": 452, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not contain information about who directed the movie \"The Last Night at Tremore Beach\". [Doc 135098] provides details about the plot and other metadata for the title, but it does not mention the director.", "latency_ms": 2651.050090789795, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director information is not present in the provided context, matching the gold answer in substance. It does not introduce any unsupported facts and is fully grounded in the given document reference, so both correctness and faithfulness are maximal.", "main_total_tokens": 2041, "main_prompt_tokens": 1989, "main_completion_tokens": 52, "judge_total_tokens": 525, "judge_prompt_tokens": 444, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" [Doc 98375]. In this film, the students not only awaken the mummy but also find themselves trapped in a time loop where they must defeat the mummy to escape.", "latency_ms": 2954.8864364624023, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.7, "justification": "The generated answer correctly identifies the movie as 'Rise of the Mummy', matching the gold answer, so correctness is 1. However, the added detail about the students being trapped in a time loop and needing to defeat the mummy is extra content not supported by the gold reference and may be fabricated, so faithfulness is reduced.", "main_total_tokens": 2182, "main_prompt_tokens": 2117, "main_completion_tokens": 65, "judge_total_tokens": 546, "judge_prompt_tokens": 447, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The plot described in the question matches with the movie \"Stranger in My Bed\" [Doc 35237]. This is where Sarah fakes her own death to escape her abusive spouse, who subsequently hunts her down.", "latency_ms": 2696.0203647613525, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Stranger in My Bed,' which matches the gold answer. It does not introduce contradictions or unsupported claims beyond restating and slightly elaborating on the plot, remaining faithful to the given information.", "main_total_tokens": 2166, "main_prompt_tokens": 2118, "main_completion_tokens": 48, "judge_total_tokens": 513, "judge_prompt_tokens": 440, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie in which a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years is \"Game of Silence\" [Doc 123938].", "latency_ms": 2332.4172496795654, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'Game of Silence' and the generated answer correctly identifies 'Game of Silence' as well, matching exactly. It does not introduce contradictions or extra unsupported information beyond restating the plot clue, so it is both correct and faithful.", "main_total_tokens": 1892, "main_prompt_tokens": 1853, "main_completion_tokens": 39, "judge_total_tokens": 511, "judge_prompt_tokens": 430, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie where a killer is based on a true story involving Craigslist is titled \"The Craigslist Killer\" [Doc 53039]. This film, released in 2011, tells the story of Philip Markoff, a medical student at Boston University who leads a double life as a brutal sexual deviant, using Craigslist to find and abuse prostitutes.", "latency_ms": 3233.8576316833496, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer' and provides an accurate brief description that is consistent with known information about the film. It remains faithful to the given gold answer and does not introduce contradictions.", "main_total_tokens": 2210, "main_prompt_tokens": 2135, "main_completion_tokens": 75, "judge_total_tokens": 535, "judge_prompt_tokens": 461, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie where Yuki Hirano fails his university entrance exam and then sets his eyes on the role of a forestry trainee is \"Wood Job!\" [Doc 66094].", "latency_ms": 3179.938554763794, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", which matches the gold answer exactly, and it stays faithful to the provided information without adding incorrect details.", "main_total_tokens": 3047, "main_prompt_tokens": 3007, "main_completion_tokens": 40, "judge_total_tokens": 499, "judge_prompt_tokens": 435, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure is titled \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 3082.4344158172607, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, \"Sesame Street: The Magical Wand Chase,\" and correctly restates the question context. There is no added incorrect information, so it is both correct and faithful.", "main_total_tokens": 2577, "main_prompt_tokens": 2522, "main_completion_tokens": 55, "judge_total_tokens": 533, "judge_prompt_tokens": 460, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The plot described happens in the movie titled \"Beyond\" [Doc 74597]. This matches exactly with the scenario where a 7-year-old boy must find heroism within himself to save his parents trapped inside a burning car due to a rock fall.", "latency_ms": 2733.8595390319824, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title matching the described plot. The gold answer is \"Beyond\". The generated answer also identifies the movie as \"Beyond\" and restates that this matches the given scenario. There are no contradictions or hallucinations about different titles; the reference to a document ID does not alter the core answer. Thus, it is fully correct and faithful to the gold answer.", "main_total_tokens": 2082, "main_prompt_tokens": 2027, "main_completion_tokens": 55, "judge_total_tokens": 568, "judge_prompt_tokens": 459, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman, fired from a financial corporation during the Asia crisis, returns home with no money is \"6ixtynin9\" [Doc 22478].", "latency_ms": 2517.8685188293457, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the film as \"6ixtynin9.\" It is correct and fully supported by the provided information, with no unsupported additions.", "main_total_tokens": 2102, "main_prompt_tokens": 2061, "main_completion_tokens": 41, "judge_total_tokens": 507, "judge_prompt_tokens": 439, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The described scenario occurs in the movie \"Club Dread\" [Doc 32410]. This movie involves a serial killer disrupting activities at Coconut Pete's Coconut Beach Resort, which is subsequently dealt with or concealed by the club's staff.", "latency_ms": 2858.5877418518066, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread', matching the gold answer exactly. The added description is consistent with the plot and does not introduce contradictions or hallucinations, maintaining faithfulness to the question and given information.", "main_total_tokens": 2415, "main_prompt_tokens": 2365, "main_completion_tokens": 50, "judge_total_tokens": 548, "judge_prompt_tokens": 471, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie where a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is titled \"Toilet\" [Doc 48873].", "latency_ms": 3137.838840484619, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" which matches the gold answer. It stays faithful to the given information without adding unsupported details, only slightly rephrasing the question into a declarative answer.", "main_total_tokens": 2614, "main_prompt_tokens": 2558, "main_completion_tokens": 56, "judge_total_tokens": 539, "judge_prompt_tokens": 465, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie that matches the description provided is \"Electric Angel\" [Doc 1024]. This experimental film uses various audiovisual techniques to create a sense of polymorphic eroticism as developed by European and Mediterranean cinematography in the 20th century, combining animation and live action methods.", "latency_ms": 3069.8676109313965, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Electric Angel,' which matches the gold label. It stays faithful to the given description, only modestly elaborating on techniques in a way that is consistent with the prompt and not contradicted by it.", "main_total_tokens": 2450, "main_prompt_tokens": 2389, "main_completion_tokens": 61, "judge_total_tokens": 552, "judge_prompt_tokens": 472, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 based on the provided documents is \"Fall\" with a rating of 4.0 [Doc 106810].", "latency_ms": 1865.7703399658203, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold label identifies \"Mega Ape\" as the lowest rated movie from 2023, while the model answer claims it is \"Fall\" with a rating of 4.0. This contradicts the gold answer, so correctness is 0. It is also unfaithful to the provided gold reference, so faithfulness is 0.", "main_total_tokens": 2359, "main_prompt_tokens": 2321, "main_completion_tokens": 38, "judge_total_tokens": 513, "judge_prompt_tokens": 412, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 1204.3232917785645, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Tea House', but the model responded with 'I don't know.' This is incorrect because a correct answer existed and was provided in the gold. However, the response is faithful in that it does not hallucinate or contradict its own stated uncertainty; it honestly indicates lack of knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 477, "judge_prompt_tokens": 386, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 539.6640300750732, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because the gold label specifies a concrete movie title, while the model responded with not knowing the answer. However, the answer is faithful in that it does not invent or hallucinate any incorrect specific movie title; it simply admits ignorance.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 479, "judge_prompt_tokens": 395, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 based on the provided documents is \"Cataclysm\" with a rating of 3.889 [Doc 744]. The language of this movie is English [Doc 744].", "latency_ms": 3183.018207550049, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the language of the lowest-rated movie from 1980 is English. The generated answer also concludes that the movie (\"Cataclysm\") is in English, explicitly citing the document. This matches the gold answer and correctly uses the provided information, so it is both correct and faithful.", "main_total_tokens": 2555, "main_prompt_tokens": 2501, "main_completion_tokens": 54, "judge_total_tokens": 522, "judge_prompt_tokens": 428, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "Based on the provided documents, the TV show with the highest popularity in 1999 was \"The Amanda Show\" with a popularity rating of 3.4197 [Doc 115219].", "latency_ms": 2923.539876937866, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect: the gold label indicates that the TV show with the highest popularity in 1999 was 'Law & Order: Special Victims Unit', but the answer claims it was 'The Amanda Show'. However, the answer is faithful to the cited document, explicitly stating it is based on the provided documents and giving a specific popularity rating from Doc 115219, so it does not hallucinate beyond its evidence.", "main_total_tokens": 2581, "main_prompt_tokens": 2533, "main_completion_tokens": 48, "judge_total_tokens": 546, "judge_prompt_tokens": 429, "judge_completion_tokens": 117}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 2380.8348178863525, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 for \"Bluefin\" and does not introduce contradictions. It appears directly grounded in the provided document citation [Doc 79355].", "main_total_tokens": 2412, "main_prompt_tokens": 2390, "main_completion_tokens": 22, "judge_total_tokens": 470, "judge_prompt_tokens": 400, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in two different years according to the provided documents:\n\n1. [Doc 22481] The first document indicates that it was released in 1999.\n2. [Doc 88811] The second document suggests a release year of 2019.\n\nTherefore, \"The Story of Us\" refers to two separate movies with different release years.", "latency_ms": 3621.368885040283, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer specifies 2019 as the correct release year, but the generated response claims there are two movies with different release years (1999 and 2019) and does not directly answer the question with the single correct year. This is incorrect relative to the gold. It is also unfaithful to the task, which asks for the year of the movie, not for an analysis of multiple documents or multiple films; the model introduces an extra 1999 release that contradicts the gold answer.", "main_total_tokens": 2425, "main_prompt_tokens": 2335, "main_completion_tokens": 90, "judge_total_tokens": 595, "judge_prompt_tokens": 463, "judge_completion_tokens": 132}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "1", "latency_ms": 32.69243240356445, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 279 votes, but the generated answer is 1, which is numerically incorrect. The model clearly attempted to answer the question, so it is faithful to the user query, but the content is wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 460, "judge_prompt_tokens": 383, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "7", "latency_ms": 32.70220756530762, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 12 votes but the generated answer says 7 votes, so the content is factually incorrect. However, the model directly answers the question without adding unsupported details, so it is faithful to the (incorrect) stated value.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 463, "judge_prompt_tokens": 383, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The provided documents do not contain information about the movie \"The Last Days of Lehman Brothers\". Therefore, based on the given context, it's not possible to determine the release year of that specific movie [None Found].", "latency_ms": 2718.1193828582764, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of the movie, which is 2009. The generated answer states that the year cannot be determined from the provided documents, which is incorrect relative to the gold answer. However, the generated response accurately reflects the limitation of the given context and does not fabricate an answer, so it is fully faithful to the described constraints but incorrect in terms of the task evaluation.", "main_total_tokens": 2249, "main_prompt_tokens": 2204, "main_completion_tokens": 45, "judge_total_tokens": 540, "judge_prompt_tokens": 430, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 2448.2979774475098, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the language is English, and the generated answer correctly states that the original language is English. The citation tag does not contradict the claim, so the response is both correct and faithful.", "main_total_tokens": 2420, "main_prompt_tokens": 2396, "main_completion_tokens": 24, "judge_total_tokens": 473, "judge_prompt_tokens": 402, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "I don't know.", "latency_ms": 2207.0159912109375, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 1987, but the model said it did not know. This is factually incorrect, so correctness is 0. However, the model did not fabricate an answer and honestly stated uncertainty, so it is fully faithful to its own reasoning, giving faithfulness 1.", "main_total_tokens": 653, "main_prompt_tokens": 582, "main_completion_tokens": 71, "judge_total_tokens": 472, "judge_prompt_tokens": 382, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Tokyo Gore School", "latency_ms": 4748.114109039307, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer, 'Tokyo Gore School', is incorrect. The correct movie matching the description is 'Brain Fluid Explosion Girl'. However, the model remains faithful to its own (incorrect) output and does not contradict it, so faithfulness is high while correctness is low.", "main_total_tokens": 2865, "main_prompt_tokens": 2752, "main_completion_tokens": 113, "judge_total_tokens": 476, "judge_prompt_tokens": 391, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 4324.311971664429, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question refers to the animated TV show where six friends are transported to a fantasy world and guided by the Dungeon Master to find a way home. Both the gold answer and the generated answer correctly give the title as 'Dungeons & Dragons.' The generated answer exactly matches the gold and is both correct and faithful.", "main_total_tokens": 2544, "main_prompt_tokens": 2444, "main_completion_tokens": 100, "judge_total_tokens": 487, "judge_prompt_tokens": 394, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "I don't know.", "latency_ms": 2007.7643394470215, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model response is incorrect because the correct answer is 'Nacht der Wölfe', while the generated answer says 'I don't know.' However, the answer is faithful to the model's uncertainty and does not hallucinate any incorrect movie title.", "main_total_tokens": 643, "main_prompt_tokens": 577, "main_completion_tokens": 66, "judge_total_tokens": 482, "judge_prompt_tokens": 402, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900 [Doc 126191]", "latency_ms": 4151.882886886597, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (1900) and is fully supported by the provided information.", "main_total_tokens": 2825, "main_prompt_tokens": 2736, "main_completion_tokens": 89, "judge_total_tokens": 446, "judge_prompt_tokens": 394, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The Friends At The Margherita Café", "latency_ms": 543.1921482086182, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a 2004 movie whose title is also a calendar month and describes a young violinist; the gold answer is 'June'. The generated answer 'The Friends At The Margherita Café' is incorrect. It does not match the gold answer, and there is no support in the question for this alternative title, so it is unfaithful to the given reference.", "main_total_tokens": 174, "main_prompt_tokens": 165, "main_completion_tokens": 9, "judge_total_tokens": 549, "judge_prompt_tokens": 440, "judge_completion_tokens": 109}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 106.02164268493652, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is known and provided in the gold, but the generated answer says 'I don't know,' which is incorrect. However, the model did not fabricate an incorrect movie title and instead admitted uncertainty, so it remains faithful to its own stated knowledge limits.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 482, "judge_prompt_tokens": 399, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 122.70450592041016, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 7.6, but the model said it did not know, which is incorrect. However, it did not hallucinate or provide any fabricated information, so the response is fully faithful to the model's stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 468, "judge_prompt_tokens": 389, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "11", "latency_ms": 2436.784029006958, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3535124404757273, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (11) does not match the gold answer (4), so it is incorrect. However, the model clearly attempts to answer the specific question asked (number of movies matching the criteria), so it is faithful to the user query.", "main_total_tokens": 1588, "main_prompt_tokens": 1561, "main_completion_tokens": 27, "judge_total_tokens": 474, "judge_prompt_tokens": 394, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 105.33404350280762, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, while the generated answer asserts there are none matching the criteria, so the content is factually incorrect. However, the model does not fabricate specific numbers or details and instead admits inability to find matches, so it is faithful to its own (limited) retrieval rather than hallucinating an answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 499, "judge_prompt_tokens": 404, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 141.20960235595703, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific TV show and the gold answer is 'Fantômas'. The generated answer is 'I don't know,' which does not provide the correct show, so correctness is 0. However, saying 'I don't know' is honest and does not fabricate information, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 479, "judge_prompt_tokens": 389, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise", "latency_ms": 3080.310821533203, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the movie as 'Kill Cruise.'", "main_total_tokens": 2731, "main_prompt_tokens": 2700, "main_completion_tokens": 31, "judge_total_tokens": 436, "judge_prompt_tokens": 387, "judge_completion_tokens": 49}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 4457.981586456299, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the film title.", "main_total_tokens": 3049, "main_prompt_tokens": 2958, "main_completion_tokens": 91, "judge_total_tokens": 454, "judge_prompt_tokens": 408, "judge_completion_tokens": 46}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018 [Doc 84838]", "latency_ms": 4725.791454315186, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer gives the correct year (2018), which matches the gold answer, and it does not introduce any contradictory or extraneous claims. The bracketed document reference does not alter the factual content.", "main_total_tokens": 3191, "main_prompt_tokens": 3099, "main_completion_tokens": 92, "judge_total_tokens": 458, "judge_prompt_tokens": 386, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "I don't know.", "latency_ms": 1991.3341999053955, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Astro Boy'. The generated answer 'I don't know.' is incorrect. However, it does not introduce any hallucinated or contradictory information; it simply expresses a lack of knowledge, so it is faithful to the model's stated uncertainty.", "main_total_tokens": 665, "main_prompt_tokens": 602, "main_completion_tokens": 63, "judge_total_tokens": 474, "judge_prompt_tokens": 391, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 years [Doc 67513]", "latency_ms": 3660.2108478546143, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly states that they were held captive for 7 years, matching the gold answer, so correctness is 1. However, it includes an unexplained citation-like reference '[Doc 67513]' that does not clearly correspond to the provided context, so faithfulness to the given information is not fully assured, resulting in a faithfulness score of 0.", "main_total_tokens": 2497, "main_prompt_tokens": 2421, "main_completion_tokens": 76, "judge_total_tokens": 508, "judge_prompt_tokens": 403, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m [Doc 133469]", "latency_ms": 2211.7390632629395, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the provided gold answer exactly (£26m), and it clearly uses only the information given in the prompt, so it is both correct and faithful.", "main_total_tokens": 2341, "main_prompt_tokens": 2326, "main_completion_tokens": 15, "judge_total_tokens": 468, "judge_prompt_tokens": 404, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 405.9491157531738, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score, which is provided in the gold answer as 80.5772. The generated answer says 'I don't know,' which does not provide the required value and is therefore incorrect. However, the generated response does not hallucinate or fabricate information; it honestly states a lack of knowledge, so it is fully faithful to the model’s uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 504, "judge_prompt_tokens": 396, "judge_completion_tokens": 108}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 115.60559272766113, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 Russian language TV show released in 1997, but the generated answer claims there are none. This is factually incorrect with respect to the gold, so correctness is 0. However, the generated answer does not invent specific incorrect facts; it simply states inability to find matches, which is consistent with the model’s apparent lack of information, so faithfulness to its own reasoning is high, thus faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 518, "judge_prompt_tokens": 394, "judge_completion_tokens": 124}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes [Doc 121928]", "latency_ms": 4836.705923080444, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Angel Eyes') and does not introduce any unsupported information beyond the correct title, so it is both correct and faithful.", "main_total_tokens": 3441, "main_prompt_tokens": 3348, "main_completion_tokens": 93, "judge_total_tokens": 466, "judge_prompt_tokens": 403, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of this movie is English [Doc 14842]. However, there are multiple movies titled \"Angel Eyes\" listed here, and they all have the language as English except for one which is Korean [Doc 121928]. But since the question does not specify which \"Angel Eyes\" movie it refers to, I will provide the most common answer based on the documents provided.", "latency_ms": 6748.629093170166, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.4, "justification": "The gold answer clearly states the language is Korean, but the generated answer concludes the language is English, so it is factually incorrect. It also explicitly chooses English as the “most common answer” despite acknowledging that one listed movie titled \"Angel Eyes\" is Korean, which contradicts the gold label. This reasoning departs from the ground truth and introduces an incorrect heuristic, reducing faithfulness to the task and gold answer.", "main_total_tokens": 3343, "main_prompt_tokens": 3149, "main_completion_tokens": 194, "judge_total_tokens": 575, "judge_prompt_tokens": 453, "judge_completion_tokens": 122}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 476.1693477630615, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 2 such movies, but the generated answer claims there are none, which is factually incorrect, so correctness is 0. The answer is internally consistent with its own (incorrect) premise and does not contradict itself or invent extra details, so faithfulness to its own reasoning is high, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 505, "judge_prompt_tokens": 403, "judge_completion_tokens": 102}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Raging Bull", "latency_ms": 6031.127214431763, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that there are no movies from 1979 in the database, so the system should respond with that limitation. The generated answer names 'Raging Bull', which is incorrect both because it contradicts the stated database limitation and because 'Raging Bull' was released in 1980, not 1979. Therefore it is neither correct relative to the gold nor faithful to the given constraints.", "main_total_tokens": 3094, "main_prompt_tokens": 2923, "main_completion_tokens": 171, "judge_total_tokens": 511, "judge_prompt_tokens": 398, "judge_completion_tokens": 113}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "I don't know.", "latency_ms": 2203.7265300750732, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is not strictly correct because the appropriate response should indicate that no such movie exists, not simply say 'I don't know.' However, it is faithful in that it does not hallucinate a release year or contradict the provided information.", "main_total_tokens": 696, "main_prompt_tokens": 623, "main_completion_tokens": 73, "judge_total_tokens": 483, "judge_prompt_tokens": 404, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that there are no movies from 1980 about the Chernobyl disaster, which is correct because the Chernobyl disaster occurred in 1986. The generated answer is faithful to the information in the gold answer, differing only in wording and an added suggestion to re-check the time period, but not contradicting the core fact.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 539, "judge_prompt_tokens": 434, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 66.00069999694824, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 titles, while the generated answer claims there are none, which is factually incorrect, so correctness is 0. The generated answer is internally consistent and does not hallucinate specific incorrect titles or numbers; it simply states inability to find any, so it is faithful to its own (albeit incorrect) retrieval limits, giving faithfulness a 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 506, "judge_prompt_tokens": 397, "judge_completion_tokens": 109}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 3010.9994411468506, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer claims there are no such movies, which is factually incorrect relative to the gold; thus correctness is 0. The model does honestly state its inability to find matches instead of fabricating a number, so it is faithful to its (claimed) knowledge limits, giving faithfulness a 1.", "main_total_tokens": 3625, "main_prompt_tokens": 3623, "main_completion_tokens": 2, "judge_total_tokens": 496, "judge_prompt_tokens": 395, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 66.01190567016602, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there was 1 TV show in the 1990s about Super Mario Bros, but the generated answer claims there were none, so it is factually incorrect. However, the model does not fabricate specific incorrect titles or numbers; it honestly states it cannot find any, which is faithful to its stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 495, "judge_prompt_tokens": 398, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 66.00284576416016, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The gold answer states there were 0 TV shows, while the generated answer says it cannot find any movies or shows matching the criteria. This is effectively agreeing with the gold that there are none, so it is correct with respect to the question. However, as an evaluation of a model that has access to the gold label, it should have directly stated the known answer (0) rather than deferring to lack of information, so it is not fully faithful to the provided gold answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 528, "judge_prompt_tokens": 400, "judge_completion_tokens": 128}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 88.00339698791504, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because a reasonable expected answer is 'Naruto', while the model said 'I don't know'. However, it is faithful to its own (claimed) lack of knowledge and does not hallucinate an alternative answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 464, "judge_prompt_tokens": 386, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 3345.0894355773926, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates that the correct response is that no such movie was released. The generated answer says 'I don't know,' which does not assert any incorrect fact; it simply expresses ignorance. Therefore, it is factually compatible with the gold answer (no false claim) and thus fully faithful to the gold, even though it is less informative. Both correctness (no wrong info) and faithfulness (no contradiction with gold) are 1.", "main_total_tokens": 697, "main_prompt_tokens": 567, "main_completion_tokens": 130, "judge_total_tokens": 509, "judge_prompt_tokens": 389, "judge_completion_tokens": 120}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 1777.332067489624, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that the movie was not released or does not exist, while the model answered 'I don't know.' This is incorrect, so correctness is 0. However, the model did not fabricate any details and honestly expressed uncertainty, so faithfulness is 1.", "main_total_tokens": 628, "main_prompt_tokens": 572, "main_completion_tokens": 56, "judge_total_tokens": 472, "judge_prompt_tokens": 386, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "I don't know.", "latency_ms": 2038.9292240142822, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the 4th oldest Star Wars movie and the gold answer is a specific title, but the generated answer says 'I don't know,' which is incorrect. However, it does not invent any facts or contradict known information; it simply expresses a lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 652, "main_prompt_tokens": 582, "main_completion_tokens": 70, "judge_total_tokens": 491, "judge_prompt_tokens": 393, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "I don't know.", "latency_ms": 2834.2010974884033, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model says it does not know the answer, which is faithful to its uncertainty but incorrect because the correct year is 1986.", "main_total_tokens": 783, "main_prompt_tokens": 690, "main_completion_tokens": 93, "judge_total_tokens": 439, "judge_prompt_tokens": 381, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 66.36357307434082, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a precise numeric rating (8.466), while the generated answer says 'I don't know.' This is incorrect because the correct value is known and provided. However, the generated answer is faithful in that it does not fabricate or hallucinate a rating; it truthfully expresses uncertainty instead of inventing data.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 494, "judge_prompt_tokens": 398, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med [Doc 121125]", "latency_ms": 4734.337329864502, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Mighty Med'. It is correct and faithful to the query.", "main_total_tokens": 2560, "main_prompt_tokens": 2433, "main_completion_tokens": 127, "judge_total_tokens": 453, "judge_prompt_tokens": 400, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina [Doc 53582]", "latency_ms": 4544.196844100952, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Argentina as the country, matching the gold answer exactly and not introducing any unsupported information.", "main_total_tokens": 3000, "main_prompt_tokens": 2902, "main_completion_tokens": 98, "judge_total_tokens": 451, "judge_prompt_tokens": 398, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "I don't know.", "latency_ms": 2184.821844100952, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Jiang Jie'. The generated answer says 'I don't know', which is incorrect. However, it does not invent any wrong specific information and openly admits uncertainty, so it is fully faithful to the model's stated uncertainty.", "main_total_tokens": 677, "main_prompt_tokens": 604, "main_completion_tokens": 73, "judge_total_tokens": 468, "judge_prompt_tokens": 388, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "2", "latency_ms": 3203.0680179595947, "retrieval_recall": 0.0, "retrieval_mrr": 0.25, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.16812753627111746, "hit_rate_at_5": 1.0, "recall_at_10": 0.25, "precision_at_10": 0.1, "ndcg_at_10": 0.16812753627111746, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (2) does not match the gold answer (4), so it is incorrect. However, the model directly answered the user’s question without adding extra fabricated details, so it is faithful to its own (albeit incorrect) reasoning.", "main_total_tokens": 3759, "main_prompt_tokens": 3753, "main_completion_tokens": 6, "judge_total_tokens": 466, "judge_prompt_tokens": 385, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "30", "latency_ms": 2825.141429901123, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '30' is factually incorrect given the gold explanation that there are no movies featuring both Bruce Lee and Steven Seagal together. It also directly contradicts the provided gold answer, so it is not faithful to the reference.", "main_total_tokens": 2468, "main_prompt_tokens": 2415, "main_completion_tokens": 53, "judge_total_tokens": 474, "judge_prompt_tokens": 395, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3 [Doc 42750]", "latency_ms": 2118.1201934814453, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (6.3) and does not introduce unsupported information. It is both correct and faithful.", "main_total_tokens": 2226, "main_prompt_tokens": 2213, "main_completion_tokens": 13, "judge_total_tokens": 446, "judge_prompt_tokens": 393, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "drift-style", "latency_ms": 4582.719087600708, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'drift-style' clearly conveys the same concept as the gold answer 'Drift', identifying the style of racing correctly. It is faithful to the prompt and does not introduce new or conflicting information.", "main_total_tokens": 2888, "main_prompt_tokens": 2796, "main_completion_tokens": 92, "judge_total_tokens": 454, "judge_prompt_tokens": 380, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer [Doc 131298]", "latency_ms": 4645.113468170166, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Confronting a Serial Killer' with no additions that change the meaning. The bracketed document reference does not alter the correctness or faithfulness of the movie title answer.", "main_total_tokens": 2953, "main_prompt_tokens": 2846, "main_completion_tokens": 107, "judge_total_tokens": 492, "judge_prompt_tokens": 418, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838 [Doc 20603]", "latency_ms": 2337.0273113250732, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold popularity score exactly (7.838) and does not introduce any unsupported information. It is both correct and faithful.", "main_total_tokens": 2548, "main_prompt_tokens": 2533, "main_completion_tokens": 15, "judge_total_tokens": 455, "judge_prompt_tokens": 395, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense", "latency_ms": 3980.605125427246, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'The Sixth Sense' as the 1999 psychological thriller alluding to an extra sense, and it is fully supported by the question.", "main_total_tokens": 2950, "main_prompt_tokens": 2880, "main_completion_tokens": 70, "judge_total_tokens": 464, "judge_prompt_tokens": 394, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002 [Doc 27318]", "latency_ms": 4029.6833515167236, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2002) exactly and does not introduce any contradictory information. It is both correct and faithful.", "main_total_tokens": 3070, "main_prompt_tokens": 3001, "main_completion_tokens": 69, "judge_total_tokens": 447, "judge_prompt_tokens": 389, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled", "latency_ms": 3788.379907608032, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly giving the title as 'The Transporter Refueled' and faithfully reflecting the provided reference.", "main_total_tokens": 3142, "main_prompt_tokens": 3087, "main_completion_tokens": 55, "judge_total_tokens": 457, "judge_prompt_tokens": 397, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015 [Doc 67500]", "latency_ms": 4182.7802658081055, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It is also faithful to the provided information, with no contradictions or unsupported claims.", "main_total_tokens": 3086, "main_prompt_tokens": 3010, "main_completion_tokens": 76, "judge_total_tokens": 466, "judge_prompt_tokens": 393, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "I don't know.", "latency_ms": 2062.541961669922, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no such movie description can be found, i.e., the correct response is effectively not knowing of such a romantic comedy. The generated answer says, 'I don't know,' which matches that stance. Therefore it is correct and faithful to the gold.", "main_total_tokens": 663, "main_prompt_tokens": 596, "main_completion_tokens": 67, "judge_total_tokens": 479, "judge_prompt_tokens": 394, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "I don't know.", "latency_ms": 1882.1136951446533, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Havana Darkness,' but the model responded with 'I don't know,' which is incorrect, so correctness is 0. However, the model did not hallucinate or fabricate an answer and honestly expressed uncertainty, so faithfulness is 1.", "main_total_tokens": 665, "main_prompt_tokens": 604, "main_completion_tokens": 61, "judge_total_tokens": 476, "judge_prompt_tokens": 392, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 4562.053918838501, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Doll & Em', so it is both correct and faithful.", "main_total_tokens": 2870, "main_prompt_tokens": 2768, "main_completion_tokens": 102, "judge_total_tokens": 452, "judge_prompt_tokens": 392, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "I don't know.", "latency_ms": 1773.8087177276611, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Hallam Foe'. The generated answer says 'I don't know,' which is incorrect. However, the model does not fabricate an incorrect specific movie title; it honestly expresses uncertainty, so faithfulness to its own stated knowledge is high.", "main_total_tokens": 658, "main_prompt_tokens": 603, "main_completion_tokens": 55, "judge_total_tokens": 486, "judge_prompt_tokens": 403, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "I don't know.", "latency_ms": 2682.945966720581, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'A foe'. The model response 'I don't know.' is incorrect because it fails to provide the required classification. However, it does not introduce any fabricated or contradictory information; it simply expresses uncertainty, so it is fully faithful to what the model could know.", "main_total_tokens": 832, "main_prompt_tokens": 752, "main_completion_tokens": 80, "judge_total_tokens": 477, "judge_prompt_tokens": 390, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18 [Doc 124583]", "latency_ms": 4438.727855682373, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer gives Sam's age as 18, which matches the gold answer exactly. It is therefore both correct and faithful to the reference.", "main_total_tokens": 3104, "main_prompt_tokens": 3017, "main_completion_tokens": 87, "judge_total_tokens": 455, "judge_prompt_tokens": 395, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam is on the autism spectrum [Doc 124583].", "latency_ms": 4190.375566482544, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which matches the gold answer exactly. It is fully faithful to the provided information and does not introduce contradictions or unsupported details.", "main_total_tokens": 3025, "main_prompt_tokens": 2944, "main_completion_tokens": 81, "judge_total_tokens": 467, "judge_prompt_tokens": 399, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix [Doc 59659]", "latency_ms": 3913.63263130188, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Chamonix') and does not introduce any additional unsupported information, so it is both correct and faithful.", "main_total_tokens": 3314, "main_prompt_tokens": 3260, "main_completion_tokens": 54, "judge_total_tokens": 467, "judge_prompt_tokens": 406, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481]", "latency_ms": 4192.615985870361, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and simply repeats it with a document reference, staying fully faithful to the given information.", "main_total_tokens": 3733, "main_prompt_tokens": 3676, "main_completion_tokens": 57, "judge_total_tokens": 458, "judge_prompt_tokens": 398, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland [Doc 114810]", "latency_ms": 4271.45791053772, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The answer matches the gold answer exactly ('Greenland') and is fully supported by the provided information, with no added or conflicting details.", "main_total_tokens": 3248, "main_prompt_tokens": 3174, "main_completion_tokens": 74, "judge_total_tokens": 459, "judge_prompt_tokens": 401, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon [Doc 96838]", "latency_ms": 4725.572347640991, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('One Afternoon'). It is correct and fully faithful to the provided gold reference; the added document tag does not alter the core answer.", "main_total_tokens": 2663, "main_prompt_tokens": 2542, "main_completion_tokens": 121, "judge_total_tokens": 478, "judge_prompt_tokens": 412, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "None of the provided documents contain information about a low-rated Italian language movie where a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her. The closest match in terms of language and rating is \"Tutti giù\" [Doc 55728], but it does not fit the plot description provided in the question. Therefore, based strictly on the context given, there is no correct answer.", "latency_ms": 4597.060918807983, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "recall_at_10": 0.0, "precision_at_5": 0.0, "precision_at_10": 0.0, "ndcg_at_5": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_5": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'The Life Ahead.' The generated answer incorrectly claims there is no correct answer based on the documents, so it is factually wrong and gets a correctness score of 0. However, it accurately reflects that the context provided (the documents) do not mention the movie and explains its reasoning strictly from that context, so it is faithful to the given evidence and reasoning process, yielding a faithfulness score of 1.", "main_total_tokens": 3186, "main_prompt_tokens": 3014, "main_completion_tokens": 172, "judge_total_tokens": 599, "judge_prompt_tokens": 481, "judge_completion_tokens": 118}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The documentary \"McLaren\" from 2016 is about Bruce McLaren, a New Zealander who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 4772.79806137085, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary, matching the gold answer. It does not contradict the provided information and stays faithful to it, with only a minor addition (his nationality and role) that is accurate and reasonable.", "main_total_tokens": 3158, "main_prompt_tokens": 3048, "main_completion_tokens": 110, "judge_total_tokens": 503, "judge_prompt_tokens": 419, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren Motor Racing team [Doc 73859]", "latency_ms": 5564.65220451355, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer in content (team name) and does not introduce contradictions or unsupported information. It is both correct and faithful.", "main_total_tokens": 3224, "main_prompt_tokens": 3088, "main_completion_tokens": 136, "judge_total_tokens": 455, "judge_prompt_tokens": 394, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "I don't know.", "latency_ms": 1845.4744815826416, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'New Zealand', but the generated answer is 'I don't know.', which is factually incorrect, so correctness is 0. However, saying 'I don't know' does not invent any unsupported facts and is faithful to having no information, so faithfulness is 1.", "main_total_tokens": 683, "main_prompt_tokens": 638, "main_completion_tokens": 45, "judge_total_tokens": 470, "judge_prompt_tokens": 380, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "I don't know.", "latency_ms": 1545.6712245941162, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer admits not knowing, which is effectively equivalent in correctness to the gold answer stating that such a movie is not in the database; both convey that the system cannot provide a specific movie. It is faithful to the question and does not hallucinate any movie title.", "main_total_tokens": 625, "main_prompt_tokens": 580, "main_completion_tokens": 45, "judge_total_tokens": 487, "judge_prompt_tokens": 402, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit [Doc 28589]", "latency_ms": 3973.564624786377, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2002 Japanese horror film as 'Tomie: Forbidden Fruit'. It is fully correct and faithful to the provided information.", "main_total_tokens": 3110, "main_prompt_tokens": 3039, "main_completion_tokens": 71, "judge_total_tokens": 496, "judge_prompt_tokens": 428, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 [Doc 98566]", "latency_ms": 4197.662591934204, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years) and directly uses the information without adding unsupported details, so it is both correct and faithful.", "main_total_tokens": 3366, "main_prompt_tokens": 3298, "main_completion_tokens": 68, "judge_total_tokens": 463, "judge_prompt_tokens": 402, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "I don't know.", "latency_ms": 2539.9844646453857, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that they search for the Book of Souls. The generated answer says 'I don't know,' which is incorrect, so correctness is 0. However, it does not introduce any incorrect alternative and simply admits lack of knowledge, so it is fully faithful to the model’s uncertainty, giving faithfulness a score of 1.", "main_total_tokens": 703, "main_prompt_tokens": 612, "main_completion_tokens": 91, "judge_total_tokens": 492, "judge_prompt_tokens": 398, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1660.3400707244873, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, which is equivalent in practice to not knowing the release year. The generated answer says \"I don't know,\" which is consistent with the gold. There is no incorrect factual claim and the response is fully aligned with the gold intent.", "main_total_tokens": 638, "main_prompt_tokens": 588, "main_completion_tokens": 50, "judge_total_tokens": 484, "judge_prompt_tokens": 395, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1725.7473468780518, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, i.e., the system lacks the information. The generated answer \"I don't know\" is consistent with that and does not introduce incorrect details. It is both correct (no false information) and faithful to the gold response.", "main_total_tokens": 655, "main_prompt_tokens": 603, "main_completion_tokens": 52, "judge_total_tokens": 481, "judge_prompt_tokens": 393, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1877.8741359710693, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, effectively conveying lack of knowledge. The generated answer \"I don't know.\" is consistent with this and does not introduce fabricated information. Therefore it is both correct and fully faithful to the gold.", "main_total_tokens": 647, "main_prompt_tokens": 585, "main_completion_tokens": 62, "judge_total_tokens": 476, "judge_prompt_tokens": 395, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1864.3662929534912, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer convey that the model does not know or have the information about the movie. The generated answer does not hallucinate a year and is consistent with the gold response, so it is both correct and faithful.", "main_total_tokens": 647, "main_prompt_tokens": 585, "main_completion_tokens": 62, "judge_total_tokens": 474, "judge_prompt_tokens": 395, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1971.921443939209, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, which is equivalent in content to saying 'I don't know.' The generated response correctly reflects the lack of knowledge without inventing a year, so it is both correct and faithful to the gold.", "main_total_tokens": 662, "main_prompt_tokens": 595, "main_completion_tokens": 67, "judge_total_tokens": 477, "judge_prompt_tokens": 395, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1494.798183441162, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly indicate lack of knowledge about the movie; the generated answer is fully consistent with the gold.", "main_total_tokens": 597, "main_prompt_tokens": 554, "main_completion_tokens": 43, "judge_total_tokens": 449, "judge_prompt_tokens": 393, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1524.9693393707275, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly indicate a lack of knowledge about the movie. The generated answer is fully consistent with the gold answer, so it is both correct and faithful.", "main_total_tokens": 640, "main_prompt_tokens": 597, "main_completion_tokens": 43, "judge_total_tokens": 462, "judge_prompt_tokens": 396, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 1888.6206150054932, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the information is not available in the database, so the correct behavior is to not provide a director. The generated answer says 'I don't know,' which aligns with the lack of information and does not introduce external facts. It is therefore both correct and faithful to the database constraint.", "main_total_tokens": 645, "main_prompt_tokens": 583, "main_completion_tokens": 62, "judge_total_tokens": 486, "judge_prompt_tokens": 401, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 1720.3712463378906, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the database does not provide the director information. The generated answer says “I don't know,” which is consistent with not having that information. It does not introduce any new facts and matches the epistemic limitation, so it is both correct and faithful.", "main_total_tokens": 647, "main_prompt_tokens": 594, "main_completion_tokens": 53, "judge_total_tokens": 478, "judge_prompt_tokens": 399, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 2457.2842121124268, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer appropriately indicates a lack of knowledge, which matches the gold response that the director information is not provided. It is both correct and faithful to the available information.", "main_total_tokens": 665, "main_prompt_tokens": 575, "main_completion_tokens": 90, "judge_total_tokens": 462, "judge_prompt_tokens": 397, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 2263.294219970703, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the database does not provide the director information. The generated answer admits not knowing, which is effectively equivalent in this context. It does not hallucinate a director name and aligns with the lack of information, so it is both correct and faithful to the gold answer.", "main_total_tokens": 654, "main_prompt_tokens": 573, "main_completion_tokens": 81, "judge_total_tokens": 489, "judge_prompt_tokens": 401, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 3691.392660140991, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Rise of the Mummy', correctly identifying the movie and fully aligning with the provided reference.", "main_total_tokens": 2818, "main_prompt_tokens": 2755, "main_completion_tokens": 63, "judge_total_tokens": 454, "judge_prompt_tokens": 396, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 4538.242816925049, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Stranger in My Bed') and correctly identifies the movie based on the description. It is both correct and fully faithful to the given gold response.", "main_total_tokens": 2877, "main_prompt_tokens": 2779, "main_completion_tokens": 98, "judge_total_tokens": 469, "judge_prompt_tokens": 400, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 4356.352806091309, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question actually describes the premise of the TV series 'Game of Silence,' not a movie. The gold answer is 'Game of Silence,' and the generated answer exactly matches this. Therefore it is fully correct relative to the gold and fully faithful.", "main_total_tokens": 2584, "main_prompt_tokens": 2485, "main_completion_tokens": 99, "judge_total_tokens": 480, "judge_prompt_tokens": 400, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 3586.4133834838867, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'The Craigslist Killer'. It is both correct and fully faithful to the gold response.", "main_total_tokens": 2785, "main_prompt_tokens": 2728, "main_completion_tokens": 57, "judge_total_tokens": 457, "judge_prompt_tokens": 395, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 4295.218706130981, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer, 'Wood Job!', exactly matches the gold answer and correctly identifies the movie based on the described plot. It is both correct and fully faithful to the given gold standard.", "main_total_tokens": 3706, "main_prompt_tokens": 3648, "main_completion_tokens": 58, "judge_total_tokens": 471, "judge_prompt_tokens": 403, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 4341.172933578491, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the correct movie as 'Sesame Street: The Magical Wand Chase'. It is both factually correct and completely faithful to the provided gold response.", "main_total_tokens": 3296, "main_prompt_tokens": 3223, "main_completion_tokens": 73, "judge_total_tokens": 487, "judge_prompt_tokens": 417, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond [Doc 74597]", "latency_ms": 4550.289630889893, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title, and the gold answer is 'Beyond'. The generated answer is 'Beyond [Doc 74597]', which correctly identifies the same movie title, only adding an extraneous document tag. This is fully correct and faithful to the gold answer, despite the minor annotation artifact.", "main_total_tokens": 2752, "main_prompt_tokens": 2644, "main_completion_tokens": 108, "judge_total_tokens": 507, "judge_prompt_tokens": 415, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9 [Doc 22478]", "latency_ms": 3668.8287258148193, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, identifying the correct movie as '6ixtynin9'. There are no additions or contradictions, so it is fully correct and faithful.", "main_total_tokens": 2737, "main_prompt_tokens": 2675, "main_completion_tokens": 62, "judge_total_tokens": 483, "judge_prompt_tokens": 415, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread [Doc 32410]", "latency_ms": 4508.519649505615, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Club Dread'), so it is correct. It does not introduce contradictions or unsupported claims, and the extra bracketed reference does not change the factual answer, so it is faithful to the given information.", "main_total_tokens": 3096, "main_prompt_tokens": 3003, "main_completion_tokens": 93, "judge_total_tokens": 513, "judge_prompt_tokens": 433, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Title: Toilet", "latency_ms": 4529.818058013916, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title; the gold answer is 'Toilet' and the generated answer is 'Title: Toilet', which correctly identifies the same movie. It is correct and consistent with the given gold answer.", "main_total_tokens": 3281, "main_prompt_tokens": 3194, "main_completion_tokens": 87, "judge_total_tokens": 492, "judge_prompt_tokens": 417, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel [Doc 1024]", "latency_ms": 3855.393171310425, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Electric Angel'), and there is no reliance on external or fabricated information. The mention of '[Doc 1024]' does not alter the correctness or faithfulness of the movie title provided.", "main_total_tokens": 3019, "main_prompt_tokens": 2957, "main_completion_tokens": 62, "judge_total_tokens": 501, "judge_prompt_tokens": 424, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "Fall [Doc 106810]", "latency_ms": 2853.118896484375, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates that the lowest rated movie from 2023 is 'Mega Ape', while the generated answer is 'Fall [Doc 106810]'. This is factually incorrect relative to the gold, so correctness is 0. There is no evidence the generated answer is derived from or consistent with the provided gold reference; it contradicts it, so faithfulness is also 0.", "main_total_tokens": 2970, "main_prompt_tokens": 2892, "main_completion_tokens": 78, "judge_total_tokens": 498, "judge_prompt_tokens": 388, "judge_completion_tokens": 110}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 109.38715934753418, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the highest rated TV show from 2010, with the gold answer being 'Tea House'. The generated answer is 'I don't know.', which does not match the gold answer and thus is incorrect (correctness 0). However, the model does not fabricate an answer and honestly admits lack of knowledge, so it is faithful to its own uncertainty (faithfulness 1).", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 496, "judge_prompt_tokens": 386, "judge_completion_tokens": 110}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 185.5752468109131, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know', which does not provide the correct movie title and thus is incorrect. However, it does not introduce any fabricated information, so it is faithful to the model's stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 395, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The language of the lowest-rated movie from 1980 is en [Doc 705].", "latency_ms": 5636.887311935425, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer uses the ISO code 'en' instead of the full language name 'English' required by the gold answer, so it is incorrect. It also adds an unsupported citation '[Doc 705]' that is not present in the gold reference, making it unfaithful to the provided ground truth.", "main_total_tokens": 3271, "main_prompt_tokens": 3129, "main_completion_tokens": 142, "judge_total_tokens": 493, "judge_prompt_tokens": 401, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The Amanda Show", "latency_ms": 5436.749696731567, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because 'The Amanda Show' was not the 1999 TV show with the highest popularity according to the gold label, which is 'Law & Order: Special Victims Unit'. However, the model’s answer is internally consistent and directly responds to the question, so it is faithful to the prompt, just factually wrong relative to the gold answer.", "main_total_tokens": 3287, "main_prompt_tokens": 3150, "main_completion_tokens": 137, "judge_total_tokens": 499, "judge_prompt_tokens": 393, "judge_completion_tokens": 106}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2 [Doc 79355]", "latency_ms": 2177.2665977478027, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (7.2) and does not introduce unsupported information. It is both correct and faithful.", "main_total_tokens": 2416, "main_prompt_tokens": 2403, "main_completion_tokens": 13, "judge_total_tokens": 450, "judge_prompt_tokens": 392, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "1999 [Doc 22481]", "latency_ms": 4407.062768936157, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1999) does not match the provided gold answer (2019), so it is incorrect w.r.t. the gold. However, the model directly used the information it stated (1999 [Doc 22481]) without contradicting its own cited source, so it is internally faithful.", "main_total_tokens": 3040, "main_prompt_tokens": 2949, "main_completion_tokens": 91, "judge_total_tokens": 486, "judge_prompt_tokens": 393, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 2055.9186935424805, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that 'My Dog Skip' received 279 votes. The generated answer says it cannot find any movies or shows matching that criteria, which is incorrect relative to the gold. However, it does not fabricate any incorrect number or contradict known facts about the movie; it simply expresses inability to find it. Thus, correctness is 0, but faithfulness to the model's stated uncertainty is 1.", "main_total_tokens": 2408, "main_prompt_tokens": 2406, "main_completion_tokens": 2, "judge_total_tokens": 506, "judge_prompt_tokens": 392, "judge_completion_tokens": 114}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 3450.101375579834, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 12 votes, but the generated response claims it cannot find any movies or shows matching the criteria instead of providing the numeric answer. This is incorrect. However, the model does not fabricate a number or contradictory details; it simply states inability to find relevant information, which is faithful to its (incorrect) retrieval rather than hallucinating specifics.", "main_total_tokens": 2543, "main_prompt_tokens": 2458, "main_completion_tokens": 85, "judge_total_tokens": 494, "judge_prompt_tokens": 392, "judge_completion_tokens": 102}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "I don't know.", "latency_ms": 1876.4221668243408, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 2009, but the model responded with 'I don't know,' which is incorrect. However, the response is faithful because it does not fabricate an answer and honestly states a lack of knowledge.", "main_total_tokens": 619, "main_prompt_tokens": 558, "main_completion_tokens": 61, "judge_total_tokens": 465, "judge_prompt_tokens": 391, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is en [Doc 48035].", "latency_ms": 4993.136167526245, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The answer correctly identifies English as the original language of the movie, matching the gold label 'en'. However, it adds an unsupported reference '[Doc 48035]' that is not grounded in the provided information, which slightly reduces faithfulness.", "main_total_tokens": 3138, "main_prompt_tokens": 3017, "main_completion_tokens": 121, "judge_total_tokens": 478, "judge_prompt_tokens": 402, "judge_completion_tokens": 76}
