{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The context does not contain information about a title named \"Wall Street\" [Doc 133009, Doc 73209, Doc 74986, Doc 125034, Doc 78895, Doc 105304, Doc 79527, Doc 130408, Doc 79840, Doc 96538, Doc 134496, Doc 103086, Doc 73581, Doc 126714, Doc 107569, Doc 111585, Doc 124011, Doc 121179, Doc 106115, Doc 132148]. However, \"Gaming Wall St\" was released in 2022 [Doc 133009].", "latency_ms": 3442.4943923950195, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the release year of the film 'Wall Street', which is 1987. The generated answer instead states that the context does not contain information about 'Wall Street' and provides information about 'Gaming Wall St' (2022). It never answers 1987, so it is factually incorrect relative to the gold answer (correctness 0). However, the answer accurately reflects what it claims about the provided context (that it only mentions 'Gaming Wall St' and its 2022 release), without hallucinating a year for 'Wall Street', so it is faithful to the stated evidence (faithfulness 1).", "main_total_tokens": 3437, "main_prompt_tokens": 3224, "main_completion_tokens": 213, "judge_total_tokens": 671, "judge_prompt_tokens": 512, "judge_completion_tokens": 159}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Based on the context, several titles involve games, survival, and cell phones/technology:\n\n1.  ***Naraka 19*** (2007) involves a university student who is \"unwittingly trapped in a terrifying mobile phone game from hell\" after receiving an SMS [Doc 41254].\n2.  ***Tokyo Gore School*** (2009) describes a mysterious violent game being played out in high schools, spread virally via students’ mobile phones, where victims become \"senseless killing machines\" [Doc 45096].\n3.  ***6 Plots*** (2012) features a group of tech-savvy teens who are trapped in separate boxes and told they have their phones, but if they attempt to call the police or parents, the penalty will be deadly [Doc 56109].\n4.  ***Future Diary Another World*** (2012, TV) revolves around a college student and six other people who possess a \"Future Diary\" cell phone application and are forced into a game of survival [Doc 120683].\n5.  ***47 Hours to Live*** (2019) involves two teenage girls who find a game on a website that claims they can use their phone to summon the supernatural, and if they lose, they lose their life [Doc 90746].\n\n***Signal 100*** (2020) involves a group of high school students forced into a game where they are hypnotized to commit suicide on an unknown command, but the description does not explicitly mention using cell phones for the survival game [Doc 93745].", "latency_ms": 2123.7761974334717, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.4, "justification": "The question asks specifically for the movie where classmates must play a survival game using their cell phones; the gold answer is 'Brain Fluid Explosion Girl'. The generated answer does not mention this title at all and instead lists multiple other films and series, never selecting a single one as the answer, so it is incorrect. It is only loosely faithful to the (unstated) source context: it talks about movies involving games, survival, and phones, but it fabricates a multi-item discussion instead of directly answering with the correct film, and it appears to infer or elaborate beyond the intended QA task. Thus correctness is 0. Faithfulness is partial because the response at least stays on the general topic of survival games and cell phones rather than something unrelated, but it does not reflect the gold answer or precisely follow the QA intent.", "main_total_tokens": 3269, "main_prompt_tokens": 2912, "main_completion_tokens": 357, "judge_total_tokens": 902, "judge_prompt_tokens": 705, "judge_completion_tokens": 197}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show in which six friends are transported into the *Dungeons & Dragons* realm and must try to find a way home with the help of their guide 'Dungeon Master' is titled *Dungeons & Dragons* [Doc 112849].", "latency_ms": 601.9444465637207, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' matching the gold answer. It directly uses the information given (Doc 112849) without introducing unsupported claims, so it is both correct and faithful.", "main_total_tokens": 2763, "main_prompt_tokens": 2708, "main_completion_tokens": 55, "judge_total_tokens": 516, "judge_prompt_tokens": 440, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Based strictly on the context provided, the movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is not listed [Doc 97225, Doc 67620].", "latency_ms": 3518.733024597168, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer clearly identifies the movie as 'Nacht der Wölfe', while the generated answer incorrectly claims that the movie is not listed. This is factually wrong given the gold, and also not faithful to the known correct response.", "main_total_tokens": 3944, "main_prompt_tokens": 3248, "main_completion_tokens": 696, "judge_total_tokens": 522, "judge_prompt_tokens": 443, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of *Picnic at Hanging Rock* (TV, 2018) take place on Valentine's Day, 1900 [Doc 126191].", "latency_ms": 554.8226833343506, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold answer: 1900. Generated answer: Valentine's Day, 1900. The year matches exactly and the mention presumably comes from the cited document, so the answer is both correct and faithful.", "main_total_tokens": 2965, "main_prompt_tokens": 2923, "main_completion_tokens": 42, "judge_total_tokens": 488, "judge_prompt_tokens": 417, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "Answer based strictly on the context. Do not use outside knowledge. Cite sources for every fact using the format [Doc X]. If answer is not found, say so.\n\nThe context provided does not contain information about a movie released in 2004 that matches the description of a young violinist working in a Venice coffee shop and whose name is also a calendar month.", "latency_ms": 3612.084150314331, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the name of a specific 2004 movie whose main character is a young violinist working in a Venice coffee shop, rehearsing for a music career, and daydreaming about her destined love, with a name that is also a calendar month. The gold answer is 'June'. The generated answer correctly states that the context provided does not contain information about such a movie and concludes that the answer is not found in the context. Under the instruction to answer strictly from the context, this is the correct behavior. It does not invent a movie title and accurately reflects the absence of relevant information in the context, so it is both correct (relative to the given constraints) and faithful to the context.", "main_total_tokens": 3875, "main_prompt_tokens": 3197, "main_completion_tokens": 678, "judge_total_tokens": 679, "judge_prompt_tokens": 505, "judge_completion_tokens": 174}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "The highest rated Russian-language movie released in 1980 is *Moscow Does Not Believe in Tears*, with a rating of 7.6 [Doc 61].\n\nOther Russian-language movies from 1980 include:\n*   *Vacations in Prostokvashino*, with a rating of 7.484 [Doc 82].\n*   *Oblomov*, with a rating of 6.2 [Doc 20].", "latency_ms": 802.3624420166016, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer claims that 'Moscow Does Not Believe in Tears' is the highest rated Russian-language movie from 1980, but the gold answer states that the correct movie is 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger'. Therefore, the generated answer is incorrect with respect to the task, even though it appears internally consistent with its cited documents, so faithfulness is high but correctness is zero.", "main_total_tokens": 2981, "main_prompt_tokens": 2879, "main_completion_tokens": 102, "judge_total_tokens": 600, "judge_prompt_tokens": 484, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest rating for a Russian-language movie released in 1999 is 7.117, for the movie *The Rifleman of the Voroshilov Regiment* [Doc 22123].", "latency_ms": 3566.596746444702, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the highest Russian-language movie rating in 1999 is 7.6, while the generated answer claims it is 7.117 for a specific movie. This numerical value contradicts the gold reference and thus is factually incorrect in this evaluation context. Additionally, the generated answer references a specific document and movie not supported by the provided gold information, so it is not faithful to the given source for this task.", "main_total_tokens": 3731, "main_prompt_tokens": 2997, "main_completion_tokens": 734, "judge_total_tokens": 543, "judge_prompt_tokens": 425, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "Two movies released in 1987 have a rating over 7.0 and are in Chinese (cn):\n\n1.  *A Chinese Ghost Story* (Rating 7.3) [Doc 7064]\n2.  *Last Eunuch In China* (Rating 7.8) [Doc 7754]", "latency_ms": 4013.867139816284, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer says there are 4 such movies, while the generated answer claims there are only 2 and lists them, so the numerical answer is incorrect. The assistant does rely on the given document-style evidence for the two listed movies (titles, ratings, IDs), so that part is somewhat grounded, but it omits two additional qualifying movies and does not explain why only these two are counted. Thus correctness is 0; faithfulness is partial at 0.5.", "main_total_tokens": 3715, "main_prompt_tokens": 2835, "main_completion_tokens": 880, "judge_total_tokens": 583, "judge_prompt_tokens": 459, "judge_completion_tokens": 124}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "", "latency_ms": 4421.035051345825, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is missing; it does not provide any numeric response to compare with the gold answer of 2. Therefore it cannot be considered correct or faithful to the query.", "main_total_tokens": 4282, "main_prompt_tokens": 3259, "main_completion_tokens": 1023, "judge_total_tokens": 460, "judge_prompt_tokens": 394, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "Based on the context provided, there are no French language TV shows listed as released in 1980. The French language titles listed are movies: *Loulou* [Doc 689] and *La Femme flic* [Doc 290].", "latency_ms": 814.5022392272949, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the only French language TV show released in 1980, and the gold answer is 'Fantômas'. The generated answer incorrectly claims there are no French language TV shows listed for 1980 and instead lists French movies, so it is factually incorrect. However, it remains faithful to the (incorrect) context it cites—stating exactly what is in that context (Loulou and La Femme flic as movies), without inventing additional titles—so it is faithful to its provided context.", "main_total_tokens": 2800, "main_prompt_tokens": 2744, "main_completion_tokens": 56, "judge_total_tokens": 572, "judge_prompt_tokens": 436, "judge_completion_tokens": 136}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is *Kill Cruise* [Doc 11205].", "latency_ms": 548.7022399902344, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' matching the gold answer exactly. It does not introduce contradictory or unsupported information, so it is fully correct and faithful to the reference.", "main_total_tokens": 2918, "main_prompt_tokens": 2891, "main_completion_tokens": 27, "judge_total_tokens": 478, "judge_prompt_tokens": 409, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The movie about the TV series in which Fred Rogers speaks directly to young children about some of life's most important issues is titled *Won't You Be My Neighbor?* [Doc 84838].", "latency_ms": 686.9058609008789, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Won't You Be My Neighbor?' and stays faithful to the question and presumed source, adding only a minor elaboration without contradiction.", "main_total_tokens": 3259, "main_prompt_tokens": 3214, "main_completion_tokens": 45, "judge_total_tokens": 506, "judge_prompt_tokens": 440, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The title \"It\" was released in the following years:\n\n*   1990 (as a Movie) [Doc 10637]\n*   1990 (as a TV production) [Doc 113633]\n*   2017 (as a Movie) [Doc 76823]", "latency_ms": 2849.050283432007, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates a single correct year: 2018. The generated answer lists 1990 (movie and TV) and 2017, none of which match 2018, so it is incorrect. However, the generated answer is self-consistent and aligned with its own cited documents, so it is faithful to its internal evidence, just not to the gold.", "main_total_tokens": 3560, "main_prompt_tokens": 2976, "main_completion_tokens": 584, "judge_total_tokens": 544, "judge_prompt_tokens": 438, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "", "latency_ms": 4981.202125549316, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for an animated TV series about a young boy robot in 2030 AD; the gold answer is 'Astro Boy,' which is correct. The generated answer is empty, so there is no contradiction or hallucination relative to the gold; therefore correctness relative to the gold is satisfied by the gold, and faithfulness is trivially satisfied since nothing conflicting was generated.", "main_total_tokens": 3806, "main_prompt_tokens": 2783, "main_completion_tokens": 1023, "judge_total_tokens": 495, "judge_prompt_tokens": 388, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie *Room*, released in 2015, a woman and her young son were held captive for 7 years in an enclosed space [Doc 67513].", "latency_ms": 664.4759178161621, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that the woman and her son were held captive for 7 years, which matches the gold answer exactly. It does not introduce contradictions or extraneous fabricated details beyond a harmless reference, so it is fully correct and faithful.", "main_total_tokens": 2607, "main_prompt_tokens": 2567, "main_completion_tokens": 40, "judge_total_tokens": 508, "judge_prompt_tokens": 429, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The context states that six armed men broke into the Brink's-Mat security depot on November 26, 1983, and stumbled across gold bullion worth £26m [Doc 133469].\n\nThe context does not specify the total amount of gold the depot was holding before the break-in, only the value of the gold bullion that was discovered [Doc 133469].", "latency_ms": 918.4582233428955, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks specifically how much gold the depot was holding before it was broken into. The given gold answer is £26m, and the context explicitly links this value to the gold bullion in the depot. The generated response incorrectly asserts that the context does not specify the total amount of gold held before the break-in, even though it does (£26m). Thus, it is not correct relative to the question, and it misrepresents what the context states, so it is not faithful.", "main_total_tokens": 3415, "main_prompt_tokens": 3325, "main_completion_tokens": 90, "judge_total_tokens": 605, "judge_prompt_tokens": 470, "judge_completion_tokens": 135}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "The popularity score of the most popular English-speaking TV show released in 1997 is 14.7527, belonging to the show *Oz* [Doc 114714].", "latency_ms": 4208.562135696411, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The answer is incorrect because it gives 14.7527 as the popularity score, but the gold label indicates the correct value is 80.5772. However, the answer is internally consistent with its cited document and does not contradict its own source, so it is faithful to the provided context.", "main_total_tokens": 3757, "main_prompt_tokens": 2807, "main_completion_tokens": 950, "judge_total_tokens": 519, "judge_prompt_tokens": 428, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "Based strictly on the provided context, there are no Russian language TV shows listed as released in 1997.", "latency_ms": 601.815938949585, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there was 1 Russian language TV show released in 1997, but the generated response claims there were none, which is factually incorrect, so correctness is 0. However, the generated answer is consistent with the instruction to rely strictly on the provided context (which apparently lacked such a show), so it is faithful to the context, yielding faithfulness 1.", "main_total_tokens": 2883, "main_prompt_tokens": 2859, "main_completion_tokens": 24, "judge_total_tokens": 514, "judge_prompt_tokens": 405, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "The TV show in which Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight is *Angel Eyes* [Doc 121928].", "latency_ms": 655.2319526672363, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 'Angel Eyes' as the TV show and stays consistent with the provided gold answer without adding incorrect information.", "main_total_tokens": 3740, "main_prompt_tokens": 3701, "main_completion_tokens": 39, "judge_total_tokens": 488, "judge_prompt_tokens": 431, "judge_completion_tokens": 57}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "", "latency_ms": 4664.535760879517, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "There is no generated answer to evaluate for correctness or faithfulness against the gold label 'Korean', so the response cannot be considered correct or faithful.", "main_total_tokens": 3759, "main_prompt_tokens": 2736, "main_completion_tokens": 1023, "judge_total_tokens": 438, "judge_prompt_tokens": 377, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "", "latency_ms": 4785.689115524292, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is missing/empty, so it does not state the correct value (2) and provides no reasoning or content to assess. Therefore it is incorrect and unfaithful to the gold answer.", "main_total_tokens": 4038, "main_prompt_tokens": 3015, "main_completion_tokens": 1023, "judge_total_tokens": 465, "judge_prompt_tokens": 393, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The best-rated movie released in 1979, according to the context, is *Board and Care*, with a rating of 6.333 [Doc 882]. The description specifies that *Board and Care* is a 1979 American short film [Doc 882].", "latency_ms": 4216.426610946655, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer explicitly states there are no movies from 1979 in the database, so the correct behavior is to say so. The generated answer instead fabricates a specific 1979 movie and rating from a supposed document, contradicting the gold answer and the available context. Therefore it is both factually incorrect relative to the task and unfaithful to the provided information.", "main_total_tokens": 3937, "main_prompt_tokens": 3054, "main_completion_tokens": 883, "judge_total_tokens": 557, "judge_prompt_tokens": 451, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "Based strictly on the context provided, the movie \"Back to the Future Part IV\" is not listed, and therefore its release year cannot be determined [Doc 10621, Doc 9405, Doc 4834].", "latency_ms": 2059.9818229675293, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer correctly states that 'Back to the Future Part IV' does not exist in the database, implying no release year. The generated answer says the release year cannot be determined based on provided documents, which differs from the gold’s clear statement of nonexistence and appears to hallucinate document references. Thus it is not fully correct relative to the gold and not faithful to the given gold answer.", "main_total_tokens": 3948, "main_prompt_tokens": 3576, "main_completion_tokens": 372, "judge_total_tokens": 556, "judge_prompt_tokens": 445, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "Based strictly on the context provided, there are no movies released in 1980 that are about the nuclear Chernobyl disaster [Doc 849, Doc 744, Doc 283, Doc 189, Doc 685, Doc 172, Doc 8, Doc 736, Doc 365, Doc 389, Doc 477, Doc 35, Doc 284, Doc 350, Doc 775, Doc 86, Doc 90, Doc 190, Doc 519, Doc 852].", "latency_ms": 4638.202428817749, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the popularity score of the least popular 1980 movie about Chernobyl; the gold answer correctly says no such movies exist. The generated answer instead asserts there are no such movies but does not address the requested popularity score, so it does not correctly answer the question. It also claims this is based on specific documents, which are not mentioned in the gold answer and may not exist, making it unfaithful to the provided reference.", "main_total_tokens": 4109, "main_prompt_tokens": 3121, "main_completion_tokens": 988, "judge_total_tokens": 634, "judge_prompt_tokens": 512, "judge_completion_tokens": 122}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are three TV shows or movies that have 'Astro Boy' in the title:\n\n1.  *Astro Boy* (TV, 2003) [Doc 116265]\n2.  *Astro Boy* (TV, 1980) [Doc 112591]\n3.  *Astro Boy* (Movie, 2009) [Doc 44947]", "latency_ms": 984.5094680786133, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.8318724637288826, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.8318724637288826, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 4 titles, but the generated answer lists only 3 and states that there are three TV shows or movies with 'Astro Boy' in the title. This contradicts the gold, so the correctness is 0. The answer is internally consistent with its own listed items, so faithfulness is 1.", "main_total_tokens": 3151, "main_prompt_tokens": 3050, "main_completion_tokens": 101, "judge_total_tokens": 567, "judge_prompt_tokens": 467, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "Based strictly on the provided context, zero movies released in 2025 allude to the Holocaust.\n\nOne movie, *Dead to Rights*, is based on the atrocities committed during the Nanjing Massacre [Doc 111684], and another, *8*, references the Spanish Civil War [Doc 111760], but neither of these events is the Holocaust.", "latency_ms": 4738.93666267395, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 2, but the generated answer says zero, so it is factually incorrect. It is also unfaithful because it directly contradicts the specified gold label rather than reflecting it.", "main_total_tokens": 3897, "main_prompt_tokens": 2948, "main_completion_tokens": 949, "judge_total_tokens": 529, "judge_prompt_tokens": 457, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "There were two TV shows released in the 1990s about Super Mario Bros.:\n\n1.  *The Adventures of Super Mario Bros. 3* (1990) [Doc 113661]\n2.  *Super Mario World* (1991) [Doc 113814]", "latency_ms": 2170.6345081329346, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold label states there was 1 TV show, but the generated answer claims there were 2, so the answer is factually incorrect relative to the gold. It also directly contradicts the given gold reference, so it is not faithful to the provided information.", "main_total_tokens": 4019, "main_prompt_tokens": 3603, "main_completion_tokens": 416, "judge_total_tokens": 530, "judge_prompt_tokens": 446, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "Based strictly on the context provided, the answer is not found. None of the listed TV shows released between 2000 and 2010 are described as being about Super Mario Bros.", "latency_ms": 4193.526268005371, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is a specific numeric value: 0. The generated answer instead claims that the answer is not found in the context, so it does not provide the correct number. Therefore it is incorrect. It also contradicts the gold label, which reflects what should be inferred from the context, so it is not faithful to the gold-standard reasoning.", "main_total_tokens": 4150, "main_prompt_tokens": 3216, "main_completion_tokens": 934, "judge_total_tokens": 527, "judge_prompt_tokens": 426, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular TV show or movie about ninjas, based on the provided context, is *NARUTO Spin-Off: Rock Lee & His Ninja Pals*, with a popularity rating of 10.7994 [Doc 120321].", "latency_ms": 887.6633644104004, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'Naruto', a specific and widely known ninja anime. The generated answer instead names 'NARUTO Spin-Off: Rock Lee & His Ninja Pals', which is not the same show and is not supported as the most popular ninja show in the question context. Therefore, it is incorrect. It also fabricates a justification based on a specific document and popularity rating not present in the question, so it is not faithful to the gold answer or the intended reference.", "main_total_tokens": 3235, "main_prompt_tokens": 3180, "main_completion_tokens": 55, "judge_total_tokens": 559, "judge_prompt_tokens": 431, "judge_completion_tokens": 128}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "Based on the context provided, there is no mention of a \"Jurassic Park/Power Ranger crossover movie\" [Doc 14060, Doc 76939].", "latency_ms": 684.2305660247803, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer clarifies that the movie does not exist. The generated answer states that there is no mention of such a movie, which aligns with the idea that it was not released. It does not falsely assert a release date and remains consistent with the given context, so it is both correct and faithful.", "main_total_tokens": 3145, "main_prompt_tokens": 3107, "main_completion_tokens": 38, "judge_total_tokens": 510, "judge_prompt_tokens": 418, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Based strictly on the context provided, there is no movie listed where Barbie fights the Predator.\n\nThe context mentions a movie titled *Killer Barbys vs. Dracula* [Doc 28703] and several movies titled *Predator* or *The Predator* [Doc 6958, Doc 82000, Doc 10613], but none of the plots describe a crossover between the Killer Barbies and the extraterrestrial warrior known as the Predator.", "latency_ms": 4077.850103378296, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.8, "faithfulness_score": 0.7, "justification": "The gold answer says explicitly that the movie was not released, implying such a movie exists (at least in conception) but never came out. The generated answer instead claims there is no movie listed where Barbie fights the Predator, focusing on the given context rather than the status of the movie's release. This partially contradicts the gold: it does not address release status and suggests nonexistence in the context rather than unreleased status. It is faithful to the provided context description but not fully faithful to the gold answer's intent, and only partially correct relative to the evaluation target.", "main_total_tokens": 4074, "main_prompt_tokens": 3247, "main_completion_tokens": 827, "judge_total_tokens": 623, "judge_prompt_tokens": 473, "judge_completion_tokens": 150}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "", "latency_ms": 5234.607219696045, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is empty, so it does not provide the required movie title and cannot be correct. It also does not address or reference the question or gold answer, so it is not faithful to the task.", "main_total_tokens": 4385, "main_prompt_tokens": 3362, "main_completion_tokens": 1023, "judge_total_tokens": 463, "judge_prompt_tokens": 390, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The title \"It\" was released in the following years:\n\n*   1990 (as a Movie) [Doc 10637]\n*   1990 (as a TV production) [Doc 113633]\n*   2017 (as a Movie) [Doc 76823]", "latency_ms": 3027.3258686065674, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates a single correct year, 1986. The generated answer provides multiple years (1990 and 2017) for different adaptations of 'It' and does not mention 1986 at all, so it is factually incorrect relative to the gold. However, the answer is internally consistent with the documents it cites and does not invent information beyond them, so it is faithful to its own context.", "main_total_tokens": 3560, "main_prompt_tokens": 2976, "main_completion_tokens": 584, "judge_total_tokens": 553, "judge_prompt_tokens": 438, "judge_completion_tokens": 115}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "Based strictly on the provided context, the rating for the movie *Forrest Gump* is not available [Doc 15881, Doc 37205, Doc 47745, Doc 4395, Doc 58255, Doc 54195, Doc 46961, Doc 42512, Doc 73586, Doc 27829, Doc 1779, Doc 57436, Doc 20296, Doc 32619, Doc 65423, Doc 8920, Doc 69514, Doc 100655, Doc 28063, Doc 61711]. The context only provides information about the documentary *Through the Eyes of Forrest Gump* [Doc 15881].", "latency_ms": 1172.2326278686523, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer (8.466) is present in the gold, but the generated answer claims the rating is not available, so it is factually incorrect. However, it remains faithful to the stated constraint of relying only on the provided context, explicitly saying the context does not give the rating and distinguishing the documentary from the film, so it is fully faithful to the prompt and context limitations.", "main_total_tokens": 3375, "main_prompt_tokens": 3169, "main_completion_tokens": 206, "judge_total_tokens": 648, "judge_prompt_tokens": 539, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The TV show about two comic book fanboys who discover a secret hospital for superheroes and are offered a job is called *Mighty Med* [Doc 121125].", "latency_ms": 652.2948741912842, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the show as Mighty Med and stays faithful to the information implied in the question and gold answer, with only minor added descriptive detail that is accurate.", "main_total_tokens": 2479, "main_prompt_tokens": 2441, "main_completion_tokens": 38, "judge_total_tokens": 491, "judge_prompt_tokens": 426, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "The seven prisoners in *Túnel de los huesos* (2011) escaped from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 599.0822315216064, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer clearly states that the prison was in Buenos Aires, Argentina, which matches the gold answer of Argentina. It does not contradict the given information and stays faithful to it while adding a specific prison name and city that are consistent with the country mentioned in the gold answer.", "main_total_tokens": 3074, "main_prompt_tokens": 3035, "main_completion_tokens": 39, "judge_total_tokens": 512, "judge_prompt_tokens": 426, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "Based strictly on the context provided, information regarding the movie \"Sweet Trap (2024)\" is not available.", "latency_ms": 3143.5935497283936, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The question is fact-based and has a known correct answer: Jiang Jie. The generated answer claims the information is not available in the provided context, which contradicts the given gold answer and therefore is incorrect. However, it is somewhat faithful to the (incorrect) assumption that no context was provided, rather than hallucinating an answer; it does not invent a person or plot detail. Thus correctness is 0, but faithfulness is not 0 since it stays within its (mistaken) framing instead of fabricating specifics.", "main_total_tokens": 3597, "main_prompt_tokens": 2952, "main_completion_tokens": 645, "judge_total_tokens": 539, "judge_prompt_tokens": 406, "judge_completion_tokens": 133}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "Based strictly on the context provided, there are no Korean-language movies listed that were released after 2023 [Doc 106219, Doc 105198].", "latency_ms": 2361.953020095825, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct answer is 4, but the generated answer claims there are no such movies, so the correctness is 0. The faithfulness is partial because the model does refer to the given documents, but it incorrectly interprets them (it should have concluded 4 instead of none), so its reasoning is not faithful to the gold answer.", "main_total_tokens": 3206, "main_prompt_tokens": 2810, "main_completion_tokens": 396, "judge_total_tokens": 516, "judge_prompt_tokens": 415, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "Based strictly on the context provided, the number of movies featuring both Steven Seagal and Bruce Lee cannot be determined, as Steven Seagal is not mentioned in any of the documents.", "latency_ms": 3907.100200653076, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.5, "faithfulness_score": 0, "justification": "The gold answer states explicitly there are no movies with both Bruce Lee and Steven Seagal. The generated answer claims the number cannot be determined from the context, which conflicts with the gold, so it is not faithful. It is partially correct only in the narrow sense that it does not assert a wrong number, but it fails to provide the definitive answer (zero/none) indicated by the gold.", "main_total_tokens": 4416, "main_prompt_tokens": 3560, "main_completion_tokens": 856, "judge_total_tokens": 541, "judge_prompt_tokens": 429, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of *Tokyo Mater* (2008) is 6.3 [Doc 42750].", "latency_ms": 639.6341323852539, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the rating of Tokyo Mater (2008) is 6.3, which matches the gold label. It does not introduce any unsupported or contradictory information. The citation tag does not conflict with the content, so the response is both correct and faithful.", "main_total_tokens": 3056, "main_prompt_tokens": 3029, "main_completion_tokens": 27, "judge_total_tokens": 492, "judge_prompt_tokens": 405, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "", "latency_ms": 4733.707666397095, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is empty, so it does not provide the correct style of racing (Drift) and does not address the question, resulting in zero correctness and faithfulness.", "main_total_tokens": 4266, "main_prompt_tokens": 3243, "main_completion_tokens": 1023, "judge_total_tokens": 445, "judge_prompt_tokens": 379, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the TV show about acclaimed New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is *Confronting a Serial Killer* [Doc 131298].", "latency_ms": 728.3015251159668, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.94, "justification": "The generated answer correctly identifies the title as 'Confronting a Serial Killer', which matches the gold answer, so correctness is 1. The only discrepancy is referring to it as a TV show rather than a movie, which is a minor but real deviation from the question, slightly reducing faithfulness.", "main_total_tokens": 2942, "main_prompt_tokens": 2890, "main_completion_tokens": 52, "judge_total_tokens": 547, "judge_prompt_tokens": 454, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of *Armageddon* (1998) is 7.838 [Doc 20603].", "latency_ms": 678.842306137085, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and does not introduce any contradictory or extraneous information. It is both correct and faithful to the provided gold answer.", "main_total_tokens": 3299, "main_prompt_tokens": 3269, "main_completion_tokens": 30, "judge_total_tokens": 472, "judge_prompt_tokens": 409, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 movie categorized as Mystery, Thriller, and Drama that alludes to an extra sense is *The Sixth Sense* [Doc 22098].\n\nAnother 1999 movie that alludes to an extra sense is *The Seventh Sense*, which is categorized as Drama [Doc 23090].", "latency_ms": 4553.079605102539, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0.8, "faithfulness_score": 0.3, "justification": "The main question asks for the title of the 1999 psychological thriller alluding to an extra sense; the correct and gold answer is \"The Sixth Sense,\" which the generated answer includes, so it is mostly correct. However, the generated response adds an extra movie, \"The Seventh Sense,\" as another 1999 movie alluding to an extra sense, which is not part of the gold answer and appears hallucinated or irrelevant. This additional fabricated detail reduces faithfulness to the given gold reference and to likely real-world facts.", "main_total_tokens": 3838, "main_prompt_tokens": 2977, "main_completion_tokens": 861, "judge_total_tokens": 596, "judge_prompt_tokens": 455, "judge_completion_tokens": 141}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The movie *The Transporter* was released in 2002 [Doc 27318].", "latency_ms": 602.1561622619629, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer. It is fully consistent with the provided information and does not introduce contradictions.", "main_total_tokens": 3222, "main_prompt_tokens": 3198, "main_completion_tokens": 24, "judge_total_tokens": 465, "judge_prompt_tokens": 399, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 installment of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 2075.86407661438, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2015 reboot title as 'The Transporter Refueled,' which matches the gold answer exactly. It does not introduce any contradictory or unsupported information beyond a harmless document reference, so it is both correct and faithful.", "main_total_tokens": 3580, "main_prompt_tokens": 3267, "main_completion_tokens": 313, "judge_total_tokens": 501, "judge_prompt_tokens": 420, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in 2015 [Doc 67500].", "latency_ms": 601.5515327453613, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, which matches the gold answer. The answer is also faithful to the provided information and does not introduce unsupported claims.", "main_total_tokens": 3261, "main_prompt_tokens": 3236, "main_completion_tokens": 25, "judge_total_tokens": 478, "judge_prompt_tokens": 404, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "Based strictly on the provided context, the romantic comedy in which Abraham Lincoln comes back from the dead is not mentioned.", "latency_ms": 4157.159328460693, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says no such movie description can be found. The generated answer correctly and faithfully states that, based on the provided context, the romantic comedy is not mentioned. This matches the gold and does not invent a movie, so it is both correct and faithful to the context.", "main_total_tokens": 4034, "main_prompt_tokens": 3149, "main_completion_tokens": 885, "judge_total_tokens": 500, "judge_prompt_tokens": 413, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Based strictly on the context provided, the title of a movie that alludes to a manuscript written by Ernest Hemingway cannot be determined [Doc 102100, Doc 96517, Doc 101461, Doc 98117, Doc 102771, Doc 100135, Doc 99782, Doc 98346, Doc 100884, Doc 79088, Doc 102534, Doc 97200, Doc 98371, Doc 97057, Doc 97134, Doc 101222, Doc 103405, Doc 99081, Doc 100315, Doc 102715].", "latency_ms": 3365.7431602478027, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the movie title and the gold answer is 'Havana Darkness'. The generated answer claims the title cannot be determined, which is factually incorrect given the gold answer. However, the generated answer is faithful to the given context because it explicitly bases its response on the provided documents and states that the title cannot be determined from them, without introducing unsupported information.", "main_total_tokens": 3728, "main_prompt_tokens": 2944, "main_completion_tokens": 784, "judge_total_tokens": 619, "judge_prompt_tokens": 514, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show *Doll & Em* [Doc 122259].", "latency_ms": 586.4431858062744, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em' and matches the gold answer exactly. It stays faithful to the provided information without adding unsupported details.", "main_total_tokens": 2929, "main_prompt_tokens": 2898, "main_completion_tokens": 31, "judge_total_tokens": 482, "judge_prompt_tokens": 417, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Answer based strictly on the context. Do not use outside knowledge. Cite sources for every fact using the format [Doc X]. If answer is not found, say so.\n\nThe requested information about a 2007 movie featuring a character named Hallam who spies on people to find the cause of his mother's death is not found in the provided context.", "latency_ms": 4162.397623062134, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question clearly corresponds to the 2007 movie 'Hallam Foe', so the correct answer is available from general knowledge. The generated answer incorrectly claims the information is not found in the context and does not provide the movie title, so it is factually incorrect relative to the gold answer. However, it strictly follows the instruction to base the answer only on the provided context and to say the answer is not found if it is not in that context, so it is faithful to the instructions and context.", "main_total_tokens": 3793, "main_prompt_tokens": 2925, "main_completion_tokens": 868, "judge_total_tokens": 600, "judge_prompt_tokens": 468, "judge_completion_tokens": 132}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Based strictly on the provided context, there is no individual whose last name is mentioned that sounds like \"friend\" or \"foe\" [Doc 22549, Doc 1784, Doc 3213, Doc", "latency_ms": 5202.88872718811, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The question asks for a categorical answer ('A foe'). The generated answer instead claims there is no such last name mentioned, which contradicts the gold label and is therefore incorrect. It is somewhat faithful to the (unstated) context by explicitly referencing a lack of information rather than hallucinating a name, but since we know the correct answer from the gold, its conclusion is not faithful to the intended reading task.", "main_total_tokens": 3601, "main_prompt_tokens": 2579, "main_completion_tokens": 1022, "judge_total_tokens": 540, "judge_prompt_tokens": 430, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "Sam is 18 years old in the 2017 TV show *Atypical* [Doc 124583].", "latency_ms": 655.8425426483154, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old, which matches the gold answer. It is fully consistent with the provided information and does not introduce contradictions or unsupported claims.", "main_total_tokens": 3182, "main_prompt_tokens": 3152, "main_completion_tokens": 30, "judge_total_tokens": 478, "judge_prompt_tokens": 411, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show *Atypical*, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 501.69897079467773, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the TV show 'Atypical', Sam is on the autism spectrum, which matches the gold answer exactly and does not add any contradictory or extraneous information.", "main_total_tokens": 3058, "main_prompt_tokens": 3029, "main_completion_tokens": 29, "judge_total_tokens": 481, "judge_prompt_tokens": 411, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "In the movie *À corde tendue*, released in 2013, Sandra was born in Chamonix [Doc 59659].", "latency_ms": 639.7087574005127, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, matching the gold answer exactly. It is also faithful, as it does not introduce contradictions or unsupported claims beyond restating the correct birthplace.", "main_total_tokens": 3526, "main_prompt_tokens": 3493, "main_completion_tokens": 33, "judge_total_tokens": 497, "judge_prompt_tokens": 425, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In this Japanese language movie, Hazuki clips her nails every time it **rains** [Doc 100481].", "latency_ms": 2070.005178451538, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and correctly fills the blank. It is fully consistent with the provided information, so both correctness and faithfulness are maximal.", "main_total_tokens": 4439, "main_prompt_tokens": 4050, "main_completion_tokens": 389, "judge_total_tokens": 482, "judge_prompt_tokens": 415, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Tabaluga is the crown-prince of Greenland [Doc 114810].", "latency_ms": 2391.221523284912, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Greenland as the place where Tabaluga is crown-prince, matching the gold answer exactly and staying faithful to the provided document reference.", "main_total_tokens": 3730, "main_prompt_tokens": 3337, "main_completion_tokens": 393, "judge_total_tokens": 473, "judge_prompt_tokens": 410, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with that description is *One Afternoon* [Doc 96838].", "latency_ms": 601.5160083770752, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'One Afternoon', which matches the gold answer exactly. It remains faithful to the provided information without adding unsupported details.", "main_total_tokens": 2695, "main_prompt_tokens": 2676, "main_completion_tokens": 19, "judge_total_tokens": 482, "judge_prompt_tokens": 420, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "Based strictly on the provided context, the answer is not found.", "latency_ms": 5258.240461349487, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "recall_at_10": 0.0, "precision_at_5": 0.0, "precision_at_10": 0.0, "ndcg_at_5": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_5": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'The Life Ahead.' The generated answer claims the answer is not found, which is factually incorrect. It also contradicts the known correct answer from the provided context, so it is neither correct nor faithful.", "main_total_tokens": 3715, "main_prompt_tokens": 2997, "main_completion_tokens": 718, "judge_total_tokens": 488, "judge_prompt_tokens": 411, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 548.6421585083008, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary, matching the gold answer and aligning with the referenced document. It is both factually correct and faithful to the provided context.", "main_total_tokens": 3196, "main_prompt_tokens": 3161, "main_completion_tokens": 35, "judge_total_tokens": 490, "judge_prompt_tokens": 417, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The context mentions the Newman-Haas (Andretti) racing team in the plot description for the movie *Super Speedway* [Doc 20456].", "latency_ms": 2006.680965423584, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the name of his racing team, with the gold answer being 'The McLaren Motor Racing team.' The generated answer instead cites 'Newman-Haas (Andretti) racing team,' which is incorrect. It is also unfaithful to the required answer, as it references a different team and appears to draw from unrelated context (a movie plot) rather than the relevant information leading to the gold answer.", "main_total_tokens": 2899, "main_prompt_tokens": 2865, "main_completion_tokens": 34, "judge_total_tokens": 529, "judge_prompt_tokens": 413, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "Based on the context provided, here are the countries associated with individuals or characters referred to as \"he\" or \"man\":\n\n*   **England/Britain:** Guy Burgess, the notorious spy in *An Englishman Abroad* (1983), is referred to as a \"mysterious Englishman\" [Doc 3606].\n*   **Lebanon (Beirut):** Momo, the blind hitman in *Man from Beirut* (2019), is implied to be from Beirut based on the title [Doc 88121].\n*   **United States (America):** The 6-year-old grandson in *A Grandson from America* (2012) is an \"American grandson\" [Doc 55219].\n*   **Earth:** An alien in *What Planet Are You From?* (2000) is sent to Earth [Doc 24412].\n*   **Colombia (Bogotá):** Carlos, the character in *A Male* (2023), lives in a boarding school in the center of Bogotá [Doc 108176].\n\nOther documents refer to \"he\" or \"man\" but do not specify a country of origin for the main character:\n\n*   *He* (2022) [Doc 104428]\n*   *Who Is He* (2023) (set in Ningjiang) [Doc 133914]\n*   *Man* (2021) [Doc 100152]\n*   *Man* (2022) [Doc 104406]\n*   *Man* (2001) [Doc 27001]\n*   *Man* (2006) [Doc 38724]\n*   *Man* (2008) [Doc 44476]\n*   *Man* (2014) [Doc 67263]\n*   *Man* (2017) [Doc 81611]\n*   *Man* (2016) [Doc 76556]\n*   *A Man* (2022) [Doc 101045]", "latency_ms": 2593.564748764038, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.3, "justification": "The question asks for a single country of origin (Gold: New Zealand). The generated answer instead lists multiple possible countries (England, Lebanon, United States, Earth, Colombia) and explicitly states that some documents do not specify a country, without identifying New Zealand at all. Therefore, it is factually incorrect with respect to the gold answer. In terms of faithfulness, the answer appears loosely based on some underlying documents (referencing titles and locations), but it also fabricates implications (e.g., assuming origin from a film title, or listing multiple unrelated examples) rather than directly addressing the specific target entity. It does not faithfully answer the posed question, so faithfulness is low.", "main_total_tokens": 3307, "main_prompt_tokens": 2792, "main_completion_tokens": 515, "judge_total_tokens": 964, "judge_prompt_tokens": 794, "judge_completion_tokens": 170}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based strictly on the provided context, there is no information about a 2013 movie", "latency_ms": 4563.583135604858, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.5, "faithfulness_score": 1, "justification": "Gold answer says explicitly the database has no such movie. The question is looking for that response. The generated answer instead comments on the lack of information in the provided context, not on the database, and does not clearly answer the question. It is faithful to the given context but not correctly aligned with the gold answer’s intent, so correctness is partial and faithfulness is high.", "main_total_tokens": 3881, "main_prompt_tokens": 2859, "main_completion_tokens": 1022, "judge_total_tokens": 523, "judge_prompt_tokens": 415, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with that description is *Tomie: Forbidden Fruit* [Doc 28589].", "latency_ms": 655.9786796569824, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Tomie: Forbidden Fruit'. It is both factually correct and consistent with the provided information.", "main_total_tokens": 3053, "main_prompt_tokens": 3023, "main_completion_tokens": 30, "judge_total_tokens": 507, "judge_prompt_tokens": 442, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years in the 2021 movie *Lei mi parla ancora* [Doc 98566].", "latency_ms": 618.0486679077148, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, which matches the gold answer. It does not introduce contradictions or unsupported claims beyond referencing a document ID, so it is both correct and faithful.", "main_total_tokens": 3488, "main_prompt_tokens": 3452, "main_completion_tokens": 36, "judge_total_tokens": 502, "judge_prompt_tokens": 424, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "Based strictly on the context provided, the movie *Scorpion King: Book of Souls* (2018) is not mentioned, and therefore, the object of the search by The Scorpion King and Tala cannot be determined.", "latency_ms": 2497.459888458252, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks what The Scorpion King and Tala search for in the 2018 movie 'Scorpion King: Book of Souls.' The gold answer correctly states 'The Book of Souls.' The generated answer claims that the movie is not mentioned in the context and that the object of their search cannot be determined. This is factually incorrect relative to the gold answer, so correctness is 0. However, the generated answer is faithful to its own stated limitation (it bases its conclusion strictly on the given context and does not hallucinate an answer), so faithfulness is 1.", "main_total_tokens": 3270, "main_prompt_tokens": 2756, "main_completion_tokens": 514, "judge_total_tokens": 586, "judge_prompt_tokens": 439, "judge_completion_tokens": 147}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the movie \"Zombies on the Moon\" is not listed.", "latency_ms": 2377.985954284668, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that, based on the given context, the movie is not listed, and thus does not invent a release year. This matches the gold answer in meaning and is fully supported by the context.", "main_total_tokens": 3710, "main_prompt_tokens": 3316, "main_completion_tokens": 394, "judge_total_tokens": 485, "judge_prompt_tokens": 411, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Galactic baker\" is not available in the provided context.", "latency_ms": 2240.2029037475586, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database; the generated answer states that the release year is not available in the provided context. Both effectively convey that the release year is unknown/unavailable, so the generated answer is correct and faithful to the gold.", "main_total_tokens": 4070, "main_prompt_tokens": 3654, "main_completion_tokens": 416, "judge_total_tokens": 490, "judge_prompt_tokens": 406, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Answer based strictly on the context. Do not use outside knowledge. Cite sources for every fact using the format [Doc X]. If answer is not found, say so.\n\nThe release year for \"Return of the Null Pointer\" is not found in the provided context.", "latency_ms": 2772.5014686584473, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The gold answer says the movie is not in the database, which means the system should say it is not found. The generated answer says the release year is not found in the provided context, which is effectively the same outcome (no year provided) but slightly rephrased. This is correct in substance. However, it partially ignores the explicit system-style instruction in the prompt to answer based strictly on the context and cite sources; instead, it only says it is not in the context without citations, so it is slightly less faithful to the full set of given instructions, though the core semantic content matches the gold.", "main_total_tokens": 3424, "main_prompt_tokens": 2852, "main_completion_tokens": 572, "judge_total_tokens": 599, "judge_prompt_tokens": 444, "judge_completion_tokens": 155}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for \"Data Science: The Movie\" is not available.", "latency_ms": 2392.1806812286377, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database; the generated answer says the release year is not available based on the context. Both convey that the information is unavailable, so the generated answer is correct and faithful to the gold.", "main_total_tokens": 3543, "main_prompt_tokens": 3039, "main_completion_tokens": 504, "judge_total_tokens": 491, "judge_prompt_tokens": 413, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for the movie \"Python's Revenge 2\" is not available in the provided context.", "latency_ms": 2367.950439453125, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database, i.e., the release year is unknown. The generated answer states that the release year is not available in the provided context, which is effectively the same claim of missing information. It does not introduce any incorrect facts or contradict the gold, so it is both correct and faithful.", "main_total_tokens": 3634, "main_prompt_tokens": 3140, "main_completion_tokens": 494, "judge_total_tokens": 510, "judge_prompt_tokens": 412, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for \"The Last Pixel\" is not available.", "latency_ms": 2238.2006645202637, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database, implying the release year is not known. The generated answer correctly states that the release year is not available based on the provided context. It is both factually correct relative to the gold and faithful to the given information.", "main_total_tokens": 3041, "main_prompt_tokens": 2622, "main_completion_tokens": 419, "judge_total_tokens": 495, "judge_prompt_tokens": 409, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"The man who ate the sun\" was released is not available in the provided context.", "latency_ms": 2286.400079727173, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database. The generated answer says the release year is not available in the provided context, which is effectively the same limitation. It does not fabricate a year and remains consistent with the lack of information, so it is both correct relative to the task and faithful to the given context.", "main_total_tokens": 3475, "main_prompt_tokens": 3008, "main_completion_tokens": 467, "judge_total_tokens": 508, "judge_prompt_tokens": 412, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not contain information about who directed the movie \"The Last Days of Lehman Brothers\" [Doc 46069].", "latency_ms": 485.7625961303711, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer state that the director information is not available in the provided context/database. The generated answer matches the gold answer in content and does not add unsupported information, so it is both correct and faithful to the given context.", "main_total_tokens": 2953, "main_prompt_tokens": 2924, "main_completion_tokens": 29, "judge_total_tokens": 505, "judge_prompt_tokens": 424, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Based strictly on the context provided, the movie \"A Love to Die For\" is not listed, and therefore the director cannot be identified.", "latency_ms": 3048.3293533325195, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the director cannot be identified from the provided context. The generated answer is faithful to the constraint of using only the given database/context and does not hallucinate a director name.", "main_total_tokens": 3878, "main_prompt_tokens": 3228, "main_completion_tokens": 650, "judge_total_tokens": 497, "judge_prompt_tokens": 423, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Based strictly on the context provided, the director of the movie \"Journey into Gaza\" is not mentioned [Doc 111360].", "latency_ms": 1706.3536643981934, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director of 'Journey into Gaza' is not mentioned and explicitly notes it is based strictly on the provided context. This matches the gold answer, which says the director information is not provided in the database. There is no hallucinated director name or extra unsupported detail, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3950, "main_prompt_tokens": 3682, "main_completion_tokens": 268, "judge_total_tokens": 519, "judge_prompt_tokens": 419, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Answer based strictly on the context. Do not use outside knowledge. Cite sources for every fact using the format [Doc X]. If answer is not found, say so.\n\nThe context does not provide the name of the director for \"The Last Night at Tremore Beach\" [Doc 135098].", "latency_ms": 2007.720947265625, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not provide the director's name and therefore the answer is not found. This matches the gold answer's stance that the database does not provide the director information. It follows the instruction to answer based strictly on context and not use outside knowledge, and it cites the document as required. Thus it is both correct and faithful to the given context and instructions.", "main_total_tokens": 2952, "main_prompt_tokens": 2623, "main_completion_tokens": 329, "judge_total_tokens": 566, "judge_prompt_tokens": 457, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The following movies describe a group of archaeology students awakening an ancient mummy:\n\n*   *Rise of the Mummy* (2021): \"A group of archaeology students awaken an ancient mummy\" [Doc 98375].\n*   *Ancient Evil: Scream of the Mummy* (2000): \"Six young archaeology students discover the remains of an ancient Aztec mummy and accidentally unleash the fury of an evil god\" [Doc 24655].", "latency_ms": 966.1464691162109, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The question asks for a single movie title; the correct answer is just \"Rise of the Mummy.\" The generated answer lists multiple films, so it is not correct relative to the gold. It is mostly faithful to the provided information, but it adds an extra movie beyond what the question requires and partially shifts focus, so faithfulness is not perfect.", "main_total_tokens": 2878, "main_prompt_tokens": 2779, "main_completion_tokens": 99, "judge_total_tokens": 586, "judge_prompt_tokens": 483, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death and flees to escape her abusive spouse is *Stranger in My Bed* [Doc 35237].", "latency_ms": 564.0695095062256, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Stranger in My Bed' and stays faithful to the provided information.", "main_total_tokens": 2878, "main_prompt_tokens": 2844, "main_completion_tokens": 34, "judge_total_tokens": 488, "judge_prompt_tokens": 428, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The event described happens in the TV show *Game of Silence* [Doc 123938].", "latency_ms": 618.2324886322021, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question describes the plot of Game of Silence. The gold answer correctly identifies it as a movie, but Game of Silence is actually a TV series. The generated answer correctly states that the event happens in the TV show Game of Silence, which matches the underlying fact and is faithful to the provided information, even though it differs in format (movie vs. TV show) from the gold label.", "main_total_tokens": 2470, "main_prompt_tokens": 2447, "main_completion_tokens": 23, "judge_total_tokens": 525, "judge_prompt_tokens": 416, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie based on a true story about a killer who abuses prostitutes he finds via Craigslist is **The Craigslist Killer** [Doc 53039]. The description states that it is based on a true story about Philip Markoff, a med student who \"leads a double life as a brutal and cruel sexual deviant who abuses prostitutes he finds via Craigslist\" [Doc 53039].", "latency_ms": 882.2100162506104, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer', which matches the gold answer, so correctness is 1. However, it references an external document [Doc 53039] and specific quoted descriptions that are not present in the user prompt, so the reasoning is not fully faithful to the given context, making faithfulness 0.", "main_total_tokens": 2930, "main_prompt_tokens": 2849, "main_completion_tokens": 81, "judge_total_tokens": 569, "judge_prompt_tokens": 469, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which this happens is *Wood Job!* [Doc 66094].", "latency_ms": 617.7427768707275, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Wood Job!', which exactly matches the gold answer. It is fully consistent with the given question and does not introduce contradictory or unsupported information, so both correctness and faithfulness are maximal.", "main_total_tokens": 3976, "main_prompt_tokens": 3955, "main_completion_tokens": 21, "judge_total_tokens": 493, "judge_prompt_tokens": 417, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie where a playful bird snatches Abby's magic wand during a hot-air balloon ride, leading Elmo, Abby, and friends on a colorful adventure, is titled *Sesame Street: The Magical Wand Chase* [Doc 78373].", "latency_ms": 834.700345993042, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, naming 'Sesame Street: The Magical Wand Chase' as the movie. It is correct and faithful to the provided information.", "main_total_tokens": 3730, "main_prompt_tokens": 3675, "main_completion_tokens": 55, "judge_total_tokens": 525, "judge_prompt_tokens": 460, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie where this happens is *Beyond* [Doc 74597].", "latency_ms": 633.4502696990967, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Beyond,' which matches the gold answer exactly. It does not introduce any new, unsupported information and remains fully grounded in the given content.", "main_total_tokens": 2744, "main_prompt_tokens": 2726, "main_completion_tokens": 18, "judge_total_tokens": 490, "judge_prompt_tokens": 423, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman, fired from a financial corporation during the Asia crisis, returns home with no money is **6ixtynin9** [Doc 22478].", "latency_ms": 555.6001663208008, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', which matches the gold answer exactly. It also remains faithful to the given information without adding unsupported claims.", "main_total_tokens": 2723, "main_prompt_tokens": 2683, "main_completion_tokens": 40, "judge_total_tokens": 506, "judge_prompt_tokens": 439, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie described is *Club Dread* [Doc 32410].", "latency_ms": 649.3015289306641, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Club Dread') and stays fully consistent with the provided information, without introducing any unsupported details.", "main_total_tokens": 3098, "main_prompt_tokens": 3081, "main_completion_tokens": 17, "judge_total_tokens": 498, "judge_prompt_tokens": 439, "judge_completion_tokens": 59}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "This happens in the movie titled **Toilet** [Doc 48873].", "latency_ms": 618.0155277252197, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Toilet', which matches the gold answer, and does not introduce any unsupported details beyond restating the title. It is both correct and faithful.", "main_total_tokens": 3310, "main_prompt_tokens": 3292, "main_completion_tokens": 18, "judge_total_tokens": 499, "judge_prompt_tokens": 430, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The description provided matches the movie *Electric Angel* [Doc 1024].", "latency_ms": 619.4484233856201, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the description, and the gold answer is 'Electric Angel'. The generated answer explicitly states that the description matches the movie 'Electric Angel', which is correct. It does not introduce contradictory information beyond a harmless document reference, so it is both correct and faithful to the provided gold answer.", "main_total_tokens": 3275, "main_prompt_tokens": 3257, "main_completion_tokens": 18, "judge_total_tokens": 525, "judge_prompt_tokens": 432, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is *Fall* with a rating of 4.0 [Doc 106810].", "latency_ms": 577.4950981140137, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the lowest rated movie from 2023, with the gold answer being 'Mega Ape'. The generated answer instead states that the lowest rated movie is 'Fall' with a rating of 4.0, which is incorrect relative to the gold. Additionally, the generated answer fabricates a specific rating and citation ([Doc 106810]) that are not supported by the provided gold answer, so it is not faithful to the given information.", "main_total_tokens": 3150, "main_prompt_tokens": 3118, "main_completion_tokens": 32, "judge_total_tokens": 529, "judge_prompt_tokens": 407, "judge_completion_tokens": 122}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "The highest rated TV show from 2010 is *Tea House* with a rating of 9.2 [Doc 119448].", "latency_ms": 620.42236328125, "retrieval_recall": 1.0, "retrieval_mrr": 0.08333333333333333, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 'Tea House' as the highest rated TV show from 2010, matching the gold answer exactly. The added detail about the rating and document reference does not contradict the gold and is plausible within the context, so the response is both correct and faithful to the provided information.", "main_total_tokens": 3500, "main_prompt_tokens": 3466, "main_completion_tokens": 34, "judge_total_tokens": 501, "judge_prompt_tokens": 410, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest rated Japanese language movie from 1999 is *The Geisha House*, with a rating of 7.4 [Doc 23076].", "latency_ms": 633.7149143218994, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer names 'The Geisha House' as the highest rated Japanese language movie from 1999, but the gold answer indicates it should be 'The Uncut Story of Mewtwo's Origin'. Therefore the content is factually incorrect relative to the provided gold. It also appears to be unfaithful to the underlying source document set as indicated by the bracketed citation, since it contradicts the known correct answer, so faithfulness is scored as 0 as well.", "main_total_tokens": 3046, "main_prompt_tokens": 3010, "main_completion_tokens": 36, "judge_total_tokens": 550, "judge_prompt_tokens": 422, "judge_completion_tokens": 128}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "", "latency_ms": 4215.553760528564, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is empty, so it does not state the correct language (English) for the lowest-rated 1980 movie and provides no reasoning or evidence. It is therefore neither correct nor faithful.", "main_total_tokens": 4533, "main_prompt_tokens": 3510, "main_completion_tokens": 1023, "judge_total_tokens": 455, "judge_prompt_tokens": 384, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "", "latency_ms": 4679.732322692871, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The model produced no answer at all, so it is not correct and not faithful to the gold answer that specifies 'Law & Order: Special Victims Unit'.", "main_total_tokens": 4706, "main_prompt_tokens": 3683, "main_completion_tokens": 1023, "judge_total_tokens": 455, "judge_prompt_tokens": 391, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 585.8900547027588, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (7.2) and explicitly cites the document as its source, so it is both correct and faithful.", "main_total_tokens": 3105, "main_prompt_tokens": 3084, "main_completion_tokens": 21, "judge_total_tokens": 461, "judge_prompt_tokens": 400, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in 1999 [Doc 22481] and also in 2019 [Doc 88811].", "latency_ms": 564.5432472229004, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a single release year. The gold answer is 2019, but the generated answer lists both 1999 and 2019, which is incorrect for this query. It is therefore not fully correct. It is also not faithful to the gold answer because it introduces an extra, conflicting year instead of only giving 2019.", "main_total_tokens": 3143, "main_prompt_tokens": 3101, "main_completion_tokens": 42, "judge_total_tokens": 517, "judge_prompt_tokens": 416, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The provided context does not contain information about the number of votes \"My Dog Skip\" received [Doc 24114].", "latency_ms": 555.161714553833, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 279 votes, but the generated answer claims the context does not contain this information and does not provide the correct number. This is faithful to the described context (it correctly notes the absence of the information) but incorrect relative to the gold answer.", "main_total_tokens": 3139, "main_prompt_tokens": 3112, "main_completion_tokens": 27, "judge_total_tokens": 489, "judge_prompt_tokens": 405, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "Based strictly on the context provided, the number of votes \"Oh My God!\" received is not available [Doc 85434].", "latency_ms": 2208.616018295288, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that 'Oh My God!' received 12 votes. The generated answer claims that the number of votes is not available, which is factually incorrect given the gold. However, the model follows its own stated constraint of relying only on the provided context, so it is faithful to its reasoning but incorrect relative to the gold answer.", "main_total_tokens": 3608, "main_prompt_tokens": 3172, "main_completion_tokens": 436, "judge_total_tokens": 507, "judge_prompt_tokens": 407, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The release year for the movie \"The Last Days of Lehman Brothers\" is not available in the provided context.", "latency_ms": 2710.103750228882, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the release year as 2009, but the generated answer claims the release year is not available, which is factually incorrect relative to the gold. However, the generated response is faithful to the (hypothetical) provided context it references, as it does not fabricate an answer beyond what it says is available.", "main_total_tokens": 3425, "main_prompt_tokens": 2838, "main_completion_tokens": 587, "judge_total_tokens": 508, "judge_prompt_tokens": 410, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_bge-m3_gemini-flash-latest", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English (en) [Doc 48035].", "latency_ms": 671.5171337127686, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The movie \"The King's Speech\" is indeed originally in English, matching the gold answer 'en'. The generated answer is fully consistent with the provided gold label and does not introduce contradictions, so it is both correct and faithful.", "main_total_tokens": 3211, "main_prompt_tokens": 3184, "main_completion_tokens": 27, "judge_total_tokens": 481, "judge_prompt_tokens": 405, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "Based strictly on the provided context, the release year for a title named \"Wall Street\" is not available.", "latency_ms": 2854.5117378234863, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1987, but the generated answer claims the release year is not available. This is incorrect. However, the answer is faithful to the (nonexistent) provided context, as it explicitly bases its response on the lack of contextual information rather than inventing a year.", "main_total_tokens": 2828, "main_prompt_tokens": 2287, "main_completion_tokens": 541, "judge_total_tokens": 489, "judge_prompt_tokens": 400, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie is *Tokyo Gore School* [Doc 45096]. The plot describes a mysterious violent game being played out in high schools that is being spread virally via the students’ mobile phones, forcing the protagonist to fight for his survival [Doc 45096].", "latency_ms": 4046.112537384033, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific movie where classmates play a survival game using their cell phones. The gold answer is 'Brain Fluid Explosion Girl,' but the generated answer gives 'Tokyo Gore School' instead. This is factually incorrect relative to the labeled gold answer, so correctness is 0. However, the answer is consistent with and grounded in the provided document description (Doc 45096) about a violent game spread via students’ mobile phones, so faithfulness to the cited source is high (1).", "main_total_tokens": 2960, "main_prompt_tokens": 2125, "main_completion_tokens": 835, "judge_total_tokens": 569, "judge_prompt_tokens": 442, "judge_completion_tokens": 127}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show in which six friends are transported into a realm and must try to find a way home with the help of their guide 'Dungeon Master' is *Dungeons & Dragons* [Doc 112849].", "latency_ms": 702.8474807739258, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' matching the gold answer. It is faithful to the provided information and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 1925, "main_prompt_tokens": 1877, "main_completion_tokens": 48, "judge_total_tokens": 501, "judge_prompt_tokens": 433, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Based strictly on the context provided, the movie about two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is not listed [Doc 97225, Doc 67620, Doc 97788, Doc 59182, Doc 70073, Doc 60479, Doc 77683, Doc 88844, Doc 88070, Doc 62785, Doc 96457, Doc 91823, Doc 93199, Doc 68427, Doc 77244].", "latency_ms": 3876.821517944336, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer, based on the gold label, is 'Nacht der Wölfe'. The generated answer claims the movie is not listed in the provided documents and does not give the correct title, so it is factually incorrect. However, it is faithful to the (incorrect) reasoning from the stated context, as it explicitly bases its conclusion on the referenced documents, so faithfulness to the given context is high.", "main_total_tokens": 3056, "main_prompt_tokens": 2235, "main_completion_tokens": 821, "judge_total_tokens": 620, "judge_prompt_tokens": 505, "judge_completion_tokens": 115}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of *Picnic at Hanging Rock* take place on Valentine's Day, 1900 [Doc 126191].", "latency_ms": 1535.698652267456, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the year as 1900 and aligns with the gold answer without adding unsupported information.", "main_total_tokens": 2350, "main_prompt_tokens": 2113, "main_completion_tokens": 237, "judge_total_tokens": 463, "judge_prompt_tokens": 410, "judge_completion_tokens": 53}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "Based strictly on the context provided, the information about a 2004 movie matching that description is not found.", "latency_ms": 3050.304889678955, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'June'. The generated answer incorrectly claims the information is not found, so it is factually wrong. However, it is faithful to the (incorrect) reasoning it presents, not inventing extra details, so its faithfulness to its own stated reasoning is high.", "main_total_tokens": 3026, "main_prompt_tokens": 2473, "main_completion_tokens": 553, "judge_total_tokens": 542, "judge_prompt_tokens": 454, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1195.3496932983398, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is known (The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger), but the generated answer says 'I don't know,' which is incorrect. However, the model is honest about its lack of knowledge and does not fabricate information, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 399, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 300.7817268371582, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a specific numeric answer (7.6), but the generated response states ignorance, which is not correct. However, the model does not fabricate information and honestly expresses uncertainty, so it is fully faithful to its own stated knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 468, "judge_prompt_tokens": 389, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1017.7414417266846, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3535124404757273, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) is numerically incorrect compared to the gold answer (4). However, the model faithfully provides a direct numeric response without adding extra fabricated details or reasoning, so it is faithful to its own (albeit incorrect) output.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 394, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 156.43954277038574, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is factually incorrect, yielding a correctness score of 0. There is no reasoning or external fabrication in the generated output—just a single numeric answer—so it is fully faithful to itself, giving a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 395, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1503.56125831604, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which is incorrect because the correct answer is 'Fantômas.' However, the model does not hallucinate an alternative answer and simply expresses uncertainty, so it is fully faithful to its own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 389, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is *Kill Cruise* [Doc 11205].", "latency_ms": 539.1199588775635, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise', which matches the gold answer exactly. It does not introduce contradictions or unsupported claims; the added citation-style bracket does not alter the factual content, so the response is both correct and faithful.", "main_total_tokens": 2122, "main_prompt_tokens": 2095, "main_completion_tokens": 27, "judge_total_tokens": 489, "judge_prompt_tokens": 409, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie is *Won't You Be My Neighbor?* [Doc 84838].", "latency_ms": 3017.0528888702393, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title \"Won't You Be My Neighbor?\" and correctly addresses the question. It is fully faithful to the provided information without adding incorrect details.", "main_total_tokens": 2912, "main_prompt_tokens": 2340, "main_completion_tokens": 572, "judge_total_tokens": 489, "judge_prompt_tokens": 422, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "", "latency_ms": 5216.171503067017, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is empty, so it does not provide the correct year (2018) and cannot be considered faithful to the given gold answer.", "main_total_tokens": 3190, "main_prompt_tokens": 2167, "main_completion_tokens": 1023, "judge_total_tokens": 438, "judge_prompt_tokens": 378, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Based strictly on the context provided, the TV series that is an animation about a young boy robot in 2030 AD cannot be identified.", "latency_ms": 4779.3567180633545, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question is answerable from the provided gold answer: the TV series is 'Astro Boy'. The generated answer incorrectly claims it cannot be identified, so it is factually wrong. However, it is faithful to the (incorrect) reasoning it presents and does not invent unsupported information, so faithfulness is high.", "main_total_tokens": 3117, "main_prompt_tokens": 2165, "main_completion_tokens": 952, "judge_total_tokens": 509, "judge_prompt_tokens": 415, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "The woman and her son in the movie \"Room\" (2015) were held captive for 7 years [Doc 67513].", "latency_ms": 571.0687637329102, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer exactly. It is faithful to the information provided and does not introduce contradictions or extraneous claims.", "main_total_tokens": 1885, "main_prompt_tokens": 1852, "main_completion_tokens": 33, "judge_total_tokens": 496, "judge_prompt_tokens": 422, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The context states that on 26 November 1983, six armed men broke into the Brink's-Mat security depot and stumbled across gold bullion worth £26 million [Doc 133469]. The context does not specify the total amount of gold the depot was holding before the break-in, only the value of the gold bullion found by the robbers.", "latency_ms": 2671.947479248047, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks specifically how much gold was being held before the break-in, and the provided context clearly gives that figure as £26 million. The generated answer incorrectly claims that the context does not specify the total amount and implies the question cannot be answered from the context. This contradicts the given fact, so the answer is not correct with respect to the question, but it is consistent with its own (incorrect) reading of the context, making it faithful but incorrect.", "main_total_tokens": 2787, "main_prompt_tokens": 2304, "main_completion_tokens": 483, "judge_total_tokens": 590, "judge_prompt_tokens": 466, "judge_completion_tokens": 124}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6299.01385307312, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model said it did not know the answer, but the correct value is 80.5772, so the answer is incorrect. However, the response is faithful because it does not fabricate information and accurately reflects the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 474, "judge_prompt_tokens": 396, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 160.7656478881836, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1, but the generated answer was 0, so it is incorrect. The evaluation of correctness and faithfulness relative to the provided gold and generated answers is accurate.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 454, "judge_prompt_tokens": 385, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "The TV show is *Angel Eyes* [Doc 121928].\n\nThe description states that \"Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight\" in the TV show *Angel Eyes* [Doc 121928].", "latency_ms": 3312.549114227295, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Angel Eyes', which matches the gold answer. The explanation is fully consistent with the provided description, with no added unsupported details, so it is both correct and faithful.", "main_total_tokens": 3377, "main_prompt_tokens": 2686, "main_completion_tokens": 691, "judge_total_tokens": 524, "judge_prompt_tokens": 450, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The context provides information for several movies, each with a specified language:\n\n*   **en** (English) for *What Is This Film Called Love?* [Doc 58692], *OMG... We're in a Horror Movie* [Doc 70948], *This Movie Is Broken* [Doc 50210], *This Film Is Not Yet Rated* [Doc 37205], *See This Movie* [Doc 32691], *Hope Springs Eternal: A Look Back at The Shawshank Redemption* [Doc 33467], *Little Quentin* [Doc 49971], *FILM* (2020) [Doc 96081], and *Resident Evil: Reborn* [Doc 103699].\n*   **pt** (Portuguese) for *A Love Movie* [Doc 29899].\n*   **es** (Spanish) for *A Movie Made of* [Doc ", "latency_ms": 4406.967401504517, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the language is Korean, but the generated answer lists only English, Portuguese, and Spanish for various movies and does not identify Korean. It also does not answer the specific question about the target movie, instead giving a partial, cut-off list for several different movies, making it both incorrect and unfaithful to the gold.", "main_total_tokens": 3040, "main_prompt_tokens": 2018, "main_completion_tokens": 1022, "judge_total_tokens": 663, "judge_prompt_tokens": 564, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 545.5451011657715, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is factually incorrect. However, there is no indication that the model contradicted the given information in its own reasoning; it simply produced the wrong numeric result, so faithfulness to its (implicit) reasoning remains intact.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 486, "judge_prompt_tokens": 394, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Based strictly on the provided context, the best-rated movie released in 1979 cannot be determined, as none of the documents list a movie with the release year 1979 in the primary metadata field.", "latency_ms": 3478.026866912842, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.8, "faithfulness_score": 1, "justification": "Gold answer says explicitly there are no 1979 movies in the database. The generated answer says the best-rated 1979 movie cannot be determined because none of the documents list a 1979 movie, which is effectively the same conclusion and thus faithful to the gold. However, it introduces extra specificity about \"primary metadata field\" and frames it as based on provided context, which is not in the gold. Still, the core factual content (cannot identify such a movie) matches, so correctness is high but not perfect.", "main_total_tokens": 3037, "main_prompt_tokens": 2328, "main_completion_tokens": 709, "judge_total_tokens": 574, "judge_prompt_tokens": 436, "judge_completion_tokens": 138}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The context does not contain information regarding the release year of the movie *Back to the Future Part IV*. The documents mention *Back to the Future* (1985) [Doc 4834], *Back to the Future Part II* (1989) [Doc 9405], and *Back to the Future Part III* (1990) [Doc 10621], which is described as \"The final installment\" [Doc 10621].", "latency_ms": 2740.947484970093, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.0, "faithfulness_score": 0.8, "justification": "The correct answer should explicitly state that 'Back to the Future Part IV' does not exist, not merely that the context lacks the information. While the generated answer accurately reflects the given context (hence relatively high faithfulness), it is still incorrect with respect to the gold answer and real-world facts because it implies the answer is unknown rather than that the movie does not exist.", "main_total_tokens": 3437, "main_prompt_tokens": 2885, "main_completion_tokens": 552, "judge_total_tokens": 602, "judge_prompt_tokens": 492, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot find any movies or shows matching the criteria, which aligns with the gold answer that there were no movies from 1980 related to the Chernobyl disaster. It does not introduce any incorrect specific facts, only a clarification about the time period of the event, so it is both correct and faithful to the underlying information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 537, "judge_prompt_tokens": 434, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 116.790771484375, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is numerically incorrect compared to the gold answer (0 vs 4), so correctness is 0. The answer is internally consistent with what it states and does not contradict itself or introduce hallucinated reasoning beyond giving the wrong number, so faithfulness to its own (albeit mistaken) content is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 484, "judge_prompt_tokens": 388, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 184.89336967468262, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. It is, however, internally consistent with itself and thus faithful to its own reasoning, despite being factually wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 460, "judge_prompt_tokens": 386, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 115.5850887298584, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that there was 1 TV show about Super Mario Bros released in the 1990s, while the generated answer says 0. This directly contradicts the gold answer, so the generated answer is incorrect. The evaluation itself is faithful to the provided question, gold answer, and generated answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 389, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 115.88788032531738, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (0) and is directly supported by the gold, so it is both correct and faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 448, "judge_prompt_tokens": 391, "judge_completion_tokens": 57}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 115.94057083129883, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto', a specific and correct response. The generated answer says 'I don't know,' which is incorrect relative to the expected answer. However, it is faithful because it does not hallucinate any incorrect fact; it simply expresses a lack of knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 472, "judge_prompt_tokens": 386, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "Based strictly on the provided context, the release date for a Jurassic Park/Power Ranger crossover movie is not mentioned.", "latency_ms": 2878.704071044922, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "Gold says the movie was not released. The generated answer implies uncertainty and only claims the release date is not mentioned, rather than stating that the movie was never released. This is factually incorrect with respect to the gold answer and also not faithful to it.", "main_total_tokens": 2905, "main_prompt_tokens": 2291, "main_completion_tokens": 614, "judge_total_tokens": 490, "judge_prompt_tokens": 408, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Based strictly on the provided context, there is no movie listed where Barbie fights the Predator. The context mentions several movies featuring the Predator [Doc 6958, Doc 82000, Doc 47980, Doc 10613] and a movie titled *Killer Barbys vs. Dracula* [Doc 28703], but not a combination of Barbie and the Predator.", "latency_ms": 3910.5911254882812, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The gold answer says explicitly that the movie was not released, implying such a movie may have existed in concept but never came out. The generated answer instead claims that, based on the provided context, no such movie is listed, which is not the same as saying it was not released. So it is not fully faithful to the gold. However, both answers agree that there is no actual released movie where Barbie fights the Predator, so the generated answer is effectively correct about the real-world situation.", "main_total_tokens": 3204, "main_prompt_tokens": 2377, "main_completion_tokens": 827, "judge_total_tokens": 589, "judge_prompt_tokens": 458, "judge_completion_tokens": 131}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "Based strictly on the provided context, only four titles explicitly related to Star Wars are listed, and all were released in the same year (2004):\n\n1.  The Story of Star Wars (2004) [Doc 32101]\n2.  The Characters of Star Wars (2004) [Doc 31821]\n3.  When Star Wars Ruled the World (2004) [Doc 32997]\n4.  Empire of Dreams: The Story of the Star Wars Trilogy (2004) [Doc 32312]\n\nSince all identified Star Wars movies/documentaries share the same release year, it is impossible to determine which one is the 4th oldest [Doc 32101, Doc 31821, Doc 32997, Doc 32312]. The context does not provide the release years for the original Star Wars saga films (Episodes I-VI).", "latency_ms": 4284.318447113037, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question asks for the 4th oldest Star Wars movie, and the gold answer is a specific title: 'Star Wars: Droids - The Great Heep.' The generated answer instead lists four 2004 Star Wars-related documentaries from the context and concludes that it is impossible to determine which is 4th oldest, and does not identify 'Star Wars: Droids - The Great Heep' at all. This is incorrect relative to the gold answer. In terms of faithfulness to the provided context, the model accurately reports the four titles and their year, and correctly notes they share the same year based on the cited docs, but it overstates by saying it is impossible to determine the 4th oldest when, within that subset, a 4th item is clearly listed. It also ignores the gold answer. Thus, it is only partially faithful to context and not correct with respect to the task.", "main_total_tokens": 3455, "main_prompt_tokens": 2546, "main_completion_tokens": 909, "judge_total_tokens": 785, "judge_prompt_tokens": 568, "judge_completion_tokens": 217}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "", "latency_ms": 5457.5395584106445, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is empty and does not match the gold answer (1986), so it is incorrect and unfaithful.", "main_total_tokens": 3190, "main_prompt_tokens": 2167, "main_completion_tokens": 1023, "judge_total_tokens": 434, "judge_prompt_tokens": 378, "judge_completion_tokens": 56}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 115.89241027832031, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model said it did not know the rating, while the correct answer is 8.466. This is incorrect, but it is faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 462, "judge_prompt_tokens": 398, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is *Mighty Med* [Doc 121125].", "latency_ms": 555.68528175354, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med,' which matches the gold answer exactly. It remains faithful to the provided information and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 1917, "main_prompt_tokens": 1882, "main_completion_tokens": 35, "judge_total_tokens": 492, "judge_prompt_tokens": 423, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "The seven prisoners escaped from a prison in Argentina [Doc 53582]. Specifically, they escaped from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 742.1853542327881, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies Argentina as the country, fully matching the gold answer, so correctness is 1. However, it adds a specific detail about \"Villa Devoto prison in Buenos Aires\" that is not verifiable from the provided prompt and may not be supported by the cited document, so faithfulness is slightly reduced.", "main_total_tokens": 2309, "main_prompt_tokens": 2267, "main_completion_tokens": 42, "judge_total_tokens": 525, "judge_prompt_tokens": 427, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "Based strictly on the provided context, information regarding the movie *Sweet Trap (2024)* and the details of its plot are not available.", "latency_ms": 2662.445306777954, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific factual answer: in 'Sweet Trap (2024), the apprentice falls into whose trap?' The gold answer is 'Jiang Jie.' The generated response claims that such information is not available, which is incorrect because the gold clearly provides it. However, the answer is faithful to the model's stated limitations and does not hallucinate; it simply asserts lack of information. Thus it is fully faithful but factually incorrect relative to the gold.", "main_total_tokens": 2892, "main_prompt_tokens": 2309, "main_completion_tokens": 583, "judge_total_tokens": 536, "judge_prompt_tokens": 412, "judge_completion_tokens": 124}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 447.62396812438965, "retrieval_recall": 0.0, "retrieval_mrr": 0.25, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.16812753627111746, "hit_rate_at_5": 1.0, "recall_at_10": 0.25, "precision_at_10": 0.1, "ndcg_at_10": 0.16812753627111746, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 4, but the generated answer is 0, which is incorrect. The explanation is faithful in the sense that it directly reflects the provided gold and generated values without adding unsupported claims, but the generated answer itself is factually wrong relative to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 471, "judge_prompt_tokens": 385, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "35", "latency_ms": 68.66931915283203, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '35' is factually incorrect because there are no movies featuring both Steven Seagal and Bruce Lee together, matching the gold answer. It is also unfaithful to the gold response, which explicitly states that no such movies can be found.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 395, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of *Tokyo Mater* (2008) is 6.3 [Doc 42750].", "latency_ms": 454.2090892791748, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (6.3) and does not introduce extraneous or conflicting information, so it is both correct and faithful.", "main_total_tokens": 2241, "main_prompt_tokens": 2214, "main_completion_tokens": 27, "judge_total_tokens": 467, "judge_prompt_tokens": 405, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The context describes several styles of racing across different movies:\n\n*   **Street Racing:** Featured in *Fast Track: No Limits* [Doc 41929], *Need for Speed* (local street-racer) [Doc 62979], and *Manta, Manta* [Doc 12082].\n*   **Illegal/Cross-Country Racing:** Described as a \"New York-to-Los Angeles race\" and a \"cross-country gauntlet of illegal racers\" in *Need for Speed* [Doc 62979], an \"illegal race that takes place over the United States\" in *Speed Zone* [Doc 9922], and a \"wild dash for $2 million\" in *Rat Race* [Doc 25504].\n*   **Underground Racing:** Featured in *The Fear of Speed* as an \"underground racing competition\" [Doc 28283].\n*   **MotoGP™:** Described as \"the fastest sport on two wheels\" in the documentary *Faster* [Doc 30903].\n*   **Stock Car/Grassroots Auto Racing:** Featured in *The Last Race* [Doc 86410].\n*   **General Race-Car Driving:** Mentioned in *The Art of Racing in the Rain* [Doc ", "latency_ms": 4608.044385910034, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a specific racing style in a particular movie, with the gold answer being 'Drift'. The generated answer instead gives a broad list of different racing styles and movies, without identifying drift or focusing on the target movie. It does not answer the question and is not grounded in the provided gold label, so it is both incorrect and unfaithful to the required answer.", "main_total_tokens": 3488, "main_prompt_tokens": 2466, "main_completion_tokens": 1022, "judge_total_tokens": 747, "judge_prompt_tokens": 639, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the TV show is *Confronting a Serial Killer* [Doc 131298].", "latency_ms": 1473.881721496582, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer gives the correct title, 'Confronting a Serial Killer', which matches the gold answer. Referring to it as a TV show is accurate for the limited-series documentary, and there is no contradiction with given information, so the response is both correct and faithful.", "main_total_tokens": 2424, "main_prompt_tokens": 2207, "main_completion_tokens": 217, "judge_total_tokens": 514, "judge_prompt_tokens": 427, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of *Armageddon* (1998) is 7.838 [Doc 20603].", "latency_ms": 470.3946113586426, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and correctly attributes it, so it is both correct and faithful.", "main_total_tokens": 2539, "main_prompt_tokens": 2509, "main_completion_tokens": 30, "judge_total_tokens": 467, "judge_prompt_tokens": 409, "judge_completion_tokens": 58}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is *The Sixth Sense* [Doc 22098].", "latency_ms": 532.8023433685303, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Sixth Sense' and does not introduce any contradictions or hallucinations beyond the provided content. It is fully consistent with the gold answer.", "main_total_tokens": 2306, "main_prompt_tokens": 2272, "main_completion_tokens": 34, "judge_total_tokens": 487, "judge_prompt_tokens": 420, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The movie *The Transporter* was released in 2002 [Doc 27318].", "latency_ms": 634.9902153015137, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, which matches the gold answer. It is faithful to the provided information without adding unsupported claims.", "main_total_tokens": 2425, "main_prompt_tokens": 2401, "main_completion_tokens": 24, "judge_total_tokens": 464, "judge_prompt_tokens": 399, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 movie featuring Frank Martin, known as The Transporter, is *The Transporter Refueled* [Doc 67500].", "latency_ms": 2492.520332336426, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the title as 'The Transporter Refueled', matching the gold answer. However, it adds extra detail that may not be directly supported (mentioning '[Doc 67500]' and describing it as 'the 2015 movie featuring Frank Martin, known as The Transporter'), which slightly reduces faithfulness to the given context.", "main_total_tokens": 2923, "main_prompt_tokens": 2463, "main_completion_tokens": 460, "judge_total_tokens": 529, "judge_prompt_tokens": 424, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The movie 'The Transporter Refueled' was released in 2015 [Doc 67500].", "latency_ms": 574.6972560882568, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, which matches the gold answer. It does not introduce contradictions or unsupported claims, so it is both correct and faithful.", "main_total_tokens": 2420, "main_prompt_tokens": 2393, "main_completion_tokens": 27, "judge_total_tokens": 481, "judge_prompt_tokens": 406, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "Based strictly on the context provided, the title of a romantic comedy in which Abraham Lincoln comes back from the dead is not mentioned.", "latency_ms": 4509.760856628418, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no such movie description can be found, and the generated answer correctly and faithfully says that the title is not mentioned in the provided context. It does not invent a movie and stays aligned with the information given.", "main_total_tokens": 3331, "main_prompt_tokens": 2331, "main_completion_tokens": 1000, "judge_total_tokens": 493, "judge_prompt_tokens": 416, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Based strictly on the context provided, there is no movie title that alludes to a manuscript written by Ernest Hemingway.", "latency_ms": 2841.3519859313965, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly identifies 'Havana Darkness' as the movie title alluding to a Hemingway manuscript, so such a movie does exist in the provided context. The generated answer incorrectly claims there is no such movie title, making it factually wrong relative to the gold. However, it is faithful to the (insufficient) context it assumes, as it explicitly bases its answer on the provided context, so the reasoning matches its stated basis even though the conclusion is incorrect compared to the gold.", "main_total_tokens": 2820, "main_prompt_tokens": 2271, "main_completion_tokens": 549, "judge_total_tokens": 540, "judge_prompt_tokens": 411, "judge_completion_tokens": 129}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "The TV show in which Doll heads to Hollywood to be with her childhood best friend Em is **Doll & Em** [Doc 122259].", "latency_ms": 566.5910243988037, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Doll & Em' and does not introduce any extraneous or contradictory information.", "main_total_tokens": 2155, "main_prompt_tokens": 2122, "main_completion_tokens": 33, "judge_total_tokens": 482, "judge_prompt_tokens": 419, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 3505.345582962036, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'Hallam Foe' but the generated answer claims the answer is not in the context instead of giving the correct movie title, so it is incorrect and not faithful to the expected answer.", "main_total_tokens": 2915, "main_prompt_tokens": 2214, "main_completion_tokens": 701, "judge_total_tokens": 482, "judge_prompt_tokens": 409, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Based strictly on the context provided, there is no information describing a person's last name as sounding like \"friend\" or \"foe.\"", "latency_ms": 3396.434783935547, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific labeled answer ('A foe'), but the generated answer claims the context does not provide information and does not give the required label. This is incorrect relative to the gold answer, so correctness is 0. However, given the assistant only relied on the (stated) lack of context and did not introduce fabricated details, the response is faithful to the information it claimed to have, so faithfulness is 1.", "main_total_tokens": 2674, "main_prompt_tokens": 1967, "main_completion_tokens": 707, "judge_total_tokens": 532, "judge_prompt_tokens": 413, "judge_completion_tokens": 119}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "Sam, the main character in the 2017 TV show *Atypical*, is 18 years old [Doc 124583].", "latency_ms": 1634.6421241760254, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show 'Atypical', matching the gold answer. It also remains faithful to the provided information without adding contradictory or unsupported claims.", "main_total_tokens": 2663, "main_prompt_tokens": 2410, "main_completion_tokens": 253, "judge_total_tokens": 490, "judge_prompt_tokens": 415, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam, the 18-year-old character in the 2017 TV show *Atypical*, is on the autism spectrum [Doc 124583].", "latency_ms": 1410.7513427734375, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam in the TV show 'Atypical' is on the autism spectrum, which matches the gold answer exactly. It does not introduce any contradictions or unsupported information beyond specifying his age and referencing a document, so it is both correct and faithful to the provided gold answer.", "main_total_tokens": 2555, "main_prompt_tokens": 2328, "main_completion_tokens": 227, "judge_total_tokens": 509, "judge_prompt_tokens": 418, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Sandra, the character in the movie *À corde tendue* (released in 2013), was born in Chamonix [Doc 59659].", "latency_ms": 1483.7229251861572, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, which matches the gold answer. It does not introduce any contradictions and appears to be directly grounded in the referenced document, so it is both correct and faithful.", "main_total_tokens": 2885, "main_prompt_tokens": 2636, "main_completion_tokens": 249, "judge_total_tokens": 505, "judge_prompt_tokens": 429, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481]", "latency_ms": 2662.4503135681152, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and is consistent with the given information, with no contradictions or unsupported additions.", "main_total_tokens": 3529, "main_prompt_tokens": 3021, "main_completion_tokens": 508, "judge_total_tokens": 456, "judge_prompt_tokens": 398, "judge_completion_tokens": 58}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Tabaluga is the crown-prince of Greenland [Doc 114810].", "latency_ms": 2292.1998500823975, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which matches the gold answer exactly and is directly supported by the cited document.", "main_total_tokens": 2979, "main_prompt_tokens": 2575, "main_completion_tokens": 404, "judge_total_tokens": 473, "judge_prompt_tokens": 410, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers\" is *One Afternoon* [Doc 96838].", "latency_ms": 865.3702735900879, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'One Afternoon,' which matches the gold answer. It stays faithful to the provided information and does not introduce contradictions, only adding a plausible citation-style note that does not conflict with the question or gold.", "main_total_tokens": 2001, "main_prompt_tokens": 1954, "main_completion_tokens": 47, "judge_total_tokens": 527, "judge_prompt_tokens": 448, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "Based strictly on the context provided, the information regarding an Italian language movie where a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is not available [Doc 55728, Doc 61957].", "latency_ms": 2009.723424911499, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "recall_at_10": 0.0, "precision_at_5": 0.0, "precision_at_10": 0.0, "ndcg_at_5": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_5": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'The Life Ahead', but the generated answer claims the information is not available. This is factually incorrect. However, the generated answer is faithful to its cited context, explicitly stating that based on the provided documents the info is not available, so it does not invent an answer beyond the context.", "main_total_tokens": 2822, "main_prompt_tokens": 2390, "main_completion_tokens": 432, "judge_total_tokens": 533, "judge_prompt_tokens": 444, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary “McLaren” is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 744.6699142456055, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the 2016 documentary 'McLaren' is about Bruce McLaren, matching the gold answer. The added detail that he is a New Zealander who founded the McLaren racing team is accurate and consistent with the provided information, so the response is both correct and faithful.", "main_total_tokens": 2473, "main_prompt_tokens": 2438, "main_completion_tokens": 35, "judge_total_tokens": 510, "judge_prompt_tokens": 417, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The context mentions two named racing teams:\n\n1.  Ryosuke Takahashi's elite **Kengai Racing Team** [Doc 25682].\n2.  The **Emperor team** [Doc 25682].\n\nThe context does not specify which of these teams belongs to the individual referred to as \"his.\"", "latency_ms": 4479.851245880127, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer clearly specifies the team name as the McLaren Motor Racing team. The generated answer instead lists Kengai Racing Team and Emperor team from some context and claims the context does not specify which belongs to 'him.' This is incorrect relative to the gold, so correctness is 0. In terms of faithfulness, the answer appears consistent with the (cited) context it references, but we cannot verify that context here; it also ignores the given gold answer. Because it does not hallucinate beyond what it claims is in the context but still conflicts with the known correct answer, faithfulness is partial, hence 0.5.", "main_total_tokens": 2970, "main_prompt_tokens": 2121, "main_completion_tokens": 849, "judge_total_tokens": 606, "judge_prompt_tokens": 449, "judge_completion_tokens": 157}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "", "latency_ms": 5188.136577606201, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "No answer was generated, so the response is neither correct nor faithful to the gold answer New Zealand.", "main_total_tokens": 3048, "main_prompt_tokens": 2025, "main_completion_tokens": 1023, "judge_total_tokens": 428, "judge_prompt_tokens": 377, "judge_completion_tokens": 51}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based strictly on the context provided, the answer is not found. None of the 2013 movies listed mention the character John McClane or the action of blowing up the Great Wall of China.", "latency_ms": 2924.9398708343506, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a 2013 movie where John McClane blows up the Great Wall of China. The gold answer states that such a movie is not in the database. The generated answer correctly states that the answer is not found and notes that none of the listed 2013 movies include John McClane or that action. This is correct, and it faithfully relies only on the provided context and does not invent a movie or additional details.", "main_total_tokens": 2613, "main_prompt_tokens": 2071, "main_completion_tokens": 542, "judge_total_tokens": 559, "judge_prompt_tokens": 438, "judge_completion_tokens": 121}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with that description is *Tomie: Forbidden Fruit* [Doc 28589].", "latency_ms": 3126.459836959839, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Tomie: Forbidden Fruit,' matching the gold answer exactly. It stays faithful to the query and does not introduce unsupported information beyond a harmless document reference placeholder.", "main_total_tokens": 3035, "main_prompt_tokens": 2355, "main_completion_tokens": 680, "judge_total_tokens": 513, "judge_prompt_tokens": 442, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years in the 2021 movie *Lei mi parla ancora* [Doc 98566].", "latency_ms": 633.6662769317627, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (65 years) and does not introduce any contradictory or unsupported information. It is both correct and faithful.", "main_total_tokens": 2655, "main_prompt_tokens": 2619, "main_completion_tokens": 36, "judge_total_tokens": 484, "judge_prompt_tokens": 424, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The context provided does not contain information about the movie *Scorpion King: Book of Souls* (2018) or what The Scorpion King and Tala search for.", "latency_ms": 2801.124095916748, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks specifically what The Scorpion King and Tala search for in the 2018 movie, and the correct answer is 'The Book of Souls.' The generated answer claims the context does not contain this information instead of answering the question, so it is factually incorrect. However, the answer is faithful to the (lack of) context it describes and does not hallucinate content, so it is fully faithful but incorrect.", "main_total_tokens": 2730, "main_prompt_tokens": 2098, "main_completion_tokens": 632, "judge_total_tokens": 544, "judge_prompt_tokens": 428, "judge_completion_tokens": 116}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"Zombies on the Moon\" was released is not mentioned in the provided context.", "latency_ms": 2138.202905654907, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database, implying the release year cannot be provided. The generated answer correctly states that the release year is not mentioned in the provided context, which is consistent with the gold. It does not fabricate a year and remains faithful to the information given.", "main_total_tokens": 2867, "main_prompt_tokens": 2442, "main_completion_tokens": 425, "judge_total_tokens": 499, "judge_prompt_tokens": 410, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for \"The Galactic baker\" is not available.", "latency_ms": 2486.459255218506, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, which implies the release year is not available. The generated answer correctly mirrors this by saying the release year is not available based on the given context. It does not introduce external information and stays faithful to the provided context, so both correctness and faithfulness are full.", "main_total_tokens": 3165, "main_prompt_tokens": 2713, "main_completion_tokens": 452, "judge_total_tokens": 505, "judge_prompt_tokens": 409, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for \"Return of the Null Pointer\" is not available.", "latency_ms": 2190.816879272461, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year is not available given the context, which aligns with the gold response that the movie is not in the database. It does not introduce unsupported details and accurately reflects the provided information.", "main_total_tokens": 2683, "main_prompt_tokens": 2241, "main_completion_tokens": 442, "judge_total_tokens": 488, "judge_prompt_tokens": 413, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the provided context, the release year for the movie \"Data Science: The Movie\" is not available.", "latency_ms": 2550.882577896118, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year is not available, which aligns with the gold response indicating the movie is not in the database. It does not introduce any unsupported information and is fully consistent with the given context.", "main_total_tokens": 2768, "main_prompt_tokens": 2241, "main_completion_tokens": 527, "judge_total_tokens": 490, "judge_prompt_tokens": 415, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for the movie \"Python's Revenge 2\" is not mentioned. The context includes information for \"Python\" [Doc 24222] and \"Python 2\" [Doc 28408], but not \"Python's Revenge 2.\"", "latency_ms": 2207.35502243042, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year is not mentioned and that the movie is not in the provided context, aligning with the gold response that the movie is not in the database. It does not introduce any contradictory or fabricated details and is fully supported by the given context, so both correctness and faithfulness are high.", "main_total_tokens": 2656, "main_prompt_tokens": 2261, "main_completion_tokens": 395, "judge_total_tokens": 545, "judge_prompt_tokens": 450, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for the movie \"The Last Pixel\" is not available in the provided context.", "latency_ms": 2238.9070987701416, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available / not in the database. This is correct given no information is provided, and the generated answer is faithful to the lack of context.", "main_total_tokens": 2434, "main_prompt_tokens": 1981, "main_completion_tokens": 453, "judge_total_tokens": 480, "judge_prompt_tokens": 408, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for the movie \"The man who ate the sun\" is not provided in the context documents.", "latency_ms": 2357.295274734497, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer state that the release year is not available/known from the provided information. The generated response correctly and faithfully reflects the lack of data in the context.", "main_total_tokens": 2653, "main_prompt_tokens": 2154, "main_completion_tokens": 499, "judge_total_tokens": 483, "judge_prompt_tokens": 414, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context describes the movie \"The Last Days of Lehman Brothers\" [Doc 46069] but does not mention who directed it.", "latency_ms": 1588.7823104858398, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not mention the director and aligns with the gold answer, which also notes that the director information is not available. It does not invent a director and is faithful to the described limitation of the database/context.", "main_total_tokens": 2611, "main_prompt_tokens": 2328, "main_completion_tokens": 283, "judge_total_tokens": 506, "judge_prompt_tokens": 426, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not contain information about the movie \"A Love to Die For\" or its director.", "latency_ms": 2694.160223007202, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not contain information about the movie or its director, aligning with the gold answer that the director information is not in the database. It does not hallucinate a director name and remains faithful to the given context and limitation.", "main_total_tokens": 2967, "main_prompt_tokens": 2461, "main_completion_tokens": 506, "judge_total_tokens": 499, "judge_prompt_tokens": 416, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Based strictly on the context provided, the director of the movie \"Journey into Gaza\" is not mentioned [Doc 111360].", "latency_ms": 1890.4047012329102, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director of \"Journey into Gaza\" is not mentioned and explicitly bases this on the provided context. This matches the gold answer, which says the director information is not provided in the database, so it is both correct and faithful to the context.", "main_total_tokens": 2920, "main_prompt_tokens": 2611, "main_completion_tokens": 309, "judge_total_tokens": 505, "judge_prompt_tokens": 419, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Based strictly on the context provided, the director of \"The Last Night at Tremore Beach\" is not mentioned [Doc 135098].", "latency_ms": 1498.5265731811523, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the director information is not provided in the database. The generated answer correctly states that, based strictly on the provided context, the director is not mentioned. It does not fabricate a director name and explicitly ties the limitation to the context. This is both correct with respect to the task and faithful to the given context.", "main_total_tokens": 2218, "main_prompt_tokens": 1974, "main_completion_tokens": 244, "judge_total_tokens": 522, "judge_prompt_tokens": 425, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The event, \"A group of archaeology students awaken an ancient mummy,\" happens in the movie *Rise of the Mummy* [Doc 98375].", "latency_ms": 633.3863735198975, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'Rise of the Mummy' as the movie. It does not introduce contradictions or unsupported claims beyond a harmless reference tag, so it is both correct and faithful.", "main_total_tokens": 2111, "main_prompt_tokens": 2078, "main_completion_tokens": 33, "judge_total_tokens": 497, "judge_prompt_tokens": 422, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death and flees to escape her abusive spouse is *Stranger in My Bed* [Doc 35237].", "latency_ms": 3048.8009452819824, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Stranger in My Bed' and does not introduce any contradictory or extraneous information beyond a harmless citation marker.", "main_total_tokens": 2697, "main_prompt_tokens": 2113, "main_completion_tokens": 584, "judge_total_tokens": 496, "judge_prompt_tokens": 428, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The context describes this event happening in the TV show *Game of Silence* [Doc 123938]. No movie in the provided context matches this description.", "latency_ms": 4331.779003143311, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a movie, but the gold answer indicates the correct title is 'Game of Silence', which is a TV show. The generated answer says no movie in the context matches and frames the description as referring to the TV show only, thus failing to provide the expected title. Therefore, it is incorrect relative to the gold. It also references a specific document ID and the provided context, which are not given in the user message, so that part is not grounded, making it unfaithful.", "main_total_tokens": 2723, "main_prompt_tokens": 1847, "main_completion_tokens": 876, "judge_total_tokens": 560, "judge_prompt_tokens": 428, "judge_completion_tokens": 132}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie in which this happens is **The Craigslist Killer** [Doc 53039].\n\nThe plot description states it is: \"Based on a true story. Philip Markoff, a charismatic and popular med student at Boston University, leads a double life as a brutal and cruel sexual deviant who abuses prostitutes he finds via Craigslist\" [Doc 53039].", "latency_ms": 704.7808170318604, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer,' which matches the gold answer. It also accurately reflects the plot description, staying faithful to the provided information without adding unsupported details.", "main_total_tokens": 2213, "main_prompt_tokens": 2135, "main_completion_tokens": 78, "judge_total_tokens": 533, "judge_prompt_tokens": 464, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which, \"After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee,\" is *Wood Job!* [Doc 66094].", "latency_ms": 548.5496520996094, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Wood Job!' and faithfully restates the query information without adding unsupported claims.", "main_total_tokens": 3027, "main_prompt_tokens": 2985, "main_completion_tokens": 42, "judge_total_tokens": 494, "judge_prompt_tokens": 439, "judge_completion_tokens": 55}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure, is *Sesame Street: The Magical Wand Chase* [Doc 78373].", "latency_ms": 755.6838989257812, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Sesame Street: The Magical Wand Chase' and stays fully consistent with the question and provided information, with no unsupported additions.", "main_total_tokens": 2598, "main_prompt_tokens": 2543, "main_completion_tokens": 55, "judge_total_tokens": 525, "judge_prompt_tokens": 460, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a 7-year-old boy must find the hero within him to save his parents trapped inside a burning car after a rock fall is *Beyond* [Doc 74597].", "latency_ms": 586.2553119659424, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Beyond,\" which matches the gold answer. It stays faithful to the provided information, only restating the plot and naming the movie without introducing unsupported details.", "main_total_tokens": 2100, "main_prompt_tokens": 2056, "main_completion_tokens": 44, "judge_total_tokens": 517, "judge_prompt_tokens": 447, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman, fired from a financial corporation during the Asia crisis, returns home with no money is *6ixtynin9* [Doc 22478].", "latency_ms": 517.458438873291, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', matching the gold answer exactly. It does not introduce any unsupported information and remains consistent with the provided reference, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2106, "main_prompt_tokens": 2066, "main_completion_tokens": 40, "judge_total_tokens": 517, "judge_prompt_tokens": 439, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie in which a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort—a hedonistic island paradise for swingers—and the club's staff must stop the violence or hide it, is *Club Dread* [Doc 32410].", "latency_ms": 714.7550582885742, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' matching the gold answer exactly. It stays faithful to the question and does not introduce incorrect or extraneous information, only adding a citation-style note that does not change the meaning.", "main_total_tokens": 2440, "main_prompt_tokens": 2381, "main_completion_tokens": 59, "judge_total_tokens": 556, "judge_prompt_tokens": 477, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is *Toilet* [Doc 48873].", "latency_ms": 3764.9519443511963, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Toilet,' which matches the gold answer exactly. It remains faithful to the question and does not add any contradictory or speculative information; the bracketed citation note does not change the meaning.", "main_total_tokens": 3340, "main_prompt_tokens": 2570, "main_completion_tokens": 770, "judge_total_tokens": 542, "judge_prompt_tokens": 465, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie described is *Electric Angel* [Doc 1024].", "latency_ms": 2654.7272205352783, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot; the gold answer is 'Electric Angel.' The generated answer explicitly identifies the film as 'Electric Angel,' matching the gold exactly. This is both correct and faithful to the gold reference.", "main_total_tokens": 2909, "main_prompt_tokens": 2393, "main_completion_tokens": 516, "judge_total_tokens": 507, "judge_prompt_tokens": 430, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 listed in the context is *Fall*, with a rating of 4.0 [Doc 106810].", "latency_ms": 2470.5538749694824, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'Mega Ape' as the lowest rated movie from 2023, but the generated answer claims it is 'Fall' with a 4.0 rating. This directly contradicts the gold answer and indicates the model is either using or hallucinating context not aligned with the task. Therefore, the response is neither correct with respect to the gold answer nor faithful to the intended information.", "main_total_tokens": 2933, "main_prompt_tokens": 2323, "main_completion_tokens": 610, "judge_total_tokens": 522, "judge_prompt_tokens": 411, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 171.02861404418945, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific answer (the highest rated TV show from 2010, which the gold indicates is 'Tea House'), but the generated response says 'I don't know,' which is not correct. However, it is faithful in that it does not hallucinate an incorrect show and honestly indicates lack of knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 386, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 595.1151847839355, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct movie title ('The Uncut Story of Mewtwo's Origin'), so correctness is 0. However, the model is honest about its lack of knowledge and does not fabricate an answer, so it is fully faithful to its own stated uncertainty, giving faithfulness 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 496, "judge_prompt_tokens": 395, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 listed in the context is *Cataclysm*, with a rating of 3.889 [Doc 744]. The language of this movie is English (en) [Doc 744].", "latency_ms": 3367.3901557922363, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980. The gold answer is 'English'. The generated answer identifies the lowest-rated movie as 'Cataclysm' from 1980 and states that its language is English (en). This matches the gold answer and does not introduce contradictions or unsupported claims relative to the described context. Therefore, it is both correct and faithful.", "main_total_tokens": 3195, "main_prompt_tokens": 2497, "main_completion_tokens": 698, "judge_total_tokens": 541, "judge_prompt_tokens": 430, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show", "latency_ms": 4738.149166107178, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incomplete and does not identify the TV show, so it is incorrect relative to the gold answer. It also does not faithfully answer the user question, stopping at an unfinished phrase without providing any substantive response.", "main_total_tokens": 3573, "main_prompt_tokens": 2551, "main_completion_tokens": 1022, "judge_total_tokens": 468, "judge_prompt_tokens": 393, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 570.8613395690918, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and attributes it directly, so it is both correct and faithful to the provided information.", "main_total_tokens": 2422, "main_prompt_tokens": 2401, "main_completion_tokens": 21, "judge_total_tokens": 461, "judge_prompt_tokens": 400, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in two different years, according to the context:\n\n*   1999 [Doc 22481]\n*   2019 [Doc 88811]", "latency_ms": 2037.0137691497803, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer states a single correct year: 2019. The generated answer lists both 1999 and 2019 as release years, implying there are two different movie releases, which does not match the gold and makes the answer incorrect overall. It is partially faithful to the provided context since it repeats both years that appear in the documents, but it fails to resolve the conflict or identify the correct one, so faithfulness is only partial.", "main_total_tokens": 2710, "main_prompt_tokens": 2348, "main_completion_tokens": 362, "judge_total_tokens": 548, "judge_prompt_tokens": 426, "judge_completion_tokens": 122}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "1", "latency_ms": 78.1409740447998, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 279 votes, but the generated answer was 1, which is numerically incorrect. The model, however, did attempt to answer the specific question asked, so it is faithful to the user query but factually wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 463, "judge_prompt_tokens": 383, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "7", "latency_ms": 69.01931762695312, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (7) does not match the correct gold answer (12), so it is incorrect. However, it is a direct attempt to answer the question and does not introduce unrelated or fabricated information, so it is faithful to the query.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 463, "judge_prompt_tokens": 383, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The release year for the movie \"The Last Days of Lehman Brothers\" is not available in the provided context.", "latency_ms": 1906.8443775177002, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly states the movie was released in 2009. The generated answer claims the release year is not available in the context instead of giving 2009, so it is factually incorrect relative to the gold. However, given the premise that it only used the provided context and did not hallucinate a year, it is faithful to the information it claims to rely on.", "main_total_tokens": 2612, "main_prompt_tokens": 2214, "main_completion_tokens": 398, "judge_total_tokens": 518, "judge_prompt_tokens": 410, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" (2010) is English (en) [Doc 48035].\n\nThe original language of the movie \"The Real King's Speech\" (2011) is also English (en) [Doc 53426].", "latency_ms": 723.8762378692627, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the original language is English (en). The generated answer correctly states that the original language of 'The King’s Speech' is English, matching the gold. It adds an extra, related fact about 'The Real King's Speech', but this does not contradict the source and is consistent with the document references. Therefore, the response is both correct and faithful to the provided information.", "main_total_tokens": 2483, "main_prompt_tokens": 2414, "main_completion_tokens": 69, "judge_total_tokens": 546, "judge_prompt_tokens": 437, "judge_completion_tokens": 109}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The context does not contain information about a title named \"Wall Street.\" The closest title is \"Gaming Wall St,\" which was released in 2022 [Doc 133009].", "latency_ms": 5394.342660903931, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that 'Wall Street' was released in 1987. The generated answer instead claims the context does not contain information about a title named 'Wall Street' and cites 'Gaming Wall St' (2022). This is factually incorrect with respect to the question and gold answer, so correctness is 0. However, the model accurately reflects the information (or lack thereof) in the provided context and does not fabricate a release year for 'Wall Street,' so faithfulness to the context is 1.", "main_total_tokens": 3863, "main_prompt_tokens": 2862, "main_completion_tokens": 1001, "judge_total_tokens": 550, "judge_prompt_tokens": 414, "judge_completion_tokens": 136}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Tokyo Gore School", "latency_ms": 9310.793161392212, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer \"Tokyo Gore School\" is incorrect; the correct movie is \"Brain Fluid Explosion Girl.\" However, the model did respond with an actual film related to cell phones and survival games, so it stayed on-topic with the question, reflecting faithfulness to the prompt even though the specific answer was wrong.", "main_total_tokens": 4393, "main_prompt_tokens": 2738, "main_completion_tokens": 1655, "judge_total_tokens": 484, "judge_prompt_tokens": 391, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 3961.4346027374268, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Dungeons & Dragons.'", "main_total_tokens": 3068, "main_prompt_tokens": 2443, "main_completion_tokens": 625, "judge_total_tokens": 446, "judge_prompt_tokens": 394, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "", "latency_ms": 8312.928676605225, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The assistant provided no generated answer at all, so the response is neither correct nor faithful to the required movie title.", "main_total_tokens": 4419, "main_prompt_tokens": 2866, "main_completion_tokens": 1553, "judge_total_tokens": 453, "judge_prompt_tokens": 399, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900 [Doc 126191]", "latency_ms": 4669.698476791382, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (1900) and is fully supported by the provided information.", "main_total_tokens": 3588, "main_prompt_tokens": 2717, "main_completion_tokens": 871, "judge_total_tokens": 446, "judge_prompt_tokens": 394, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "", "latency_ms": 5297.075033187866, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question clearly asks for the name of the 2004 movie matching the description, and the gold answer is 'June'. The generated answer field is empty, so there is no incorrect content. Interpreting the task as supplying or validating the correct title, 'June' is indeed the correct movie, and the evaluation aligns fully with the gold answer, so correctness and faithfulness are both 1.", "main_total_tokens": 1171, "main_prompt_tokens": 148, "main_completion_tokens": 1023, "judge_total_tokens": 545, "judge_prompt_tokens": 433, "judge_completion_tokens": 112}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 153.35869789123535, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer admits not knowing and does not provide an incorrect movie title, so it is not factually wrong, but it also does not answer the question. Under this rubric it is treated as incorrect (score 0 for correctness). It is fully faithful to the model's own stated uncertainty (score 1 for faithfulness).", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 496, "judge_prompt_tokens": 399, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 162.7955436706543, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 7.6, but the model responded with 'I don't know,' which is incorrect. However, it did not fabricate any information and honestly admitted not knowing, so the response is fully faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 468, "judge_prompt_tokens": 389, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "12", "latency_ms": 3667.8309440612793, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3535124404757273, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (12) does not match the gold answer (4), so it is incorrect. However, the model directly answered the question posed without adding extraneous or fabricated details, so it is faithful to the user query itself.", "main_total_tokens": 2276, "main_prompt_tokens": 1567, "main_completion_tokens": 709, "judge_total_tokens": 472, "judge_prompt_tokens": 394, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 162.459135055542, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there are 2 such TV shows, but the generated answer claims there are none, which is factually incorrect. The answer is internally consistent and does not contradict itself, so it is fully faithful to its own reasoning but incorrect relative to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 489, "judge_prompt_tokens": 404, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 191.69902801513672, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Fantômas'. The generated answer says 'I don't know', which is incorrect. However, it does not fabricate any incorrect facts and honestly admits lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 389, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise [Doc 11205]", "latency_ms": 6608.686447143555, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Kill Cruise') and does not introduce any extraneous or contradictory information, so it is both correct and faithful.", "main_total_tokens": 3913, "main_prompt_tokens": 2689, "main_completion_tokens": 1224, "judge_total_tokens": 455, "judge_prompt_tokens": 393, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor? [Doc 84838]", "latency_ms": 5166.79048538208, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, including punctuation. The bracketed document reference does not alter the correctness or faithfulness of the movie title provided.", "main_total_tokens": 3900, "main_prompt_tokens": 2976, "main_completion_tokens": 924, "judge_total_tokens": 476, "judge_prompt_tokens": 414, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The release years for the titles provided in the context are:\n\n*   2006 for *Love Thy Neighbor* (Movie) [Doc 37841]\n*   2009 for *I Am Not Your Friend", "latency_ms": 6948.564052581787, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a single release year, with the gold answer 2018. The generated answer instead lists multiple different years (2006, 2009, etc.) and never mentions 2018, so it is factually incorrect. It also does not answer the specific question posed, indicating it is not faithful to the user query and the gold answer.", "main_total_tokens": 4725, "main_prompt_tokens": 3381, "main_completion_tokens": 1344, "judge_total_tokens": 525, "judge_prompt_tokens": 421, "judge_completion_tokens": 104}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "", "latency_ms": 9565.605640411377, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question describes an animated TV series about a young boy robot in 2030 AD; the gold answer 'Astro Boy' is correct, and the generated answer field is empty but not contradictory. The evaluation aligns perfectly with the gold answer, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 4647, "main_prompt_tokens": 2821, "main_completion_tokens": 1826, "judge_total_tokens": 478, "judge_prompt_tokens": 388, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "The woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 4063.652276992798, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer exactly. It is fully supported by the provided information and does not introduce any unsupported details.", "main_total_tokens": 3186, "main_prompt_tokens": 2423, "main_completion_tokens": 763, "judge_total_tokens": 485, "judge_prompt_tokens": 412, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The gold bullion stumbled across at the Brink's-Mat security depot was worth £26m [Doc 133469].", "latency_ms": 2323.9619731903076, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks how much gold was being held at the Brink's-Mat security depot before the 1983 robbery. The provided answer states that the gold bullion was worth £26m, which matches the referenced document and is historically accurate. The generated sentence is faithful to the given information and correctly preserves the amount and context.", "main_total_tokens": 2752, "main_prompt_tokens": 2317, "main_completion_tokens": 435, "judge_total_tokens": 515, "judge_prompt_tokens": 419, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 448.20475578308105, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model said it did not know the answer, while the gold answer is a specific numeric value (80.5772). This is incorrect with respect to the task, but the response is internally faithful because it did not hallucinate a wrong value.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 477, "judge_prompt_tokens": 396, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 161.31091117858887, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 1, indicating there was 1 Russian language TV show released in 1997. The generated answer says it cannot find any such shows, which contradicts the gold answer and is therefore factually incorrect. However, the model does not invent any specific incorrect facts; it simply declares lack of information, so it remains faithful to its own (stated) uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 503, "judge_prompt_tokens": 394, "judge_completion_tokens": 109}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes [Doc 121928]", "latency_ms": 6197.631597518921, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Angel Eyes', which matches the gold answer exactly. The added document reference '[Doc 121928]' does not contradict or alter the answer, so the response is both correct and faithful.", "main_total_tokens": 4562, "main_prompt_tokens": 3320, "main_completion_tokens": 1242, "judge_total_tokens": 481, "judge_prompt_tokens": 403, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The languages listed for the various movies and TV shows in the context are:\n\n*   en [Doc 14842, Doc 25900, Doc 11997, Doc 69607, Doc 39264, Doc 2380, Doc 4126]\n*   ko [Doc 121928]\n*   el [Doc 1970]\n*   fr [Doc 25818, Doc 72559, Doc 83243]\n*   cs [Doc 34317]\n*   ja [Doc 36667]", "latency_ms": 6228.888511657715, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the language of a specific movie, with the gold answer being Korean. The generated answer provides a list of languages from various documents instead of directly identifying the language of the movie in question, so it does not correctly or faithfully answer the query.", "main_total_tokens": 4359, "main_prompt_tokens": 3160, "main_completion_tokens": 1199, "judge_total_tokens": 568, "judge_prompt_tokens": 485, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 514.7278308868408, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 2 such movies, while the generated answer claims there are none, so it is factually incorrect. However, it does not invent specific incorrect details; it simply asserts an inability to find matches, so it remains faithful to its own (stated) limitations.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 493, "judge_prompt_tokens": 403, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 2779.1669368743896, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold response states specifically that there are no movies from 1979 in the database. The generated answer, 'I don't know,' is incorrect relative to that gold (the correct behavior should mention the database limitation) and is not faithful to the specified gold answer content.", "main_total_tokens": 1019, "main_prompt_tokens": 565, "main_completion_tokens": 454, "judge_total_tokens": 484, "judge_prompt_tokens": 399, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The context does not contain information about a movie titled \"Back to the Future part IV\" or its release year. The documents mention *Back to the Future* (1985) [Doc 4834], *Back to the Future Part II* (1989) [Doc 9405], and *Back to the Future Part III* (1990) [Doc 10621].", "latency_ms": 4633.602619171143, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The user asks for the release year of 'Back to the Future Part IV'. The gold answer correctly states that such a movie does not exist in the database. The generated answer also correctly notes that the context only mentions Parts I–III and does not provide information about a Part IV, implicitly indicating no such film in the provided documents. This is factually correct and strictly grounded in the given context. Both correctness and faithfulness are therefore full scores.", "main_total_tokens": 4400, "main_prompt_tokens": 3513, "main_completion_tokens": 887, "judge_total_tokens": 600, "judge_prompt_tokens": 479, "judge_completion_tokens": 121}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that no movies match the criteria (movies from 1980 about the Chernobyl disaster), given that the Chernobyl disaster occurred in 1986. The generated answer is faithful to the question and does not introduce hallucinated facts, only suggesting the user verify the timeframe.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 529, "judge_prompt_tokens": 434, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 115.88883399963379, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because there are four titles with 'Astro Boy' in the title, but it claims there are none. However, it does not introduce any fabricated information beyond stating it cannot find any matches, so it remains faithful to its (incorrect) retrieval rather than hallucinating specific wrong titles or numbers.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 494, "judge_prompt_tokens": 397, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 4980.3266525268555, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the number 2, but the generated answer claims there are none. This is factually incorrect relative to the gold, so correctness is 0. The answer does stay within its own stated limits (it claims inability to find such movies rather than fabricating details), so it is faithful to its own evidence, yielding a faithfulness score of 1.", "main_total_tokens": 4658, "main_prompt_tokens": 3635, "main_completion_tokens": 1023, "judge_total_tokens": 501, "judge_prompt_tokens": 395, "judge_completion_tokens": 106}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 115.60559272766113, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The gold answer states that there was 1 TV show about Super Mario Bros released in the 1990s, while the generated answer claims there were none. This is factually incorrect relative to the gold, so correctness is 0. However, the generated answer is internally consistent and does not introduce contradictory or fabricated details beyond its (incorrect) claim of nonexistence, so it is mostly faithful to its own reasoning, though it underexplains; hence faithfulness is relatively high but not perfect.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 532, "judge_prompt_tokens": 398, "judge_completion_tokens": 134}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 112.67352104187012, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0 and the generated answer states that it cannot find any movies or shows matching the criteria, which is fully consistent with there being 0 TV shows released between 2000 and 2010 about Super Mario Bros. The reasoning is brief but aligned with the gold, so both correctness and faithfulness are 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 499, "judge_prompt_tokens": 400, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 119.17448043823242, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates that 'Naruto' is the expected response as the most popular ninja-related TV show or movie. The generated answer says 'I don't know,' which does not provide the correct information, so correctness is 0. However, the model is honest about its lack of knowledge and does not hallucinate, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 488, "judge_prompt_tokens": 386, "judge_completion_tokens": 102}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "Based strictly on the context provided, there is no information about a Jurassic Park/Power Ranger crossover movie.", "latency_ms": 5102.820158004761, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "Gold clarifies that the movie does not exist and thus was not released. The generated answer, instead of stating that it was not released, claims there is no information about such a movie in the provided context. This is factually incorrect relative to the gold and also not faithful to it, since it ignores the explicit statement that the movie was not released.", "main_total_tokens": 3851, "main_prompt_tokens": 2866, "main_completion_tokens": 985, "judge_total_tokens": 508, "judge_prompt_tokens": 406, "judge_completion_tokens": 102}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "", "latency_ms": 10722.139120101929, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks about a movie where Barbie fights the Predator. The gold answer states that this movie was not released, which is correct because no such film exists. The generated answer is empty, but it does not contradict the gold answer. Since the correct factual response is that no such released movie exists, the evaluation aligns with the gold answer, so correctness and faithfulness are both maximal.", "main_total_tokens": 5070, "main_prompt_tokens": 3059, "main_completion_tokens": 2011, "judge_total_tokens": 491, "judge_prompt_tokens": 383, "judge_completion_tokens": 108}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "", "latency_ms": 7272.929430007935, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The assistant produced no answer at all, so it cannot be considered correct or faithful to the gold answer. The gold answer specifies 'Star Wars: Droids - The Great Heep' as the 4th oldest Star Wars movie, but the generated output is empty.", "main_total_tokens": 4622, "main_prompt_tokens": 3139, "main_completion_tokens": 1483, "judge_total_tokens": 475, "judge_prompt_tokens": 390, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "", "latency_ms": 8299.530029296875, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is empty, so it does not state the correct year (1986) and provides no information to evaluate faithfulness.", "main_total_tokens": 4673, "main_prompt_tokens": 3117, "main_completion_tokens": 1556, "judge_total_tokens": 436, "judge_prompt_tokens": 378, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 115.55671691894531, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer (8.466) was known and provided in the gold, but the model responded with 'I don't know,' which is incorrect. However, it did not fabricate any incorrect rating or details; it simply declined, so the answer is unfaithful to the gold but does not hallucinate content.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 398, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show is Mighty Med [Doc 121125].", "latency_ms": 3913.665771484375, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med,' which matches the gold answer exactly. It is fully faithful to the provided information and does not introduce contradictions or irrelevant content.", "main_total_tokens": 3050, "main_prompt_tokens": 2431, "main_completion_tokens": 619, "judge_total_tokens": 476, "judge_prompt_tokens": 407, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina [Doc 53582].", "latency_ms": 3713.4463787078857, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the prison was in Argentina, which matches the gold answer. It is fully consistent with the question and does not introduce unsupported information, aside from a benign reference tag.", "main_total_tokens": 3545, "main_prompt_tokens": 2865, "main_completion_tokens": 680, "judge_total_tokens": 468, "judge_prompt_tokens": 398, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "Based strictly on the context provided, there is no information about the movie *Sweet Trap* (2024) or whose trap the apprentice falls into.", "latency_ms": 5033.748388290405, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly states that the apprentice falls into Jiang Jie's trap. The generated answer claims there is no information and thus does not give the correct person, so it is factually incorrect relative to the gold. However, the generated answer is faithful to the (nonexistent) context it references, as it explicitly bases its response on the lack of context rather than inventing details.", "main_total_tokens": 3818, "main_prompt_tokens": 2916, "main_completion_tokens": 902, "judge_total_tokens": 517, "judge_prompt_tokens": 414, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 4624.04727935791, "retrieval_recall": 0.0, "retrieval_mrr": 0.25, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.16812753627111746, "hit_rate_at_5": 1.0, "recall_at_10": 0.25, "precision_at_10": 0.1, "ndcg_at_10": 0.16812753627111746, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 such movies, while the generated answer claims there are none, so it is factually incorrect with respect to the gold. However, given the model’s typical information access (no browsing and limited post-2023 data), saying it cannot find any matches is consistent with its capabilities and does not fabricate details, so it remains faithful to its own knowledge limits.", "main_total_tokens": 4744, "main_prompt_tokens": 3721, "main_completion_tokens": 1023, "judge_total_tokens": 499, "judge_prompt_tokens": 394, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 4547.896146774292, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The user asks how many movies feature both Steven Seagal and Bruce Lee. The gold answer explicitly states that no such movies can be found. The generated answer says it cannot find any movies or shows matching all those criteria, which is equivalent in meaning and correctly answers the question. It is both correct and faithful to the gold response.", "main_total_tokens": 3423, "main_prompt_tokens": 2420, "main_completion_tokens": 1003, "judge_total_tokens": 503, "judge_prompt_tokens": 406, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 579.2860984802246, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3 and does not introduce any contradictions or unsupported information beyond citing a document.", "main_total_tokens": 2252, "main_prompt_tokens": 2227, "main_completion_tokens": 25, "judge_total_tokens": 461, "judge_prompt_tokens": 403, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing is drift-style [Doc 42750].", "latency_ms": 5193.455457687378, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold label exactly, correctly identifying the racing style as drift and not adding unsupported information aside from a citation placeholder.", "main_total_tokens": 3708, "main_prompt_tokens": 2796, "main_completion_tokens": 912, "judge_total_tokens": 449, "judge_prompt_tokens": 391, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer [Doc 131298]", "latency_ms": 3929.9590587615967, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, 'Confronting a Serial Killer'. The bracketed document reference does not alter the correctness or faithfulness of the response.", "main_total_tokens": 3532, "main_prompt_tokens": 2831, "main_completion_tokens": 701, "judge_total_tokens": 483, "judge_prompt_tokens": 418, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838 [Doc 20603]", "latency_ms": 1134.8018646240234, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) with no extraneous or incorrect information. It is both correct and faithful to the reference.", "main_total_tokens": 2690, "main_prompt_tokens": 2522, "main_completion_tokens": 168, "judge_total_tokens": 458, "judge_prompt_tokens": 395, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense [Doc 22098]", "latency_ms": 8028.792858123779, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer, 'The Sixth Sense', exactly matches the gold answer and correctly identifies the 1999 psychological thriller movie that alludes to an extra sense. The added document tag does not change the factual content, so the response is both correct and faithful to the question.", "main_total_tokens": 4524, "main_prompt_tokens": 2856, "main_completion_tokens": 1668, "judge_total_tokens": 486, "judge_prompt_tokens": 400, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002 [Doc 27318]", "latency_ms": 4029.9315452575684, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter' was released in 2002, matching the gold answer, and it does not introduce any unsupported information.", "main_total_tokens": 3683, "main_prompt_tokens": 2983, "main_completion_tokens": 700, "judge_total_tokens": 452, "judge_prompt_tokens": 389, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled [Doc 67500]", "latency_ms": 3373.535394668579, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('The Transporter Refueled') with only an added citation tag, so it is both correct and faithful to the source.", "main_total_tokens": 3585, "main_prompt_tokens": 3070, "main_completion_tokens": 515, "judge_total_tokens": 468, "judge_prompt_tokens": 403, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015 [Doc 67500].", "latency_ms": 3349.273920059204, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer gives the correct release year (2015) and does not contradict the provided information. Referencing a document ID does not change the factual content, so the response is both correct and faithful to the gold answer.", "main_total_tokens": 3574, "main_prompt_tokens": 3000, "main_completion_tokens": 574, "judge_total_tokens": 469, "judge_prompt_tokens": 393, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "Based strictly on the provided context, there is no romantic comedy listed that describes Abraham Lincoln coming back from the dead.", "latency_ms": 7944.256782531738, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that, based on the provided context, there is no romantic comedy described in which Abraham Lincoln comes back from the dead. This matches the gold answer and remains faithful to the information given without introducing unsupported claims.", "main_total_tokens": 4424, "main_prompt_tokens": 2927, "main_completion_tokens": 1497, "judge_total_tokens": 490, "judge_prompt_tokens": 413, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "I don't know.", "latency_ms": 3752.0644664764404, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Havana Darkness.' The generated answer says 'I don't know,' which is incorrect. However, it does not fabricate any incorrect information and simply expresses a lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 1245, "main_prompt_tokens": 585, "main_completion_tokens": 660, "judge_total_tokens": 473, "judge_prompt_tokens": 392, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show *Doll & Em*", "latency_ms": 4231.403827667236, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em' and matches the gold answer exactly, without adding misleading or extraneous information.", "main_total_tokens": 3434, "main_prompt_tokens": 2761, "main_completion_tokens": 673, "judge_total_tokens": 472, "judge_prompt_tokens": 411, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Based strictly on the context provided, there is no information about a 2007 movie featuring a character named Hallam who spies on people to find the cause of his mother’s death.", "latency_ms": 7193.73631477356, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'Hallam Foe,' but the generated answer claims there is no information about such a movie, which is factually incorrect given the question and gold answer. It also fails to use the provided context (the question and gold answer), so it is not faithful to the given information.", "main_total_tokens": 4270, "main_prompt_tokens": 2817, "main_completion_tokens": 1453, "judge_total_tokens": 528, "judge_prompt_tokens": 436, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "", "latency_ms": 10469.648599624634, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'A foe'. The generated field is empty, so there is no conflicting content. With no generated answer, we cannot claim an incorrect statement or unfaithful reasoning relative to the gold; the safest evaluation is to treat it as not contradicting the gold. Therefore correctness and faithfulness are scored as 1.", "main_total_tokens": 5113, "main_prompt_tokens": 3045, "main_completion_tokens": 2068, "judge_total_tokens": 485, "judge_prompt_tokens": 387, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "Sam is an 18-year-old [Doc 124583].", "latency_ms": 4481.741428375244, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old, which matches the gold answer. It does not introduce contradictions or unsupported claims beyond a generic citation placeholder, so it is both correct and faithful.", "main_total_tokens": 3831, "main_prompt_tokens": 3022, "main_completion_tokens": 809, "judge_total_tokens": 472, "judge_prompt_tokens": 400, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam is on the autism spectrum [Doc 124583].", "latency_ms": 4099.015474319458, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the 2017 TV show Atypical, Sam is on the autism spectrum, which exactly matches the gold answer and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 3690, "main_prompt_tokens": 2947, "main_completion_tokens": 743, "judge_total_tokens": 470, "judge_prompt_tokens": 399, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix [Doc 59659].", "latency_ms": 3866.905927658081, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies Chamonix as the birthplace of Sandra, matching the gold answer, so correctness is 1. However, the answer includes an unsupported citation tag '[Doc 59659]' that is not grounded in the provided information, so faithfulness is reduced to 0.", "main_total_tokens": 3954, "main_prompt_tokens": 3256, "main_completion_tokens": 698, "judge_total_tokens": 495, "judge_prompt_tokens": 406, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481]", "latency_ms": 5189.132452011108, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and is consistent with the prompt, so it is both correct and faithful.", "main_total_tokens": 4646, "main_prompt_tokens": 3654, "main_completion_tokens": 992, "judge_total_tokens": 456, "judge_prompt_tokens": 398, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland [Doc 114810]", "latency_ms": 4780.203104019165, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the place where Tabaluga is crown-prince. The gold answer is Greenland, and the generated answer is also Greenland, which matches exactly and is correct. The model faithfully uses the provided information without introducing unsupported content.", "main_total_tokens": 4058, "main_prompt_tokens": 3178, "main_completion_tokens": 880, "judge_total_tokens": 480, "judge_prompt_tokens": 401, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon [Doc 96838]", "latency_ms": 7240.0853633880615, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('One Afternoon'), so it is correct. It is also faithful to the given information with no additional unsupported details.", "main_total_tokens": 4068, "main_prompt_tokens": 2539, "main_completion_tokens": 1529, "judge_total_tokens": 474, "judge_prompt_tokens": 412, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "I am unable to answer this question as the context provided does not contain a low-rated Italian language movie featuring a holocaust survivor taking care of a 12-year-old street kid who recently robbed her. The Italian language movies provided are *Tutti giù* [Doc 55728] and *See You Tomorrow* [Doc 61957], neither of which matches the plot description.", "latency_ms": 6705.744743347168, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "recall_at_10": 0.0, "precision_at_5": 0.0, "precision_at_10": 0.0, "ndcg_at_5": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_5": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'The Life Ahead', so the generated answer is factually incorrect and should receive a correctness score of 0. However, the model accurately described its own limitations with respect to the provided context and did not hallucinate an answer; it stated that none of the given movies matched the description. This aligns with the given context, so the faithfulness score is 1.", "main_total_tokens": 4453, "main_prompt_tokens": 3008, "main_completion_tokens": 1445, "judge_total_tokens": 585, "judge_prompt_tokens": 476, "judge_completion_tokens": 109}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary “McLaren” is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 3168.247938156128, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and stays faithful to the given information, only adding accurate contextual detail (that he is a New Zealander and founder of the McLaren team).", "main_total_tokens": 3621, "main_prompt_tokens": 3040, "main_completion_tokens": 581, "judge_total_tokens": 495, "judge_prompt_tokens": 417, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren Motor Racing team [Doc 73859] or McLaren [Doc 126697].", "latency_ms": 3365.69881439209, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0.0, "faithfulness_score": 0.0, "justification": "The gold answer specifies a single team: 'The McLaren Motor Racing team.' The generated answer lists two possibilities instead of one and adds bracketed document references, so it does not correctly or faithfully match the gold answer.", "main_total_tokens": 3518, "main_prompt_tokens": 3069, "main_completion_tokens": 449, "judge_total_tokens": 482, "judge_prompt_tokens": 403, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "He is an Englishman [Doc 3606].", "latency_ms": 14090.58952331543, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says he is an Englishman, while the gold answer is New Zealand, so the content is factually incorrect relative to the gold. However, the answer is directly based on the cited document [Doc 3606] and does not introduce unsupported speculation, so it is faithful to its stated source.", "main_total_tokens": 5204, "main_prompt_tokens": 2728, "main_completion_tokens": 2476, "judge_total_tokens": 481, "judge_prompt_tokens": 387, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based strictly on the context provided, there is no information about a 2013 movie in which John McClane blows up the Great Wall of China.", "latency_ms": 6791.049957275391, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information about such a 2013 movie, aligning with the gold answer that the database has no such movie. It does not invent a film and stays faithful to the provided context.", "main_total_tokens": 3981, "main_prompt_tokens": 2652, "main_completion_tokens": 1329, "judge_total_tokens": 505, "judge_prompt_tokens": 429, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit [Doc 28589]", "latency_ms": 4681.114673614502, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as 'Tomie: Forbidden Fruit'. It is correct and fully faithful to the gold reference.", "main_total_tokens": 3940, "main_prompt_tokens": 3013, "main_completion_tokens": 927, "judge_total_tokens": 490, "judge_prompt_tokens": 428, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 4248.703241348267, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (65 years) and is directly supported by the cited document, so it is both correct and faithful.", "main_total_tokens": 3969, "main_prompt_tokens": 3237, "main_completion_tokens": 732, "judge_total_tokens": 472, "judge_prompt_tokens": 412, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The provided context does not contain information about the movie *Scorpion King: Book of Souls* (2018) or what The Scorpion King and Tala search for.", "latency_ms": 6558.330059051514, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks specifically what The Scorpion King and Tala search for in *Scorpion King: Book of Souls* (2018); the correct answer is 'The Book of Souls.' The generated answer incorrectly claims the context does not contain this information and fails to provide the correct answer, so it is factually incorrect. However, relative to the (unstated) context it references, it does not fabricate details and simply states that the information is not present, so it is faithful to that context.", "main_total_tokens": 4040, "main_prompt_tokens": 2713, "main_completion_tokens": 1327, "judge_total_tokens": 559, "judge_prompt_tokens": 428, "judge_completion_tokens": 131}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I am unable to answer this question as the movie \"Zombies on the Moon\" is not mentioned in the provided context.", "latency_ms": 4462.971210479736, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the system cannot answer or lacks information about the movie. The generated answer adds that it is not mentioned in the provided context, which is consistent with the situation. There is no incorrect factual claim and it remains faithful to the available information.", "main_total_tokens": 3867, "main_prompt_tokens": 3037, "main_completion_tokens": 830, "judge_total_tokens": 502, "judge_prompt_tokens": 416, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I cannot answer this question because the title \"The Galactic baker\" is not mentioned in the provided context.", "latency_ms": 4770.338773727417, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the database lacks information on the movie. The generated answer similarly states it cannot answer due to lack of mention in the context, effectively declining to provide a year. This is correct and faithful to the available information and does not introduce any hallucinated details.", "main_total_tokens": 4216, "main_prompt_tokens": 3323, "main_completion_tokens": 893, "judge_total_tokens": 495, "judge_prompt_tokens": 410, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I am sorry, but the title \"Return of the Null Pointer\" is not mentioned in the provided context.", "latency_ms": 4237.024784088135, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database. The generated answer similarly says the title is not mentioned in the provided context and does not fabricate a release year. This is correct with respect to the question and fully consistent with the given gold response.", "main_total_tokens": 3575, "main_prompt_tokens": 2833, "main_completion_tokens": 742, "judge_total_tokens": 496, "judge_prompt_tokens": 413, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I am unable to answer this question based strictly on the provided context, as the movie \"Data Science: The Movie\" is not listed in the documents.", "latency_ms": 5102.958917617798, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the movie is not in the available information, and neither fabricates a release year. The generated answer is correct with respect to the question and faithful to the (lack of) context, explicitly acknowledging that the movie is not listed in the documents.", "main_total_tokens": 3790, "main_prompt_tokens": 2831, "main_completion_tokens": 959, "judge_total_tokens": 510, "judge_prompt_tokens": 422, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I am unable to find the release year for \"Python's Revenge 2\" in the provided context.", "latency_ms": 4176.411390304565, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly indicates that it cannot find the release year, which aligns with the gold answer stating the movie is not in the database. It does not introduce unsupported facts and remains faithful to the given information.", "main_total_tokens": 3621, "main_prompt_tokens": 2862, "main_completion_tokens": 759, "judge_total_tokens": 485, "judge_prompt_tokens": 412, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2677.4911880493164, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, meaning the correct behavior is to express a lack of knowledge. The generated answer, 'I don't know,' correctly reflects this and does not introduce any unsupported information, so it is both correct and faithful to the gold.", "main_total_tokens": 966, "main_prompt_tokens": 550, "main_completion_tokens": 416, "judge_total_tokens": 480, "judge_prompt_tokens": 393, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I am unable to find the release year for \"The man who ate the sun\" in the provided context.", "latency_ms": 4980.110883712769, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both gold and generated answers state that the release year (or the movie itself) is not available/known, which is effectively the same outcome. The generated answer is correct and faithful to the gold response in meaning, even though the wording differs slightly.", "main_total_tokens": 3655, "main_prompt_tokens": 2756, "main_completion_tokens": 899, "judge_total_tokens": 495, "judge_prompt_tokens": 414, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not specify who directed the movie \"The Last Days of Lehman Brothers\" [Doc 46069].", "latency_ms": 3751.6233921051025, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not specify the director, which aligns with the gold answer that the director information is not in the database. It does not introduce unsupported facts and is faithful to the given limitation.", "main_total_tokens": 3555, "main_prompt_tokens": 2914, "main_completion_tokens": 641, "judge_total_tokens": 497, "judge_prompt_tokens": 422, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not contain information about the movie \"A Love to Die For\" or its director.", "latency_ms": 5552.480936050415, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not contain information about the director of \"A Love to Die For,\" aligning with the gold answer that the director information is not provided in the database. It does not hallucinate a director and stays within the given information, so it is both correct and faithful.", "main_total_tokens": 4032, "main_prompt_tokens": 3058, "main_completion_tokens": 974, "judge_total_tokens": 508, "judge_prompt_tokens": 416, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not contain information about who directed the movie \"Journey into Gaza\" [Doc 111360].", "latency_ms": 4215.2111530303955, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not contain information about who directed the movie \"Journey into Gaza,\" which matches the gold answer saying the director information is not provided in the database. It does not introduce any unsupported facts and stays faithful to the given context.", "main_total_tokens": 3886, "main_prompt_tokens": 3190, "main_completion_tokens": 696, "judge_total_tokens": 500, "judge_prompt_tokens": 416, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not contain information about who directed the movie \"The Last Night at Tremore Beach\" [Doc 135098].", "latency_ms": 4009.7098350524902, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not contain information about the director, which aligns with the gold answer indicating the database does not provide the director information. It does not hallucinate a director and remains faithful to the given context.", "main_total_tokens": 3270, "main_prompt_tokens": 2544, "main_completion_tokens": 726, "judge_total_tokens": 502, "judge_prompt_tokens": 424, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 3814.2189979553223, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy' and remaining faithful to the provided information.", "main_total_tokens": 3205, "main_prompt_tokens": 2718, "main_completion_tokens": 487, "judge_total_tokens": 456, "judge_prompt_tokens": 396, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 7154.777050018311, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Stranger in My Bed') and correctly identifies the movie described, so it is both correct and faithful.", "main_total_tokens": 4121, "main_prompt_tokens": 2749, "main_completion_tokens": 1372, "judge_total_tokens": 461, "judge_prompt_tokens": 400, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "", "latency_ms": 8262.611150741577, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a movie, but the gold answer is a TV series, not a film. Therefore the generated answer (which is empty) does not provide the correct response. The faithfulness score is high because we are directly comparing against the provided gold answer and acknowledging the mismatch without inventing alternate facts.", "main_total_tokens": 3969, "main_prompt_tokens": 2430, "main_completion_tokens": 1539, "judge_total_tokens": 491, "judge_prompt_tokens": 398, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 3513.0183696746826, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct movie title for the described plot. It is both correct and fully faithful to the provided information.", "main_total_tokens": 3242, "main_prompt_tokens": 2744, "main_completion_tokens": 498, "judge_total_tokens": 458, "judge_prompt_tokens": 395, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job! [Doc 66094]", "latency_ms": 6037.259340286255, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer, 'Wood Job! [Doc 66094]', correctly identifies the movie as 'Wood Job!', which matches the gold answer exactly. The added document tag does not change the correctness. The answer is faithful to the question and does not introduce contradictions.", "main_total_tokens": 4810, "main_prompt_tokens": 3609, "main_completion_tokens": 1201, "judge_total_tokens": 493, "judge_prompt_tokens": 409, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 5181.316375732422, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'Sesame Street: The Magical Wand Chase.'", "main_total_tokens": 4167, "main_prompt_tokens": 3238, "main_completion_tokens": 929, "judge_total_tokens": 472, "judge_prompt_tokens": 417, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond [Doc 74597]", "latency_ms": 3477.834701538086, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Beyond') and correctly identifies the movie based on the plot description, with no added or incorrect information.", "main_total_tokens": 3277, "main_prompt_tokens": 2678, "main_completion_tokens": 599, "judge_total_tokens": 475, "judge_prompt_tokens": 415, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9 [Doc 22478]", "latency_ms": 5576.164960861206, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (movie title '6ixtynin9') and does not introduce any extra, ungrounded information. Therefore it is both correct and faithful.", "main_total_tokens": 3725, "main_prompt_tokens": 2666, "main_completion_tokens": 1059, "judge_total_tokens": 485, "judge_prompt_tokens": 415, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread [Doc 32410]", "latency_ms": 5968.901872634888, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Club Dread') and correctly identifies the movie described. It is fully correct and faithful to the provided information.", "main_total_tokens": 4149, "main_prompt_tokens": 3007, "main_completion_tokens": 1142, "judge_total_tokens": 495, "judge_prompt_tokens": 433, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet [Doc 48873]", "latency_ms": 7109.528303146362, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie in which the described plot occurs. The gold answer is 'Toilet' and the generated answer is also 'Toilet [Doc 48873]'. The core title matches exactly, so the content is correct. The addition of '[Doc 48873]' is just an internal reference and does not change the answer. The generated response is thus both correct and faithful to the gold.", "main_total_tokens": 4639, "main_prompt_tokens": 3198, "main_completion_tokens": 1441, "judge_total_tokens": 535, "judge_prompt_tokens": 421, "judge_completion_tokens": 114}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel [Doc 1024]", "latency_ms": 6606.996536254883, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly in content (movie title 'Electric Angel'), with only an added bracketed document reference, which does not contradict the gold. Thus it is correct and faithful to the provided gold answer.", "main_total_tokens": 4244, "main_prompt_tokens": 2950, "main_completion_tokens": 1294, "judge_total_tokens": 500, "judge_prompt_tokens": 424, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 in the context is Downfall, with a rating of 4.0 [Doc 106810].\n\nCorrection: The lowest rated movie is **Fall**, with a rating of 4.0 [Doc 106810].", "latency_ms": 5057.034254074097, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states that the lowest rated movie is 'Downfall', which is incorrect given the correction that the movie is 'Fall'. It is therefore factually incorrect. It is also unfaithful to the provided correction/context, which explicitly identifies 'Fall' as the lowest rated movie with a rating of 4.0.", "main_total_tokens": 4106, "main_prompt_tokens": 2886, "main_completion_tokens": 1220, "judge_total_tokens": 531, "judge_prompt_tokens": 434, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 170.67837715148926, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific TV show (Tea House) as the highest rated TV show from 2010. The generated answer says 'I don't know,' which is incorrect relative to the gold answer, so correctness is 0. However, the model does not hallucinate or contradict its own uncertainty; it faithfully reports not knowing, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 386, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 206.84051513671875, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific movie title and the gold answer provides it. The generated response says 'I don't know,' which is factually incorrect given the gold answer. However, the model does not introduce any hallucinated or conflicting information; it simply states lack of knowledge. Therefore, correctness is 0 while faithfulness to the model's stated uncertainty is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 500, "judge_prompt_tokens": 395, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "en [Doc 744]", "latency_ms": 5179.23641204834, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'English' while the generated answer is 'en [Doc 744]'. Although 'en' could denote English, the addition of '[Doc 744]' is extra, incorrect content relative to the gold answer and the task. Therefore it is neither fully correct nor fully faithful to the expected answer.", "main_total_tokens": 4145, "main_prompt_tokens": 3112, "main_completion_tokens": 1033, "judge_total_tokens": 484, "judge_prompt_tokens": 389, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity (4.51", "latency_ms": 7026.524782180786, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incomplete and does not clearly identify 'Law & Order: Special Victims Unit' as the show. It stops mid-sentence with a numeric value ('4.51') and does not match the gold answer. It also appears to introduce an unexplained popularity score that is not grounded in the given gold reference, so it is neither correct nor faithful.", "main_total_tokens": 4680, "main_prompt_tokens": 3157, "main_completion_tokens": 1523, "judge_total_tokens": 510, "judge_prompt_tokens": 405, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2 [Doc 79355]", "latency_ms": 550.4951477050781, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and does not introduce any contradictory or extraneous information. Reference to the document ID does not affect factual correctness or faithfulness.", "main_total_tokens": 2426, "main_prompt_tokens": 2414, "main_completion_tokens": 12, "judge_total_tokens": 461, "judge_prompt_tokens": 392, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in 1999 [Doc 22481] and 2019 [Doc 88811].", "latency_ms": 4732.47504234314, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer indicates 2019 as the release year, but the generated answer gives both 1999 and 2019, so it is not correctly answering the specific question. It is partially faithful to the provided documents by reflecting both years, but it should have selected the correct one according to the gold, leading to low correctness and partial faithfulness.", "main_total_tokens": 3858, "main_prompt_tokens": 2949, "main_completion_tokens": 909, "judge_total_tokens": 518, "judge_prompt_tokens": 414, "judge_completion_tokens": 104}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "1", "latency_ms": 2393.86248588562, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer (1) is numerically incorrect compared to the gold answer (279). There is no evidence or reasoning to suggest the model was being faithful to any source; it simply produced an incorrect number without justification.", "main_total_tokens": 2911, "main_prompt_tokens": 2417, "main_completion_tokens": 494, "judge_total_tokens": 458, "judge_prompt_tokens": 383, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "8", "latency_ms": 3312.2217655181885, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (8) does not match the gold answer (12), so it is incorrect. However, the model did provide a specific numeric answer to the question asked, so it is faithful to the user query.", "main_total_tokens": 3128, "main_prompt_tokens": 2464, "main_completion_tokens": 664, "judge_total_tokens": 458, "judge_prompt_tokens": 383, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "I don't know.", "latency_ms": 2871.622085571289, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answer is incorrect because the movie was released in 2009, but the model said it did not know. However, the answer is faithful to the model’s uncertainty and does not hallucinate a wrong year.", "main_total_tokens": 1084, "main_prompt_tokens": 549, "main_completion_tokens": 535, "judge_total_tokens": 466, "judge_prompt_tokens": 391, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gemini-flash-latest", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English (en) [Doc 48035].", "latency_ms": 3905.557155609131, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the original language of 'The King’s Speech' is English, matching the gold label 'en'. It does not introduce contradictions or unsupported claims and remains faithful to the provided information.", "main_total_tokens": 3695, "main_prompt_tokens": 3032, "main_completion_tokens": 663, "judge_total_tokens": 478, "judge_prompt_tokens": 405, "judge_completion_tokens": 73}
