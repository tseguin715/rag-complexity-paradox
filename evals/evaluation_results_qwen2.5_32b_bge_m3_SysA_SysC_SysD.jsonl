{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The provided documents do not mention a release year for a film or show titled \"Wall Street.\" Therefore, based solely on the given context, I cannot provide the release year for \"Wall Street.\"", "latency_ms": 25999.924659729004, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1987, but the generated answer claims it cannot determine the year from the provided documents and does not give the correct year, so it is factually incorrect. However, it is faithful to the stated constraint of using only the given context, explicitly acknowledging the lack of information instead of hallucinating an answer.", "main_total_tokens": 3259, "main_prompt_tokens": 3219, "main_completion_tokens": 40, "judge_total_tokens": 514, "judge_prompt_tokens": 417, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The context does not provide information about a movie where classmates specifically have to play a survival game using their cell phones. However, there are movies that involve games or sinister activities linked to mobile phones or similar devices:\n\n1. \"Tokyo Gore School\" [Doc 45096] involves a mysterious violent game being played out in the corridors and playgrounds of Tokyo’s high schools, spread via students’ mobile phones.\n   \n2. \"Naraka 19\" [Doc 41254] features a terrifying mobile phone game from hell that traps the main character.\n\n3. \"47 Hours to Live\" [Doc 90746] involves a game on a creepy website that claims you can use your phone to summon the supernatural.\n\nNone of these exactly match the prompt's specific criteria of classmates playing a survival game using their cell phones, so based strictly on the provided context, there is no exact match.", "latency_ms": 20148.492097854614, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Brain Fluid Explosion Girl', but the generated answer claims no exact match in the context and does not provide this title, so it is factually incorrect. However, it accurately reflects what is (and is not) stated in the provided documents, so it is faithful to the given context.", "main_total_tokens": 3083, "main_prompt_tokens": 2890, "main_completion_tokens": 193, "judge_total_tokens": 660, "judge_prompt_tokens": 567, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show in which six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" [Doc 112849].", "latency_ms": 5132.673740386963, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' which matches the gold answer exactly. It is fully faithful to the information provided in the prompt.", "main_total_tokens": 2726, "main_prompt_tokens": 2691, "main_completion_tokens": 35, "judge_total_tokens": 485, "judge_prompt_tokens": 420, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The provided documents do not contain information about a movie that tells the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones. [Doc 97225] mentions a movie set in Munich but it is about spies during the 1938 Munich Conference. None of the other documents describe a plot involving rival gangs as described in the question.", "latency_ms": 9864.127397537231, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answer is faithful to the (implied) retrieval context by stating that the documents do not mention such a movie, but it is factually incorrect relative to the question because the correct answer is 'Nacht der Wölfe' and the model fails to provide it.", "main_total_tokens": 3308, "main_prompt_tokens": 3227, "main_completion_tokens": 81, "judge_total_tokens": 560, "judge_prompt_tokens": 473, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of Picnic at Hanging Rock take place in the year 1900 [Doc 126191].", "latency_ms": 4748.302459716797, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of Picnic at Hanging Rock take place in 1900, which matches the gold answer, and it does not introduce any unsupported information.", "main_total_tokens": 2950, "main_prompt_tokens": 2921, "main_completion_tokens": 29, "judge_total_tokens": 472, "judge_prompt_tokens": 406, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The provided context does not contain information about a 2004 movie matching the description given. [Doc X]", "latency_ms": 4643.860101699829, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the name of the movie, which is 'June'. The generated answer instead claims the context does not contain the information, and does not provide the correct title, so it is factually incorrect. However, it is faithful to the (assumed) lack of information in the provided context and does not hallucinate a movie title, so it is fully faithful to the given context.", "main_total_tokens": 3244, "main_prompt_tokens": 3219, "main_completion_tokens": 25, "judge_total_tokens": 565, "judge_prompt_tokens": 454, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "The highest rated Russian-language movie released in 1980, based on the provided documents, is \"Moscow Does Not Believe in Tears\" with a rating of 7.6 [Doc 61].", "latency_ms": 6149.829626083374, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger', while the generated answer claims 'Moscow Does Not Believe in Tears' is the highest rated; this is factually incorrect given the gold. Additionally, the generated answer references 'the provided documents' and Doc 61, but we have no such context here, so its appeal to those documents is not verifiably grounded in the given information, making it unfaithful to the evaluation context.", "main_total_tokens": 2932, "main_prompt_tokens": 2887, "main_completion_tokens": 45, "judge_total_tokens": 565, "judge_prompt_tokens": 436, "judge_completion_tokens": 129}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest Russian-language movie rating in 1999 based on the provided documents is 7.117 for \"The Rifleman of the Voroshilov Regiment\" [Doc 22123].", "latency_ms": 6427.788734436035, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the highest Russian-language movie rating in 1999 is 7.6, while the generated answer claims it is 7.117 for a specific film, which is numerically incorrect. Additionally, the generated response references an external document and specific title not supported by the gold answer, so it is not faithful to the given information.", "main_total_tokens": 3050, "main_prompt_tokens": 3003, "main_completion_tokens": 47, "judge_total_tokens": 526, "judge_prompt_tokens": 424, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "Based on the provided documents, there are two movies from 1987 with a rating over 7 that are in Chinese:\n\n1. \"A Chinese Ghost Story\" with a rating of 7.3 [Doc 7064]\n2. \"Last Eunuch In China\" with a rating of 7.8 [Doc 7754] \n\nTherefore, the answer is 2 movies.", "latency_ms": 10140.962362289429, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 4 such movies, but the generated answer concludes there are 2. This is factually incorrect relative to the gold label, so correctness is 0. However, the model consistently reasons from the documents it cites and does not introduce unsupported information; it accurately reports what it found in the provided docs and explains its conclusion based on them, so faithfulness to the given context is 1.", "main_total_tokens": 2920, "main_prompt_tokens": 2832, "main_completion_tokens": 88, "judge_total_tokens": 588, "judge_prompt_tokens": 472, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "Based on the provided documents, there are no TV shows that were released in 1987, have a rating over 7, and are in Chinese. The only Chinese language entry is \"A Chinese Ghost Story,\" but it is a movie, not a TV show [Doc 7064].", "latency_ms": 8032.157182693481, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 2 such TV shows, but the generated answer claims there are none, so it is factually incorrect relative to the gold. However, the generated answer is consistent with and grounded in the cited document, correctly noting that the only Chinese entry mentioned is a movie, not a TV show, so it is faithful to the provided evidence.", "main_total_tokens": 3333, "main_prompt_tokens": 3269, "main_completion_tokens": 64, "judge_total_tokens": 556, "judge_prompt_tokens": 452, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "Based on the provided documents, there isn't a French language TV show released in 1980. All French entries listed are of type \"Movie\" - \"Loulou\" and \"La Femme flic\". [Doc 689], [Doc 290]", "latency_ms": 7568.66455078125, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the only French language TV show released in 1980, and the gold answer is 'Fantômas'. The generated answer claims there is no such French TV show and lists only movies, which is factually incorrect relative to the gold. However, the generated answer is faithful to the cited documents, explicitly basing its conclusion on them, so it does not fabricate information beyond the provided sources.", "main_total_tokens": 2817, "main_prompt_tokens": 2757, "main_completion_tokens": 60, "judge_total_tokens": 546, "judge_prompt_tokens": 437, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie that features two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" [Doc 11205].", "latency_ms": 4898.352146148682, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise', which matches the gold answer exactly. It does not introduce contradictions or fabricate extra information beyond a brief citation-style reference, so it is both correct and faithful to the given information.", "main_total_tokens": 2918, "main_prompt_tokens": 2887, "main_completion_tokens": 31, "judge_total_tokens": 488, "judge_prompt_tokens": 410, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 6206.119537353516, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Won't You Be My Neighbor?' and remains faithful to the information given without adding unsupported claims.", "main_total_tokens": 3225, "main_prompt_tokens": 3182, "main_completion_tokens": 43, "judge_total_tokens": 506, "judge_prompt_tokens": 440, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The provided context includes multiple movies, so I need to specify which movie you are asking about. For example:\n\n- The movie \"Title:\" with genres 'Horror', 'Science Fiction', 'Thriller' was released in 1989 [Doc 9460].\n- The movie \"It\" from 2017 was released in 2017 [Doc 76823].\n- The movie \"Album\" was released in 2016 [Doc 73431].\n\nPlease specify the title of the movie you are asking about.", "latency_ms": 13507.104873657227, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question clearly refers to a single item with a known answer (2018), but the generated answer does not provide that year and instead claims ambiguity and lists unrelated examples. This is incorrect relative to the gold answer and not faithful to the provided supervision that the correct year is 2018.", "main_total_tokens": 3076, "main_prompt_tokens": 2953, "main_completion_tokens": 123, "judge_total_tokens": 572, "judge_prompt_tokens": 483, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The provided documents do not contain information about a TV series that is an animation about a young boy robot in 2030 AD.", "latency_ms": 4622.793197631836, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question clearly refers to the animated TV series 'Astro Boy', so the correct answer is known and does not require the provided documents. The generated answer incorrectly claims the information is not available, so it is factually wrong. However, it is faithful to the (stated) use of the provided documents, as it honestly reflects that it did not find the answer there.", "main_total_tokens": 2798, "main_prompt_tokens": 2769, "main_completion_tokens": 29, "judge_total_tokens": 520, "judge_prompt_tokens": 413, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 5062.487363815308, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, matching the gold answer. It does not introduce any unsupported details beyond the cited document reference, so it is both correct and faithful.", "main_total_tokens": 2585, "main_prompt_tokens": 2549, "main_completion_tokens": 36, "judge_total_tokens": 500, "judge_prompt_tokens": 424, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26m before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 6426.554441452026, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer directly matches the provided gold amount of £26m and correctly restates it with no added contradictions or unsupported details beyond the citation placeholder.", "main_total_tokens": 3369, "main_prompt_tokens": 3325, "main_completion_tokens": 44, "judge_total_tokens": 489, "judge_prompt_tokens": 428, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "The most popular English-speaking TV show released in 1997 based on the provided documents is \"Oz\" with a popularity score of 14.7527 [Doc 114714].", "latency_ms": 6366.807222366333, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The answer is incorrect because it gives 14.7527 as the popularity score, while the gold answer is 80.5772. It is also unfaithful to the gold/reference since it identifies a different show and score than the correct one, indicating it did not follow the underlying ground truth.", "main_total_tokens": 2857, "main_prompt_tokens": 2809, "main_completion_tokens": 48, "judge_total_tokens": 521, "judge_prompt_tokens": 429, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "None of the TV shows listed in the provided documents from 1997 are in the Russian language. [Doc 114830], [Doc 114860], [Doc 114854], [Doc 114791], [Doc 114908], [Doc 114813], [Doc 114820], [Doc 114817], [Doc 114730], [Doc 114899], [Doc 114849], [Doc 114782], [Doc 114851], [Doc 114858], [Doc 114844], [Doc 114751], [Doc 114846], and [Doc 114838] all specify languages other than Russian.", "latency_ms": 21599.141597747803, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 1, but the generated answer implies there were 0 Russian-language TV shows released in 1997, so it is factually incorrect. However, the explanation is internally consistent and grounded in the cited documents, so it is faithful to its own evidence even though that evidence does not match the gold.", "main_total_tokens": 3070, "main_prompt_tokens": 2859, "main_completion_tokens": 211, "judge_total_tokens": 615, "judge_prompt_tokens": 519, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 6466.320514678955, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, identifying the TV show as 'Angel Eyes', and does not introduce any contradictions or unsupported claims beyond a harmless document citation.", "main_total_tokens": 3752, "main_prompt_tokens": 3712, "main_completion_tokens": 40, "judge_total_tokens": 493, "judge_prompt_tokens": 429, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"What Is This Film Called Love?\" is English [Doc 58692].", "latency_ms": 4211.100339889526, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states the movie is in English, but the gold label says Korean, so the answer is incorrect. It also contradicts the provided gold information, so it is not faithful to the reference.", "main_total_tokens": 2743, "main_prompt_tokens": 2718, "main_completion_tokens": 25, "judge_total_tokens": 469, "judge_prompt_tokens": 397, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "None of the English-speaking Horror movies listed from 2009 have a popularity score greater than 7. The highest popularity score among them is 4.9425 for \"Friday the 13th\" [Doc 44859].", "latency_ms": 7281.779050827026, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a count of such movies (gold answer: 2), but the generated answer incorrectly asserts there are none, so the correctness is 0. The answer is consistent with the (incorrect) reasoning it gives about the listed movies and does not contradict its own use of the document, so faithfulness is 1.", "main_total_tokens": 3057, "main_prompt_tokens": 3001, "main_completion_tokens": 56, "judge_total_tokens": 532, "judge_prompt_tokens": 439, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The provided documents do not contain information about any movies released in 1979. The earliest release year mentioned in the context is 1980. Therefore, based on the given information, there is no answer to the question about the best-rated movie released in 1979.", "latency_ms": 7777.63557434082, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information in the provided documents about movies released in 1979, which aligns with the gold answer that the database has no such movies. It does not hallucinate an answer and stays faithful to the context, explaining the lack of data and thus yielding no answer to the question.", "main_total_tokens": 3108, "main_prompt_tokens": 3046, "main_completion_tokens": 62, "judge_total_tokens": 545, "judge_prompt_tokens": 450, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The provided documents do not mention a release year for \"Back to the Future Part IV\" as it does not exist within the given context. There is no information about a fourth installment of the Back to the Future series in the documents provided. [Doc 10621], [Doc 9405], and [Doc 4834] detail parts I, II, and III respectively, but there is no reference to a Part IV.", "latency_ms": 11399.977445602417, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information about a release year because Back to the Future Part IV does not exist, matching the gold answer’s core point. It remains faithful to the question and to the described documents, explicitly noting that only Parts I–III are mentioned and that Part IV is absent. Both correctness and faithfulness are therefore fully satisfied.", "main_total_tokens": 3673, "main_prompt_tokens": 3578, "main_completion_tokens": 95, "judge_total_tokens": 590, "judge_prompt_tokens": 487, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "The provided documents do not contain information about a movie from 1980 that is specifically about the nuclear Chernobyl disaster. Therefore, I cannot provide a popularity score for such a movie based on the given context.", "latency_ms": 6503.108024597168, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the popularity score of the least popular 1980 movie about the Chernobyl disaster. The gold answer notes that no such movies can be found. The generated answer correctly states that there is no information in the provided documents about such a movie and thus no popularity score can be given. This matches the gold answer in substance and does not introduce unsupported claims, so it is both correct regarding the question and faithful to the context.", "main_total_tokens": 3172, "main_prompt_tokens": 3126, "main_completion_tokens": 46, "judge_total_tokens": 564, "judge_prompt_tokens": 449, "judge_completion_tokens": 115}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are three TV shows or movies with 'Astro Boy' in the title:\n\n1. Astro Boy (2003) [Doc 116265]\n2. Astro Boy (1980) [Doc 112591]\n3. Astro Boy (2009) [Doc 44947]", "latency_ms": 9321.671724319458, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.8318724637288826, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.8318724637288826, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 4, but the generated answer states there are three TV shows or movies with 'Astro Boy' in the title and lists only three. This is factually incorrect relative to the gold. However, the answer is internally consistent and fully grounded in the information it presents, so it is faithful to its own reasoning but not correct with respect to the gold.", "main_total_tokens": 3090, "main_prompt_tokens": 3013, "main_completion_tokens": 77, "judge_total_tokens": 552, "judge_prompt_tokens": 445, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "None of the provided movie plots from 2025 allude to the Holocaust. [Doc 112027] mentions a historical backdrop but focuses on the Georgian community in Nice and does not reference the Holocaust specifically. [Doc 112161] and [Doc 111760] have historical elements but are set in different contexts (underground drug trafficking and Spain's Civil War, respectively). None of the other documents mention anything related to the Holocaust.", "latency_ms": 11827.077150344849, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that there are 2 movies in 2025 that allude to the Holocaust, but the generated answer claims that none of the provided movie plots allude to the Holocaust, which directly contradicts the gold. Thus the correctness with respect to the gold is 0. The reasoning in the generated answer is internally consistent with the cited documents and does not hallucinate beyond them, so it is faithful to the evidence it references.", "main_total_tokens": 3036, "main_prompt_tokens": 2932, "main_completion_tokens": 104, "judge_total_tokens": 593, "judge_prompt_tokens": 473, "judge_completion_tokens": 120}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "Based on the provided documents, there are three TV shows related to Super Mario Bros that were released in the 1990s:\n\n1. The Super Mario Bros. Super Show! [Doc 113517] - Year: 1989 (though it's just outside the 1990s decade, it's worth noting as a precursor)\n2. The Adventures of Super Mario Bros. 3 [Doc 113661] - Year: 1990\n3. Super Mario World [Doc 113814] - Year: 1991\n\nTherefore, two TV shows about Super Mario Bros were released in the 1990s.", "latency_ms": 17189.36276435852, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the number of TV shows released in the 1990s about Super Mario Bros, and the gold answer is 1. The generated answer concludes that there were 2 shows released in the 1990s (1990 and 1991), which contradicts the gold answer, so it is incorrect. However, the reasoning in the generated answer is consistent with the documents it cites and does not introduce information beyond them, so it is faithful to the provided context.", "main_total_tokens": 3732, "main_prompt_tokens": 3578, "main_completion_tokens": 154, "judge_total_tokens": 643, "judge_prompt_tokens": 514, "judge_completion_tokens": 129}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "The provided documents do not contain any information about TV shows released between 2000 and 2010 that are about Super Mario Bros. Therefore, based on the given context, the answer is not found. [Doc N/A]", "latency_ms": 7085.057497024536, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question expects the numeric answer 0, but the generated answer says the answer is not found in the documents rather than giving the correct value. This makes the factual answer incorrect relative to the gold label. However, the response is faithful to the stated limitation of the provided documents and does not hallucinate any specific number or TV shows, so it is faithful to the (implied) context.", "main_total_tokens": 3279, "main_prompt_tokens": 3228, "main_completion_tokens": 51, "judge_total_tokens": 545, "judge_prompt_tokens": 435, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular TV show or movie about ninjas, based on the popularity score provided, is \"NARUTO Spin-Off: Rock Lee & His Ninja Pals\" with a popularity of 10.7994 [Doc 120321].", "latency_ms": 7693.18962097168, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'Naruto', while the generated answer gives a different show, 'NARUTO Spin-Off: Rock Lee & His Ninja Pals', so it is incorrect. It also fabricates a specific popularity score and citation not present in the prompt, so it is not faithful to the provided information.", "main_total_tokens": 3222, "main_prompt_tokens": 3164, "main_completion_tokens": 58, "judge_total_tokens": 525, "judge_prompt_tokens": 431, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The provided documents do not mention a Jurassic Park/Power Rangers crossover movie. Therefore, based on the given information, the release date for such a crossover cannot be determined.", "latency_ms": 5342.843055725098, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states explicitly that the movie was not released. The generated answer instead says the date cannot be determined from the documents, which is incorrect relative to the gold; it should have stated that the movie does not exist or was never released. However, the generated answer is faithful to the provided documents by not inventing a release date and clearly indicating the documents do not mention such a movie.", "main_total_tokens": 3098, "main_prompt_tokens": 3063, "main_completion_tokens": 35, "judge_total_tokens": 529, "judge_prompt_tokens": 419, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "None of the provided documents mention a movie where Barbie fights the Predator. [Doc 33795] describes \"The Batman vs. Dracula,\" [Doc 28703] describes \"Killer Barbys vs. Dracula,\" and other entries discuss various Predators or Barbies, but there is no document that describes a movie where Barbie fights the Predator.", "latency_ms": 9681.63514137268, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the movie was not released, implying that such a movie exists conceptually but was unreleased. The generated answer instead claims there is no document mentioning such a movie, and does not address the question about the movie’s release status. It therefore does not correctly answer the question and is not faithful to the gold answer’s key point that the movie was not released.", "main_total_tokens": 3393, "main_prompt_tokens": 3314, "main_completion_tokens": 79, "judge_total_tokens": 560, "judge_prompt_tokens": 452, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The provided documents do not contain a comprehensive list of all Star Wars movies in chronological order by release date, so I cannot definitively state which is the 4th oldest Star Wars movie based solely on the information given. The documents mention \"Star Wars: The Last Jedi\" from 2017 and some documentaries and movies related to Star Wars, but no other main series entries are listed to determine their order. [Doc 32101], [Doc 31821], [Doc 32997], [Doc 32312] all refer to Star Wars-related content from 2004, but they are not part of the main film series.", "latency_ms": 16172.96314239502, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the 4th oldest Star Wars movie, with the gold answer being 'Star Wars: Droids - The Great Heep'. The generated answer instead claims it cannot determine the 4th oldest movie from the documents and does not provide the correct title, so it is incorrect. However, it accurately describes the limitations of the provided documents and does not invent a list or a wrong movie; it stays faithful to the information it says it has, so faithfulness is high.", "main_total_tokens": 3487, "main_prompt_tokens": 3341, "main_completion_tokens": 146, "judge_total_tokens": 648, "judge_prompt_tokens": 518, "judge_completion_tokens": 130}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The provided context includes multiple movies, so I need to specify which movie you are asking about. For example:\n\n- The movie \"Title:\" with genres 'Horror', 'Science Fiction', 'Thriller' was released in 1989 [Doc 9460].\n- The movie \"It\" from 2017 was released in 2017 [Doc 76823].\n- The movie \"Album\" was released in 2016 [Doc 73431].\n\nPlease specify the title of the movie you are asking about.", "latency_ms": 13567.423105239868, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly indicates the correct year is 1986, but the generated answer does not provide this year and instead asks for clarification, listing other examples. Therefore, it is incorrect with respect to the gold answer. However, given only the provided generated text and the described context (multiple movies and years), the assistant’s response is consistent with that context and does not invent facts beyond it, so it is faithful.", "main_total_tokens": 3076, "main_prompt_tokens": 2953, "main_completion_tokens": 123, "judge_total_tokens": 593, "judge_prompt_tokens": 483, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The provided documents do not contain the rating for the movie \"Forrest Gump\". [Doc 15881] only provides information about a documentary titled \"Through the Eyes of Forrest Gump\" which looks behind the scenes of Robert Zemeckis' 1994 Oscar-winning film, 'Forrest Gump', but does not give the rating for the original movie \"Forrest Gump\".", "latency_ms": 10321.611881256104, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the numerical rating of the movie Forrest Gump (8.466). The generated answer instead claims the documents do not contain the rating and never provides the requested value, so it is incorrect. However, the answer is faithful to the cited document, accurately stating that the document is about the documentary and does not give the rating for the original movie, so faithfulness is high.", "main_total_tokens": 3220, "main_prompt_tokens": 3134, "main_completion_tokens": 86, "judge_total_tokens": 584, "judge_prompt_tokens": 474, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 4987.662076950073, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med,' which matches the gold answer exactly and is fully supported by the given information.", "main_total_tokens": 2465, "main_prompt_tokens": 2428, "main_completion_tokens": 37, "judge_total_tokens": 483, "judge_prompt_tokens": 423, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "The seven prisoners escaped from a prison in Argentina in the movie Túnel de los huesos (2011) [Doc 53582].", "latency_ms": 5286.071300506592, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the seven prisoners escaped from a prison in Argentina, which matches the gold answer. It is fully consistent with the provided information and does not introduce unsupported claims.", "main_total_tokens": 3082, "main_prompt_tokens": 3047, "main_completion_tokens": 35, "judge_total_tokens": 488, "judge_prompt_tokens": 420, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The provided documents do not contain any information about a movie titled \"Sweet Trap\" from 2024 or details about an apprentice falling into someone's trap. Therefore, based on the given context, this information is not found.", "latency_ms": 6466.222763061523, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks specifically whose trap the apprentice falls into in Sweet Trap (2024), and the gold answer is Jiang Jie. The generated answer claims that the information is not found in the provided documents, rather than guessing incorrectly. This contradicts the gold answer, so it is factually incorrect relative to the gold. However, given the model’s stated constraint (no such info in the provided documents), it accurately reflects the absence of evidence and does not fabricate an answer, so it is faithful to the context and the instructions.", "main_total_tokens": 2994, "main_prompt_tokens": 2946, "main_completion_tokens": 48, "judge_total_tokens": 564, "judge_prompt_tokens": 428, "judge_completion_tokens": 136}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "Based on the provided documents, there are two Korean-language movies that mention assassins and are from the year 2023:\n\n1. \"The Assassin\" [Doc 106219]\n2. \"Kill Boksoon\" [Doc 105198]\n\nSince the context only provides information for the year 2023 and no future years, the answer to how many Korean-language movies mention assassins after 2023 is not found in the given documents. [Doc 106219], [Doc 105198]", "latency_ms": 13763.170719146729, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the number of Korean-language movies mentioning assassins after 2023, and the gold answer is 4. The generated answer instead discusses 2023 titles and concludes that the answer is not found, giving no numeric answer and not matching the gold value, so it is incorrect. However, the assistant accurately describes what is (and is not) in the provided context and does not fabricate unseen data, so it is faithful to the given documents.", "main_total_tokens": 2925, "main_prompt_tokens": 2799, "main_completion_tokens": 126, "judge_total_tokens": 609, "judge_prompt_tokens": 485, "judge_completion_tokens": 124}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "The provided documents do not contain any information about Steven Seagal and Bruce Lee appearing together in movies. [Not Found]", "latency_ms": 5004.763841629028, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that no information (or movies) involving both Steven Seagal and Bruce Lee together can be found. The generated answer is correct given the context and is faithful to the information available (explicitly notes absence in the documents).", "main_total_tokens": 3556, "main_prompt_tokens": 3531, "main_completion_tokens": 25, "judge_total_tokens": 499, "judge_prompt_tokens": 417, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 4524.978876113892, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (6.3) and does not introduce unsupported information. It is both correct and faithful to the provided reference.", "main_total_tokens": 3040, "main_prompt_tokens": 3014, "main_completion_tokens": 26, "judge_total_tokens": 465, "judge_prompt_tokens": 403, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The provided context does not specify the style of racing in any particular movie, so I cannot determine the style of racing based on the given information.", "latency_ms": 5093.731641769409, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question-answer pair clearly indicates that the correct answer is 'Drift'. The generated response incorrectly claims the information is not specified and refuses to answer, making it factually incorrect. However, it is faithful to the (stated) provided context from its own perspective, as it does not hallucinate an answer and instead acknowledges missing information.", "main_total_tokens": 3259, "main_prompt_tokens": 3229, "main_completion_tokens": 30, "judge_total_tokens": 506, "judge_prompt_tokens": 407, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 6716.101884841919, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the title as 'Confronting a Serial Killer' and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 2944, "main_prompt_tokens": 2893, "main_completion_tokens": 51, "judge_total_tokens": 515, "judge_prompt_tokens": 452, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 5049.652814865112, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold value exactly (7.838) and clearly attributes it to the document, so it is both correct and faithful.", "main_total_tokens": 3308, "main_prompt_tokens": 3278, "main_completion_tokens": 30, "judge_total_tokens": 467, "judge_prompt_tokens": 407, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 5277.689695358276, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'The Sixth Sense' and matches the gold answer exactly. It is fully supported by the prompt and introduces no unsupported information aside from a harmless document reference tag.", "main_total_tokens": 3030, "main_prompt_tokens": 2995, "main_completion_tokens": 35, "judge_total_tokens": 492, "judge_prompt_tokens": 420, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 4436.957359313965, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002 and aligns with the provided gold answer. It does not introduce any unsupported information.", "main_total_tokens": 3224, "main_prompt_tokens": 3201, "main_completion_tokens": 23, "judge_total_tokens": 459, "judge_prompt_tokens": 397, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 5500.592231750488, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, providing the correct title 'The Transporter Refueled' for the 2015 reboot. It remains faithful to the information implied by the gold without adding unsupported claims.", "main_total_tokens": 3298, "main_prompt_tokens": 3263, "main_completion_tokens": 35, "judge_total_tokens": 494, "judge_prompt_tokens": 420, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in the year 2015 [Doc 67500].", "latency_ms": 4760.457992553711, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It does not introduce contradictions or unsupported claims, so it is both correct and faithful.", "main_total_tokens": 3263, "main_prompt_tokens": 3235, "main_completion_tokens": 28, "judge_total_tokens": 480, "judge_prompt_tokens": 406, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "The provided documents do not contain any information about a romantic comedy involving Abraham Lincoln coming back from the dead. [Not Found]", "latency_ms": 4706.486940383911, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which romantic comedy features Abraham Lincoln coming back from the dead. Both the gold and generated answers state that no such movie is found / not in the provided documents. Given the context (answering from provided documents), the generated answer correctly reflects that the information is not present and does not invent a movie title. It matches the gold answer in substance and is fully faithful to the (lack of) source information.", "main_total_tokens": 3181, "main_prompt_tokens": 3155, "main_completion_tokens": 26, "judge_total_tokens": 530, "judge_prompt_tokens": 415, "judge_completion_tokens": 115}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The provided documents do not contain information about a movie that alludes to a manuscript written by Ernest Hemingway.", "latency_ms": 4300.124883651733, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the title of a movie that alludes to a manuscript written by Ernest Hemingway, and the correct answer is 'Havana Darkness.' The generated answer instead claims that the documents do not contain this information and does not provide the correct title, so it is factually incorrect. However, it is faithful to the (stated) lack of information in the provided documents, so it is not hallucinating beyond them.", "main_total_tokens": 2964, "main_prompt_tokens": 2940, "main_completion_tokens": 24, "judge_total_tokens": 526, "judge_prompt_tokens": 409, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show \"Doll & Em\" [Doc 122259].", "latency_ms": 5127.474308013916, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em,' which exactly matches the gold answer and remains faithful to the provided information.", "main_total_tokens": 2925, "main_prompt_tokens": 2891, "main_completion_tokens": 34, "judge_total_tokens": 477, "judge_prompt_tokens": 417, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The provided documents do not contain information about a character named Hallam or a movie from 2007 where a character spies on people to find the cause of their mother's death. Therefore, based solely on the given context, I cannot answer this question. [Doc Not Found]", "latency_ms": 7505.980730056763, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Hallam Foe'. The generated answer incorrectly claims the information is not in the documents and does not provide the correct movie title, so it is factually incorrect. However, it remains faithful to the (assumed) provided context by not hallucinating an answer and explicitly stating inability to answer from the documents, so faithfulness is high.", "main_total_tokens": 2962, "main_prompt_tokens": 2903, "main_completion_tokens": 59, "judge_total_tokens": 558, "judge_prompt_tokens": 454, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The provided documents do not contain information about a character whose last name sounds like they could be either a friend or a foe. Therefore, based strictly on the given context, there is no answer to this question.", "latency_ms": 5707.540988922119, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'A foe'. The generated answer claims there is no answer, which is incorrect with respect to the gold, so correctness is 0. However, given it explicitly reasons from the (stated) lack of information in the provided documents and does not fabricate details, it is faithful to its stated context, so faithfulness is 1.", "main_total_tokens": 2620, "main_prompt_tokens": 2577, "main_completion_tokens": 43, "judge_total_tokens": 531, "judge_prompt_tokens": 428, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "The age of Sam in the 2017 TV show Atypical is 18 years old [Doc 124583].", "latency_ms": 5300.104856491089, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, matching the gold answer exactly and not adding unsupported information beyond a generic citation.", "main_total_tokens": 3168, "main_prompt_tokens": 3136, "main_completion_tokens": 32, "judge_total_tokens": 481, "judge_prompt_tokens": 412, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 4772.443056106567, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies that in the TV show Atypical, Sam is on the autism spectrum, which matches the gold answer exactly. It is fully supported by the prompt and does not introduce any extraneous or incorrect information, so both correctness and faithfulness are perfect.", "main_total_tokens": 3043, "main_prompt_tokens": 3014, "main_completion_tokens": 29, "judge_total_tokens": 496, "judge_prompt_tokens": 410, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Sandra was born in Chamonix according to the plot description of the movie \"À corde tendue\" [Doc 59659].", "latency_ms": 5516.868591308594, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, which matches the gold answer. It is also faithful to the provided information, explicitly referencing the plot description of the movie and not introducing unsupported details.", "main_total_tokens": 3521, "main_prompt_tokens": 3488, "main_completion_tokens": 33, "judge_total_tokens": 498, "judge_prompt_tokens": 424, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "in this japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 5365.360260009766, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and accurately fills in the blank, so it is both correct and faithful.", "main_total_tokens": 4088, "main_prompt_tokens": 4062, "main_completion_tokens": 26, "judge_total_tokens": 470, "judge_prompt_tokens": 412, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the German language animation \"Tabaluga\", Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 5760.551691055298, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Greenland as the place where Tabaluga is crown-prince, which matches the gold answer. It is also faithful, directly answering the question without adding unsupported information.", "main_total_tokens": 3367, "main_prompt_tokens": 3330, "main_completion_tokens": 37, "judge_total_tokens": 495, "judge_prompt_tokens": 426, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is \"One Afternoon\" [Doc 96838].", "latency_ms": 6525.737047195435, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'One Afternoon', which matches the provided gold answer. It does not introduce contradictions or unsupported claims and remains faithful to the given information.", "main_total_tokens": 2711, "main_prompt_tokens": 2661, "main_completion_tokens": 50, "judge_total_tokens": 514, "judge_prompt_tokens": 448, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The provided documents do not contain information about a low-rated Italian language movie where a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her. None of the movies listed match this specific plot description.", "latency_ms": 8514.793395996094, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "recall_at_10": 0.0, "precision_at_5": 0.0, "precision_at_10": 0.0, "ndcg_at_5": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_5": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'The Life Ahead'. The generated answer incorrectly states that none of the movies match the description, so it is factually wrong, yielding a correctness score of 0. However, it accurately reflects what is (or is not) in the provided documents and does not introduce unsupported claims, so it is faithful to the given context, resulting in a faithfulness score of 1.", "main_total_tokens": 3031, "main_prompt_tokens": 2985, "main_completion_tokens": 46, "judge_total_tokens": 553, "judge_prompt_tokens": 442, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 5930.8836460113525, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary, matching the gold answer, so correctness is 1. However, it introduces an unverified citation/reference [Doc 73859] that is not part of the provided context, so faithfulness to the given information is not fully ensured, resulting in a faithfulness score of 0.", "main_total_tokens": 3183, "main_prompt_tokens": 3145, "main_completion_tokens": 38, "judge_total_tokens": 523, "judge_prompt_tokens": 417, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The description does not mention a specific name for the protagonist's racing team. However, it mentions the Newman-Haas (Andretti) racing team in the context of the documentary \"Super Speedway,\" but this is not directly related to a specific protagonist's team [Doc 20456].", "latency_ms": 7820.395708084106, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.75, "justification": "The gold answer clearly identifies the team as 'The McLaren Motor Racing team'. The generated answer incorrectly claims no specific team name is mentioned and instead references Newman-Haas, so it is factually wrong relative to the gold, yielding a correctness score of 0. It is partially grounded in the provided document snippet (it cites Newman-Haas from Doc 20456), so it shows some faithfulness to the source, but it contradicts the gold and misinterprets the question, so faithfulness is not full; hence 0.75.", "main_total_tokens": 2902, "main_prompt_tokens": 2839, "main_completion_tokens": 63, "judge_total_tokens": 578, "judge_prompt_tokens": 440, "judge_completion_tokens": 138}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The provided context does not specify a particular country for \"he\" without additional context about which character or film is being referred to. Therefore, the answer cannot be determined from the given information.", "latency_ms": 5574.546337127686, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'New Zealand', so the correct output should be that country. The generated answer says it cannot be determined, which is factually incorrect given the gold, so correctness is 0. However, the model is faithful to its stated premise that the context does not specify the country, so faithfulness to the (assumed) provided context is high.", "main_total_tokens": 2820, "main_prompt_tokens": 2781, "main_completion_tokens": 39, "judge_total_tokens": 519, "judge_prompt_tokens": 414, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "None of the movies listed in the provided documents from 2013 feature a plot where John McClane blows up the Great Wall of China. [Doc 59033] to [Doc 65829] do not contain such information.", "latency_ms": 7169.503211975098, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because it implies the answer is derived from specific documents and restricts the claim to movies listed there, while the correct response is simply that no such movie exists in the database. It invents references to document ranges that are not in the prompt, so it is not faithful to the given information.", "main_total_tokens": 2901, "main_prompt_tokens": 2845, "main_completion_tokens": 56, "judge_total_tokens": 541, "judge_prompt_tokens": 446, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with the given description is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 5084.282159805298, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the film as 'Tomie: Forbidden Fruit'. It aligns fully with the provided description and does not introduce any unsupported information, so both correctness and faithfulness are complete.", "main_total_tokens": 3053, "main_prompt_tokens": 3021, "main_completion_tokens": 32, "judge_total_tokens": 519, "judge_prompt_tokens": 443, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years in the 2021 movie \"Lei mi parla ancora\" [Doc 98566].", "latency_ms": 6100.564956665039, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (65 years) exactly and stays consistent with the provided information without adding unsupported claims.", "main_total_tokens": 3538, "main_prompt_tokens": 3499, "main_completion_tokens": 39, "judge_total_tokens": 479, "judge_prompt_tokens": 424, "judge_completion_tokens": 55}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The provided documents do not contain any information about a movie titled \"Scorpion King: Book of Souls\" from 2018 or details about The Scorpion King and Tala searching for something. Therefore, based on the given context, I cannot answer your question. [Information not found in provided documents]", "latency_ms": 7773.098945617676, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks what the Scorpion King and Tala search for in the 2018 movie 'Scorpion King: Book of Souls.' The gold answer is 'The Book of Souls.' The generated answer states that the documents do not contain information and therefore the question cannot be answered from the context. This is incorrect relative to the gold answer (so correctness is 0) but it is a faithful statement about not having information from the provided documents and does not hallucinate an answer (so faithfulness is 1).", "main_total_tokens": 2815, "main_prompt_tokens": 2751, "main_completion_tokens": 64, "judge_total_tokens": 588, "judge_prompt_tokens": 454, "judge_completion_tokens": 134}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document does not contain information about a movie titled \"Zombies on the Moon\". [Doc 91873] mentions a movie titled \"Zombies\" from 2019, and [Doc 32087] mentions a movie titled \"Zombie Honeymoon\" from 2004, but there is no movie with the exact title \"Zombies on the Moon\".", "latency_ms": 10474.462747573853, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie that is not actually represented, and the gold answer explicitly states it is not in the database. The generated answer correctly indicates the document does not contain information about a movie titled \"Zombies on the Moon\" and clarifies that other similarly named movies are present instead, aligning with the gold response. No incorrect facts about the requested movie are introduced and the answer stays grounded in the described documents, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3404, "main_prompt_tokens": 3318, "main_completion_tokens": 86, "judge_total_tokens": 593, "judge_prompt_tokens": 464, "judge_completion_tokens": 129}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain any information about a movie or show titled \"The Galactic baker\". Therefore, I cannot provide a release year for this title based on the given context.", "latency_ms": 5990.99588394165, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of 'The Galactic baker'. The gold answer states that the movie is not in the database. The generated answer correctly says that the provided documents do not contain information about such a title and therefore it cannot provide a release year. This is factually correct given the context and does not hallucinate a year. It is fully faithful to the premise that the answer must be based on the provided documents and matches the gold answer in spirit (acknowledging lack of information).", "main_total_tokens": 3663, "main_prompt_tokens": 3626, "main_completion_tokens": 37, "judge_total_tokens": 557, "judge_prompt_tokens": 425, "judge_completion_tokens": 132}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain any information about a movie titled \"Return of the Null Pointer\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 5515.049934387207, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year cannot be provided and does not invent a date. It is faithful to the (implicit) context and aligns with the gold answer in acknowledging lack of information, even though it phrases it differently by referencing the provided documents rather than the database.", "main_total_tokens": 2878, "main_prompt_tokens": 2840, "main_completion_tokens": 38, "judge_total_tokens": 515, "judge_prompt_tokens": 428, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain any information about a movie titled \"Data Science: The Movie\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 5578.244924545288, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie that is not in the reference data. The gold answer correctly states that the movie is not in the database. The generated answer similarly notes that the provided documents lack information about that movie and that it therefore cannot provide the release year. This is correct (it does not fabricate a year) and faithful to the given context (it explicitly grounds its inability to answer in the lack of information in the documents).", "main_total_tokens": 3015, "main_prompt_tokens": 2977, "main_completion_tokens": 38, "judge_total_tokens": 549, "judge_prompt_tokens": 428, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"Python's Revenge 2\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 5580.586671829224, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the information about the movie is not available. The generated response correctly notes that the documents/context do not contain information about 'Python's Revenge 2' and appropriately refuses to fabricate a release year. This is correct regarding the question and faithful to the (lack of) provided context.", "main_total_tokens": 3145, "main_prompt_tokens": 3108, "main_completion_tokens": 37, "judge_total_tokens": 522, "judge_prompt_tokens": 427, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document does not contain information about a movie titled \"The Last Pixel\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 4850.457668304443, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie that is not in the provided data. The gold answer correctly states that the movie is not in the database. The generated answer similarly states that the document does not contain information about the movie and therefore cannot provide the release year. This is correct with respect to the available context and faithful to it, without hallucinating a year or extra facts.", "main_total_tokens": 2631, "main_prompt_tokens": 2597, "main_completion_tokens": 34, "judge_total_tokens": 531, "judge_prompt_tokens": 422, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"The man who ate the sun\". Therefore, I cannot provide a release year for this specific title based on the given context.", "latency_ms": 5595.834732055664, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database. The generated answer similarly states that the documents do not contain information about the movie and that it cannot provide a release year. This is correct given no evidence about the movie and is faithful to the (implied) context of only using provided documents.", "main_total_tokens": 3040, "main_prompt_tokens": 3002, "main_completion_tokens": 38, "judge_total_tokens": 521, "judge_prompt_tokens": 429, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about who directed the movie \"The Last Days of Lehman Brothers\". [Doc 46069]", "latency_ms": 4886.793375015259, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not contain information about the director, which matches the gold answer indicating the database lacks this information. It does not introduce unsupported facts and remains faithful to the source constraints.", "main_total_tokens": 2949, "main_prompt_tokens": 2918, "main_completion_tokens": 31, "judge_total_tokens": 497, "judge_prompt_tokens": 424, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about who directed the movie \"A Love to Die For\". [Doc 15670] mentions a movie titled \"A Friend to Die For\", [Doc 88425] mentions \"An Affair to Die For\", and other documents list various titles with \"to die for\" in them, but there is no document that provides details about a movie called \"A Love to Die For\" or its director.", "latency_ms": 11337.403059005737, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the director information for \"A Love to Die For\" is not available in the provided documents/database. The generated answer stays grounded in the cited documents and explicitly notes the absence of relevant information, so it is both correct and faithful.", "main_total_tokens": 3293, "main_prompt_tokens": 3197, "main_completion_tokens": 96, "judge_total_tokens": 569, "judge_prompt_tokens": 483, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not specify who directed the movie \"Journey into Gaza\". [Doc 111360]", "latency_ms": 5254.669189453125, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director of 'Journey into Gaza' is not specified in the provided documents, which matches the gold answer that the director information is not available in the database. It does not introduce any unsupported claims and stays grounded in the given information.", "main_total_tokens": 3721, "main_prompt_tokens": 3694, "main_completion_tokens": 27, "judge_total_tokens": 498, "judge_prompt_tokens": 414, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about who directed the movie \"The Last Night at Tremore Beach\". [Doc 135098] provides details about the TV show with this title, including its year of release (2024), rating (7.0), and a brief plot description, but does not mention any director.", "latency_ms": 8559.08489227295, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not provide information about the director of 'The Last Night at Tremore Beach' and notes only the available details, matching the gold answer's stance that the director information is not in the database. It does not introduce any unsupported facts and closely follows the given document context, so it is both correct and faithful.", "main_total_tokens": 2700, "main_prompt_tokens": 2628, "main_completion_tokens": 72, "judge_total_tokens": 564, "judge_prompt_tokens": 462, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "This scenario occurs in the movie \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 4221.307277679443, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Rise of the Mummy' and does not introduce unsupported information beyond a harmless document reference tag, so it is both correct and faithful.", "main_total_tokens": 2833, "main_prompt_tokens": 2809, "main_completion_tokens": 24, "judge_total_tokens": 477, "judge_prompt_tokens": 410, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The scenario where Sarah fakes her own death to escape an abusive spouse occurs in the movie \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 5168.080568313599, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Stranger in My Bed,\" which matches the gold answer exactly. It does not introduce contradictions or unsupported claims beyond a harmless citation-style note, so it is both correct and faithful to the provided information.", "main_total_tokens": 2867, "main_prompt_tokens": 2832, "main_completion_tokens": 35, "judge_total_tokens": 507, "judge_prompt_tokens": 427, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The scenario where a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years is described in the plot of \"Game of Silence\" [Doc 123938]. However, it should be noted that \"Game of Silence\" is listed as a TV show and not a movie.", "latency_ms": 7671.495676040649, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 'Game of Silence' as matching the described plot. It also accurately notes that 'Game of Silence' is a TV show rather than a movie, which is faithful to the information and does not contradict the gold answer. Both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2515, "main_prompt_tokens": 2449, "main_completion_tokens": 66, "judge_total_tokens": 546, "judge_prompt_tokens": 457, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie in which a true story about a killer from the site Craigslist is depicted is \"The Craigslist Killer\" [Doc 53039].", "latency_ms": 4968.056201934814, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Craigslist Killer,\" which matches the gold answer exactly. It is fully faithful to the information implied in the question and does not introduce contradictions or extraneous, incorrect details.", "main_total_tokens": 2872, "main_prompt_tokens": 2840, "main_completion_tokens": 32, "judge_total_tokens": 494, "judge_prompt_tokens": 420, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The described scenario happens in the movie \"Wood Job!\" [Doc 66094].", "latency_ms": 4880.630970001221, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Wood Job!', which matches the gold answer exactly. It does not introduce contradictions or unsupported claims; the brief citation-like note does not alter the core answer, so the response is both correct and faithful.", "main_total_tokens": 3997, "main_prompt_tokens": 3976, "main_completion_tokens": 21, "judge_total_tokens": 497, "judge_prompt_tokens": 417, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The event where \"A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure\" happens in the movie \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 8078.4752368927, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, correctly identifying the movie as \"Sesame Street: The Magical Wand Chase.\" It stays faithful to the provided information without adding unsupported details.", "main_total_tokens": 3710, "main_prompt_tokens": 3654, "main_completion_tokens": 56, "judge_total_tokens": 528, "judge_prompt_tokens": 461, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The plot you described occurs in the movie \"Beyond\" from the year 2016 [Doc 74597].", "latency_ms": 4582.674741744995, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title matching the described plot; the gold answer is 'Beyond'. The generated answer identifies the movie as 'Beyond' and adds only the release year and a document reference. This is correct with respect to the gold and remains faithful to the provided information without contradiction or hallucinated plot details.", "main_total_tokens": 2716, "main_prompt_tokens": 2687, "main_completion_tokens": 29, "judge_total_tokens": 525, "judge_prompt_tokens": 431, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The described scenario happens in the movie \"6ixtynin9\" [Doc 22478].", "latency_ms": 4244.500160217285, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"6ixtynin9,\" matching the gold answer exactly. It remains fully faithful to the given information without adding unsupported details.", "main_total_tokens": 2700, "main_prompt_tokens": 2675, "main_completion_tokens": 25, "judge_total_tokens": 489, "judge_prompt_tokens": 423, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The plot you described happens in the movie \"Club Dread\" [Doc 32410].", "latency_ms": 4161.819934844971, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer exactly. It is fully consistent with the provided information and does not introduce contradictions or unsupported claims, so it is both correct and faithful.", "main_total_tokens": 3082, "main_prompt_tokens": 3060, "main_completion_tokens": 22, "judge_total_tokens": 519, "judge_prompt_tokens": 443, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The described plot occurs in the movie \"Toilet\" [Doc 48873].", "latency_ms": 4330.331325531006, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Toilet', which matches the gold answer. It does not add any unsupported details beyond naming the film and a neutral citation, so it is both correct and faithful.", "main_total_tokens": 3298, "main_prompt_tokens": 3277, "main_completion_tokens": 21, "judge_total_tokens": 504, "judge_prompt_tokens": 431, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The description provided matches the plot of the movie \"Electric Angel\" [Doc 1024].", "latency_ms": 4347.813129425049, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Electric Angel,\" which matches the gold answer. It does not introduce contradictions or unfounded claims and stays faithful to the information given in the prompt.", "main_total_tokens": 3272, "main_prompt_tokens": 3250, "main_completion_tokens": 22, "judge_total_tokens": 504, "judge_prompt_tokens": 435, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 based on the provided documents is \"Fall\" with a rating of 4.0 [Doc 106810].", "latency_ms": 4432.654619216919, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold answer identifies \"Mega Ape\" as the lowest rated movie from 2023, but the model states it is \"Fall\" with a rating of 4.0. This contradicts the gold label and misidentifies the movie. It is also unfaithful to the provided gold information, as it introduces a different title and rating that do not match the reference.", "main_total_tokens": 3151, "main_prompt_tokens": 3113, "main_completion_tokens": 38, "judge_total_tokens": 520, "judge_prompt_tokens": 412, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "The highest rated TV show from 2010 based on the provided documents is \"Tea House\" with a rating of 9.2 [Doc 119448].", "latency_ms": 4728.731155395508, "retrieval_recall": 1.0, "retrieval_mrr": 0.08333333333333333, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the highest rated TV show from 2010, and the gold answer is 'Tea House'. The generated answer correctly identifies 'Tea House' as the highest rated show from 2010 and adds a plausible rating and document reference. There is no contradiction with the gold answer, and the added detail is presented as coming from the provided documents, so it is faithful to the given information model. Therefore, both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3531, "main_prompt_tokens": 3490, "main_completion_tokens": 41, "judge_total_tokens": 539, "judge_prompt_tokens": 415, "judge_completion_tokens": 124}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest rated Japanese language movie from 1999 based on the provided documents is \"Yu☆Gi☆Oh!\" with a rating of 7.0 [Doc 22216].", "latency_ms": 6011.976718902588, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold answer indicates that the highest rated Japanese language movie from 1999 is 'The Uncut Story of Mewtwo's Origin', whereas the model claims it is 'Yu☆Gi☆Oh!'. Therefore, the content is not factually correct relative to the gold. Additionally, the model asserts its answer is based on provided documents (e.g., [Doc 22216]), but given the evaluation context and mismatch with the gold, this reference is not reliable, so the answer is not faithful to the ground truth being evaluated.", "main_total_tokens": 3043, "main_prompt_tokens": 3000, "main_completion_tokens": 43, "judge_total_tokens": 573, "judge_prompt_tokens": 428, "judge_completion_tokens": 145}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 in the provided context is \"Cataclysm\" with a rating of 3.889, and its language is English [Doc 744].", "latency_ms": 6712.787628173828, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980. The generated answer states that the lowest-rated movie is \"Cataclysm\" with a rating of 3.889 and that its language is English, which matches the gold answer (English). It also correctly refers to the provided context (Doc 744), so it is both factually correct and faithful to the source.", "main_total_tokens": 3551, "main_prompt_tokens": 3506, "main_completion_tokens": 45, "judge_total_tokens": 533, "judge_prompt_tokens": 421, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was \"The Amanda Show\" with a popularity score of 3.4197 [Doc 115219].", "latency_ms": 6597.567558288574, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is factually incorrect compared to the gold label; it names 'The Amanda Show' instead of 'Law & Order: Special Victims Unit.' It is also unfaithful because it fabricates a specific popularity score and citation without any provided supporting document context in the prompt.", "main_total_tokens": 3729, "main_prompt_tokens": 3687, "main_completion_tokens": 42, "judge_total_tokens": 512, "judge_prompt_tokens": 423, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 4149.901866912842, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (7.2) and clearly attributes it, so it is both correct and faithful.", "main_total_tokens": 3092, "main_prompt_tokens": 3070, "main_completion_tokens": 22, "judge_total_tokens": 457, "judge_prompt_tokens": 400, "judge_completion_tokens": 57}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in two different years according to the provided documents: 1999 [Doc 22481] and 2019 [Doc 88811].", "latency_ms": 6740.7472133636475, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2019, but the generated answer states that the movie was released in both 1999 and 2019, which is incorrect for this question; thus correctness is 0. However, the generated answer accurately reflects the conflicting information explicitly mentioned in the provided documents (1999 in one and 2019 in another), so it is faithful to the given evidence, yielding faithfulness 1.", "main_total_tokens": 3129, "main_prompt_tokens": 3079, "main_completion_tokens": 50, "judge_total_tokens": 538, "judge_prompt_tokens": 423, "judge_completion_tokens": 115}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The provided documents do not contain specific information about the number of votes \"My Dog Skip\" received. [Doc 24114] provides a rating of 6.7 and popularity of 1.7749 for the movie, but it does not specify the number of votes.", "latency_ms": 7799.07488822937, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the specific number of votes (279). The generated answer incorrectly states that the documents do not contain this information and does not provide the correct value, so it is factually incorrect. However, the answer is faithful to the reasoning it presents and does not hallucinate a number; it accurately reflects what it claims about the document contents, so faithfulness is high.", "main_total_tokens": 3159, "main_prompt_tokens": 3097, "main_completion_tokens": 62, "judge_total_tokens": 544, "judge_prompt_tokens": 437, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The provided documents do not contain information about the number of votes for \"Oh My God!\". Therefore, I cannot provide that information based on the given context.", "latency_ms": 5230.755090713501, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 12 votes, but the generated answer claims the information is not available, which is factually incorrect. However, given the assumption that the model must rely only on the provided documents, the response is faithful to its stated limitation and does not hallucinate a number, so faithfulness is high while correctness relative to the gold is 0.", "main_total_tokens": 3179, "main_prompt_tokens": 3147, "main_completion_tokens": 32, "judge_total_tokens": 510, "judge_prompt_tokens": 412, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The provided documents do not contain information about the movie \"The Last Days of Lehman Brothers\". Therefore, I cannot provide the release year based on the given context.", "latency_ms": 5138.591051101685, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answer is faithful to the provided context—it correctly states that the documents do not contain the information. However, it is factually incorrect relative to the question and gold answer; the movie was released in 2009, and the model failed to provide this.", "main_total_tokens": 2856, "main_prompt_tokens": 2822, "main_completion_tokens": 34, "judge_total_tokens": 504, "judge_prompt_tokens": 420, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_bge-m3_qwen2.5:32b", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [en] [Doc 48035].", "latency_ms": 4811.152935028076, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies English as the original language of 'The King's Speech' and is consistent with the gold label 'en'. The reference to a document does not contradict the provided information.", "main_total_tokens": 3188, "main_prompt_tokens": 3161, "main_completion_tokens": 27, "judge_total_tokens": 474, "judge_prompt_tokens": 405, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The provided documents do not contain information about the release year of the movie \"Wall Street.\" Therefore, based solely on the given context, the answer is not found.", "latency_ms": 4788.5425090789795, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1987, but the generated answer claims the information is not found, so it is factually incorrect. However, the response is faithful to the (stated) constraint of only using the provided documents and not adding outside knowledge, so faithfulness is high.", "main_total_tokens": 2328, "main_prompt_tokens": 2294, "main_completion_tokens": 34, "judge_total_tokens": 499, "judge_prompt_tokens": 411, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie where classmates have to play a survival game using their cell phones is \"Tokyo Gore School\" [Doc 45096]. In this film, students discover a mysterious violent game being played out in the corridors and playgrounds of Tokyo’s high schools, which is spread via their mobile phones, turning victims into senseless killing machines.", "latency_ms": 8148.94700050354, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the movie where classmates have to play a survival game using their cell phones, with the gold answer 'Brain Fluid Explosion Girl'. The generated answer instead identifies 'Tokyo Gore School', which is a different film. Thus, the answer is incorrect. However, it stays consistent with the (incorrect) cited document and does not invent contradictions relative to its own source, so it is faithful to the provided doc description.", "main_total_tokens": 2190, "main_prompt_tokens": 2118, "main_completion_tokens": 72, "judge_total_tokens": 571, "judge_prompt_tokens": 455, "judge_completion_tokens": 116}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show where six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" [Doc 112849].", "latency_ms": 4350.429534912109, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' which matches the gold answer exactly. The reasoning is directly supported by the prompt and does not introduce unsupported information, so it is both correct and faithful.", "main_total_tokens": 1898, "main_prompt_tokens": 1864, "main_completion_tokens": 34, "judge_total_tokens": 495, "judge_prompt_tokens": 419, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "None of the provided documents describe a movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones. [Doc 68427] talks about a movie set in Berlin with elements of punk culture but does not match the description given in the question. Similarly, [Doc 77244] is about a woman recovering from an attempted suicide who meets an aspiring doctor through dance and is set in Turkey, not about rival gangs in Munich. Therefore, based on the provided context, there is no movie that matches the description given in the question.", "latency_ms": 12843.913793563843, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the movie title, which is 'Nacht der Wölfe'. The generated answer incorrectly claims no such movie is identified in the documents, so it does not supply the correct title and is therefore factually incorrect (correctness=0). However, the answer accurately describes what is and is not in the cited documents and does not invent a movie from the provided context, so it remains faithful to the given evidence (faithfulness=1).", "main_total_tokens": 2349, "main_prompt_tokens": 2225, "main_completion_tokens": 124, "judge_total_tokens": 638, "judge_prompt_tokens": 515, "judge_completion_tokens": 123}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of Picnic at Hanging Rock take place in the year 1900 [Doc 126191].", "latency_ms": 4263.9405727386475, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of Picnic at Hanging Rock take place in 1900, which matches the gold answer exactly and is fully supported by the cited document.", "main_total_tokens": 2145, "main_prompt_tokens": 2116, "main_completion_tokens": 29, "judge_total_tokens": 472, "judge_prompt_tokens": 406, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "None of the provided documents describe a 2004 movie fitting the given description about a young violinist named after a calendar month working in a Venice coffee shop and daydreaming about her destined lover. [Doc X]", "latency_ms": 5958.723068237305, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question is a standard movie-trivia style query whose correct answer is 'June'. The generated answer claims that none of the documents contain such a movie and does not provide the correct title, so it is factually incorrect with respect to the question. However, given the apparent constraint to use only the provided documents, the model truthfully states what is (presumably) in them and does not hallucinate an answer, so it is faithful to the provided context.", "main_total_tokens": 2540, "main_prompt_tokens": 2493, "main_completion_tokens": 47, "judge_total_tokens": 600, "judge_prompt_tokens": 476, "judge_completion_tokens": 124}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1179.3510913848877, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific movie, and the gold answer provides it. The generated answer says 'I don't know,' which is incorrect because the correct answer is available, so correctness is 0. However, the model does not invent any incorrect information; it simply admits not knowing, so it is fully faithful to its own (lack of) knowledge, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 510, "judge_prompt_tokens": 399, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 281.2654972076416, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered 'I don't know' instead of providing the correct rating 7.6, so the answer is incorrect. However, it did not fabricate any information and honestly stated its uncertainty, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 389, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 984.4615459442139, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3535124404757273, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is factually incorrect relative to the labeled ground truth. However, there is no indication that the model contradicted the given question or introduced unrelated content; it simply produced an incorrect number, so faithfulness to the question itself is preserved.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 489, "judge_prompt_tokens": 394, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 156.36682510375977, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. There is no indication that the model contradicted its own prior reasoning or context, so it is fully faithful to its (incorrect) conclusion.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 474, "judge_prompt_tokens": 395, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1470.512866973877, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answer is incorrect because it says 'I don't know' instead of providing 'Fantômas.' However, the answer is faithful to the model's stated uncertainty and does not hallucinate, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 389, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie that matches the description of being about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" from 1990 [Doc 11205].", "latency_ms": 5249.629974365234, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise', which matches the gold answer exactly. The answer is fully grounded in the provided description and does not introduce unsupported information beyond an innocuous citation placeholder, so it is both correct and faithful.", "main_total_tokens": 2132, "main_prompt_tokens": 2090, "main_completion_tokens": 42, "judge_total_tokens": 499, "judge_prompt_tokens": 419, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 5515.361785888672, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Won't You Be My Neighbor?'. It does not introduce contradictions or unsupported information beyond a benign document reference, so it is both correct and faithful.", "main_total_tokens": 2361, "main_prompt_tokens": 2318, "main_completion_tokens": 43, "judge_total_tokens": 517, "judge_prompt_tokens": 440, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The question \"What year was it released?\" does not provide enough context to determine which specific movie or item is being referred to. Multiple movies are listed in the provided documents, each with different release years. Therefore, a precise answer cannot be given without additional information about which movie or item \"it\" refers to. [Doc 9460, Doc 76823, Doc 73431, Doc 25850, Doc 10351, Doc 10637, Doc 113633, Doc 22768, Doc 6950, Doc 6160, Doc 3711, Doc 92877, Doc 75757, Doc 25228, Doc 25942]", "latency_ms": 18293.632745742798, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The gold answer specifies the year 2018, so the correct behavior was to answer with that year. The generated answer instead claims that there is not enough context and refuses to give a year, which is incorrect relative to the gold. Thus correctness is 0. However, in terms of faithfulness to the provided documents, the model explains that multiple movies with different years are present and that it cannot disambiguate, which is a plausible and grounded interpretation of ambiguous context, so faithfulness is relatively high though not perfect, as it should have aligned with the QA task requirement to pick the gold year.", "main_total_tokens": 2335, "main_prompt_tokens": 2153, "main_completion_tokens": 182, "judge_total_tokens": 670, "judge_prompt_tokens": 516, "judge_completion_tokens": 154}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "None of the provided documents describe a TV series that is an animation about a young boy robot in 2030 AD.", "latency_ms": 4027.6803970336914, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Astro Boy', but the generated answer claims none of the documents describe such a series instead of identifying Astro Boy. This is incorrect relative to the gold answer, so correctness is 0. However, given the generated answer is explicitly based on the (stated) absence of information in the provided documents, it does not introduce unsupported facts, so it is faithful to its cited basis, giving faithfulness 1.", "main_total_tokens": 2179, "main_prompt_tokens": 2152, "main_completion_tokens": 27, "judge_total_tokens": 530, "judge_prompt_tokens": 411, "judge_completion_tokens": 119}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 4473.396301269531, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: both state that the woman and her son were held captive for 7 years. It is both factually correct and faithful to the gold reference.", "main_total_tokens": 1883, "main_prompt_tokens": 1847, "main_completion_tokens": 36, "judge_total_tokens": 493, "judge_prompt_tokens": 424, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot held gold bullion worth £26m before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 5442.205429077148, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot held gold bullion worth £26 million before the robbery on 26 November 1983. It directly matches the provided gold answer and does not introduce unsupported information, aside from a harmless reference formatting to a document. The core factual content is accurate and faithful to the given data.", "main_total_tokens": 2356, "main_prompt_tokens": 2313, "main_completion_tokens": 43, "judge_total_tokens": 526, "judge_prompt_tokens": 427, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6194.685935974121, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model said it did not know the answer, while a specific numeric value is known from the gold answer. This is incorrect with respect to the task, but it is faithful because the model did not fabricate a value and accurately reflected its uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 476, "judge_prompt_tokens": 396, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 158.21385383605957, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 Russian language TV show released in 1997, while the generated answer is 0. This directly contradicts the gold, making the generated answer factually incorrect relative to the labeled ground truth. However, there is no sign of hallucinated reasoning or misuse of the question context beyond the incorrect numeric answer, so it remains faithful to the (albeit wrong) claim it makes.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 500, "judge_prompt_tokens": 385, "judge_completion_tokens": 115}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 5715.182065963745, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Angel Eyes' and does not introduce unsupported information beyond the citation marker.", "main_total_tokens": 2739, "main_prompt_tokens": 2699, "main_completion_tokens": 40, "judge_total_tokens": 490, "judge_prompt_tokens": 429, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"What Is This Film Called Love?\" is English [Doc 58692].", "latency_ms": 3625.9186267852783, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states the movie is in English, while the gold label indicates the language is Korean. This is factually incorrect. It also contradicts the provided gold answer, so it is not faithful to the reference.", "main_total_tokens": 2027, "main_prompt_tokens": 2002, "main_completion_tokens": 25, "judge_total_tokens": 472, "judge_prompt_tokens": 397, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 484.41290855407715, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. However, the model directly provided a numeric answer to the question without adding unsupported claims, so it is faithful to its (incorrect) prediction.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 473, "judge_prompt_tokens": 394, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The provided context does not contain any movies released in 1979. Therefore, based strictly on the given information, there is no best-rated movie from 1979 to identify. [Doc Not Applicable]", "latency_ms": 5875.998735427856, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the best-rated movie from 1979, but the gold answer specifies that there are no 1979 movies in the database. The generated answer correctly states that the provided context does not contain any movies from 1979 and thus no best-rated movie can be identified, which matches the gold’s constraint. It does not introduce any unsupported facts and stays within the given information, so it is both correct and faithful.", "main_total_tokens": 2376, "main_prompt_tokens": 2329, "main_completion_tokens": 47, "judge_total_tokens": 554, "judge_prompt_tokens": 436, "judge_completion_tokens": 118}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The provided context does not contain any information about a movie titled \"Back to the Future Part IV\" or its release year. [Doc X]", "latency_ms": 4742.8929805755615, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information about a movie titled 'Back to the Future Part IV' or its release year, which aligns with the gold answer indicating that such a movie does not exist in the database. It is also faithful to the given context, as it does not invent a release year or additional details.", "main_total_tokens": 2927, "main_prompt_tokens": 2897, "main_completion_tokens": 30, "judge_total_tokens": 526, "judge_prompt_tokens": 429, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 15.3350830078125, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly indicates that it cannot find any movies or shows matching the criteria, which aligns with the gold answer that there are no movies from 1980 related to the Chernobyl disaster (the disaster occurred in 1986). It is faithful to the question and does not fabricate a movie or a popularity score, and it reasonably prompts the user to re-check the time period, which is consistent with the known facts.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 551, "judge_prompt_tokens": 434, "judge_completion_tokens": 117}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 69.99707221984863, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (4), so it is incorrect. However, we have no evidence of additional hallucinated details or contradictions beyond the incorrect numeric response, so it is fully unfaithful only with respect to the gold label, not to any provided context; thus faithfulness to the (nonexistent) context is maintained.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 388, "judge_completion_tokens": 104}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 168.61581802368164, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says 2 while the generated answer says 0, so the generated answer is factually incorrect relative to the labeled gold. However, the model did not fabricate any reasoning or external facts; it simply provided a different numeric answer, so it remains fully faithful to its own (albeit incorrect) output.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 386, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 109.39431190490723, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 1, but the generated answer is 0, so it is factually incorrect relative to the gold label, giving a correctness score of 0. The model clearly and directly answers the question with a specific number without contradiction or extraneous fabrication, so it is internally consistent and fully faithful to the (incorrect) reasoning it implies, giving a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 500, "judge_prompt_tokens": 389, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 109.80105400085449, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer and the generated answer both state 0 TV shows, so the generated answer matches the gold exactly and is fully faithful to it.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 451, "judge_prompt_tokens": 391, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 109.39216613769531, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto'. The generated answer says 'I don't know', which is factually incorrect given the reference and does not attempt to answer the question. However, it does not fabricate any information or contradict the prompt beyond expressing lack of knowledge, so it is fully faithful to the (incorrect) reasoning it presents.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 386, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The provided documents do not contain information about a Jurassic Park/Power Ranger crossover movie. Therefore, based on the given context, there is no release date for such a crossover movie.", "latency_ms": 5113.022327423096, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer frames the absence of a release date as being only based on the provided documents, implying uncertainty and not clearly stating that the movie does not exist. The gold answer asserts that the movie was not released, so the generated answer is not fully correct. However, it is faithful to the given context because it accurately reflects that the provided documents contain no such information.", "main_total_tokens": 2296, "main_prompt_tokens": 2259, "main_completion_tokens": 37, "judge_total_tokens": 526, "judge_prompt_tokens": 421, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Based on the provided context, there is no movie where Barbie fights the Predator. The closest entry is \"Killer Barbys vs. Dracula\" [Doc 28703], but it involves Barbies fighting Dracula, not a Predator.", "latency_ms": 6450.71268081665, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer is essentially correct: there is no released movie in which Barbie fights the Predator, which matches the gold answer that such a movie was not released. However, the answer introduces extra, speculative information about a specific film (\"Killer Barbys vs. Dracula\" and a document reference) that is not present in the gold and cannot be verified from the given prompt alone, so faithfulness is slightly reduced.", "main_total_tokens": 2473, "main_prompt_tokens": 2420, "main_completion_tokens": 53, "judge_total_tokens": 546, "judge_prompt_tokens": 429, "judge_completion_tokens": 117}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The provided context does not contain a list or information about the chronological order of Star Wars movies, so it's not possible to determine the 4th oldest Star Wars movie from the given documents. [Doc 32101] mentions Episodes IV, V, and VI but does not provide enough information to establish the fourth oldest movie in the franchise.", "latency_ms": 8418.975591659546, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the 4th oldest Star Wars movie, and the gold answer is a specific title, 'Star Wars: Droids - The Great Heep.' The generated answer claims it is impossible to determine the 4th oldest movie from the provided context and does not attempt to answer the question directly. Therefore it is incorrect with respect to the task. However, it accurately reflects the stated limitation of the provided context and does not invent a list or movie order, so it is faithful to the (implied) document constraints, yielding high faithfulness.", "main_total_tokens": 2609, "main_prompt_tokens": 2536, "main_completion_tokens": 73, "judge_total_tokens": 601, "judge_prompt_tokens": 457, "judge_completion_tokens": 144}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The question \"What year was it released?\" does not provide enough context to determine which specific movie or item is being referred to. Multiple movies are listed in the provided documents, each with different release years. Therefore, a precise answer cannot be given without additional information about which movie or item \"it\" refers to. [Doc 9460, Doc 76823, Doc 73431, Doc 25850, Doc 10351, Doc 10637, Doc 113633, Doc 22768, Doc 6950, Doc 6160, Doc 3711, Doc 92877, Doc 75757, Doc 25228, Doc 25942]", "latency_ms": 18340.428590774536, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer clearly indicates that a specific year, 1986, was the correct response, implying the context did identify a particular item. The generated answer instead claims there is insufficient context and refuses to provide a year, which is incorrect relative to the gold answer. It also contradicts the given supervision signal that there was a determinate answer, so it is not faithful to the task context as represented by the gold label.", "main_total_tokens": 2335, "main_prompt_tokens": 2153, "main_completion_tokens": 182, "judge_total_tokens": 632, "judge_prompt_tokens": 516, "judge_completion_tokens": 116}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 62.52455711364746, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is a specific numerical rating (8.466). The generated answer says 'I don't know,' which does not provide the correct rating, so correctness is 0. However, the model does not hallucinate an incorrect value and honestly states a lack of knowledge, so the response is faithful to its uncertainty, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 502, "judge_prompt_tokens": 398, "judge_completion_tokens": 104}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 4649.590015411377, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Mighty Med') and correctly states the name of the TV show. It is fully correct and aligned with the provided information.", "main_total_tokens": 1911, "main_prompt_tokens": 1874, "main_completion_tokens": 37, "judge_total_tokens": 488, "judge_prompt_tokens": 423, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in Argentina [Doc 53582].", "latency_ms": 4488.832235336304, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Argentina as the country where the prison break occurred in 'Túnel de los huesos (2011)', matching the gold answer exactly and staying faithful to the provided information.", "main_total_tokens": 2315, "main_prompt_tokens": 2283, "main_completion_tokens": 32, "judge_total_tokens": 488, "judge_prompt_tokens": 417, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "None of the provided documents contain information about a movie titled \"Sweet Trap\" from 2024 or any related context about an apprentice falling into someone's trap. Therefore, based on the given context, the answer cannot be found.", "latency_ms": 6041.548728942871, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Jiang Jie', but the generated answer claims the answer cannot be found in the provided documents. This is incorrect with respect to the gold answer, so correctness is 0. However, the model explicitly bases its statement on the absence of information in the context and does not fabricate an answer, so it is faithful to the stated evidence, giving faithfulness a score of 1.", "main_total_tokens": 2359, "main_prompt_tokens": 2310, "main_completion_tokens": 49, "judge_total_tokens": 541, "judge_prompt_tokens": 429, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 418.3175563812256, "retrieval_recall": 0.0, "retrieval_mrr": 0.25, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.16812753627111746, "hit_rate_at_5": 1.0, "recall_at_10": 0.25, "precision_at_10": 0.1, "ndcg_at_10": 0.16812753627111746, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (4), so it is incorrect. There is no additional reasoning given that contradicts the stated answer, so faithfulness to its own (albeit wrong) conclusion is maintained.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 464, "judge_prompt_tokens": 385, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "35", "latency_ms": 30.95078468322754, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '35' is incorrect because the gold indicates there are no movies featuring both Bruce Lee and Steven Seagal together. It is also unfaithful to the gold reference, which explicitly states an inability to find any such movies, not a count of 35.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 395, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 3945.7879066467285, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer matches the gold rating exactly (6.3), so it is correct. However, it fabricates a citation to '[Doc 42750]' which is not provided in the prompt, making the explanation about the source unfaithful to the given information.", "main_total_tokens": 2226, "main_prompt_tokens": 2200, "main_completion_tokens": 26, "judge_total_tokens": 488, "judge_prompt_tokens": 403, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in \"Need for Speed\" is street-racing, as evidenced by the description which mentions a local street-racer as the protagonist who later participates in an illegal New York-to-Los Angeles race [Doc 62979].", "latency_ms": 6479.698181152344, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question expects the specific style of racing to be 'Drift'. The generated answer instead identifies the style as 'street-racing', which is incorrect. However, the answer is faithful to the cited document, which describes a street-racer protagonist and an illegal New York-to-Los Angeles race, so it accurately reflects the provided evidence even though it does not match the gold label.", "main_total_tokens": 2501, "main_prompt_tokens": 2448, "main_completion_tokens": 53, "judge_total_tokens": 530, "judge_prompt_tokens": 427, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie or TV show about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 6404.074668884277, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, 'Confronting a Serial Killer', and stays faithful to the referenced document without adding incorrect details.", "main_total_tokens": 2265, "main_prompt_tokens": 2211, "main_completion_tokens": 54, "judge_total_tokens": 515, "judge_prompt_tokens": 455, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 4392.755031585693, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and does not introduce any unsupported information. It is both correct and faithful to the provided data.", "main_total_tokens": 2550, "main_prompt_tokens": 2520, "main_completion_tokens": 30, "judge_total_tokens": 466, "judge_prompt_tokens": 407, "judge_completion_tokens": 59}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 4776.955604553223, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Sixth Sense' and stays faithful to the gold answer without adding conflicting information.", "main_total_tokens": 2316, "main_prompt_tokens": 2281, "main_completion_tokens": 35, "judge_total_tokens": 476, "judge_prompt_tokens": 420, "judge_completion_tokens": 56}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 3785.752534866333, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, which matches the gold answer, and it appears to be supported by the cited document.", "main_total_tokens": 2430, "main_prompt_tokens": 2407, "main_completion_tokens": 23, "judge_total_tokens": 461, "judge_prompt_tokens": 397, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 4806.937217712402, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the title as 'The Transporter Refueled' and does not introduce any contradictory or unsupported information.", "main_total_tokens": 2505, "main_prompt_tokens": 2470, "main_completion_tokens": 35, "judge_total_tokens": 483, "judge_prompt_tokens": 420, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The movie 'The Transporter Refueled' was released in the year 2015 [Doc 67500].", "latency_ms": 4331.749439239502, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer exactly. It does not introduce any contradictory or extraneous claims and appears well supported by the cited document, so it is both correct and faithful.", "main_total_tokens": 2424, "main_prompt_tokens": 2394, "main_completion_tokens": 30, "judge_total_tokens": 493, "judge_prompt_tokens": 408, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "None of the provided documents mention a romantic comedy where Abraham Lincoln comes back from the dead. [Doc 101219] to [Doc 102914] do not contain such information.", "latency_ms": 5739.625930786133, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no such movie description can be found, and the generated answer correctly states that none of the provided documents mention such a romantic comedy, aligning with the gold. It does not invent a movie or contradict the given evidence, so it is both correct with respect to the task and faithful to the documents.", "main_total_tokens": 2389, "main_prompt_tokens": 2343, "main_completion_tokens": 46, "judge_total_tokens": 522, "judge_prompt_tokens": 427, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The provided context does not contain information about a movie that alludes to a manuscript written by Ernest Hemingway. [Doc 102100 - Doc 97134]", "latency_ms": 5368.255138397217, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Havana Darkness'. The generated answer incorrectly claims the context lacks this information, so it is factually wrong. However, it does not invent any alternative movie title or contradictory detail beyond stating the absence of information, so it remains faithful to the (assumed) lack of visible context, even though the answer is incorrect relative to the gold.", "main_total_tokens": 2316, "main_prompt_tokens": 2274, "main_completion_tokens": 42, "judge_total_tokens": 524, "judge_prompt_tokens": 420, "judge_completion_tokens": 104}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show \"Doll & Em\" [Doc 122259].", "latency_ms": 4728.919982910156, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Doll & Em,\" which matches the gold answer exactly and stays faithful to the provided information.", "main_total_tokens": 2157, "main_prompt_tokens": 2123, "main_completion_tokens": 34, "judge_total_tokens": 477, "judge_prompt_tokens": 417, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The provided documents do not contain information about a character named Hallam or a movie where a character attempts to find the cause of his mother’s death in 2007. Therefore, based strictly on the given context, there is no answer to this question.", "latency_ms": 6509.859800338745, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question clearly refers to the 2007 film 'Hallam Foe'. The generated answer claims there is no answer based on the provided context, which is incorrect relative to the gold answer, so correctness is 0. However, the model explicitly conditions its response on the lack of information in the provided documents and does not fabricate an answer, so it is faithful to its stated constraints, giving a faithfulness score of 1.", "main_total_tokens": 2263, "main_prompt_tokens": 2209, "main_completion_tokens": 54, "judge_total_tokens": 568, "judge_prompt_tokens": 450, "judge_completion_tokens": 118}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The provided context does not contain information about an individual whose last name might initially suggest whether they are a friend or foe. Therefore, based strictly on the given documents, there is no answer to the question.", "latency_ms": 5128.722429275513, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'A foe'. The generated answer claims there is no answer, which contradicts the gold, so it is incorrect. However, it is faithful to the (apparently missing or unprovided) context since it does not invent details and explicitly states there is no information in the documents, so faithfulness is high.", "main_total_tokens": 2010, "main_prompt_tokens": 1968, "main_completion_tokens": 42, "judge_total_tokens": 520, "judge_prompt_tokens": 427, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "In the 2017 TV show Atypical, Sam is 18 years old [Doc 124583].", "latency_ms": 4421.985387802124, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, which matches the gold answer exactly. It is fully consistent with the provided information and does not introduce unsupported claims.", "main_total_tokens": 2433, "main_prompt_tokens": 2403, "main_completion_tokens": 30, "judge_total_tokens": 486, "judge_prompt_tokens": 410, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 4205.269575119019, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the 2017 TV show Atypical, Sam is on the autism spectrum, which matches the gold answer exactly. It is also faithful to the information provided without adding unsupported details.", "main_total_tokens": 2349, "main_prompt_tokens": 2320, "main_completion_tokens": 29, "judge_total_tokens": 485, "judge_prompt_tokens": 410, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "In the movie \"À corde tendue,\" which was released in 2013, Sandra was born in Chamonix [Doc 59659].", "latency_ms": 5315.07134437561, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Chamonix as the place where Sandra was born, matching the gold answer exactly. It is fully grounded in the provided information, with no additional unsupported claims beyond restating the question context.", "main_total_tokens": 2667, "main_prompt_tokens": 2630, "main_completion_tokens": 37, "judge_total_tokens": 501, "judge_prompt_tokens": 427, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "in this japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 4509.302139282227, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly fills in the blank with 'rains,' matching the gold answer exactly and preserving the original meaning.", "main_total_tokens": 3053, "main_prompt_tokens": 3027, "main_completion_tokens": 26, "judge_total_tokens": 467, "judge_prompt_tokens": 412, "judge_completion_tokens": 55}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 4191.965103149414, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which matches the gold answer. It remains faithful to the given information and only adds a citation marker without altering the fact.", "main_total_tokens": 2600, "main_prompt_tokens": 2573, "main_completion_tokens": 27, "judge_total_tokens": 488, "judge_prompt_tokens": 416, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is titled \"One Afternoon\" [Doc 96838].", "latency_ms": 5842.3779010772705, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'One Afternoon' and does not introduce any unsupported information beyond a document citation placeholder. Both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2000, "main_prompt_tokens": 1949, "main_completion_tokens": 51, "judge_total_tokens": 514, "judge_prompt_tokens": 449, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The provided documents do not contain information about a low-rated Italian language movie where a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her. Therefore, based on the given context, the answer cannot be found.", "latency_ms": 3783.1149101257324, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "recall_at_10": 0.0, "precision_at_5": 0.0, "precision_at_10": 0.0, "ndcg_at_5": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_5": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'The Life Ahead'. The generated answer says the answer cannot be found, which is incorrect; the correct movie is identifiable as 'The Life Ahead'. However, the generated answer is faithful to the (stated) limitation of the provided documents and does not fabricate an answer, so it is faithful but factually incorrect.", "main_total_tokens": 2433, "main_prompt_tokens": 2384, "main_completion_tokens": 49, "judge_total_tokens": 544, "judge_prompt_tokens": 445, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary “McLaren” is about Bruce McLaren, a New Zealander who founded the McLaren Motor Racing team. [Doc 73859]", "latency_ms": 5340.687990188599, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and adds accurate contextual information about him founding the McLaren racing team. This is consistent with the gold answer and does not contradict it.", "main_total_tokens": 2474, "main_prompt_tokens": 2434, "main_completion_tokens": 40, "judge_total_tokens": 494, "judge_prompt_tokens": 419, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "Based on the provided context, there is no specific mention of a racing team name for any individual. However, one relevant piece of information is from [Doc 25682], which mentions the \"Kengai Racing Team\" that seeks to assemble the greatest racers in the Gunma area for a tour of Japan's mountain passes. But this does not specify it as the racing team of an individual. Therefore, the name of his racing team is not found in the given documents.", "latency_ms": 10743.261814117432, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is explicitly given in the gold as 'The McLaren Motor Racing team', so claiming that the name is not found in the documents is factually incorrect. Additionally, the generated answer introduces 'Kengai Racing Team' from a cited document instead of following the specified gold answer, so it is not faithful to the reference.", "main_total_tokens": 2203, "main_prompt_tokens": 2102, "main_completion_tokens": 101, "judge_total_tokens": 578, "judge_prompt_tokens": 479, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The provided context does not specify a country for \"he\" in general or any specific individual referred to as \"he.\" Therefore, based strictly on the given information, there is no answer to the question \"What country is he from?\" [Doc 5569]-[Doc 76556]", "latency_ms": 7207.349061965942, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer explicitly states that the person is from New Zealand, so the correct answer is available. The generated response incorrectly claims there is no answer, making it factually incorrect. However, the model is faithful to its stated reasoning and to the (assumed) lack of context it references, so it is internally consistent and faithful to its own premise.", "main_total_tokens": 2076, "main_prompt_tokens": 2011, "main_completion_tokens": 65, "judge_total_tokens": 537, "judge_prompt_tokens": 435, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based on the provided context, there is no 2013 movie where John McClane blows up the great wall of China. The context does not contain any information about such an event or movie. [Doc 72056] mentions \"The Great Wall,\" but it is from 2016 and does not involve John McClane.", "latency_ms": 8089.003562927246, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question is unanswerable in the sense that no such movie exists, and the gold answer correctly states this. However, the generated answer adds an incorrect and unnecessary reference to a specific document ([Doc 72056]) and a 2016 film 'The Great Wall' that is not in the provided gold answer. This hallucinated citation makes the response factually incorrect relative to the given evaluation context, even though it reaches the same overall conclusion that no such 2013 movie exists. The answer remains faithful to the prompt and reasoning structure, but its correctness is compromised by the invented document reference and details.", "main_total_tokens": 2144, "main_prompt_tokens": 2070, "main_completion_tokens": 74, "judge_total_tokens": 619, "judge_prompt_tokens": 466, "judge_completion_tokens": 153}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie that matches the given description is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 4656.234264373779, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Tomie: Forbidden Fruit,' which matches the gold answer exactly, and it remains faithful to the provided information without adding incorrect details.", "main_total_tokens": 2397, "main_prompt_tokens": 2364, "main_completion_tokens": 33, "judge_total_tokens": 510, "judge_prompt_tokens": 444, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 3982.697010040283, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years) and is directly supported by the cited document, so it is both correct and faithful.", "main_total_tokens": 2681, "main_prompt_tokens": 2658, "main_completion_tokens": 23, "judge_total_tokens": 472, "judge_prompt_tokens": 412, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The provided context does not contain any information about \"Scorpion King: Book of Souls (2018)\" or what The Scorpion King and Tala search for. Therefore, based strictly on the given context, I cannot answer the question. [No relevant document]", "latency_ms": 6500.596761703491, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks about specific plot information from the film, but the generated answer explicitly states that the context does not contain this information and refuses to guess. This is correct and faithful to the provided context, even though it does not match the gold answer. The system correctly avoids hallucinating and bases its response strictly on the given context, so both correctness and faithfulness are maximal under the constraints.", "main_total_tokens": 2153, "main_prompt_tokens": 2097, "main_completion_tokens": 56, "judge_total_tokens": 555, "judge_prompt_tokens": 446, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"Zombies on the Moon\". Therefore, the release year for \"Zombies on the Moon\" cannot be determined from the given context.", "latency_ms": 5319.256782531738, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year cannot be provided / is not known from the available information. This is factually correct given the context and aligns fully with the gold response in substance, so the answer is both correct and faithful.", "main_total_tokens": 2488, "main_prompt_tokens": 2448, "main_completion_tokens": 40, "judge_total_tokens": 511, "judge_prompt_tokens": 430, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain any information about a release year for \"The Galactic baker\". [Doc Not Found]", "latency_ms": 4128.568649291992, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is not in the database, and the generated answer correctly states that the documents do not contain information about the release year, which is equivalent in meaning. It does not hallucinate a year and stays faithful to the lack of information in the context.", "main_total_tokens": 2708, "main_prompt_tokens": 2684, "main_completion_tokens": 24, "judge_total_tokens": 498, "judge_prompt_tokens": 412, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided context does not contain any information regarding the release year of \"Return of the Null Pointer\". [Doc X]", "latency_ms": 3834.287405014038, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year is not available in the provided context, which matches the gold answer in substance (no information about the movie). It is both factually correct, given the context, and faithful to the context by not hallucinating a release year.", "main_total_tokens": 2263, "main_prompt_tokens": 2238, "main_completion_tokens": 25, "judge_total_tokens": 501, "judge_prompt_tokens": 415, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about the release year of \"Data Science: The Movie\". [Doc X]", "latency_ms": 3689.563274383545, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the release year is not available, and the generated answer accurately reflects the absence of information in the provided documents without adding unsupported details.", "main_total_tokens": 2224, "main_prompt_tokens": 2200, "main_completion_tokens": 24, "judge_total_tokens": 480, "judge_prompt_tokens": 414, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided context does not contain any information about a movie titled \"Python's Revenge 2\". Therefore, the release year for this specific title cannot be determined from the given documents.", "latency_ms": 4913.6199951171875, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there is no information about a movie titled 'Python's Revenge 2'. The generated answer does not hallucinate a release year and is fully consistent with the provided context and the gold response.", "main_total_tokens": 2271, "main_prompt_tokens": 2234, "main_completion_tokens": 37, "judge_total_tokens": 505, "judge_prompt_tokens": 427, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided context does not contain any information about the release year of \"The Last Pixel\". [Doc X]", "latency_ms": 3418.165683746338, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not provide the release year, which is consistent with the gold answer indicating the movie is not in the database. It does not invent a date and remains faithful to the lack of information in the context.", "main_total_tokens": 1991, "main_prompt_tokens": 1968, "main_completion_tokens": 23, "judge_total_tokens": 491, "judge_prompt_tokens": 411, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"The Man Who Ate The Sun.\" Therefore, based solely on the given context, the release year of \"The Man Who Ate The Sun\" cannot be determined.", "latency_ms": 5745.871067047119, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year cannot be determined from the available information. The generated answer correctly notes that the context does not contain the movie, matching the gold response in substance. It is factually correct (given the constraints) and fully faithful to the provided context.", "main_total_tokens": 2204, "main_prompt_tokens": 2157, "main_completion_tokens": 47, "judge_total_tokens": 525, "judge_prompt_tokens": 436, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not specify who directed the movie \"The Last Days of Lehman Brothers\". [Doc 46069]", "latency_ms": 4283.878564834595, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the director information is not available in the provided documents/database. The generated answer matches the constraint of only using the given documents and does not hallucinate a director name, so it is both correct and faithful.", "main_total_tokens": 2358, "main_prompt_tokens": 2329, "main_completion_tokens": 29, "judge_total_tokens": 503, "judge_prompt_tokens": 422, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about a movie titled \"A Love to Die For\" or its director. [Doc X]", "latency_ms": 4141.634941101074, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information (or even the movie itself) is not available in the provided documents/database. The generated answer does not hallucinate a director and is consistent with the evidence. Therefore it is both correct and faithful.", "main_total_tokens": 2472, "main_prompt_tokens": 2445, "main_completion_tokens": 27, "judge_total_tokens": 503, "judge_prompt_tokens": 421, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not contain information about who directed the movie \"Journey into Gaza\". [Doc 111360]", "latency_ms": 4618.791341781616, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not contain information about who directed the movie, which matches the gold answer indicating the database does not provide the director information. It is both correct and faithful to the provided context.", "main_total_tokens": 2666, "main_prompt_tokens": 2637, "main_completion_tokens": 29, "judge_total_tokens": 491, "judge_prompt_tokens": 416, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about who directed the movie \"The Last Night at Tremore Beach\". [Doc 135098] mentions the title and some details about it being a TV type production from 2024, but does not specify the director.", "latency_ms": 6668.423891067505, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not provide the director information for 'The Last Night at Tremore Beach' and references the mentioned document. This matches the gold answer, which also says the director information is not available in the database. The response is faithful to the given information and does not introduce unsupported claims.", "main_total_tokens": 2048, "main_prompt_tokens": 1989, "main_completion_tokens": 59, "judge_total_tokens": 544, "judge_prompt_tokens": 449, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 4692.762136459351, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as 'Rise of the Mummy' with no additional conflicting information. It is both correct and faithful to the provided reference.", "main_total_tokens": 2151, "main_prompt_tokens": 2117, "main_completion_tokens": 34, "judge_total_tokens": 486, "judge_prompt_tokens": 418, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The scenario where Sarah fakes her own death to escape her abusive spouse occurs in the movie \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 4898.022890090942, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Stranger in My Bed.' It does not introduce any contradictions or hallucinations beyond referencing a document ID, which supports faithfulness to the provided information.", "main_total_tokens": 2153, "main_prompt_tokens": 2118, "main_completion_tokens": 35, "judge_total_tokens": 503, "judge_prompt_tokens": 427, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie in which a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years is \"Game of Silence\" [Doc 123938].", "latency_ms": 4790.834665298462, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Game of Silence') and does not introduce any contradictory or extraneous information, so it is both correct and faithful.", "main_total_tokens": 1892, "main_prompt_tokens": 1853, "main_completion_tokens": 39, "judge_total_tokens": 493, "judge_prompt_tokens": 430, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie where the plot is based on a true story about a killer from the site Craigslist is titled \"The Craigslist Killer\" [Doc 53039]. The film, released in 2011, tells the story of Philip Markoff, a medical student at Boston University who leads a double life as a sexual deviant who abuses prostitutes he finds via Craigslist.", "latency_ms": 8756.24394416809, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer,' matching the gold answer exactly. The additional details about the film being based on the true story of Philip Markoff and its 2011 release are accurate and consistent with the prompt, indicating the model is faithful to the described content and not contradicting any provided information.", "main_total_tokens": 2213, "main_prompt_tokens": 2135, "main_completion_tokens": 78, "judge_total_tokens": 561, "judge_prompt_tokens": 464, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which Yuki Hirano sets his eyes on the role of a forestry trainee after failing his university entrance exam is \"Wood Job!\" [Doc 66094].", "latency_ms": 5727.936506271362, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\" which matches the gold answer exactly. It remains faithful to the information in the prompt and only adds a minor citation-style note that does not contradict anything. Therefore, both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3047, "main_prompt_tokens": 3007, "main_completion_tokens": 40, "judge_total_tokens": 520, "judge_prompt_tokens": 435, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure is \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 6634.198427200317, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Sesame Street: The Magical Wand Chase' and accurately restates the query. No additional incorrect information is introduced, so it is both correct and faithful.", "main_total_tokens": 2576, "main_prompt_tokens": 2522, "main_completion_tokens": 54, "judge_total_tokens": 530, "judge_prompt_tokens": 459, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a 7-year-old boy must find the hero within him to save his parents who are trapped inside a burning car due to a rock fall is titled \"Beyond\" [Doc 74597].", "latency_ms": 5539.008617401123, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Beyond,\" which matches the gold answer. It faithfully restates the prompt scenario and adds only a citation-style note, without introducing contradictions or unsupported claims.", "main_total_tokens": 2074, "main_prompt_tokens": 2027, "main_completion_tokens": 47, "judge_total_tokens": 521, "judge_prompt_tokens": 451, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman, fired from a financial corporation during the Asia crisis, returns home with no money is \"6ixtynin9\" [Doc 22478].", "latency_ms": 5071.222543716431, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', which matches the gold answer exactly. It does not introduce any contradictions or hallucinations beyond the provided information and is fully faithful to the question and gold reference.", "main_total_tokens": 2102, "main_prompt_tokens": 2061, "main_completion_tokens": 41, "judge_total_tokens": 517, "judge_prompt_tokens": 439, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie where a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- and it's up to the club's staff to stop the violence ... or at least hide it, is titled \"Club Dread\" [Doc 32410].", "latency_ms": 7542.014598846436, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread', matching the gold answer exactly. It remains faithful to the provided information and does not introduce unsupported claims, only rephrasing and confirming the title.", "main_total_tokens": 2430, "main_prompt_tokens": 2365, "main_completion_tokens": 65, "judge_total_tokens": 558, "judge_prompt_tokens": 485, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie where a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is titled \"Toilet\" [Doc 48873].", "latency_ms": 6895.083427429199, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot, with the gold answer given as 'Toilet'. The generated answer explicitly identifies the movie as 'Toilet', matching the gold exactly. There are no contradictions or unsupported claims; referencing a document ID does not affect factuality. Thus, the response is both correct and faithful.", "main_total_tokens": 2614, "main_prompt_tokens": 2558, "main_completion_tokens": 56, "judge_total_tokens": 561, "judge_prompt_tokens": 465, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century is \"Electric Angel\" [Doc 1024].", "latency_ms": 6118.603706359863, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Electric Angel,' which matches the gold answer. It is faithful to the given information and does not introduce unsupported claims.", "main_total_tokens": 2439, "main_prompt_tokens": 2389, "main_completion_tokens": 50, "judge_total_tokens": 524, "judge_prompt_tokens": 461, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 based on the provided context is \"Fall\" with a rating of 4.0 [Doc 106810].", "latency_ms": 4063.5592937469482, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'Mega Ape', but the generated answer claims it is 'Fall', which is incorrect. Additionally, the answer fabricates context about a document and rating not present in the gold information, so it is not faithful to the provided reference.", "main_total_tokens": 2359, "main_prompt_tokens": 2321, "main_completion_tokens": 38, "judge_total_tokens": 494, "judge_prompt_tokens": 412, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 1250.6065368652344, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Tea House', but the generated answer says 'I don't know', which is incorrect. However, the model did not hallucinate or fabricate an alternative answer; it admitted not knowing, so it is fully faithful to its own (lack of) knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 472, "judge_prompt_tokens": 386, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 517.4920558929443, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific movie title; the gold answer provides it, but the generated answer says 'I don't know,' which is incorrect. However, the generated answer does not hallucinate or contradict given information; it simply expresses a lack of knowledge, so it is fully faithful to the model’s uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 488, "judge_prompt_tokens": 395, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 in the provided context is \"Cataclysm\" with a rating of 3.889, and its language is English [Doc 744].", "latency_ms": 5972.766160964966, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The answer correctly identifies the language as English, which matches the gold answer. It also remains faithful to the provided context, citing the specific movie and document reference without adding unsupported claims.", "main_total_tokens": 2546, "main_prompt_tokens": 2501, "main_completion_tokens": 45, "judge_total_tokens": 488, "judge_prompt_tokens": 421, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "Based on the provided context, the TV show in 1999 with the highest popularity was \"The Amanda Show\" with a popularity score of 3.4197 [Doc 115219].", "latency_ms": 6017.672061920166, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect: it identifies 'The Amanda Show' as the most popular TV show in 1999, while the gold answer specifies 'Law & Order: Special Victims Unit'. Additionally, the answer cites a specific document and popularity score that are not present in the given prompt, indicating it is not faithful to the provided information and instead fabricates supporting context.", "main_total_tokens": 2581, "main_prompt_tokens": 2533, "main_completion_tokens": 48, "judge_total_tokens": 535, "judge_prompt_tokens": 429, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 3670.9017753601074, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (7.2) and correctly attributes the value to the movie \"Bluefin\". It is consistent with the provided evidence, so it is both correct and faithful.", "main_total_tokens": 2412, "main_prompt_tokens": 2390, "main_completion_tokens": 22, "judge_total_tokens": 473, "judge_prompt_tokens": 400, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in two different years according to the provided documents: 1999 [Doc 22481] and 2019 [Doc 88811].", "latency_ms": 6078.308582305908, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the movie was released in 2019, but the generated answer claims there are two release years, 1999 and 2019, without resolving which is correct. This contradicts the gold answer and introduces an extra, incorrect year, so it is not correct. It is also not faithful to the gold, which specifies only 2019 as the release year.", "main_total_tokens": 2385, "main_prompt_tokens": 2335, "main_completion_tokens": 50, "judge_total_tokens": 533, "judge_prompt_tokens": 423, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "1", "latency_ms": 30.927181243896484, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer ('1') does not match the gold answer (279), so it is incorrect. However, it directly answers the question without adding unsupported details, so it is faithful to the prompt.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 454, "judge_prompt_tokens": 383, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "7", "latency_ms": 30.94935417175293, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates the correct number of votes is 12, but the generated answer says 7, which is factually incorrect. The evaluation correctly identifies the mismatch between gold and generated answers, so it is faithful to the comparison task, but the generated answer itself is not correct.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 471, "judge_prompt_tokens": 383, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The provided context does not contain information about the release year of the movie \"The Last Days of Lehman Brothers\". [Doc X]", "latency_ms": 4259.745121002197, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies 2009 as the release year, but the generated answer claims the information is not in the context and does not provide the correct year. As a result, it is incorrect with respect to the question. However, it is faithful to the given context because, according to the model, the context indeed did not mention the release year, so the model accurately stated its lack of information.", "main_total_tokens": 2232, "main_prompt_tokens": 2204, "main_completion_tokens": 28, "judge_total_tokens": 526, "judge_prompt_tokens": 414, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 3863.4679317474365, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'en' (English), and the generated answer states that the original language is English, which is correct. The answer directly reflects the gold information and does not introduce any contradictions or unsupported details, so it is both correct and faithful.", "main_total_tokens": 2420, "main_prompt_tokens": 2396, "main_completion_tokens": 24, "judge_total_tokens": 484, "judge_prompt_tokens": 402, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "I don't know.", "latency_ms": 809.1592788696289, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model explicitly answered that it did not know, which is faithful to its uncertainty but does not provide the correct year (1987).", "main_total_tokens": 584, "main_prompt_tokens": 582, "main_completion_tokens": 2, "judge_total_tokens": 440, "judge_prompt_tokens": 382, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Tokyo Gore School", "latency_ms": 3792.649030685425, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the movie where classmates have to play a survival game using their cell phones is 'Brain Fluid Explosion Girl,' not 'Tokyo Gore School.' However, it is faithful to the question asked; it attempts to answer directly without adding unrelated information.", "main_total_tokens": 2773, "main_prompt_tokens": 2752, "main_completion_tokens": 21, "judge_total_tokens": 475, "judge_prompt_tokens": 391, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 3906.3334465026855, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the TV show as 'Dungeons & Dragons.'", "main_total_tokens": 2466, "main_prompt_tokens": 2444, "main_completion_tokens": 22, "judge_total_tokens": 446, "judge_prompt_tokens": 394, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "I don't know.", "latency_ms": 648.9143371582031, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which is incorrect because the correct answer is 'Nacht der Wölfe.' However, the model does not fabricate an incorrect movie title and explicitly expresses uncertainty, so it is fully faithful to its (lack of) knowledge.", "main_total_tokens": 579, "main_prompt_tokens": 577, "main_completion_tokens": 2, "judge_total_tokens": 487, "judge_prompt_tokens": 402, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900 [Doc 126191]", "latency_ms": 3635.2617740631104, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (1900) and is consistent with the provided information, so it is both correct and faithful.", "main_total_tokens": 2753, "main_prompt_tokens": 2736, "main_completion_tokens": 17, "judge_total_tokens": 453, "judge_prompt_tokens": 394, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "Playing Mona Lisa", "latency_ms": 623.5849857330322, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer 'Playing Mona Lisa' is incorrect; the correct movie is 'June.' The evaluation correctly identifies the mismatch between the gold answer and the generated answer, so the assessment of incorrectness is faithful to the provided data.", "main_total_tokens": 169, "main_prompt_tokens": 165, "main_completion_tokens": 4, "judge_total_tokens": 512, "judge_prompt_tokens": 435, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 93.75524520874023, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The answer is incorrect because the assistant claimed not to know, while there is a specific correct movie title provided in the gold answer. However, the assistant did not fabricate any information and acknowledged not knowing, so the response is faithful to its own (stated) uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 484, "judge_prompt_tokens": 399, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 109.38096046447754, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric value (the highest Russian-language movie rating in 1999, which is 7.6). The generated answer says 'I don't know,' which is not the correct value, so correctness is 0. However, the model is honest about not knowing and does not hallucinate an incorrect answer, so it is fully faithful to its own knowledge state, giving a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 507, "judge_prompt_tokens": 389, "judge_completion_tokens": 118}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "9", "latency_ms": 3618.772029876709, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3535124404757273, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (9) does not match the gold answer (4), so it is incorrect. However, the assistant directly answered the question without adding unsupported details, so it is faithful to the user query.", "main_total_tokens": 1583, "main_prompt_tokens": 1561, "main_completion_tokens": 22, "judge_total_tokens": 467, "judge_prompt_tokens": 394, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 109.38096046447754, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 2 such TV shows, but the generated answer claims there are none, which is factually incorrect given the gold label, so correctness is 0. The model does, however, stay within the constraints of its knowledge and does not fabricate specific titles or numbers; it simply states inability to find matches, so it is fully faithful to its own (albeit limited) reasoning, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 526, "judge_prompt_tokens": 404, "judge_completion_tokens": 122}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 140.63310623168945, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states 'I don't know,' which does not provide the correct response 'Fantômas'. However, it does not introduce any incorrect facts or hallucinations, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 389, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise", "latency_ms": 2607.637643814087, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Kill Cruise') and is fully supported by the question context, so it is both correct and faithful.", "main_total_tokens": 2705, "main_prompt_tokens": 2700, "main_completion_tokens": 5, "judge_total_tokens": 447, "judge_prompt_tokens": 387, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor? [Doc 84838]", "latency_ms": 3764.9383544921875, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, including punctuation, and the added bracketed text does not alter the correctness or contradict the given information.", "main_total_tokens": 2977, "main_prompt_tokens": 2958, "main_completion_tokens": 19, "judge_total_tokens": 474, "judge_prompt_tokens": 414, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018 [Doc 84838]", "latency_ms": 4921.869516372681, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2018) exactly and does not introduce unsupported information beyond a harmless document tag.", "main_total_tokens": 3137, "main_prompt_tokens": 3107, "main_completion_tokens": 30, "judge_total_tokens": 441, "judge_prompt_tokens": 386, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "I don't know.", "latency_ms": 5564.166069030762, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Astro Boy', but the generated response said 'I don't know', which is incorrect. However, stating uncertainty is faithful to the model's apparent knowledge and does not invent incorrect information, so faithfulness is high while correctness is low.", "main_total_tokens": 660, "main_prompt_tokens": 602, "main_completion_tokens": 58, "judge_total_tokens": 474, "judge_prompt_tokens": 391, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 years [Doc 67513]", "latency_ms": 2959.3758583068848, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly states that they were held captive for 7 years, matching the gold answer. However, it introduces an unsupported citation/reference tag ([Doc 67513]) not present in the given information, so it is not fully faithful to the provided context.", "main_total_tokens": 2435, "main_prompt_tokens": 2421, "main_completion_tokens": 14, "judge_total_tokens": 487, "judge_prompt_tokens": 403, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m [Doc 133469]", "latency_ms": 3051.729679107666, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the given gold answer exactly (£26m) and does not introduce any unsupported information. It is both correct and faithful to the provided gold reference.", "main_total_tokens": 2341, "main_prompt_tokens": 2326, "main_completion_tokens": 15, "judge_total_tokens": 468, "judge_prompt_tokens": 404, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 390.64693450927734, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score (80.5772). The generated answer says, 'I don't know,' which does not provide the requested value and is thus incorrect. However, it does not invent any information and accurately reflects uncertainty, so it is fully faithful to the model's knowledge limits.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 489, "judge_prompt_tokens": 396, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 109.38119888305664, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 Russian language TV show released in 1997, but the generated answer claims there were none (or that it cannot find any), which directly contradicts the gold. This makes the generated answer incorrect. However, given the model’s typical limitations, saying it cannot find any is internally consistent and not hallucinated relative to its own stated lack of information, so it is faithful to its own reasoning process.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 513, "judge_prompt_tokens": 394, "judge_completion_tokens": 119}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes", "latency_ms": 2987.807273864746, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Angel Eyes') and correctly identifies the TV show where Soo-wan undergoes eye transplant surgery that restores her sight. It is both correct and fully faithful to the reference.", "main_total_tokens": 3353, "main_prompt_tokens": 3348, "main_completion_tokens": 5, "judge_total_tokens": 471, "judge_prompt_tokens": 397, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "en", "latency_ms": 3459.08522605896, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer 'en' is incorrect because the gold label indicates the language is Korean, not English. The evaluation itself is faithful to the provided gold and generated answers.", "main_total_tokens": 3152, "main_prompt_tokens": 3139, "main_completion_tokens": 13, "judge_total_tokens": 442, "judge_prompt_tokens": 377, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 421.06080055236816, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that there are 2 such movies, while the generated answer claims there are none. This is factually incorrect relative to the gold, so correctness is 0. However, the model does not fabricate a specific number or movies; it simply states inability to find matches, which is internally faithful to its own response style, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 509, "judge_prompt_tokens": 403, "judge_completion_tokens": 106}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 773.0748653411865, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.5, "faithfulness_score": 1, "justification": "Given the gold answer, the system should state that there are no movies from 1979 in its database. Saying 'I don't know' is partially aligned (it admits ignorance) but does not communicate the specific database limitation, so it is only partially correct relative to the gold. It is fully faithful in that it does not introduce any conflicting information or hallucinate an answer.", "main_total_tokens": 583, "main_prompt_tokens": 581, "main_completion_tokens": 2, "judge_total_tokens": 507, "judge_prompt_tokens": 399, "judge_completion_tokens": 108}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "I don't know.", "latency_ms": 6533.007621765137, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly indicate lack of knowledge / non-existence of the movie, which is appropriate because there is no official Back to the Future Part IV. The generated answer is fully consistent with the gold’s intent of denying a specific year.", "main_total_tokens": 692, "main_prompt_tokens": 623, "main_completion_tokens": 69, "judge_total_tokens": 486, "judge_prompt_tokens": 404, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no movies from 1980 related to the Chernobyl disaster could be found. The generated answer similarly says it cannot find any movies or shows matching the criteria and suggests checking the event timing, which is consistent since the Chernobyl disaster happened in 1986. Both answers correctly convey that such a movie does not exist (as far as can be found) and do not introduce unsupported facts. Therefore the generated response is both correct with respect to the question and faithful to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 568, "judge_prompt_tokens": 434, "judge_completion_tokens": 134}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 78.12929153442383, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because there are four titles with 'Astro Boy' in the title, contrary to the claim that there are none. However, it is faithful to the (incorrect) reasoning it presents, as it does not introduce fabricated details beyond stating it cannot find matches.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 397, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 3256.236791610718, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies there are 2 such movies, while the generated answer claims there are none, so it is factually incorrect relative to the gold. However, the model honestly reflects its own uncertainty and limitations in accessing specific data, so it is faithful to its own reasoning process.", "main_total_tokens": 3625, "main_prompt_tokens": 3623, "main_completion_tokens": 2, "judge_total_tokens": 483, "judge_prompt_tokens": 395, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 78.12929153442383, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1, but the generated answer says there are none, so it is factually incorrect. However, the model does not fabricate or contradict the provided information; it simply states inability to find such shows, making it faithful to its (simulated) knowledge limitations.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 486, "judge_prompt_tokens": 398, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 62.525272369384766, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that 0 TV shows were released between 2000 and 2010 about Super Mario Bros. The generated answer says it cannot find any movies or shows matching those criteria, which is equivalent to asserting there are none. This matches the gold answer, so it is correct. The response is faithful to the question and does not add unsupported details.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 504, "judge_prompt_tokens": 400, "judge_completion_tokens": 104}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 78.47332954406738, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto'. The generated answer says 'I don't know', which is incorrect because a correct, specific answer exists. However, the model is faithful to its own stated lack of knowledge and does not hallucinate an alternative, so faithfulness is high even though correctness is zero.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 477, "judge_prompt_tokens": 386, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 930.354118347168, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that the movie was not released, while the generated answer said 'I don't know,' which is incorrect relative to the gold. However, the model did not fabricate any incorrect release date or details and honestly expressed uncertainty, so it is fully faithful to its own (lack of) knowledge.", "main_total_tokens": 569, "main_prompt_tokens": 567, "main_completion_tokens": 2, "judge_total_tokens": 481, "judge_prompt_tokens": 389, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 4876.943588256836, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the movie was not released, implying an explicit assertion about the status of such a movie. The generated answer says only 'I don't know,' which does not assert that the movie was or was not released. This makes it faithful (it does not contradict the gold) but not correct, as it fails to provide the specific information requested and present in the gold answer.", "main_total_tokens": 623, "main_prompt_tokens": 572, "main_completion_tokens": 51, "judge_total_tokens": 495, "judge_prompt_tokens": 386, "judge_completion_tokens": 109}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The Empire Strikes Back [Doc 3986] is the 4th oldest Star Wars movie mentioned in the provided documents. However, since the question asks for the title and not a full description, just:\n\nThe Empire Strikes Back", "latency_ms": 13087.103366851807, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'Star Wars: Droids - The Great Heep' as the 4th oldest Star Wars movie, while the generated answer gives 'The Empire Strikes Back'. This is factually incorrect relative to the labeled gold. Additionally, the generated answer references 'provided documents' not shown in the prompt and asserts a ranking based on them, which cannot be verified here, so it is not faithful to the given information.", "main_total_tokens": 3255, "main_prompt_tokens": 3131, "main_completion_tokens": 124, "judge_total_tokens": 556, "judge_prompt_tokens": 437, "judge_completion_tokens": 119}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "1980", "latency_ms": 3768.0141925811768, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer ('1980') is incorrect; the correct year is 1986. However, the model directly answered the question without adding unsupported details, so it is fully faithful to the (incorrect) prediction it made.", "main_total_tokens": 3040, "main_prompt_tokens": 3023, "main_completion_tokens": 17, "judge_total_tokens": 456, "judge_prompt_tokens": 380, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 62.503814697265625, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific rating (8.466), whereas the generated answer says 'I don't know.' This does not match the correct rating, so correctness is 0. However, the model honestly acknowledges not knowing rather than fabricating an answer, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 398, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med [Doc 121125]", "latency_ms": 3452.8968334198, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Mighty Med'. It is fully correct and faithful to the given gold response.", "main_total_tokens": 2450, "main_prompt_tokens": 2433, "main_completion_tokens": 17, "judge_total_tokens": 463, "judge_prompt_tokens": 400, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina [Doc 53582]", "latency_ms": 3284.2512130737305, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (Argentina) exactly and remains faithful to the provided information.", "main_total_tokens": 2915, "main_prompt_tokens": 2902, "main_completion_tokens": 13, "judge_total_tokens": 448, "judge_prompt_tokens": 398, "judge_completion_tokens": 50}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "I don't know.", "latency_ms": 749.6392726898193, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states the trap belongs to Jiang Jie, but the generated answer says 'I don't know,' which does not match the gold and is therefore incorrect. However, it is faithful to the model's claimed uncertainty and does not hallucinate an incorrect name.", "main_total_tokens": 606, "main_prompt_tokens": 604, "main_completion_tokens": 2, "judge_total_tokens": 471, "judge_prompt_tokens": 388, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "3", "latency_ms": 3676.5079498291016, "retrieval_recall": 0.0, "retrieval_mrr": 0.25, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.16812753627111746, "hit_rate_at_5": 1.0, "recall_at_10": 0.25, "precision_at_10": 0.1, "ndcg_at_10": 0.16812753627111746, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the number of Korean-language movies mentioning assassins after 2023. The gold answer is 4 while the generated answer is 3, which is numerically incorrect. However, the generated answer is directly responsive to the question and does not introduce unrelated information, so it is faithful but incorrect.", "main_total_tokens": 3762, "main_prompt_tokens": 3753, "main_completion_tokens": 9, "judge_total_tokens": 479, "judge_prompt_tokens": 385, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 1862.14280128479, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that no movies or shows can be found that match the query criteria, which is consistent with the gold answer indicating there are no movies with both Bruce Lee and Steven Seagal. It is faithful to the gold response, differing only in minor wording while preserving the same meaning and conclusion.", "main_total_tokens": 2417, "main_prompt_tokens": 2415, "main_completion_tokens": 2, "judge_total_tokens": 498, "judge_prompt_tokens": 406, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3 [Doc 42750]", "latency_ms": 2825.087070465088, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (6.3) and does not introduce any unsupported information. It is both correct and faithful.", "main_total_tokens": 2226, "main_prompt_tokens": 2213, "main_completion_tokens": 13, "judge_total_tokens": 452, "judge_prompt_tokens": 393, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "Drift racing [Doc 42929]\nDrift racing [Doc 43350]\nDrift racing [Doc 33742]\nDrift racing [Doc 36394]\nDrift-style race [Doc 42750]", "latency_ms": 18815.70315361023, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold label is 'Drift'. The generated answers specify 'Drift racing' or 'Drift-style race', which are equivalent descriptions of the same racing style, so the answer is correct. The model clearly bases its response on the retrieved documents, so it is faithful as well.", "main_total_tokens": 3337, "main_prompt_tokens": 3152, "main_completion_tokens": 185, "judge_total_tokens": 511, "judge_prompt_tokens": 423, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer [Doc 131298]", "latency_ms": 3766.261577606201, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, 'Confronting a Serial Killer'. The added bracketed doc identifier does not alter the correctness or contradict any information, so the response is correct and faithful.", "main_total_tokens": 2865, "main_prompt_tokens": 2846, "main_completion_tokens": 19, "judge_total_tokens": 490, "judge_prompt_tokens": 418, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838 [Doc 20603]", "latency_ms": 3124.722480773926, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score value of 7.838, and it does not introduce any unsupported information beyond what is in the gold answer. Therefore it is both correct and faithful.", "main_total_tokens": 2548, "main_prompt_tokens": 2533, "main_completion_tokens": 15, "judge_total_tokens": 461, "judge_prompt_tokens": 395, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense", "latency_ms": 2688.2050037384033, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 1999 psychological thriller as 'The Sixth Sense' and stays fully faithful to the provided information.", "main_total_tokens": 2886, "main_prompt_tokens": 2880, "main_completion_tokens": 6, "judge_total_tokens": 457, "judge_prompt_tokens": 394, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002 [Doc 27318]", "latency_ms": 3524.996280670166, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer gives the correct release year, 2002, matching the gold answer exactly. It does not introduce any contradictions or unsupported details and remains faithful to the information provided.", "main_total_tokens": 3017, "main_prompt_tokens": 3001, "main_completion_tokens": 16, "judge_total_tokens": 456, "judge_prompt_tokens": 389, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled", "latency_ms": 3069.979429244995, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct title of the 2015 reboot of 'The Transporter'. It is both correct and faithful to the given gold response.", "main_total_tokens": 3096, "main_prompt_tokens": 3087, "main_completion_tokens": 9, "judge_total_tokens": 466, "judge_prompt_tokens": 397, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015 [Doc 67500]", "latency_ms": 3651.3946056365967, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer, and it does not introduce any contradictory or extraneous information.", "main_total_tokens": 3026, "main_prompt_tokens": 3010, "main_completion_tokens": 16, "judge_total_tokens": 462, "judge_prompt_tokens": 393, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "I don't know.", "latency_ms": 4706.780672073364, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no such movie description can be found, effectively expressing lack of knowledge. The generated answer, 'I don't know,' is consistent with that and thus correct and faithful to the gold.", "main_total_tokens": 644, "main_prompt_tokens": 596, "main_completion_tokens": 48, "judge_total_tokens": 461, "judge_prompt_tokens": 394, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "I don't know.", "latency_ms": 4367.023229598999, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Havana Darkness'. The generated answer 'I don't know.' is incorrect but does not introduce any fabricated information, so it is unfaithful to the gold answer but faithful in that it does not hallucinate content.", "main_total_tokens": 648, "main_prompt_tokens": 604, "main_completion_tokens": 44, "judge_total_tokens": 471, "judge_prompt_tokens": 392, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 4025.998592376709, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Doll & Em'. It is fully correct and faithful.", "main_total_tokens": 2790, "main_prompt_tokens": 2768, "main_completion_tokens": 22, "judge_total_tokens": 451, "judge_prompt_tokens": 392, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "I don't know.", "latency_ms": 4234.723806381226, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Hallam Foe'. The generated answer 'I don't know.' is incorrect. However, it does not introduce any fabricated or incorrect information beyond admitting ignorance, so it is fully faithful to the prompt and its own uncertainty.", "main_total_tokens": 647, "main_prompt_tokens": 603, "main_completion_tokens": 44, "judge_total_tokens": 483, "judge_prompt_tokens": 403, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "I don't know.", "latency_ms": 3239.095687866211, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a clear labeled answer: 'A foe'. The generated answer 'I don't know.' is therefore incorrect, so correctness is 0. However, the model is honest about not knowing and does not invent an answer, so faithfulness is 1.", "main_total_tokens": 780, "main_prompt_tokens": 751, "main_completion_tokens": 29, "judge_total_tokens": 474, "judge_prompt_tokens": 390, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18 [Doc 124583]", "latency_ms": 3516.887664794922, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states Sam's age as 18, matching the gold answer exactly. It is fully supported by the provided information and does not introduce unsupported details.", "main_total_tokens": 3032, "main_prompt_tokens": 3017, "main_completion_tokens": 15, "judge_total_tokens": 459, "judge_prompt_tokens": 395, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "autism spectrum [Doc 124583]", "latency_ms": 3567.1000480651855, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which matches the gold answer and is fully supported by the question context.", "main_total_tokens": 2960, "main_prompt_tokens": 2944, "main_completion_tokens": 16, "judge_total_tokens": 453, "judge_prompt_tokens": 395, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix [Doc 59659]", "latency_ms": 3749.4170665740967, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and does not introduce any extraneous or unsupported information.", "main_total_tokens": 3275, "main_prompt_tokens": 3260, "main_completion_tokens": 15, "judge_total_tokens": 456, "judge_prompt_tokens": 406, "judge_completion_tokens": 50}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481]", "latency_ms": 3949.1989612579346, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains'), and the explanation correctly reflects this match. Both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3691, "main_prompt_tokens": 3676, "main_completion_tokens": 15, "judge_total_tokens": 458, "judge_prompt_tokens": 398, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland", "latency_ms": 2937.469959259033, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Greenland') and correctly identifies the fictional place where Tabaluga is crown prince. It is both correct and faithful.", "main_total_tokens": 3179, "main_prompt_tokens": 3174, "main_completion_tokens": 5, "judge_total_tokens": 459, "judge_prompt_tokens": 395, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon [Doc 96838]", "latency_ms": 3236.6063594818115, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as 'One Afternoon'. No additional or conflicting information is introduced.", "main_total_tokens": 2557, "main_prompt_tokens": 2542, "main_completion_tokens": 15, "judge_total_tokens": 468, "judge_prompt_tokens": 412, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "I don't know.", "latency_ms": 5855.700969696045, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "recall_at_10": 0.0, "precision_at_5": 0.0, "precision_at_10": 0.0, "ndcg_at_5": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_5": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific movie title (“The Life Ahead”), but the generated answer is simply “I don't know,” which is incorrect. However, the model does not introduce any fabricated or conflicting information; it just expresses a lack of knowledge, so it is fully faithful to its own output.", "main_total_tokens": 723, "main_prompt_tokens": 617, "main_completion_tokens": 106, "judge_total_tokens": 492, "judge_prompt_tokens": 402, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "Bruce McLaren [Doc 73859]", "latency_ms": 3370.28169631958, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying Bruce McLaren as the subject of the 2016 documentary and does not introduce any contradictions or unsupported details.", "main_total_tokens": 3062, "main_prompt_tokens": 3048, "main_completion_tokens": 14, "judge_total_tokens": 457, "judge_prompt_tokens": 393, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren Motor Racing [Doc 73859]", "latency_ms": 4846.504688262939, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the team as 'McLaren Motor Racing' and matches the gold answer in substance. The bracketed document reference does not alter the factual content and remains faithful to the provided information.", "main_total_tokens": 3089, "main_prompt_tokens": 3060, "main_completion_tokens": 29, "judge_total_tokens": 465, "judge_prompt_tokens": 393, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "New Zealand [Doc 73859]", "latency_ms": 4352.442979812622, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the country as New Zealand, which matches the gold answer exactly. The added reference tag '[Doc 73859]' does not change the factual content and is consistent with the information provided, so the response is both correct and faithful to the source.", "main_total_tokens": 3104, "main_prompt_tokens": 3081, "main_completion_tokens": 23, "judge_total_tokens": 469, "judge_prompt_tokens": 384, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "I don't know.", "latency_ms": 5009.537935256958, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states there is no such movie in the database, and the generated answer responds with not knowing. Both effectively convey lack of knowledge rather than fabricating an incorrect movie, so the generated answer is correct relative to the gold and faithful to the query.", "main_total_tokens": 630, "main_prompt_tokens": 580, "main_completion_tokens": 50, "judge_total_tokens": 485, "judge_prompt_tokens": 402, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit [Doc 28589]", "latency_ms": 3619.5228099823, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, correctly identifying the 2002 Japanese horror movie as 'Tomie: Forbidden Fruit' and remaining fully faithful to the given information.", "main_total_tokens": 3056, "main_prompt_tokens": 3039, "main_completion_tokens": 17, "judge_total_tokens": 494, "judge_prompt_tokens": 428, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 [Doc 98566]", "latency_ms": 3693.4125423431396, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years) and directly uses the information provided in the prompt, so it is both correct and faithful.", "main_total_tokens": 3312, "main_prompt_tokens": 3298, "main_completion_tokens": 14, "judge_total_tokens": 463, "judge_prompt_tokens": 402, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "I don't know.", "latency_ms": 755.5022239685059, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that they search for the Book of Souls, but the generated answer says 'I don't know,' which is incorrect. However, it accurately reflects the model's lack of knowledge without inventing details, so it is faithful but not correct.", "main_total_tokens": 614, "main_prompt_tokens": 612, "main_completion_tokens": 2, "judge_total_tokens": 480, "judge_prompt_tokens": 398, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 579.5767307281494, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie that is not in the database. The gold answer appropriately states that the movie is not in the database. The generated answer, 'I don't know,' is effectively equivalent in this context: it does not hallucinate a year and correctly indicates a lack of knowledge. Therefore it is correct and fully faithful to the gold response.", "main_total_tokens": 590, "main_prompt_tokens": 588, "main_completion_tokens": 2, "judge_total_tokens": 495, "judge_prompt_tokens": 395, "judge_completion_tokens": 100}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 742.0928478240967, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly indicate a lack of knowledge about the movie, and the generated answer is fully consistent with the gold response.", "main_total_tokens": 605, "main_prompt_tokens": 603, "main_completion_tokens": 2, "judge_total_tokens": 452, "judge_prompt_tokens": 393, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 523.4229564666748, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is not in the database, meaning the correct behavior is to admit not knowing. The generated response, 'I don't know,' is effectively equivalent and thus correct. It is also perfectly faithful to the gold answer's intent and information content.", "main_total_tokens": 587, "main_prompt_tokens": 585, "main_completion_tokens": 2, "judge_total_tokens": 479, "judge_prompt_tokens": 395, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 524.8496532440186, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly admits not knowing, which aligns with the gold response that the movie is not in the database. It does not introduce any incorrect facts and is fully consistent with the information given.", "main_total_tokens": 587, "main_prompt_tokens": 585, "main_completion_tokens": 2, "judge_total_tokens": 465, "judge_prompt_tokens": 395, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 603.3115386962891, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly indicate lack of knowledge about the movie. The generated response is faithful to the information given and does not fabricate a release year.", "main_total_tokens": 597, "main_prompt_tokens": 595, "main_completion_tokens": 2, "judge_total_tokens": 458, "judge_prompt_tokens": 395, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 516.1874294281006, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer convey that the system does not have the information / does not know. This matches the gold intent, so the response is correct and faithful.", "main_total_tokens": 556, "main_prompt_tokens": 554, "main_completion_tokens": 2, "judge_total_tokens": 460, "judge_prompt_tokens": 393, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 611.9561195373535, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer correctly indicate a lack of knowledge about the movie. The generated response is fully consistent with the gold, so it is both correct and faithful.", "main_total_tokens": 599, "main_prompt_tokens": 597, "main_completion_tokens": 2, "judge_total_tokens": 463, "judge_prompt_tokens": 396, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Days of Lehman Brothers\" is not listed in the provided documents [Doc 46069].", "latency_ms": 4850.187063217163, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer state that the director information is not provided in the available sources/documents. The generated answer is consistent with the gold answer and does not introduce unsupported information, so it is both correct and faithful to the specified evidence.", "main_total_tokens": 2958, "main_prompt_tokens": 2925, "main_completion_tokens": 33, "judge_total_tokens": 506, "judge_prompt_tokens": 424, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 769.6776390075684, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the system cannot answer because the database lacks the director information. The generated answer says \"I don't know,\" which is effectively the same epistemic stance: it does not invent a director and does not contradict the gold. Therefore it is fully correct and fully faithful to the gold response.", "main_total_tokens": 596, "main_prompt_tokens": 594, "main_completion_tokens": 2, "judge_total_tokens": 485, "judge_prompt_tokens": 399, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not specify who directed the movie \"Journey into Gaza\". [Doc 111360]", "latency_ms": 4762.267589569092, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not specify who directed the movie, which aligns with the gold answer that the director information is not provided in the database. It does not invent any director name and remains faithful to the given information.", "main_total_tokens": 3254, "main_prompt_tokens": 3225, "main_completion_tokens": 29, "judge_total_tokens": 493, "judge_prompt_tokens": 414, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not contain information about who directed the movie \"The Last Night at Tremore Beach\". The document only provides details such as its type (TV), year (2024), rating (7.0), popularity (5.1693), language (es), and a brief plot description, but no director is mentioned [Doc 135098].", "latency_ms": 9271.625757217407, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not provide the director information and lists only the attributes actually present (type, year, rating, popularity, language, plot). It does not hallucinate a director or any unsupported details. This is consistent with the gold answer and faithful to the given document.", "main_total_tokens": 2658, "main_prompt_tokens": 2575, "main_completion_tokens": 83, "judge_total_tokens": 555, "judge_prompt_tokens": 469, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 2888.362407684326, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy' and faithfully adhering to the provided reference.", "main_total_tokens": 2764, "main_prompt_tokens": 2755, "main_completion_tokens": 9, "judge_total_tokens": 456, "judge_prompt_tokens": 396, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 4151.400327682495, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'Stranger in My Bed'. It is both correct and faithful.", "main_total_tokens": 2802, "main_prompt_tokens": 2779, "main_completion_tokens": 23, "judge_total_tokens": 459, "judge_prompt_tokens": 400, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 3665.602445602417, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question describes the plot of 'Game of Silence', which is actually a TV series but matches the given gold answer. The generated answer exactly matches the gold ('Game of Silence'), so it is correct and fully faithful to the provided reference.", "main_total_tokens": 2505, "main_prompt_tokens": 2485, "main_completion_tokens": 20, "judge_total_tokens": 479, "judge_prompt_tokens": 400, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 3769.9108123779297, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct movie title corresponding to the description. It is both correct and fully faithful to the gold reference.", "main_total_tokens": 2787, "main_prompt_tokens": 2768, "main_completion_tokens": 19, "judge_total_tokens": 458, "judge_prompt_tokens": 395, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 3107.419729232788, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Wood Job!') and correctly identifies the movie described. It is both correct and faithful to the reference.", "main_total_tokens": 3654, "main_prompt_tokens": 3648, "main_completion_tokens": 6, "judge_total_tokens": 464, "judge_prompt_tokens": 403, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 4244.628190994263, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase' and remains fully faithful to the given information.", "main_total_tokens": 3245, "main_prompt_tokens": 3223, "main_completion_tokens": 22, "judge_total_tokens": 481, "judge_prompt_tokens": 417, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond [Doc 74597]", "latency_ms": 3042.3176288604736, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Beyond') and does not add any unsupported details, so it is both correct and faithful.", "main_total_tokens": 2657, "main_prompt_tokens": 2644, "main_completion_tokens": 13, "judge_total_tokens": 473, "judge_prompt_tokens": 415, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9 [Doc 22478]", "latency_ms": 3680.968999862671, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the correct movie '6ixtynin9'. There are no additional incorrect details or hallucinations, so it is both correct and faithful.", "main_total_tokens": 2693, "main_prompt_tokens": 2675, "main_completion_tokens": 18, "judge_total_tokens": 485, "judge_prompt_tokens": 415, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread", "latency_ms": 2624.0179538726807, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Club Dread') and correctly identifies the movie based on the description, so it is both correct and faithful.", "main_total_tokens": 3008, "main_prompt_tokens": 3003, "main_completion_tokens": 5, "judge_total_tokens": 489, "judge_prompt_tokens": 427, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet [Doc 48873]", "latency_ms": 3571.486711502075, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, identifying the movie as 'Toilet'. It is both correct and fully faithful to the provided gold reference.", "main_total_tokens": 3208, "main_prompt_tokens": 3194, "main_completion_tokens": 14, "judge_total_tokens": 482, "judge_prompt_tokens": 421, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel [Doc 1024]", "latency_ms": 3348.795175552368, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Electric Angel'. There is no additional conflicting or hallucinated information; the bracketed document tag does not change the answer content. Thus it is both correct and faithful to the given gold response.", "main_total_tokens": 2970, "main_prompt_tokens": 2957, "main_completion_tokens": 13, "judge_total_tokens": 502, "judge_prompt_tokens": 424, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "Fall [Doc 106810]", "latency_ms": 3058.0270290374756, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'Mega Ape', but the generated answer is 'Fall [Doc 106810]', so it is factually incorrect. There is no indication that the incorrect title was derived from or grounded in the provided gold answer; thus it is also unfaithful to the reference.", "main_total_tokens": 2906, "main_prompt_tokens": 2892, "main_completion_tokens": 14, "judge_total_tokens": 477, "judge_prompt_tokens": 388, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 125.01049041748047, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the highest rated TV show from 2010 is 'Tea House'. The generated answer says 'I don't know', which is not factually correct given the gold answer, so correctness is 0. However, it does not hallucinate or contradict its own knowledge; it honestly admits not knowing, so it is fully faithful to the model's claimed knowledge, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 502, "judge_prompt_tokens": 386, "judge_completion_tokens": 116}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 156.25882148742676, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because the gold answer is known, but the model responded with uncertainty. However, it is faithful to the model's stated uncertainty, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 395, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "en", "latency_ms": 2714.0066623687744, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The correct answer is the language name 'English'. The generated answer 'en' is the ISO code, not the language name, so it is not strictly correct. It is mostly faithful to the intent (same underlying language) but not in the expected form.", "main_total_tokens": 3133, "main_prompt_tokens": 3129, "main_completion_tokens": 4, "judge_total_tokens": 469, "judge_prompt_tokens": 384, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "Popular", "latency_ms": 2562.9730224609375, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'Popular' is incorrect; the correct show is 'Law & Order: Special Victims Unit.' The model output does not align with the given gold answer or the factual data, so it is neither correct nor faithful.", "main_total_tokens": 3154, "main_prompt_tokens": 3150, "main_completion_tokens": 4, "judge_total_tokens": 469, "judge_prompt_tokens": 391, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2 [Doc 79355]", "latency_ms": 2778.1736850738525, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (7.2) and does not introduce any contradictions or unsupported information. Reference to a document ID does not affect correctness or faithfulness.", "main_total_tokens": 2416, "main_prompt_tokens": 2403, "main_completion_tokens": 13, "judge_total_tokens": 459, "judge_prompt_tokens": 392, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "1999 [Doc 22481]", "latency_ms": 3552.798271179199, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states the film was released in 1999, while the gold label says 2019, so it is incorrect relative to the provided gold. However, the assistant directly answers the question without adding fabricated details, so it is faithful to its own (though mistaken) claim.", "main_total_tokens": 2965, "main_prompt_tokens": 2949, "main_completion_tokens": 16, "judge_total_tokens": 482, "judge_prompt_tokens": 393, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 1911.3833904266357, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 279 votes, but the generated response claims it cannot find any matching movies or shows rather than providing the number. This is incorrect with respect to the gold answer. However, it is faithful to the model's (incorrect) stated inability to find the information and does not hallucinate a number or details.", "main_total_tokens": 2408, "main_prompt_tokens": 2406, "main_completion_tokens": 2, "judge_total_tokens": 488, "judge_prompt_tokens": 392, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 1780.0509929656982, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 12 votes, but the generated response claims it cannot find any matching movies or shows and does not provide the correct number. This makes the answer factually incorrect. However, it does not invent any specific false numeric value or contradictory detail beyond expressing inability to find information, so it remains faithful to the (incorrect) reasoning it presents.", "main_total_tokens": 2460, "main_prompt_tokens": 2458, "main_completion_tokens": 2, "judge_total_tokens": 494, "judge_prompt_tokens": 392, "judge_completion_tokens": 102}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "I don't know.", "latency_ms": 760.9186172485352, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 2009, but the generated answer says it does not know the answer, so it is incorrect. However, the model did not fabricate any incorrect information and simply acknowledged not knowing, so it is fully faithful to its (claimed) knowledge.", "main_total_tokens": 560, "main_prompt_tokens": 558, "main_completion_tokens": 2, "judge_total_tokens": 475, "judge_prompt_tokens": 391, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_bge-m3_qwen2.5:32b", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en [Doc 48035]", "latency_ms": 3296.807289123535, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold label indicates the original language is English ('en'), and the generated answer is also 'en', matching the gold exactly. The answer is correct and faithfully reflects the gold.", "main_total_tokens": 3030, "main_prompt_tokens": 3017, "main_completion_tokens": 13, "judge_total_tokens": 457, "judge_prompt_tokens": 390, "judge_completion_tokens": 67}
