{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The movie *Wall Street* was released in 1987 [Doc 6993].", "latency_ms": 7572.203874588013, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer. The mention of a document citation does not contradict the question and remains faithful to the provided information.", "main_total_tokens": 3128, "main_prompt_tokens": 3106, "main_completion_tokens": 22, "judge_total_tokens": 466, "judge_prompt_tokens": 396, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "", "latency_ms": 5202.174663543701, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is empty, so it does not provide the correct movie title \"Brain Fluid Explosion Girl\" and is therefore incorrect. However, it does not introduce any new or contradictory information relative to the prompt, so it is faithful to the given context.", "main_total_tokens": 3954, "main_prompt_tokens": 2931, "main_completion_tokens": 1023, "judge_total_tokens": 471, "judge_prompt_tokens": 389, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show is *Dungeons & Dragons* (1983), where six friends are transported into the Dungeons & Dragons realm and must try to find a way home with the help of their guide 'Dungeon Master' [Doc 112849].", "latency_ms": 887.5010013580322, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' matching the gold answer. It accurately describes the premise involving six friends transported to a fantasy realm and guided by the Dungeon Master to find a way home. There are no contradictions or hallucinations relative to the provided information, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3060, "main_prompt_tokens": 3003, "main_completion_tokens": 57, "judge_total_tokens": 539, "judge_prompt_tokens": 440, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is titled *Nacht der Wölfe* [Doc 2590].", "latency_ms": 833.5204124450684, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe', which matches the gold answer exactly. It stays faithful to the provided information and does not introduce contradictions or extraneous claims.", "main_total_tokens": 3381, "main_prompt_tokens": 3339, "main_completion_tokens": 42, "judge_total_tokens": 509, "judge_prompt_tokens": 438, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of *Picnic at Hanging Rock* take place in 1900 [Doc 126191].", "latency_ms": 617.5026893615723, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of 'Picnic at Hanging Rock' take place in 1900, which matches the gold answer. It is fully consistent and faithful to the provided information.", "main_total_tokens": 3111, "main_prompt_tokens": 3083, "main_completion_tokens": 28, "judge_total_tokens": 478, "judge_prompt_tokens": 407, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie is *June* [Doc 32483].\n\nIt is about \"Young, innocent, and quirky June (Felicia Day), a violinist, spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and all-hours daydreaming about the beautiful man that's destined just for her\" [Doc 32483].", "latency_ms": 1166.457176208496, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the film as 'June' and accurately reproduces the description given in the gold/reference text, staying grounded in the provided information without adding unsupported claims.", "main_total_tokens": 3383, "main_prompt_tokens": 3294, "main_completion_tokens": 89, "judge_total_tokens": 577, "judge_prompt_tokens": 511, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "The highest rated Russian-language movie released in 1980 is *Moscow Does Not Believe in Tears*, with a rating of 7.6 [Doc 61].", "latency_ms": 2175.4956245422363, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect: the highest rated Russian-language movie in 1980 per the gold answer is 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger', not 'Moscow Does Not Believe in Tears'. Additionally, the response cites a document and rating that contradict the gold label, indicating it is not faithful to the provided ground truth.", "main_total_tokens": 3628, "main_prompt_tokens": 3258, "main_completion_tokens": 370, "judge_total_tokens": 532, "judge_prompt_tokens": 429, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest rating for a Russian-language movie released in 1999 is 4.571, for the movie *Features of the Russian Bath* [Doc 22144].", "latency_ms": 3481.9071292877197, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says the highest Russian-language movie rating in 1999 is 7.6, while the generated answer claims it is 4.571 for a specific movie. This contradicts the gold fact, so the answer is incorrect. It also appears to invent a specific movie and rating that do not match the provided ground truth, so it is unfaithful to the reference information.", "main_total_tokens": 3438, "main_prompt_tokens": 2733, "main_completion_tokens": 705, "judge_total_tokens": 530, "judge_prompt_tokens": 420, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "There are two movies released in 1987 that have a rating over 7.0 and are in Chinese:\n\n1.  *A Chinese Ghost Story* (Rating: 7.3, Language: cn) [Doc 7064]\n2.  *Last Eunuch In China* (Rating: 7.8, Language: cn) [Doc 7754]", "latency_ms": 3111.5496158599854, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 such movies, but the generated answer claims there are 2. This is factually incorrect relative to the gold label, so correctness is 0. The model, however, remains internally consistent with its own listed movies and does not contradict itself or fabricate beyond its stated evidence, so faithfulness to its own reasoning is high (1).", "main_total_tokens": 3782, "main_prompt_tokens": 3174, "main_completion_tokens": 608, "judge_total_tokens": 576, "judge_prompt_tokens": 470, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "Based strictly on the context provided, there are no TV shows released in 1987 that have a rating over 7 and are in Chinese.\n\nThe only TV show released in 1987 listed is *Square One Television* [Doc 113289], which has a rating of 4.6 and is in English [Doc 113289].", "latency_ms": 3667.0961380004883, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says the correct number of TV shows is 2, but the generated answer claims there are none, so it is numerically incorrect. However, the answer is internally consistent with and supported by the cited context (only Square One Television is mentioned, which does not meet the criteria), so it is faithful to the provided documents even though the documents are apparently incomplete versus the gold label.", "main_total_tokens": 3679, "main_prompt_tokens": 3064, "main_completion_tokens": 615, "judge_total_tokens": 573, "judge_prompt_tokens": 463, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "Based strictly on the provided context, there is no French language TV show listed that was released in 1980.", "latency_ms": 4245.261669158936, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer identifies 'Fantômas' as the only French language TV show released in 1980, but the generated answer claims there is no such show listed, which contradicts the gold. Therefore, the generated answer is incorrect. However, it is faithful to the (incorrect) reasoning it states, so faithfulness is high with respect to its own claim.", "main_total_tokens": 4258, "main_prompt_tokens": 3341, "main_completion_tokens": 917, "judge_total_tokens": 508, "judge_prompt_tokens": 408, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is *Kill Cruise* [Doc 11205].", "latency_ms": 633.8121891021729, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise', which matches the gold answer exactly. It is fully consistent with the provided information and doesn’t introduce unsupported claims, so both correctness and faithfulness are maximal.", "main_total_tokens": 3195, "main_prompt_tokens": 3168, "main_completion_tokens": 27, "judge_total_tokens": 483, "judge_prompt_tokens": 409, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The movie about the TV series in which Fred Rogers (1928-2003) spoke directly to young children about some of life's most important issues is titled *Won't You Be My Neighbor?* [Doc 84838].", "latency_ms": 834.2835903167725, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'Won't You Be My Neighbor?' and faithfully reflects the information given in the gold answer without introducing contradictions or fabrications.", "main_total_tokens": 3321, "main_prompt_tokens": 3265, "main_completion_tokens": 56, "judge_total_tokens": 512, "judge_prompt_tokens": 447, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The release years for the movies mentioned in the context are:\n\n*   **YEAR:** 2023 [Doc 107925]\n*   **The Disco Years:** 1994 [Doc 15797]\n*   **663114:** 2012 [Doc 58592]\n*   **Mine:** 2013 [", "latency_ms": 4554.124116897583, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a single year of release, which according to the gold answer is 2018. The generated answer instead lists multiple different movies with various years (2023, 1994, 2012, 2013) and never mentions 2018. Therefore it is incorrect relative to the gold answer. It is also not faithful to the supposed source context for this question, since it does not answer the specific item asked and appears to introduce extraneous movies and years unrelated to the gold label.", "main_total_tokens": 4329, "main_prompt_tokens": 3307, "main_completion_tokens": 1022, "judge_total_tokens": 577, "judge_prompt_tokens": 443, "judge_completion_tokens": 134}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is *Astro Boy* [Doc 112591].", "latency_ms": 617.1464920043945, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 'Astro Boy' as the TV series described in the question and matches the gold answer. It stays faithful to the provided information and does not introduce contradictions.", "main_total_tokens": 3567, "main_prompt_tokens": 3532, "main_completion_tokens": 35, "judge_total_tokens": 484, "judge_prompt_tokens": 416, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her young son were held captive for 7 years in an enclosed space [Doc 67513].", "latency_ms": 671.0126399993896, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9463946303571863, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, matching the gold answer. It does not introduce contradictions or unsupported claims beyond specifying the confinement as an enclosed space, which is consistent with the film, so it is faithful to the given information.", "main_total_tokens": 2806, "main_prompt_tokens": 2766, "main_completion_tokens": 40, "judge_total_tokens": 517, "judge_prompt_tokens": 429, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The context states that six armed men broke into the Brink's-Mat security depot on 26 November 1983, \"stumbling across gold bullion worth £26m\" [Doc 133469]. The context does not specify the total amount of gold the depot was holding before the break-in.", "latency_ms": 972.2926616668701, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks specifically how much gold the depot was holding before the break-in; the provided gold answer says £26m. The generated explanation correctly notes that the context only mentions \"gold bullion worth £26m\" that the robbers stumbled across, and that it does not specify the depot's total gold holdings before the break-in. Thus it correctly identifies that the given answer is not supported by the context. This is faithful to the context, but the original gold answer is incorrect relative to the question, so correctness is 0 while faithfulness of the critique is 1.", "main_total_tokens": 3143, "main_prompt_tokens": 3073, "main_completion_tokens": 70, "judge_total_tokens": 608, "judge_prompt_tokens": 455, "judge_completion_tokens": 153}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "The popularity score of the most popular English-speaking TV show released in 1997 is 61.7375 (for *South Park*) [Doc 114707].", "latency_ms": 2294.724225997925, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer gives a specific score (61.7375) that does not match the gold answer (80.5772), so it is factually incorrect for this query. However, it is faithful to its cited document, as it clearly attributes the score to South Park from Doc 114707 and does not invent unsupported sources or reasoning.", "main_total_tokens": 4325, "main_prompt_tokens": 3927, "main_completion_tokens": 398, "judge_total_tokens": 526, "judge_prompt_tokens": 426, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "Based strictly on the context provided, there were no Russian language (ru) TV shows released in 1997 [Doc 114791, Doc 114736, Doc 114830, Doc 114905].", "latency_ms": 4685.2991580963135, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there was 1 Russian language TV show released in 1997, but the generated answer claims there were none, so it is factually incorrect. It also explicitly contradicts the provided gold/context, so it is not faithful to the source.", "main_total_tokens": 3550, "main_prompt_tokens": 2577, "main_completion_tokens": 973, "judge_total_tokens": 510, "judge_prompt_tokens": 426, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "The TV show in which Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight is *Angel Eyes* [Doc 121928].", "latency_ms": 772.059440612793, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Angel Eyes,' which matches the gold answer. It is faithful to the question and does not introduce any contradictions or irrelevant information.", "main_total_tokens": 3109, "main_prompt_tokens": 3070, "main_completion_tokens": 39, "judge_total_tokens": 497, "judge_prompt_tokens": 431, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "", "latency_ms": 4752.3157596588135, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "No generated answer was provided to compare against the gold label, so correctness and faithfulness cannot be confirmed.", "main_total_tokens": 4065, "main_prompt_tokens": 3042, "main_completion_tokens": 1023, "judge_total_tokens": 429, "judge_prompt_tokens": 377, "judge_completion_tokens": 52}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "Based strictly on the context provided, there were 0 English-speaking Horror movies released in 2009 with a popularity score greater than 7.\n\nThe English-language Horror movies from 2009 listed in the context are:\n*   *Perkins' 14* (Popularity: 1.1125) [Doc 46432]\n*   *Scare Zone* (Popularity: 0.3206) [Doc 47582]\n*   *Vampiro* (Popularity: 0.3315) [Doc 47574]\n*   *2:13* (Popularity: 2.1085) [Doc 45378]", "latency_ms": 4045.630693435669, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the number of English-speaking Horror movies released in 2009 with popularity > 7, and the gold answer is 2. The generated answer concludes there are 0 such movies, which is numerically incorrect. However, the reasoning is consistent with the context it cites: it lists only four 2009 English-language horror films, all with popularity well below 7, and infers 0 from that list. There is no contradiction between the cited data and the conclusion, so the answer is faithful to the provided context but factually incorrect relative to the gold label.", "main_total_tokens": 3697, "main_prompt_tokens": 2861, "main_completion_tokens": 836, "judge_total_tokens": 676, "judge_prompt_tokens": 526, "judge_completion_tokens": 150}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Based strictly on the context provided, the best-rated movie released in 1979 cannot be determined, as no movies from that year are listed.", "latency_ms": 2471.877098083496, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Given the gold answer that the database has no movies from 1979, the generated answer correctly states that the best-rated movie from 1979 cannot be determined from the provided context. It stays faithful to the limitation described in the gold response and does not introduce extra information.", "main_total_tokens": 3537, "main_prompt_tokens": 3083, "main_completion_tokens": 454, "judge_total_tokens": 510, "judge_prompt_tokens": 424, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The release year for the movie Back to the Future Part IV is not provided in the context.", "latency_ms": 2827.1381855010986, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer correctly states that Back to the Future Part IV does not exist in the database, implying no release year. The generated answer instead says the release year is not provided in the context, which is a different and misleading reason and does not clearly convey that the movie (and thus a release year) is not known or does not exist. This is not the correct or intended answer, so correctness is 0. It partially aligns in that it avoids fabricating a year and indicates absence of information, but it is not faithful to the gold explanation about nonexistence in the database, so faithfulness is partial.", "main_total_tokens": 3870, "main_prompt_tokens": 3405, "main_completion_tokens": 465, "judge_total_tokens": 576, "judge_prompt_tokens": 419, "judge_completion_tokens": 157}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "Based strictly on the provided", "latency_ms": 4596.223592758179, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incomplete and does not provide a concrete response to the question, nor does it align with the gold answer that states no such movies exist. It neither correctly answers the question nor faithfully reflects the given gold explanation.", "main_total_tokens": 4321, "main_prompt_tokens": 3299, "main_completion_tokens": 1022, "judge_total_tokens": 487, "judge_prompt_tokens": 411, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are five TV shows or movies that have \"Astro Boy\" in the title:\n\n1.  *Astro Boy* (Movie, 2009) [Doc 44947]\n2.  *Astro Boy* (TV, 1980) [Doc 112591]\n3.  *Astro Boy* (TV, 2003) [Doc 116265]\n4.  *Astro Boy vs The Junkyard Pirates* (Movie, 2010) [Doc 49121]\n5.  *Atom: The Beginning* (TV, 2017) - The description mentions \"Astro Boy\" [Doc 125127].\n\n*Self-Correction/Refinement:* The user asked for titles that *have 'Astro Boy' in the title*.\n\n1.  *Astro Boy* [Doc 44947]\n2.  *Astro Boy* [Doc 112591]\n3.  *Astro Boy* [Doc 116265]\n4.  *Astro Boy vs The Junkyard Pirates* [Doc 49121]\n5.  *Atom: The Beginning* [Doc 125127] - Title is *Atom: The Beginning*, not *Astro Boy*.\n\nTherefore, strictly based on the title field:\n\n1.  *Astro Boy* (Movie, 2009) [Doc 44947]\n2.  *Astro Boy* (TV, 1980) [Doc 112591]\n3.  *Astro Boy* (TV, 2003) [Doc 116265]\n4.  *Astro Boy vs The Junkyard Pirates* (Movie, 2010) [Doc 49121]\n\nThere are four TV shows or movies that have \"Astro Boy\" in the title:\n\n1.  *Astro Boy* (Movie, 2009) [Doc 44947]\n2.  *Astro Boy* (TV, 1980) [Doc 112591]\n3.  *Astro Boy* (TV, 2003) [Doc 116265]\n4.  *Astro Boy vs The Junkyard Pirates* (Movie, 2010) [Doc 49121]", "latency_ms": 3064.777612686157, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 1.1510196182278052, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.5330476224182168, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer concludes that there are four titles with 'Astro Boy' in the title, matching the gold answer of 4. It explicitly corrects the earlier inclusion of 'Atom: The Beginning' because that title does not contain 'Astro Boy'. The reasoning is consistent with the question constraint and does not introduce contradictions, so it is both correct and faithful.", "main_total_tokens": 3821, "main_prompt_tokens": 3247, "main_completion_tokens": 574, "judge_total_tokens": 952, "judge_prompt_tokens": 847, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "Based strictly on the context provided, there is one movie released in 2025 that alludes to the Holocaust:\n\n1.  *The Future Awaits* [Doc 112009]: The plot is set during the Vel’ d’Hiv’ Roundup of Jewish families in Paris in July 1942 [Doc 112009].", "latency_ms": 4595.267295837402, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 2 movies in 2025 that allude to the Holocaust, but the generated answer claims there is only 1, so it is numerically incorrect. However, the answer is faithful to the context it cites, explicitly grounding its claim in Doc 112009 and not inventing unsupported details, so faithfulness is high.", "main_total_tokens": 4460, "main_prompt_tokens": 3547, "main_completion_tokens": 913, "judge_total_tokens": 555, "judge_prompt_tokens": 452, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "There were two TV shows released in the 1990s about Super Mario Bros.:\n\n1.  *The Adventures of Super Mario Bros. 3*, released in 1990 [Doc 113661].\n2.  *Super Mario World*, released in 1991 [Doc 113814].", "latency_ms": 4385.889768600464, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.8010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says 1 show, but the generated answer lists 2 separate TV shows from the 1990s, so it conflicts with the gold and is therefore incorrect relative to the task. However, the generated answer is internally consistent and grounded in the provided document references, so it is faithful to its own cited evidence.", "main_total_tokens": 4312, "main_prompt_tokens": 3383, "main_completion_tokens": 929, "judge_total_tokens": 545, "judge_prompt_tokens": 448, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "Based strictly on the context provided, there were no TV shows released between 2000 and 2010 about Super Mario Bros.", "latency_ms": 3729.8660278320312, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates there were 0 such TV shows, and the generated answer explicitly says there were no TV shows released between 2000 and 2010 about Super Mario Bros. This matches the gold answer exactly and stays within the given context, so it is both correct and faithful.", "main_total_tokens": 3771, "main_prompt_tokens": 3065, "main_completion_tokens": 706, "judge_total_tokens": 504, "judge_prompt_tokens": 415, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular TV show or movie about ninjas, based on the provided popularity scores, is *House of Ninjas*, with a popularity rating of 4.4317 [Doc 135161].", "latency_ms": 4100.726127624512, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question expects the answer 'Naruto', but the generated response claims 'House of Ninjas' based on document popularity scores, which is incorrect with respect to the gold answer. It also relies on external document scoring instead of the given gold label, so it is not faithful to the specified ground truth.", "main_total_tokens": 4334, "main_prompt_tokens": 3427, "main_completion_tokens": 907, "judge_total_tokens": 515, "judge_prompt_tokens": 423, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The context does not contain information about a Jurassic Park/Power Ranger crossover movie.", "latency_ms": 3174.3013858795166, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates that the correct response is that the movie was not released. The generated answer instead claims that the context lacks information, which neither answers the question nor matches the gold. Therefore, it is incorrect relative to the gold. It is also unfaithful because, even without the context, the correct factual answer is that such a crossover movie does not exist; saying only that the context lacks information fails to provide the needed factual conclusion.", "main_total_tokens": 3908, "main_prompt_tokens": 3283, "main_completion_tokens": 625, "judge_total_tokens": 521, "judge_prompt_tokens": 401, "judge_completion_tokens": 120}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Based strictly on the context provided, there is no movie listed where Barbie fights the Predator [Doc 6958, Doc 82000, Doc 84103, Doc 104474, Doc 68094, Doc 36330, Doc 48119, Doc 72290, Doc 92886, Doc 52809, Doc 78562, Doc 48011, Doc 55216, Doc 25632, Doc 33670, Doc 73700, Doc 55268, Doc 59104, Doc 44920].", "latency_ms": 1278.8219451904297, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.0, "faithfulness_score": 0.5, "justification": "The gold answer indicates explicitly that the movie was not released. The generated answer instead claims there is no movie listed in the provided context, which is a different statement: it focuses on absence in the documents rather than on the fact that such a movie was not released. This does not correctly answer the user question as framed by the gold answer, so correctness is 0. The answer is partially faithful to the given context (it cites documents and states there is no such movie in them) but adds no unsupported factual claim beyond what is implied by lack of evidence, so faithfulness is medium (0.5).", "main_total_tokens": 3390, "main_prompt_tokens": 3220, "main_completion_tokens": 170, "judge_total_tokens": 652, "judge_prompt_tokens": 495, "judge_completion_tokens": 157}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "", "latency_ms": 4954.661130905151, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The assistant produced no answer at all, so it does not match the provided gold answer and does not attempt to answer the question. Therefore it is both incorrect and unfaithful to the task.", "main_total_tokens": 4303, "main_prompt_tokens": 3280, "main_completion_tokens": 1023, "judge_total_tokens": 460, "judge_prompt_tokens": 390, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The release years for the movies mentioned in the context are:\n\n*   **1** was released in 2008 [Doc 44750].\n*   **663114** was released in 2012 [Doc 58592].\n*   **", "latency_ms": 4762.887477874756, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a single release year (1986), but the generated answer lists different years for other items and never gives 1986. It is therefore incorrect. It also does not follow from the given gold answer and appears to rely on unrelated context, so it is not faithful to the provided information.", "main_total_tokens": 4329, "main_prompt_tokens": 3307, "main_completion_tokens": 1022, "judge_total_tokens": 520, "judge_prompt_tokens": 427, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The rating of the movie *Forrest Gump* is 8.466 [Doc 15236].", "latency_ms": 749.4015693664551, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating value (8.466) and does not introduce extra or conflicting information. It is fully correct and faithful to the provided gold answer.", "main_total_tokens": 2974, "main_prompt_tokens": 2947, "main_completion_tokens": 27, "judge_total_tokens": 482, "judge_prompt_tokens": 416, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The TV show about two comic book fanboys who discover a secret hospital for superheroes and are offered a job there is titled *Mighty Med* [Doc 121125].", "latency_ms": 695.6639289855957, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the show as 'Mighty Med' and closely matches the gold answer. It remains faithful to the question, only adding minor accurate descriptive detail without contradicting any provided information.", "main_total_tokens": 3153, "main_prompt_tokens": 3114, "main_completion_tokens": 39, "judge_total_tokens": 498, "judge_prompt_tokens": 427, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "The seven prisoners in *Túnel de los huesos* (2011) escaped from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 818.4428215026855, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies that the prisoners escaped from a prison in Argentina, matching the gold answer. It adds the specific detail 'Villa Devoto prison in Buenos Aires, Argentina' and cites a document, which goes beyond the provided gold answer; this additional specificity is not directly verifiable from the prompt alone, so it slightly reduces faithfulness, though the core country information is accurate.", "main_total_tokens": 3424, "main_prompt_tokens": 3385, "main_completion_tokens": 39, "judge_total_tokens": 536, "judge_prompt_tokens": 426, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In *Sweet Trap* (2024), the apprentice Li Nai falls into a trap designed by Jiang Jie [Doc 135231].", "latency_ms": 649.6737003326416, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is simply that the apprentice falls into Jiang Jie's trap. The generated answer incorrectly adds the detail that the apprentice is named Li Nai and attributes a specific document source, which is unverifiable and not supported by the gold. Therefore, it is not fully correct or faithful to the given information.", "main_total_tokens": 3124, "main_prompt_tokens": 3091, "main_completion_tokens": 33, "judge_total_tokens": 503, "judge_prompt_tokens": 411, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "Based strictly on the provided context, there are no Korean-language movies released after 2023 that mention assassins [Doc 106219, Doc 67739, Doc 77277, Doc 48744].", "latency_ms": 3028.3570289611816, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 such movies, whereas the generated answer claims there are none, so it is factually incorrect relative to the gold. However, the model explicitly bases its conclusion on the provided context and does not introduce external hallucinated facts, so it is faithful to the given context.", "main_total_tokens": 3612, "main_prompt_tokens": 2998, "main_completion_tokens": 614, "judge_total_tokens": 518, "judge_prompt_tokens": 426, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "Based strictly on the provided context, there is no information about any movies featuring both Steven Seagal and Bruce Lee.", "latency_ms": 2794.4109439849854, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information about any movies featuring both Steven Seagal and Bruce Lee, which aligns with the gold answer indicating none can be found. It remains faithful to the provided context and does not introduce unsupported claims.", "main_total_tokens": 4102, "main_prompt_tokens": 3530, "main_completion_tokens": 572, "judge_total_tokens": 495, "judge_prompt_tokens": 416, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of *Tokyo Mater* (2008) is 6.3 [Doc 42750].", "latency_ms": 771.2891101837158, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the rating of Tokyo Mater (2008) is 6.3, matching the gold answer exactly. It does not introduce any contradictory or extraneous information, and explicitly attributes the value to a document, so it is both correct and faithful to the provided gold data.", "main_total_tokens": 3326, "main_prompt_tokens": 3299, "main_completion_tokens": 27, "judge_total_tokens": 496, "judge_prompt_tokens": 405, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "", "latency_ms": 5134.102582931519, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The assistant failed to provide any answer to the question, so it is not correct, and it does not address the given gold answer. Therefore both correctness and faithfulness are 0.", "main_total_tokens": 4356, "main_prompt_tokens": 3333, "main_completion_tokens": 1023, "judge_total_tokens": 442, "judge_prompt_tokens": 379, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the TV show is *Confronting a Serial Killer* [Doc 131298].", "latency_ms": 755.7022571563721, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.75, "justification": "The generated answer gives the correct title, 'Confronting a Serial Killer', matching the gold answer. However, it incorrectly calls it a TV show rather than a movie, and adds an unsupported citation-style note '[Doc 131298]'. Thus, correctness is 1, but faithfulness is slightly reduced due to the mislabeling and extra fabricated reference.", "main_total_tokens": 3306, "main_prompt_tokens": 3281, "main_completion_tokens": 25, "judge_total_tokens": 533, "judge_prompt_tokens": 427, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of *Armageddon* (1998) is 7.838 [Doc 20603].", "latency_ms": 760.0011825561523, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and stays within the information given, only restating the value from the provided document reference. Both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3320, "main_prompt_tokens": 3290, "main_completion_tokens": 30, "judge_total_tokens": 476, "judge_prompt_tokens": 409, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 movie that is categorized as a Mystery, Thriller, and Drama, and alludes to an extra sense is *The Sixth Sense* [Doc 22098].", "latency_ms": 3233.0870628356934, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6895405204413555, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.7, "justification": "The generated answer correctly identifies the movie title as 'The Sixth Sense', matching the gold answer. However, it adds extra unsupported details such as specific genre labeling and a document citation [Doc 22098] that are not verifiable from the given QA pair, so faithfulness is slightly reduced.", "main_total_tokens": 3460, "main_prompt_tokens": 2888, "main_completion_tokens": 572, "judge_total_tokens": 522, "judge_prompt_tokens": 430, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The movie *The Transporter* was released in 2002 [Doc 27318].", "latency_ms": 640.308141708374, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer, and it does not introduce any contradictions or unsupported claims beyond a harmless document reference.", "main_total_tokens": 3248, "main_prompt_tokens": 3224, "main_completion_tokens": 24, "judge_total_tokens": 467, "judge_prompt_tokens": 399, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 movie is *The Transporter Refueled* [Doc 67500].", "latency_ms": 849.8308658599854, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2015 reboot title as 'The Transporter Refueled' and matches the gold answer without introducing contradictions or unsupported information.", "main_total_tokens": 3172, "main_prompt_tokens": 3144, "main_completion_tokens": 28, "judge_total_tokens": 477, "judge_prompt_tokens": 414, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in 2015 [Doc 67500].", "latency_ms": 833.9548110961914, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It does not introduce any unsupported information, so it is both correct and faithful.", "main_total_tokens": 3240, "main_prompt_tokens": 3215, "main_completion_tokens": 25, "judge_total_tokens": 477, "judge_prompt_tokens": 404, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "Based strictly on the context provided, there is no movie categorized as a romantic comedy in which Abraham Lincoln comes back from the dead.", "latency_ms": 3761.4879608154297, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says it cannot find such a movie, and the generated answer restates that there is no such romantic comedy based on the provided context. This matches the gold and does not introduce unsupported claims, so it is both correct and faithful to the given information.", "main_total_tokens": 3737, "main_prompt_tokens": 2982, "main_completion_tokens": 755, "judge_total_tokens": 500, "judge_prompt_tokens": 416, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript supposedly written by Ernest Hemingway is *Havana Darkness* [Doc 90923].", "latency_ms": 717.9760932922363, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'Havana Darkness,' which matches the gold answer exactly. It is fully supported by the provided information and does not add unsupported claims beyond referencing a document ID, so it is both correct and faithful.", "main_total_tokens": 3058, "main_prompt_tokens": 3027, "main_completion_tokens": 31, "judge_total_tokens": 496, "judge_prompt_tokens": 416, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show *Doll & Em* [Doc 122259].", "latency_ms": 890.8896446228027, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em,' matching the gold answer exactly. It stays faithful to the information given in the question and aligns with the provided reference, adding only citation-style context without altering the fact.", "main_total_tokens": 3140, "main_prompt_tokens": 3109, "main_completion_tokens": 31, "judge_total_tokens": 496, "judge_prompt_tokens": 417, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The movie is *Hallam Foe* [Doc 39389].", "latency_ms": 2363.7781143188477, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2007 movie as 'Hallam Foe', which exactly matches the gold answer. It is fully faithful to the provided information and does not add unsupported details.", "main_total_tokens": 3564, "main_prompt_tokens": 3144, "main_completion_tokens": 420, "judge_total_tokens": 484, "judge_prompt_tokens": 414, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Based strictly on the provided context, there is no information describing a person whose last name sounds like \"friend\" or \"foe.\"", "latency_ms": 4524.8706340789795, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question clearly expects the categorical answer \"A foe\", but the generated answer claims there is no information and does not provide that label, so it is incorrect. However, it does not invent or contradict any given context and faithfully reflects its own stated reasoning about the lack of information, so faithfulness is high.", "main_total_tokens": 3830, "main_prompt_tokens": 2968, "main_completion_tokens": 862, "judge_total_tokens": 506, "judge_prompt_tokens": 413, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "Sam is 18 years old in the 2017 TV show *Atypical* [Doc 124583].", "latency_ms": 740.1933670043945, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, matching the gold answer exactly. It does not introduce contradictions or unsupported claims beyond a generic document reference, so it is both correct and faithful to the provided information.", "main_total_tokens": 3018, "main_prompt_tokens": 2988, "main_completion_tokens": 30, "judge_total_tokens": 497, "judge_prompt_tokens": 411, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show *Atypical*, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 816.1892890930176, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies that Sam is on the autism spectrum, matching the gold answer exactly. It is fully supported by the provided information and does not introduce any unsupported content.", "main_total_tokens": 3201, "main_prompt_tokens": 3172, "main_completion_tokens": 29, "judge_total_tokens": 477, "judge_prompt_tokens": 411, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "The requested information is not available in the provided context.", "latency_ms": 2238.626718521118, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that Sandra was born in Chamonix, but the generated answer claims the information is not available. This is incorrect relative to the gold, so correctness is 0. However, given only the provided context (which does not include the movie details), the model truthfully stated that the information was not available, so its answer is faithful to the context, leading to a faithfulness score of 1.", "main_total_tokens": 3730, "main_prompt_tokens": 3361, "main_completion_tokens": 369, "judge_total_tokens": 524, "judge_prompt_tokens": 408, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481]", "latency_ms": 2707.767963409424, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer fills the blank with 'rains', which exactly matches the gold answer. It is correct and faithful to the provided information.", "main_total_tokens": 3694, "main_prompt_tokens": 3146, "main_completion_tokens": 548, "judge_total_tokens": 457, "judge_prompt_tokens": 398, "judge_completion_tokens": 59}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Tabaluga is the crown-prince of Greenland [Doc 114810].", "latency_ms": 2206.5155506134033, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which matches the gold answer. It is fully consistent with the provided information and does not introduce unsupported details.", "main_total_tokens": 3635, "main_prompt_tokens": 3289, "main_completion_tokens": 346, "judge_total_tokens": 479, "judge_prompt_tokens": 410, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers\" is *One Afternoon* [Doc 96838].", "latency_ms": 670.607328414917, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'One Afternoon' and does not introduce any contradictions or unsupported claims relative to the given description and gold answer. It is fully correct and faithful.", "main_total_tokens": 3063, "main_prompt_tokens": 3016, "main_completion_tokens": 47, "judge_total_tokens": 516, "judge_prompt_tokens": 448, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The Italian language movie where a Holocaust survivor running a daycare takes in a 12-year-old street kid who recently robbed her is *The Life Ahead* [Doc 93260]. It has a rating of 6.746 [Doc 93260].", "latency_ms": 9119.763374328613, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Life Ahead' and accurately describes the premise involving a Holocaust survivor caring for a 12-year-old street kid who robbed her. The added detail about it being Italian-language and including a rating of 6.746 does not contradict the gold answer and is plausibly sourced from the referenced document. The reasoning stays on-topic and grounded, so both correctness and faithfulness are high.", "main_total_tokens": 4597, "main_prompt_tokens": 3666, "main_completion_tokens": 931, "judge_total_tokens": 565, "judge_prompt_tokens": 449, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about the story of New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859]. The film shows how a man of humble beginnings could compete against and defeat the elite of motor racing [Doc 73859].", "latency_ms": 1075.2522945404053, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the subject of the 2016 documentary \"McLaren\" as New Zealander Bruce McLaren, founder of the McLaren motor racing team, which matches the gold answer exactly. The additional descriptive details stay consistent with that fact and do not contradict the source, so the response is both correct and faithful.", "main_total_tokens": 3385, "main_prompt_tokens": 3319, "main_completion_tokens": 66, "judge_total_tokens": 543, "judge_prompt_tokens": 445, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "Based strictly on the context provided, the names of racing teams mentioned are:\n\n1.  The Newman-Haas (And", "latency_ms": 5057.74712562561, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incomplete and does not provide the correct team name 'The McLaren Motor Racing team'. It also diverges from the gold answer and the apparent context, so it is neither correct nor faithful.", "main_total_tokens": 4440, "main_prompt_tokens": 3418, "main_completion_tokens": 1022, "judge_total_tokens": 480, "judge_prompt_tokens": 407, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "", "latency_ms": 5737.739324569702, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The model produced no answer, so the response is neither correct nor faithful to the gold answer (New Zealand).", "main_total_tokens": 4174, "main_prompt_tokens": 3151, "main_completion_tokens": 1023, "judge_total_tokens": 430, "judge_prompt_tokens": 377, "judge_completion_tokens": 53}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based strictly on the context provided, the answer is not found.\n\nThe 2013 movie featuring John McClane, *A Good Day to Die Hard*, involves him traveling to Moscow [Doc 58791]. No context describes a 2013 movie where John McClane blows up the Great Wall of China.", "latency_ms": 3770.6239223480225, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.9, "faithfulness_score": 0.9, "justification": "The gold answer says the database has no such movie. The generated answer correctly states that no context describes a 2013 movie where John McClane blows up the Great Wall of China, which aligns in substance with the gold (there is no such movie in the given data). However, it adds extra mention of *A Good Day to Die Hard* and a specific document reference that are not present in the provided gold/context and cannot be verified, so it's not fully faithful. Conceptually it is correct that no such film exists in the database, but tying it to a specific doc ID is speculative relative to the gold.", "main_total_tokens": 3884, "main_prompt_tokens": 3162, "main_completion_tokens": 722, "judge_total_tokens": 622, "judge_prompt_tokens": 462, "judge_completion_tokens": 160}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with that description is *Tomie: Forbidden Fruit* [Doc 28589].", "latency_ms": 940.2618408203125, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Tomie: Forbidden Fruit,' which matches the gold answer exactly. It is fully faithful to the provided information and does not introduce contradictions or extraneous claims.", "main_total_tokens": 3191, "main_prompt_tokens": 3161, "main_completion_tokens": 30, "judge_total_tokens": 513, "judge_prompt_tokens": 442, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie *Lei mi parla ancora*, Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 786.9646549224854, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, which matches the gold answer. It is fully consistent and faithful to the provided information.", "main_total_tokens": 3452, "main_prompt_tokens": 3417, "main_completion_tokens": 35, "judge_total_tokens": 491, "judge_prompt_tokens": 424, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In *Scorpion King: Book of Souls* (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 834.3222141265869, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in 'Scorpion King: Book of Souls' (2018), The Scorpion King and Tala search for the Book of Souls. This exactly matches the gold answer, only adding a brief descriptive phrase ('a legendary relic'), which is accurate and does not contradict the gold. Thus, it is both correct and faithful to the reference answer.", "main_total_tokens": 3792, "main_prompt_tokens": 3751, "main_completion_tokens": 41, "judge_total_tokens": 536, "judge_prompt_tokens": 431, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for \"Zombies on the Moon\" is not available.", "latency_ms": 2554.537534713745, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year is not available, which matches the gold response that the movie is not in the database. It does not introduce unsupported information and is fully faithful to the provided context.", "main_total_tokens": 3399, "main_prompt_tokens": 2887, "main_completion_tokens": 512, "judge_total_tokens": 486, "judge_prompt_tokens": 413, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Galactic baker\" is not available in the provided context.", "latency_ms": 2308.2988262176514, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly states that the movie is not in the database; the generated answer says the release year is not available in the provided context. This differs in meaning (database vs. context) and is not the correct behavior relative to the gold, so correctness is 0. However, the generated answer does not hallucinate a year and stays within the information given (no context), so it is faithful to its own context, yielding faithfulness 1.", "main_total_tokens": 3877, "main_prompt_tokens": 3487, "main_completion_tokens": 390, "judge_total_tokens": 529, "judge_prompt_tokens": 406, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Return of the Null Pointer\" is not available in the provided context.", "latency_ms": 2339.8711681365967, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated responses state that the release information is not available, which is correct given the context and consistent with the gold answer. The model does not fabricate a year and stays within the provided information, so it is both correct and faithful.", "main_total_tokens": 3604, "main_prompt_tokens": 3161, "main_completion_tokens": 443, "judge_total_tokens": 491, "judge_prompt_tokens": 410, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Data Science: The Movie\" is not available in the provided context.", "latency_ms": 2957.3655128479004, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available/unknown. This is correct given there's no information provided, and the generated response faithfully reflects that lack of data without inventing a year.", "main_total_tokens": 3692, "main_prompt_tokens": 3113, "main_completion_tokens": 579, "judge_total_tokens": 483, "judge_prompt_tokens": 410, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for \"Python's Revenge 2\" is not found.", "latency_ms": 3158.395290374756, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year is not found, matching the gold response that it is not in the database. It does not introduce any unsupported details, so it is both correct and faithful to the provided context.", "main_total_tokens": 4090, "main_prompt_tokens": 3472, "main_completion_tokens": 618, "judge_total_tokens": 489, "judge_prompt_tokens": 413, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for \"The Last Pixel\" cannot be determined as this title is not listed in the documents.", "latency_ms": 2340.3234481811523, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database. The generated answer correctly states that the release year cannot be determined and that the title is not listed. This matches the provided context and does not invent a year, so it is both correct and faithful.", "main_total_tokens": 3262, "main_prompt_tokens": 2826, "main_completion_tokens": 436, "judge_total_tokens": 501, "judge_prompt_tokens": 418, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for the movie \"The man who ate the sun\" is not available in the provided context.", "latency_ms": 3111.3619804382324, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold answer says the movie is not in the database. The generated answer states that the release year is not available in the provided context, which is effectively equivalent: it does not hallucinate a date. This is correct relative to the gold and faithful to the (limited) context.", "main_total_tokens": 4276, "main_prompt_tokens": 3699, "main_completion_tokens": 577, "judge_total_tokens": 501, "judge_prompt_tokens": 414, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Based strictly on the context provided, the director of the movie \"The Last Days of Lehman Brothers\" is not mentioned.", "latency_ms": 1590.714693069458, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer state that the director is not mentioned in the provided context/database. The generated answer does not introduce external information and is consistent with the given limitations, so it is both correct and faithful.", "main_total_tokens": 3676, "main_prompt_tokens": 3465, "main_completion_tokens": 211, "judge_total_tokens": 498, "judge_prompt_tokens": 422, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not contain information about a movie titled \"A Love to Die For\" or who directed it. It does mention a TV show titled \"A Love to Kill\" [Doc 117179].", "latency_ms": 971.6992378234863, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not provide information about a movie titled 'A Love to Die For' or its director, which aligns with the gold answer that the director information is not available. It does not invent a director and remains grounded in the given context, even adding a relevant note about a different title in the database. Thus it is both correct and faithful.", "main_total_tokens": 3330, "main_prompt_tokens": 3284, "main_completion_tokens": 46, "judge_total_tokens": 544, "judge_prompt_tokens": 437, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not contain information about who directed the movie \"Journey into Gaza\" [Doc 111360].", "latency_ms": 633.8968276977539, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not provide information about the director of 'Journey into Gaza', which matches the gold answer indicating the database lacks this information. It does not invent a director and is faithful to the given context.", "main_total_tokens": 3423, "main_prompt_tokens": 3396, "main_completion_tokens": 27, "judge_total_tokens": 494, "judge_prompt_tokens": 416, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not specify who directed the movie \"The Last Night at Tremore Beach\" [Doc 135098].", "latency_ms": 1474.914789199829, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not specify who directed the movie and does not attempt to hallucinate a director. This matches the gold answer in meaning (information not available in the database), so it is both correct and faithful to the provided context.", "main_total_tokens": 3112, "main_prompt_tokens": 2935, "main_completion_tokens": 177, "judge_total_tokens": 505, "judge_prompt_tokens": 422, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The following movies involve a group of archaeology students awakening an ancient mummy:\n\n*   In *Rise of the Mummy* (2021), a group of archaeology students awaken an ancient mummy [Doc 98375].\n*   In *Ancient Evil: Scream of the Mummy* (2000), six young archaeology students discover the remains of an ancient Aztec mummy and accidentally unleash the fury of an evil god [Doc 24655].", "latency_ms": 1019.411563873291, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0.5, "faithfulness_score": 0.5, "justification": "The question asks for a single movie title matching the described plot. The gold answer is \"Rise of the Mummy.\" The generated answer lists multiple films and does include \"Rise of the Mummy\" with the correct plot, but then adds another film, which is unnecessary and partially incorrect relative to the task of naming the one movie. Hence only partial correctness. In terms of faithfulness, the model correctly states that \"Rise of the Mummy\" involves archaeology students awakening a mummy, but it also asserts details about another movie and cites pseudo-doc IDs that are not grounded in the given context, so faithfulness is only partial.", "main_total_tokens": 3111, "main_prompt_tokens": 3014, "main_completion_tokens": 97, "judge_total_tokens": 642, "judge_prompt_tokens": 481, "judge_completion_tokens": 161}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie where Sarah fakes her own death to escape her abusive spouse is *Stranger in My Bed* [Doc 35237].", "latency_ms": 765.3427124023438, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold reference title 'Stranger in My Bed' and correctly restates the plot description. It does not introduce contradictions or unsupported information beyond a neutral citation marker, so it is both correct and faithful.", "main_total_tokens": 3097, "main_prompt_tokens": 3067, "main_completion_tokens": 30, "judge_total_tokens": 501, "judge_prompt_tokens": 424, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "This event happens in the TV show *Game of Silence* [Doc 123938].", "latency_ms": 686.898946762085, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question explicitly asks for a movie, while the generated answer identifies *Game of Silence* as a TV show. Although the content correctly states it is a TV show, this does not answer the question as posed (movie), so correctness is 0. The answer is partially faithful to the provided gold information by matching the title but adds an incorrect implication that the gold refers to a TV show, so faithfulness is 0.5.", "main_total_tokens": 2734, "main_prompt_tokens": 2712, "main_completion_tokens": 22, "judge_total_tokens": 536, "judge_prompt_tokens": 415, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie described as being \"Based on a true story. Philip Markoff, a charismatic and popular med student at Boston University, leads a double life as a brutal and cruel sexual deviant who abuses prostitutes he finds via Craigslist\" is titled *The Craigslist Killer* [Doc 53039].", "latency_ms": 904.9179553985596, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer,' which matches the gold answer and is consistent with the description. It is faithful to the provided information and does not introduce unsupported claims.", "main_total_tokens": 2956, "main_prompt_tokens": 2895, "main_completion_tokens": 61, "judge_total_tokens": 521, "judge_prompt_tokens": 451, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "This happens in the movie *Wood Job!* [Doc 66094].", "latency_ms": 617.840051651001, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Wood Job!' which matches the gold answer exactly. It is fully faithful to the query and does not introduce extraneous or incorrect information.", "main_total_tokens": 4331, "main_prompt_tokens": 4312, "main_completion_tokens": 19, "judge_total_tokens": 482, "judge_prompt_tokens": 415, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure, is *Sesame Street: The Magical Wand Chase* [Doc 78373].", "latency_ms": 856.9338321685791, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Sesame Street: The Magical Wand Chase,' which matches the gold answer exactly. It directly uses the information from the provided description without adding unsupported details, so it is both correct and faithful.", "main_total_tokens": 3473, "main_prompt_tokens": 3418, "main_completion_tokens": 55, "judge_total_tokens": 538, "judge_prompt_tokens": 460, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a 7-year-old boy must find the hero within him to save his parents trapped inside a burning car after a rock fall is *Beyond* [Doc 74597].", "latency_ms": 755.6548118591309, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Beyond', matching the gold answer exactly. It restates the question premise without adding contradictions or unsupported details, so it is both correct and faithful.", "main_total_tokens": 2943, "main_prompt_tokens": 2899, "main_completion_tokens": 44, "judge_total_tokens": 516, "judge_prompt_tokens": 447, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "This happens in the movie *6ixtynin9* [Doc 22478].", "latency_ms": 680.6321144104004, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as '6ixtynin9'. It is correct and directly grounded in the provided information.", "main_total_tokens": 3013, "main_prompt_tokens": 2991, "main_completion_tokens": 22, "judge_total_tokens": 484, "judge_prompt_tokens": 421, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie in which this happens is *Club Dread* [Doc 32410].", "latency_ms": 856.5883636474609, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer exactly. It remains faithful to the user query and does not introduce extraneous or incorrect information.", "main_total_tokens": 3317, "main_prompt_tokens": 3297, "main_completion_tokens": 20, "judge_total_tokens": 510, "judge_prompt_tokens": 442, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is titled *Toilet* [Doc 48873].", "latency_ms": 871.9406127929688, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Toilet,' which matches the gold answer exactly. It stays faithful to the query and does not introduce contradictory or extraneous information beyond a harmless reference marker.", "main_total_tokens": 3730, "main_prompt_tokens": 3673, "main_completion_tokens": 57, "judge_total_tokens": 537, "judge_prompt_tokens": 466, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie described is *Electric Angel* [Doc 1024].", "latency_ms": 919.1460609436035, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title matching the given description. The gold answer is 'Electric Angel.' The generated answer explicitly identifies the movie as 'Electric Angel,' which is correct. It does not introduce contradictions or unsupported claims beyond restating the title, so it is fully faithful to the provided information.", "main_total_tokens": 3474, "main_prompt_tokens": 3458, "main_completion_tokens": 16, "judge_total_tokens": 521, "judge_prompt_tokens": 430, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 among the documents provided is *Bottom of the Water*, with a rating of 4.2 [Doc 106899].", "latency_ms": 3609.1694831848145, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the lowest rated movie from 2023 is 'Mega Ape', but the generated answer claims it is 'Bottom of the Water' with a 4.2 rating, which contradicts the gold. Therefore the generated response is incorrect. It also references 'among the documents provided' and cites a specific document ID that presumably does not support the gold answer, so it is not faithful to the given ground truth for this evaluation.", "main_total_tokens": 3729, "main_prompt_tokens": 2892, "main_completion_tokens": 837, "judge_total_tokens": 530, "judge_prompt_tokens": 414, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "The highest rated TV show from 2010 is *Sherlock*, with a rating of 8.511 [Doc 119150].", "latency_ms": 2718.4159755706787, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The answer is incorrect because the gold label indicates the correct answer is 'Tea House', not 'Sherlock'. It is also unfaithful to the provided gold answer, directly contradicting it by naming a different show and adding an unsupported rating and document reference.", "main_total_tokens": 3958, "main_prompt_tokens": 3269, "main_completion_tokens": 689, "judge_total_tokens": 493, "judge_prompt_tokens": 410, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest rated Japanese language movie from 1999 is *Kikujiro*, with a rating of 7.5 [Doc 22213].", "latency_ms": 4572.654485702515, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The generated answer is factually incorrect with respect to the question and gold label: it claims *Kikujiro* is the highest rated Japanese-language movie from 1999, while the gold answer is *The Uncut Story of Mewtwo's Origin*. Therefore correctness is 0. The answer cites a document and a specific rating, so it is at least partially grounded in the provided source rather than being purely fabricated, but it still conflicts with the gold, so faithfulness is partial rather than full.", "main_total_tokens": 4298, "main_prompt_tokens": 3325, "main_completion_tokens": 973, "judge_total_tokens": 558, "judge_prompt_tokens": 422, "judge_completion_tokens": 136}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movies from 1980 have a rating of 4.5. These movies are in Chinese (cn) [Doc 354] and Spanish (es) [Doc 837].", "latency_ms": 3733.8502407073975, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the language of the lowest-rated movie from 1980, whose gold answer is English. The generated answer instead claims the lowest-rated movies are in Chinese and Spanish and discusses ratings and documents not mentioned in the question or gold. This is factually incorrect relative to the gold and introduces unsupported details, so both correctness and faithfulness are 0.", "main_total_tokens": 3667, "main_prompt_tokens": 2881, "main_completion_tokens": 786, "judge_total_tokens": 527, "judge_prompt_tokens": 423, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "", "latency_ms": 5017.759084701538, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is empty, so it does not state the correct TV show ('Law & Order: Special Victims Unit') and provides no information that can be checked against the question or the gold answer. Therefore it is neither correct nor faithful.", "main_total_tokens": 5311, "main_prompt_tokens": 4288, "main_completion_tokens": 1023, "judge_total_tokens": 471, "judge_prompt_tokens": 391, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 771.9581127166748, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and explicitly attributes it to a document, indicating it is grounded in the provided source.", "main_total_tokens": 3170, "main_prompt_tokens": 3149, "main_completion_tokens": 21, "judge_total_tokens": 461, "judge_prompt_tokens": 400, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in the following years:\n\n*   1999 [Doc 22481]\n*   2019 [Doc 88811]\n*   2025 [Doc 112077]", "latency_ms": 939.1665458679199, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the movie was released in 2019, but the generated answer lists three different years (1999, 2019, and 2025) instead of giving a single correct year. This is incorrect. It is also unfaithful because the model is supposed to answer the question, not enumerate multiple contradictory years; citing extra years that are not correct for the asked movie undermines faithfulness to the intended answer.", "main_total_tokens": 3130, "main_prompt_tokens": 3066, "main_completion_tokens": 64, "judge_total_tokens": 551, "judge_prompt_tokens": 432, "judge_completion_tokens": 119}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "Based strictly on the context provided, the number of votes \"My Dog Skip\" received is not mentioned. The context lists its Popularity as 1.7749 and its Rating as 6.7 [Doc 24114].", "latency_ms": 2709.402322769165, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer specifies that \"My Dog Skip\" received 279 votes. The generated answer incorrectly states that the number of votes is not mentioned, so it is factually incorrect relative to the gold. It also contradicts the provided gold information, so it is not faithful to the reference answer.", "main_total_tokens": 3785, "main_prompt_tokens": 3339, "main_completion_tokens": 446, "judge_total_tokens": 518, "judge_prompt_tokens": 428, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The context does not provide information regarding the number of votes received by any movie titled \"Oh My God!\" [Doc 43976, Doc 85434].", "latency_ms": 803.22265625, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that \"Oh My God!\" received 12 votes. The generated answer claims that the context does not provide this information, which directly contradicts the gold. Therefore, the generated answer is incorrect. However, assuming the model only saw the provided context and that context indeed lacked the vote count, the answer is faithful to its stated lack of information.", "main_total_tokens": 3247, "main_prompt_tokens": 3209, "main_completion_tokens": 38, "judge_total_tokens": 518, "judge_prompt_tokens": 413, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 740.4673099517822, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'The Last Days of Lehman Brothers' was released in 2009, which matches the gold answer. It is also faithful to the provided information, simply restating the correct year.", "main_total_tokens": 3483, "main_prompt_tokens": 3456, "main_completion_tokens": 27, "judge_total_tokens": 487, "judge_prompt_tokens": 410, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-small_gemini-flash-latest", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English (en) [Doc 48035].", "latency_ms": 718.6868190765381, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the original language of 'The King’s Speech' is English, matching the gold label 'en'. It does not introduce contradictions or unsupported claims; the bracketed document reference does not affect the factual content.", "main_total_tokens": 3479, "main_prompt_tokens": 3452, "main_completion_tokens": 27, "judge_total_tokens": 486, "judge_prompt_tokens": 405, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The movie *Wall Street* was released in 1987 [Doc 6993].", "latency_ms": 787.4774932861328, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, which matches the gold answer. It does not introduce any incorrect or extraneous information, and the reference to a document does not contradict the source, so it is both correct and faithful.", "main_total_tokens": 2472, "main_prompt_tokens": 2450, "main_completion_tokens": 22, "judge_total_tokens": 479, "judge_prompt_tokens": 396, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Based strictly on the provided context, there is no movie described where classmates have to play a survival game using their cell phones.\n\n*   *Perfect Strangers* involves friends playing a game with their cell phones, but it is described as a dinner party game that reveals secrets, not a survival game [Doc 103513].\n*   *47 Hours to Live* involves using a phone to play a game that results in losing one's life, but it is focused on summoning the supernatural, not a survival game played by classmates [Doc 90746].\n*   Other movies mention survival games or classmates, but not the combination of all three elements [Doc 79236, Doc 93745, Doc 45603].", "latency_ms": 4853.050708770752, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question explicitly asks for the movie title, and the gold answer is 'Brain Fluid Explosion Girl.' The generated answer claims no such movie is described in the context and does not provide the correct title, so it is incorrect with respect to the question and gold answer. However, given the constraint 'based strictly on the provided context,' the assistant accurately describes what is and is not in the cited documents, so its statements about the context are faithful. Thus correctness is 0 and faithfulness is 1.", "main_total_tokens": 3168, "main_prompt_tokens": 2199, "main_completion_tokens": 969, "judge_total_tokens": 665, "judge_prompt_tokens": 533, "judge_completion_tokens": 132}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show is *Dungeons & Dragons* [Doc 112849]. The description states that \"Six friends are transported into the Dungeons & Dragons realm and must try to find a way home with the help of their guide 'Dungeon Master'\" [Doc 112849].", "latency_ms": 856.5397262573242, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons' and accurately restates the description that six friends are transported to the realm and try to get home with the help of Dungeon Master. It matches the gold answer and stays faithful to the provided document citation without adding unsupported information.", "main_total_tokens": 2310, "main_prompt_tokens": 2245, "main_completion_tokens": 65, "judge_total_tokens": 536, "judge_prompt_tokens": 446, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is *Nacht der Wölfe* [Doc 2590].", "latency_ms": 718.7042236328125, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 'Nacht der Wölfe' as the movie and matches the gold answer exactly. It is fully supported by the provided information and does not introduce any unsupported claims.", "main_total_tokens": 2479, "main_prompt_tokens": 2438, "main_completion_tokens": 41, "judge_total_tokens": 507, "judge_prompt_tokens": 437, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of *Picnic at Hanging Rock* take place in 1900 [Doc 126191].", "latency_ms": 718.4078693389893, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of *Picnic at Hanging Rock* take place in 1900, which matches the gold answer exactly and is supported by the cited document.", "main_total_tokens": 2296, "main_prompt_tokens": 2268, "main_completion_tokens": 28, "judge_total_tokens": 475, "judge_prompt_tokens": 407, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie described is *June* [Doc 32483].\n\nThe plot details match:\n*   Year: 2004 [Doc 32483]\n*   Protagonist: June, a violinist [Doc 32483]\n*   Setting/Activities: She spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and all-hours daydreaming about the beautiful man that's destined just for her [Doc 32483].\n*   Name: Her name, June, is the name of a calendar month [Doc 32483].", "latency_ms": 4200.3045082092285, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'June', matching the gold answer. The explanation accurately reflects the described plot and character details, and does not introduce contradictions or unsupported claims relative to the question and provided description, so it is both correct and faithful.", "main_total_tokens": 3434, "main_prompt_tokens": 2535, "main_completion_tokens": 899, "judge_total_tokens": 637, "judge_prompt_tokens": 555, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1356.2848567962646, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered 'I don't know,' which is incorrect because the gold answer is known. However, it did not hallucinate or provide incorrect specific information, so it is fully faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 471, "judge_prompt_tokens": 399, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 379.0407180786133, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 7.6. The generated answer says 'I don't know,' which does not match the correct value, so it is incorrect. However, given the model's claimed uncertainty, it is internally consistent with its own reasoning and does not hallucinate a specific rating, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 389, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1281.3003063201904, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.5143371794949736, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.013881288441812, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is factually incorrect. However, there is no sign of reasoning based on external hallucinated content beyond producing a single numerical answer, so faithfulness to its (unstated) internal reasoning is assumed.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 482, "judge_prompt_tokens": 394, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 238.50703239440918, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. The model appears consistent with its own (incorrect) reasoning, so faithfulness is high even though the result is wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 470, "judge_prompt_tokens": 395, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1635.2941989898682, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not correctly identify the show 'Fantômas,' so it is incorrect. However, it does not introduce any incorrect specific claim and faithfully reflects lack of knowledge, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 389, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is *Kill Cruise* [Doc 11205].", "latency_ms": 740.2470111846924, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise' and aligns with the gold answer. It also remains faithful to the implied source citation without adding unsupported details.", "main_total_tokens": 2496, "main_prompt_tokens": 2469, "main_completion_tokens": 27, "judge_total_tokens": 473, "judge_prompt_tokens": 409, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie is *Won't You Be My Neighbor?* [Doc 84838]. The description states that the movie is about Fred Rogers, who \"spoke directly to young children about some of life's most important issues\" through his television program [Doc 84838].", "latency_ms": 3328.5446166992188, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as \"Won't You Be My Neighbor?\" which matches the gold answer exactly. It is faithful to the information implied in the question, adding only a brief explanatory description that is consistent with known context and not contradictory.", "main_total_tokens": 3246, "main_prompt_tokens": 2642, "main_completion_tokens": 604, "judge_total_tokens": 542, "judge_prompt_tokens": 459, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The context provides release years for multiple titles:\n\n*   **YEAR:** 2023 [Doc 107925]\n*   **The Disco Years:** 1994 [Doc 15797]\n*   **663114:** 2012 [Doc 58592]\n*   **Mine:** 2013 [Doc 62815]\n*   **My Favorite Year:** 1982 [Doc 2118]\n*   **1:** 2008 [Doc 44750]", "latency_ms": 4964.985609054565, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question expects the answer '2018'. The generated answer instead lists several different titles and their release years and never gives 2018, so it does not correctly answer the question. It is also unfaithful to the gold answer, since it does not mention or imply 2018 at all.", "main_total_tokens": 3571, "main_prompt_tokens": 2550, "main_completion_tokens": 1021, "judge_total_tokens": 567, "judge_prompt_tokens": 475, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is *Astro Boy* [Doc 112591].", "latency_ms": 4181.069612503052, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as Astro Boy, matching the gold answer exactly. It does not introduce any contradictions or unsupported claims and stays faithful to the provided information.", "main_total_tokens": 3365, "main_prompt_tokens": 2583, "main_completion_tokens": 782, "judge_total_tokens": 482, "judge_prompt_tokens": 416, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "The woman and her young son were held captive for 7 years [Doc 67513].", "latency_ms": 1906.7745208740234, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9463946303571863, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer exactly. It is also faithful to the provided information without adding unsupported details.", "main_total_tokens": 2363, "main_prompt_tokens": 2065, "main_completion_tokens": 298, "judge_total_tokens": 483, "judge_prompt_tokens": 413, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The gold bullion stumbled across by the six armed men who broke into the Brink's-Mat security depot on 26 November 1983 was worth £26m [Doc 133469].", "latency_ms": 2456.0301303863525, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding £26m worth of gold bullion before it was broken into on 26 November 1983. This matches the provided gold answer and is directly supported by the cited document, so it is both correct and faithful.", "main_total_tokens": 2792, "main_prompt_tokens": 2357, "main_completion_tokens": 435, "judge_total_tokens": 522, "judge_prompt_tokens": 433, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6362.814903259277, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score, which is provided in the gold answer (80.5772). The generated response says, \"I don't know,\" which is incorrect relative to the gold. However, the response does not invent any facts or contradict the provided information; it simply expresses a lack of knowledge. Therefore, correctness is 0, while faithfulness to the model's stated uncertainty is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 511, "judge_prompt_tokens": 396, "judge_completion_tokens": 115}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 332.2935104370117, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there was 1 Russian language TV show released in 1997, but the generated answer says 0, which is incorrect relative to the gold. The faithfulness is high because the evaluation accurately reflects that mismatch.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 463, "judge_prompt_tokens": 385, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "The TV show is *Angel Eyes* [Doc 121928].\n\nThe character Yoon Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in this show [Doc 121928].", "latency_ms": 3945.315361022949, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Angel Eyes' and is consistent with the gold answer. The additional detail about the character Yoon Soo-wan undergoing eye transplant surgery aligns with the question and does not contradict the source. Therefore, it is both correct and faithful.", "main_total_tokens": 2918, "main_prompt_tokens": 2251, "main_completion_tokens": 667, "judge_total_tokens": 529, "judge_prompt_tokens": 442, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The context provides information for several movies, each with a different language:\n\n*   *Foreign Language*: fr (French) [Doc 110681]\n*   *Universal Language*: fa (Persian) [Doc 111286]\n*   *LOMO: The Language of Many Others*: de (German) [Doc 84945]\n*   *Machete Language*: es (Spanish) [Doc 57266]\n*   *On Top of the Whale*: nl (Dutch) [Doc 2803]\n*   *The Film*: tr (Turkish) [Doc 100179]\n*   *Sexo*: en (English) [Doc 25596]\n*   *See This Movie*: en (English) [Doc 32691]\n*   *What Is This Film", "latency_ms": 4721.288204193115, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question refers to a single movie whose language is Korean, but the generated answer lists multiple different movies and their languages instead of identifying Korean as the language of the asked-about movie. This is incorrect relative to the gold answer and not faithful to the task of answering the question for the specific movie.", "main_total_tokens": 3197, "main_prompt_tokens": 2175, "main_completion_tokens": 1022, "judge_total_tokens": 628, "judge_prompt_tokens": 538, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 700.2148628234863, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. However, the model clearly provides a numeric answer to the query without contradicting its own reasoning, so it is internally faithful even though factually wrong with respect to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 394, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Based strictly on the context provided, there are no movies listed that were released in 1979.", "latency_ms": 2315.0010108947754, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there are no movies listed that were released in 1979, which matches the gold response. It does not introduce external information and remains fully faithful to the given context.", "main_total_tokens": 2832, "main_prompt_tokens": 2369, "main_completion_tokens": 463, "judge_total_tokens": 486, "judge_prompt_tokens": 415, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "Based strictly on the context provided, the release year for the movie *Back to the Future Part IV* is not mentioned. The context refers to *Back to the Future* (1985) [Doc 4834], *Back to the Future Part II* (1989) [Doc 9405], and *Back to the Future Part III* (1990) [Doc 10621], which is described as \"The final installment\" [Doc 10621].", "latency_ms": 3475.0359058380127, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the release year of a non-existent movie. The gold answer correctly states that 'Back to the Future Part IV' does not exist in the database, directly addressing the query. The generated answer instead talks about the context not mentioning the year and lists the existing trilogy, but it does not answer the question or state that Part IV does not exist. This fails to provide the correct response and does not follow the gold answer's intent, so it is incorrect relative to the task and unfaithful to the gold standard.", "main_total_tokens": 3375, "main_prompt_tokens": 2687, "main_completion_tokens": 688, "judge_total_tokens": 635, "judge_prompt_tokens": 497, "judge_completion_tokens": 138}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that there are no movies from 1980 about the Chernobyl disaster. The generated answer similarly says it cannot find any movies or shows matching the criteria and suggests reconsidering the time period. This aligns logically with the fact that the Chernobyl disaster happened in 1986, making 1980 movies about it highly unlikely. Thus, the generated answer is both correct relative to the question and faithful to the information in the gold answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 558, "judge_prompt_tokens": 434, "judge_completion_tokens": 124}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 231.7972183227539, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer (0) is incorrect compared to the gold answer (4) and thus not faithful to the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 443, "judge_prompt_tokens": 388, "judge_completion_tokens": 55}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 302.63733863830566, "retrieval_recall": 0.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.17723928678404774, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2 while the generated answer is 1, so the generated answer is factually incorrect relative to the provided gold. However, the model directly answers the user’s question without adding unrelated or fabricated details, so it is still faithful to the prompt itself.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 472, "judge_prompt_tokens": 386, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 200.35529136657715, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1 TV show, but the generated answer is 0, so it is factually incorrect relative to the gold. However, the generated answer is internally consistent and does not contradict itself, so it is fully faithful to its own content.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 472, "judge_prompt_tokens": 389, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 231.78482055664062, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a numeric count of TV shows between 2000 and 2010 about Super Mario Bros. The gold answer is 0 and the generated answer is also 0. Given the limited known Super Mario animated series (mostly from the late 1980s to early 1990s) and no widely recognized Super Mario Bros-focused TV shows in that 2000–2010 period, 0 is correct. The generated answer exactly matches the gold and aligns with known information, so it is both correct and faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 530, "judge_prompt_tokens": 391, "judge_completion_tokens": 139}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 247.3280429840088, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Naruto', but the generated answer says 'I don't know,' which is incorrect. However, it does not introduce any fabricated information; it simply admits not knowing, so it is fully faithful to the model's stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 467, "judge_prompt_tokens": 386, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "Based strictly on the provided context, the release date for a Jurassic Park/Power Ranger crossover movie is not mentioned.", "latency_ms": 2208.730697631836, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the movie was not released, implying explicitly that it does not exist. The generated answer claims only that the release date is not mentioned in the provided context, which fails to convey that there was no release at all. Therefore it is factually incorrect relative to the gold and also not faithful to the gold’s key information that the movie was not released.", "main_total_tokens": 2811, "main_prompt_tokens": 2420, "main_completion_tokens": 391, "judge_total_tokens": 514, "judge_prompt_tokens": 408, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Based strictly on the provided context, there is no movie listed in which Barbie fights the Predator. The documents describe various Barbie movies [Doc 68094, Doc 36330, Doc 48119, Doc 72290, Doc 104474, Doc 78562, Doc 55216, Doc 33670] and movies featuring the Predator [Doc 82000, Doc 84103], but none of the plots mention a crossover or confrontation between the two characters.", "latency_ms": 3798.9895343780518, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the movie was not released, implying such a movie might exist but is unreleased. The generated answer instead asserts that there is no movie listed in which Barbie fights the Predator, focusing on the provided documents and denying any crossover, rather than addressing release status. This conflicts with the gold answer and changes the premise from unreleased to nonexistent, so it is neither correct relative to the gold nor faithful to it.", "main_total_tokens": 3167, "main_prompt_tokens": 2424, "main_completion_tokens": 743, "judge_total_tokens": 596, "judge_prompt_tokens": 479, "judge_completion_tokens": 117}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "", "latency_ms": 4984.36713218689, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is empty, so it does not identify any movie and therefore cannot match the gold answer or any correct interpretation of the question. It is neither correct nor meaningfully related to the provided gold answer.", "main_total_tokens": 3607, "main_prompt_tokens": 2584, "main_completion_tokens": 1023, "judge_total_tokens": 463, "judge_prompt_tokens": 390, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The context provides the release years for multiple titles:\n\n*   **YEAR** was released in 2023 [Doc 107925].\n*   **The Disco Years** was released in 1994 [Doc 15797].\n*   **663114** was released in 2012 [Doc 58592].\n*   **Mine** was released in 2013 [Doc 62815].\n*   **My Favorite Year** was released in 1982 [Doc 2118].\n*   **1** was released in 2008 [Doc 44750].\n*   **Paper Year** was released in 2018 [Doc 85553].\n*   **Year of Grace** was released", "latency_ms": 4850.972414016724, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question expects the year 1986, but the generated answer lists multiple titles and years without identifying the correct one and does not mention 1986. It also appears to hallucinate or at least not clearly connect to the asked item, so it is neither correct nor faithful to the gold answer.", "main_total_tokens": 3572, "main_prompt_tokens": 2550, "main_completion_tokens": 1022, "judge_total_tokens": 612, "judge_prompt_tokens": 521, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 331.54296875, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the rating of the movie Forrest Gump, and a specific numeric gold answer (8.466) is provided. The generated answer says only 'I don't know,' which does not provide the requested rating and therefore is incorrect. However, it does not fabricate any information or contradict known facts; it simply admits lack of knowledge, so it is fully faithful to the model's stated knowledge limitations.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 511, "judge_prompt_tokens": 398, "judge_completion_tokens": 113}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is *Mighty Med* [Doc 121125].", "latency_ms": 756.0749053955078, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med' and matches the gold answer exactly. It is fully supported by the question and contains no extraneous or fabricated information beyond a harmless document citation.", "main_total_tokens": 2276, "main_prompt_tokens": 2241, "main_completion_tokens": 35, "judge_total_tokens": 496, "judge_prompt_tokens": 423, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "The seven prisoners escaped from a prison in Argentina [Doc 53582].", "latency_ms": 670.6385612487793, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Argentina as the country where the prison was located and is consistent with the gold answer, showing both correctness and faithfulness.", "main_total_tokens": 2635, "main_prompt_tokens": 2617, "main_completion_tokens": 18, "judge_total_tokens": 466, "judge_prompt_tokens": 406, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The apprentice, Li Nai, in *Sweet Trap* (2024) falls into a trap designed by Jiang Jie [Doc 135231].", "latency_ms": 1974.64919090271, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that the apprentice falls into a trap designed by Jiang Jie, which matches the gold answer exactly. It does not introduce contradictions or unsupported claims beyond naming the apprentice, and remains consistent with the provided information.", "main_total_tokens": 2647, "main_prompt_tokens": 2344, "main_completion_tokens": 303, "judge_total_tokens": 488, "judge_prompt_tokens": 413, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 701.8039226531982, "retrieval_recall": 0.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.19519002499605084, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.2, "ndcg_at_10": 0.3080351663449861, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is numerically incorrect compared to the gold (0 vs 4), so correctness is 0. The answer is a straightforward numeric response without hallucinated explanation or reasoning; it simply states 0, so with respect to its own content it is internally faithful, hence faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 385, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "46", "latency_ms": 299.76892471313477, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '46' is factually incorrect because the gold states there are no movies featuring both Bruce Lee and Steven Seagal together. It is also unfaithful to the gold answer, which explicitly says none can be found.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 473, "judge_prompt_tokens": 395, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of *Tokyo Mater* (2008) is 6.3 [Doc 42750].", "latency_ms": 653.6896228790283, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly gives the rating as 6.3, which matches the gold label, and explicitly references a document as the source. It is both correct and faithful to the provided information.", "main_total_tokens": 2530, "main_prompt_tokens": 2503, "main_completion_tokens": 27, "judge_total_tokens": 475, "judge_prompt_tokens": 405, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The context describes several different styles of racing across various movies:\n\n1.  **Street Racing / Illegal Street Racing:** Movies feature \"street racers\" [Doc 41929, Doc 42313] and \"illegal street race[s]\" [", "latency_ms": 4754.7287940979, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a specific racing style in a single movie, with the gold answer being \"Drift.\" The generated answer instead provides a general discussion of several racing styles and does not identify drifting or give a direct, correct style for the movie in question. It also invents references to multiple documents without tying them to the specific movie or answer, so it is neither correct nor faithful to the required answer.", "main_total_tokens": 3517, "main_prompt_tokens": 2495, "main_completion_tokens": 1022, "judge_total_tokens": 539, "judge_prompt_tokens": 426, "judge_completion_tokens": 113}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the TV show about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is *Confronting a Serial Killer* [Doc 131298].", "latency_ms": 3111.675024032593, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.99, "justification": "The generated answer correctly identifies the title as 'Confronting a Serial Killer,' matching the gold answer. It slightly alters the wording by saying 'TV show' instead of 'movie,' but the core factual content (the title) is accurate and grounded in the provided information, so faithfulness is very high.", "main_total_tokens": 2903, "main_prompt_tokens": 2465, "main_completion_tokens": 438, "judge_total_tokens": 548, "judge_prompt_tokens": 453, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of the movie \"Armageddon\" (1998) is 7.838 [Doc 20603].", "latency_ms": 624.3419647216797, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and does not introduce any unsupported information beyond referencing a document, so it is both correct and faithful.", "main_total_tokens": 2511, "main_prompt_tokens": 2479, "main_completion_tokens": 32, "judge_total_tokens": 476, "judge_prompt_tokens": 411, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 movie that is a thriller and alludes to an extra sense is *The Sixth Sense* [Doc 22098].", "latency_ms": 2177.0734786987305, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6895405204413555, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Sixth Sense' and stays faithful to the information implied by the question and gold answer. The added minor descriptive phrases do not introduce errors.", "main_total_tokens": 2470, "main_prompt_tokens": 2114, "main_completion_tokens": 356, "judge_total_tokens": 490, "judge_prompt_tokens": 422, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The movie *The Transporter* was released in 2002 [Doc 27318].", "latency_ms": 586.383581161499, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer. It is fully supported by the provided information and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 2460, "main_prompt_tokens": 2436, "main_completion_tokens": 24, "judge_total_tokens": 469, "judge_prompt_tokens": 399, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 movie in 'The Transporter' series is *The Transporter Refueled* [Doc 67500].", "latency_ms": 2541.121482849121, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly gives the title as 'The Transporter Refueled', which matches the gold answer exactly, and it does not introduce any contradictory or unsupported information.", "main_total_tokens": 2890, "main_prompt_tokens": 2444, "main_completion_tokens": 446, "judge_total_tokens": 486, "judge_prompt_tokens": 421, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The movie 'The Transporter Refueled' was released in 2015 [Doc 67500].", "latency_ms": 1405.411958694458, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, which matches the gold answer. It does not introduce any contradictory or extraneous information, so it is both correct and faithful.", "main_total_tokens": 2657, "main_prompt_tokens": 2476, "main_completion_tokens": 181, "judge_total_tokens": 483, "judge_prompt_tokens": 406, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "Based strictly on the provided context, there is no romantic comedy listed in which Abraham Lincoln comes back from the dead.\n\nThe documents mention movies where Abraham Lincoln deals with the undead, such as *Abraham Lincoln: Vampire Hunter* [Doc 54943] (Genres: Action, Fantasy, Horror) and *Abraham Lincoln vs. Zombies* [Doc 57213] (Genres: Horror, Action, Thriller), but none of these are categorized as romantic comedies. The only movie listed with the genres 'Romance' and 'Comedy' is *A Romantic Comedy*, which has a plot about a woman named Esra [Doc 47970].", "latency_ms": 4432.667016983032, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a romantic comedy where Abraham Lincoln comes back from the dead. Both the gold and generated answers correctly state that no such movie is found in the provided documents. The generated answer remains strictly within the provided context, citing relevant movies involving Abraham Lincoln (all non-romcom genres) and identifying the only listed romantic comedy, which does not feature Lincoln. This is correct and faithful to the given information.", "main_total_tokens": 3156, "main_prompt_tokens": 2282, "main_completion_tokens": 874, "judge_total_tokens": 635, "judge_prompt_tokens": 522, "judge_completion_tokens": 113}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Two movies mentioned in the context allude to documents or a manuscript written by Ernest Hemingway:\n\n1.  **Havana Darkness** [Doc 90923]: The plot involves traveling to Cuba \"to investigate the origins of a mysterious manuscript supposedly written by acclaimed American novelist, Ernest Hemingway\" [Doc 90923].\n2.  **Lupin the Third: The Hemingway Papers** [Doc 10975]: The plot centers on documents written by Ernest Hemingway that \"contain the location of a remarkable treasure\" [Doc 10975].", "latency_ms": 3983.4134578704834, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The question asks for the title of a single movie, whose answer is simply 'Havana Darkness'. The generated answer instead lists two movies and does not clearly select one title, so it is incorrect. It is mostly faithful to the provided documents in describing both films, but it fails the task requirement to give the one requested title.", "main_total_tokens": 2983, "main_prompt_tokens": 2240, "main_completion_tokens": 743, "judge_total_tokens": 595, "judge_prompt_tokens": 496, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "The TV show is *Doll & Em* [Doc 122259].", "latency_ms": 2462.041139602661, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'Doll & Em' as the TV show. It is fully faithful to the provided information without adding unsupported details.", "main_total_tokens": 2754, "main_prompt_tokens": 2359, "main_completion_tokens": 395, "judge_total_tokens": 471, "judge_prompt_tokens": 405, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The 2007 movie is *Hallam Foe* [Doc 39389].", "latency_ms": 3381.8156719207764, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2007 movie as 'Hallam Foe', which matches the gold answer and is faithful to the question.", "main_total_tokens": 2935, "main_prompt_tokens": 2299, "main_completion_tokens": 636, "judge_total_tokens": 477, "judge_prompt_tokens": 417, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Based strictly on the provided context, there is no information about a person whose last name sounds at first like they are a friend or a foe.", "latency_ms": 2996.3314533233643, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is an explicit label: 'A foe'. The generated answer instead claims there is no information to decide, which contradicts the gold. Thus it is incorrect relative to the task. However, given only the visible prompt and no additional context, the assistant is faithful to what it sees: there is indeed no mention of a specific last name in the provided text, so its reasoning follows from the given context.", "main_total_tokens": 2703, "main_prompt_tokens": 2185, "main_completion_tokens": 518, "judge_total_tokens": 530, "judge_prompt_tokens": 415, "judge_completion_tokens": 115}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "Sam, the main character in the 2017 TV show *Atypical*, is 18 years old [Doc 124583].", "latency_ms": 2007.4098110198975, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old, matching the gold answer. It does not introduce contradictions or unsupported claims beyond that fact, so it is both correct and faithful.", "main_total_tokens": 2528, "main_prompt_tokens": 2190, "main_completion_tokens": 338, "judge_total_tokens": 484, "judge_prompt_tokens": 415, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam, the 18-year-old character in the 2017 TV show *Atypical*, is on the autism spectrum [Doc 124583].", "latency_ms": 1859.5941066741943, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which matches the gold answer. It does not contradict the question or add incorrect information; the extra detail about his age is accurate and does not affect faithfulness to the question.", "main_total_tokens": 2719, "main_prompt_tokens": 2416, "main_completion_tokens": 303, "judge_total_tokens": 497, "judge_prompt_tokens": 418, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "The information regarding the movie *À corde tendue* (released in 2013) and the birthplace of the character Sandra is not available in the provided context.", "latency_ms": 2731.657028198242, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that Sandra was born in Chamonix, but the generated answer claims the information is not available. This is factually incorrect relative to the gold, so correctness is 0. However, the generated answer does not invent a specific incorrect birthplace and instead honestly states that the information is not available in the context, so it is faithful to the (limited) context, yielding a faithfulness score of 1.", "main_total_tokens": 2946, "main_prompt_tokens": 2487, "main_completion_tokens": 459, "judge_total_tokens": 547, "judge_prompt_tokens": 430, "judge_completion_tokens": 117}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481]", "latency_ms": 2386.3790035247803, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains'), and the bracketed document reference does not contradict or add anything unsupported about the content. Both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2780, "main_prompt_tokens": 2337, "main_completion_tokens": 443, "judge_total_tokens": 467, "judge_prompt_tokens": 398, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Tabaluga is the crown-prince of Greenland [Doc 114810].", "latency_ms": 2192.7952766418457, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies that Tabaluga is the crown-prince of Greenland, matching the gold answer exactly and remaining fully consistent with the provided information.", "main_total_tokens": 2917, "main_prompt_tokens": 2577, "main_completion_tokens": 340, "judge_total_tokens": 472, "judge_prompt_tokens": 410, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers\" is *One Afternoon* [Doc 96838].", "latency_ms": 718.7490463256836, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'One Afternoon,' which matches the provided gold answer. It does not introduce contradictions or unsupported claims beyond a harmless citation-style note, so it is both correct and faithful to the given information.", "main_total_tokens": 2426, "main_prompt_tokens": 2379, "main_completion_tokens": 47, "judge_total_tokens": 525, "judge_prompt_tokens": 448, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The movie is **The Life Ahead** [Doc 93260].\n\nDetails:\n*   Language: Italian ('it') [Doc 93260].\n*   Plot: A Holocaust survivor takes in a 12-year-old street kid who recently robbed her [Doc 93260].\n*   Rating: 6.746 [Doc 93260].", "latency_ms": 5505.837440490723, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer names the movie as 'The Life Ahead', matching the gold answer exactly. The supporting details (Italian language, plot about a Holocaust survivor and a 12-year-old street kid who robbed her, and a modest rating) are consistent with the described document and do not introduce contradictions or hallucinations. The response is both correct and faithful to the provided information.", "main_total_tokens": 4160, "main_prompt_tokens": 2785, "main_completion_tokens": 1375, "judge_total_tokens": 573, "judge_prompt_tokens": 468, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary “McLaren” is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 740.1027679443359, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies that the 2016 documentary 'McLaren' is about Bruce McLaren, matching the gold answer. The additional detail that he is a New Zealander and founder of the McLaren motor racing team is accurate and does not contradict the source, so the response is both correct and faithful.", "main_total_tokens": 2290, "main_prompt_tokens": 2255, "main_completion_tokens": 35, "judge_total_tokens": 512, "judge_prompt_tokens": 417, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The context mentions the names of two racing teams:\n\n1.  The **Newman-Haas (Andretti) racing team** [Doc 20456].\n2.  The **Chip Ganassi Racing team** (associated with Scott Dixon) [Doc 85524].", "latency_ms": 3683.5854053497314, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the name of \"his\" racing team, and the gold answer is \"The McLaren Motor Racing team.\" The generated answer instead lists two different teams (Newman-Haas and Chip Ganassi) and does not mention McLaren at all. This is incorrect relative to the gold answer and unfaithful to the required output, as it does not provide the specific team name asked for.", "main_total_tokens": 3235, "main_prompt_tokens": 2549, "main_completion_tokens": 686, "judge_total_tokens": 551, "judge_prompt_tokens": 438, "judge_completion_tokens": 113}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context provides information about several male individuals from different countries, but the specific person referred to as \"he\" is not identified in the question.\n\nKnown nationalities/origins mentioned in the context include:\n*   Karol Wojtyla, who is Polish [Doc 1545].\n*   An American journalist [Doc 4064].\n*   A minister and general in the Croatian government [Doc 85756].\n*   Xavi, who is from Uruguay [Doc 43287].", "latency_ms": 5504.437446594238, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'New Zealand', but the generated answer claims the person is not identified and lists other nationalities instead, so it is incorrect. It is also unfaithful to the gold answer because it does not state New Zealand or align with the expected response.", "main_total_tokens": 3433, "main_prompt_tokens": 2411, "main_completion_tokens": 1022, "judge_total_tokens": 556, "judge_prompt_tokens": 476, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based strictly on the context provided, there is no movie from 2013 that describes John McClane blowing up the Great Wall of China.\n\nThe context mentions one 2013 movie featuring John McClane:\n*   *A Good Day to Die Hard* (2013), where John McClane travels to Moscow to help his son stop a nuclear weapons heist [Doc 58791]. This plot summary does not mention the Great Wall of China.", "latency_ms": 3713.2019996643066, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says there is no such movie in the database. The generated answer correctly concludes that there is no 2013 movie where John McClane blows up the Great Wall of China and references the only relevant 2013 film, A Good Day to Die Hard, whose plot does not involve the Great Wall. It does not invent a movie and stays within the provided context, so it is both correct and faithful.", "main_total_tokens": 3121, "main_prompt_tokens": 2364, "main_completion_tokens": 757, "judge_total_tokens": 607, "judge_prompt_tokens": 491, "judge_completion_tokens": 116}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with that description is *Tomie: Forbidden Fruit* [Doc 28589].", "latency_ms": 3174.2048263549805, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Tomie: Forbidden Fruit', which matches the gold answer. It stays faithful to the question and only adds a citation marker, without introducing contradictions or extraneous fabricated details.", "main_total_tokens": 3100, "main_prompt_tokens": 2470, "main_completion_tokens": 630, "judge_total_tokens": 516, "judge_prompt_tokens": 442, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years in the 2021 movie *Lei mi parla ancora* [Doc 98566].", "latency_ms": 748.859167098999, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, which matches the gold answer. It is fully consistent and faithful to the provided gold information.", "main_total_tokens": 2547, "main_prompt_tokens": 2511, "main_completion_tokens": 36, "judge_total_tokens": 492, "judge_prompt_tokens": 424, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King and Tala search for a legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 2130.070924758911, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that they search for The Book of Souls, matching the gold answer. The added description 'a legendary relic' does not contradict the source and is a reasonable elaboration. Therefore, it is both correct and faithful.", "main_total_tokens": 2999, "main_prompt_tokens": 2749, "main_completion_tokens": 250, "judge_total_tokens": 496, "judge_prompt_tokens": 417, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for the movie \"Zombies on the Moon\" is not available.", "latency_ms": 2361.5987300872803, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year is not available, which is consistent with the gold response indicating the movie is not in the database. It does not introduce unsupported information and remains faithful to the provided context.", "main_total_tokens": 2573, "main_prompt_tokens": 2159, "main_completion_tokens": 414, "judge_total_tokens": 489, "judge_prompt_tokens": 415, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Galactic baker\" is not available in the provided context.", "latency_ms": 2409.7182750701904, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database; the generated answer says the release year is not available in the provided context. Both convey that the release year is unknown/unavailable, which is correct relative to the gold and faithful to the lack of information.", "main_total_tokens": 2941, "main_prompt_tokens": 2586, "main_completion_tokens": 355, "judge_total_tokens": 490, "judge_prompt_tokens": 406, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Return of the Null Pointer\" is not available in the provided context.", "latency_ms": 1791.4702892303467, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available / not in the database, which are effectively equivalent. The generated answer does not hallucinate a year and is consistent with the provided information, so it is both correct and faithful.", "main_total_tokens": 2586, "main_prompt_tokens": 2239, "main_completion_tokens": 347, "judge_total_tokens": 491, "judge_prompt_tokens": 410, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for the movie titled \"Data Science: The Movie\" is not found in the provided context.", "latency_ms": 2540.384292602539, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The gold answer clearly states the movie is not in the database, implying the system should say it does not know. The generated answer incorrectly refers to 'the provided context', which is not mentioned in the gold and changes the basis of the uncertainty; it should instead mirror the database-based limitation. This makes the answer partially unfaithful. It is also incorrect relative to the gold target behavior, which is to state that the movie is not in the database, not that it is missing from context.", "main_total_tokens": 2871, "main_prompt_tokens": 2359, "main_completion_tokens": 512, "judge_total_tokens": 545, "judge_prompt_tokens": 413, "judge_completion_tokens": 132}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the provided context, the release year for the movie \"Python's Revenge 2\" is not available. The context includes titles such as *Python 2* [Doc 28408] and various movies containing \"Revenge\" or \"2,\" but not the exact title requested.", "latency_ms": 3566.7338371276855, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold says the movie is not in the database; the generated answer correctly states that the release year is not available and explains it is not in the provided context. This is both correct and faithful to the gold/context.", "main_total_tokens": 3347, "main_prompt_tokens": 2585, "main_completion_tokens": 762, "judge_total_tokens": 526, "judge_prompt_tokens": 452, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for \"The Last Pixel\" is not available.", "latency_ms": 2408.3468914031982, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year is not available, which is consistent with the gold answer indicating the movie is not in the database. It neither fabricates a date nor contradicts the given context, so it is both correct and faithful.", "main_total_tokens": 2471, "main_prompt_tokens": 2031, "main_completion_tokens": 440, "judge_total_tokens": 490, "judge_prompt_tokens": 409, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for the movie \"The man who ate the sun\" is not available in the provided context.", "latency_ms": 2525.607109069824, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database; the generated answer says the release year is not available in the provided context. Both convey that the release year is unknown/unavailable, so the response is correct and faithful to the gold.", "main_total_tokens": 3246, "main_prompt_tokens": 2770, "main_completion_tokens": 476, "judge_total_tokens": 494, "judge_prompt_tokens": 414, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Based on the provided context, the director of the movie \"The Last Days of Lehman Brothers\" is not mentioned [Doc 46069].", "latency_ms": 617.957353591919, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not provided/mentioned in the available context. This matches the constraint of the database/context and does not fabricate a director name. The generated answer is therefore both correct and faithful to the given information.", "main_total_tokens": 2809, "main_prompt_tokens": 2778, "main_completion_tokens": 31, "judge_total_tokens": 508, "judge_prompt_tokens": 426, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Based strictly on the context provided, the movie \"A Love to Die For\" is not listed, and no director information is available for any of the titles mentioned [Doc 131220, Doc 88425, Doc 14779, Doc 16739, Doc 16243, Doc 24763, Doc 43840, Doc 68848, Doc 15670, Doc 9280, Doc 82231].", "latency_ms": 3026.698112487793, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "Gold says the model should state it cannot answer because the director info is not in the database. The generated answer adds an incorrect claim that the movie is not listed and references specific document IDs that are not in the given gold. This is not strictly grounded, so correctness is 0. It is somewhat aligned in spirit (no director info), but includes extra unverified statements, so faithfulness is slightly reduced.", "main_total_tokens": 3139, "main_prompt_tokens": 2602, "main_completion_tokens": 537, "judge_total_tokens": 598, "judge_prompt_tokens": 483, "judge_completion_tokens": 115}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provides information about the movie \"Journey into Gaza\" (Title, Year, Rating, Plot, etc.) [Doc 111360], but it does not mention who directed the film.", "latency_ms": 2207.820177078247, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not mention who directed the film and aligns with the gold answer that the director information is not in the database. It does not introduce any unsupported details and remains faithful to the provided context.", "main_total_tokens": 2953, "main_prompt_tokens": 2638, "main_completion_tokens": 315, "judge_total_tokens": 509, "judge_prompt_tokens": 432, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not contain information about who directed \"The Last Night at Tremore Beach\" [Doc 135098].", "latency_ms": 2455.7464122772217, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not provide the director information, matching the gold response in meaning. It does not hallucinate a director and remains faithful to the given database limitation.", "main_total_tokens": 2649, "main_prompt_tokens": 2263, "main_completion_tokens": 386, "judge_total_tokens": 491, "judge_prompt_tokens": 422, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "", "latency_ms": 5134.6495151519775, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is empty, so it does not provide the correct movie title ('Rise of the Mummy') and does not respond to the question at all. Therefore it is both incorrect and unfaithful to the gold answer.", "main_total_tokens": 3338, "main_prompt_tokens": 2315, "main_completion_tokens": 1023, "judge_total_tokens": 469, "judge_prompt_tokens": 392, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death and flees to escape her abusive spouse is *Stranger in My Bed* [Doc 35237].\n\nThe plot description states: \"To escape her abusive spouse, Sarah fakes her own death and flees\" [Doc 35237].", "latency_ms": 733.767032623291, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Stranger in My Bed' and correctly explains that Sarah fakes her own death to escape her abusive spouse. It does not introduce any contradictions or unsupported claims and stays fully grounded in the provided description.", "main_total_tokens": 2406, "main_prompt_tokens": 2340, "main_completion_tokens": 66, "judge_total_tokens": 536, "judge_prompt_tokens": 456, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The exact description, \"A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years,\" is associated with the title *Game of Silence* [Doc 123938]. However, *Game of Silence* is listed as a TV show, not a movie [Doc 123938].\n\nTherefore, the specific movie requested is not found in the provided context.", "latency_ms": 4262.034893035889, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks: 'In which movie does the following happen...' and the gold answer is 'Game of Silence'. The generated answer states that the description is associated with 'Game of Silence' but then incorrectly concludes that the specific movie is not found, instead of simply answering 'Game of Silence'. This makes the answer incorrect. However, it accurately reflects the information from the provided document (that Game of Silence matches the description and is categorized as a TV show), so it is faithful to the cited context, even though it mishandles the task requirement.", "main_total_tokens": 2833, "main_prompt_tokens": 2046, "main_completion_tokens": 787, "judge_total_tokens": 615, "judge_prompt_tokens": 473, "judge_completion_tokens": 142}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie is **The Craigslist Killer** [Doc 53039].\n\nThe plot description states it is \"Based on a true story. Philip Markoff, a charismatic and popular med student at Boston University, leads a double life as a brutal and cruel sexual deviant who abuses prostitutes he finds via Craigslist\" [Doc 53039].", "latency_ms": 818.3877468109131, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer', which matches the gold answer. The supporting explanation about the plot and source document is consistent with the described query and does not introduce contradictions or hallucinations, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2332, "main_prompt_tokens": 2259, "main_completion_tokens": 73, "judge_total_tokens": 543, "judge_prompt_tokens": 459, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The event described happens in the movie *Wood Job!* [Doc 66094].\n\nThe plot states: \"After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.\" [Doc 66094].", "latency_ms": 833.7016105651855, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", matching the gold answer exactly. It is also faithful to the provided plot description and does not introduce unsupported information.", "main_total_tokens": 3139, "main_prompt_tokens": 3082, "main_completion_tokens": 57, "judge_total_tokens": 516, "judge_prompt_tokens": 450, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure, is *Sesame Street: The Magical Wand Chase* [Doc 78373].", "latency_ms": 772.1712589263916, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Sesame Street: The Magical Wand Chase,' which matches the gold answer exactly. It does not introduce any contradictions or extraneous fabricated details beyond a restatement of the question, so it is fully faithful.", "main_total_tokens": 2619, "main_prompt_tokens": 2564, "main_completion_tokens": 55, "judge_total_tokens": 541, "judge_prompt_tokens": 460, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a rock fall traps his parents inside a burning car, forcing a 7-year-old boy to find the hero within him to save their lives, is *Beyond* [Doc 74597].", "latency_ms": 740.4346466064453, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Beyond', which matches the gold answer, and it restates the query faithfully without adding unsupported details.", "main_total_tokens": 2215, "main_prompt_tokens": 2167, "main_completion_tokens": 48, "judge_total_tokens": 511, "judge_prompt_tokens": 451, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman, fired from a financial corporation during the Asia crisis, returns home with no money is *6ixtynin9* [Doc 22478].", "latency_ms": 3381.2077045440674, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', which matches the gold answer exactly. It is faithful to the provided information and does not introduce contradictions or unsupported claims.", "main_total_tokens": 2910, "main_prompt_tokens": 2279, "main_completion_tokens": 631, "judge_total_tokens": 509, "judge_prompt_tokens": 439, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie in which a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort, a hedonistic island paradise for swingers, and the club's staff must stop the violence or hide it, is *Club Dread* [Doc 32410].", "latency_ms": 756.352424621582, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' matching the gold answer exactly. It remains faithful to the given information, only restating the description and naming the correct film without adding unsupported details.", "main_total_tokens": 2655, "main_prompt_tokens": 2596, "main_completion_tokens": 59, "judge_total_tokens": 552, "judge_prompt_tokens": 479, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is *Toilet* [Doc 48873].", "latency_ms": 802.9816150665283, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Toilet', which matches the gold answer exactly. It stays faithful to the question and does not introduce contradictions or extraneous, misleading information.", "main_total_tokens": 2874, "main_prompt_tokens": 2818, "main_completion_tokens": 56, "judge_total_tokens": 533, "judge_prompt_tokens": 465, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie in which this happens is *Electric Angel* [Doc 1024].\n\nThe description states: \"A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century\" [Doc 1024].", "latency_ms": 849.5585918426514, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Electric Angel', which matches the gold answer. It also stays faithful to the provided description, restating it without adding incorrect details. Therefore, both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2649, "main_prompt_tokens": 2579, "main_completion_tokens": 70, "judge_total_tokens": 559, "judge_prompt_tokens": 482, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 in the provided context is *Bottom of the Water*, which has a rating of 4.2 [Doc 106899].", "latency_ms": 3954.9715518951416, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold label indicates 'Mega Ape' as the lowest rated movie from 2023, while the answer states 'Bottom of the Water'. It is also unfaithful to the gold/reference since it introduces a different movie and rating not aligned with the provided gold answer.", "main_total_tokens": 3292, "main_prompt_tokens": 2218, "main_completion_tokens": 1074, "judge_total_tokens": 506, "judge_prompt_tokens": 415, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 287.9676818847656, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Tea House', but the generated answer says 'I don't know,' which is incorrect. However, the model does not hallucinate an alternative answer; it admits not knowing, so it remains faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 386, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 617.5661087036133, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific movie title, but the generated answer simply states 'I don't know,' which does not provide the correct movie. Thus correctness is 0. However, the answer does not fabricate or contradict any given information; it honestly admits lack of knowledge, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 395, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is *Read Lips*, with a rating of 4.5 [Doc 354]. The language of this movie is cn [Doc 354].", "latency_ms": 2981.438636779785, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the language is English, while the generated answer claims it is 'cn', so the content is factually incorrect with respect to the gold. The generated answer also invents a specific movie title and language not supported by the gold, so it is not faithful to the reference answer or expected information.", "main_total_tokens": 2854, "main_prompt_tokens": 2234, "main_completion_tokens": 620, "judge_total_tokens": 515, "judge_prompt_tokens": 421, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity score listed in the context is *Popular*, with a popularity rating of 4.5113 [Doc 115196].", "latency_ms": 4500.586748123169, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'Law & Order: Special Victims Unit', but the generated answer claims the most popular 1999 show is 'Popular', which is incorrect. It also references a specific context document and popularity score that contradict the gold label, so it is not faithful to the correct/reference information.", "main_total_tokens": 3836, "main_prompt_tokens": 2872, "main_completion_tokens": 964, "judge_total_tokens": 518, "judge_prompt_tokens": 426, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 733.8809967041016, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and repeats it correctly. It is fully faithful to the provided gold answer without adding any contradictory or extraneous claims.", "main_total_tokens": 2373, "main_prompt_tokens": 2352, "main_completion_tokens": 21, "judge_total_tokens": 467, "judge_prompt_tokens": 400, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in the following years, according to the context:\n\n*   1999 [Doc 22481]\n*   2019 [Doc 88811]\n*   2025 [Doc 112077]", "latency_ms": 702.6069164276123, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer specifies 2019 as the release year, but the generated answer lists three different years (1999, 2019, 2025) without identifying which is correct. This does not provide a single correct answer and is therefore incorrect. It is also unfaithful to the gold answer by introducing extra, conflicting years beyond the specified 2019.", "main_total_tokens": 2416, "main_prompt_tokens": 2347, "main_completion_tokens": 69, "judge_total_tokens": 542, "judge_prompt_tokens": 437, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 131.2720775604248, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 279 votes, while the generated answer is 3 votes, which is incorrect. The model clearly attempted to answer the question (not refusing or hallucinating a different task), so it is faithful to the user query but factually wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 466, "judge_prompt_tokens": 383, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "12", "latency_ms": 269.2980766296387, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (12), so it is both correct and faithful to the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 436, "judge_prompt_tokens": 383, "judge_completion_tokens": 53}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 1118.4892654418945, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie was released in 2009, matching the gold answer. It is also faithful since it directly answers the question without adding unsupported or contradictory information.", "main_total_tokens": 2451, "main_prompt_tokens": 2424, "main_completion_tokens": 27, "judge_total_tokens": 478, "judge_prompt_tokens": 410, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" (2010) is English (\"en\") [Doc 48035].", "latency_ms": 1775.4559516906738, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies English as the original language of 'The King's Speech,' matching the gold answer 'en'. It is consistent with the provided information and does not introduce any contradictions, so it is both correct and faithful.", "main_total_tokens": 2935, "main_prompt_tokens": 2654, "main_completion_tokens": 281, "judge_total_tokens": 485, "judge_prompt_tokens": 409, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "1987 [Doc 6993]", "latency_ms": 2958.083152770996, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and contains no contradictions or unsupported content beyond the correct year.", "main_total_tokens": 3400, "main_prompt_tokens": 3031, "main_completion_tokens": 369, "judge_total_tokens": 438, "judge_prompt_tokens": 387, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "47 Hours to Live", "latency_ms": 8347.423315048218, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer, '47 Hours to Live,' is incorrect. The correct movie where classmates play a survival game using their cell phones is 'Brain Fluid Explosion Girl.' The response is thus neither correct nor faithful to the gold answer.", "main_total_tokens": 4410, "main_prompt_tokens": 2847, "main_completion_tokens": 1563, "judge_total_tokens": 470, "judge_prompt_tokens": 393, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 5351.617097854614, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer. The TV show where six friends rely on a Dungeon Master to get home is indeed 'Dungeons & Dragons', so it is both correct and faithful.", "main_total_tokens": 3765, "main_prompt_tokens": 2887, "main_completion_tokens": 878, "judge_total_tokens": 464, "judge_prompt_tokens": 394, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 6525.847673416138, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct movie title for the described plot. It is both correct and fully faithful to the reference.", "main_total_tokens": 4175, "main_prompt_tokens": 3021, "main_completion_tokens": 1154, "judge_total_tokens": 465, "judge_prompt_tokens": 403, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of *Picnic at Hanging Rock* take place in 1900 [Doc 126191].", "latency_ms": 3660.2375507354736, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of 'Picnic at Hanging Rock' take place in 1900, which matches the gold answer. It is fully consistent with the provided information and does not introduce any unsupported claims.", "main_total_tokens": 3477, "main_prompt_tokens": 2839, "main_completion_tokens": 638, "judge_total_tokens": 483, "judge_prompt_tokens": 407, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June", "latency_ms": 2578.5131454467773, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question describes a 2004 movie whose main character is a young violinist named after a calendar month, and the gold answer is 'June'. The generated answer is also 'June', which exactly matches the gold and correctly satisfies the described clue.", "main_total_tokens": 564, "main_prompt_tokens": 127, "main_completion_tokens": 437, "judge_total_tokens": 514, "judge_prompt_tokens": 433, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 216.40539169311523, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered 'I don't know,' which is incorrect because the gold answer is a specific film title. However, the response is faithful in that it does not fabricate an answer or provide unsupported information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 470, "judge_prompt_tokens": 399, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 300.88162422180176, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates the correct rating is 7.6, but the generated answer says 'I don't know,' which is incorrect. However, the model does not hallucinate an incorrect rating; it honestly admits not knowing, so it is fully faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 389, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "12", "latency_ms": 4734.424829483032, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.5143371794949736, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.013881288441812, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model output (12) does not match the gold answer (4), so it is incorrect. However, it appears to faithfully answer the user’s question directly rather than changing or ignoring it, so faithfulness is high even though the content is wrong.", "main_total_tokens": 2472, "main_prompt_tokens": 1567, "main_completion_tokens": 905, "judge_total_tokens": 476, "judge_prompt_tokens": 394, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 379.0750503540039, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there are 2 such TV shows, but the generated answer claims there are none, so it is factually incorrect. However, the model does not fabricate any numbers or details and instead admits inability to find matches, so its response is internally faithful to its own (stated) limitations.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 497, "judge_prompt_tokens": 404, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 271.2421417236328, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer admits not knowing, which is faithful to the model's uncertainty but does not provide the correct gold answer ('Fantômas'), so correctness is 0 while faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 460, "judge_prompt_tokens": 389, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise [Doc 11205]", "latency_ms": 3531.1007499694824, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, identifying the movie as 'Kill Cruise'. It is correct and fully consistent with the provided information.", "main_total_tokens": 3566, "main_prompt_tokens": 3048, "main_completion_tokens": 518, "judge_total_tokens": 452, "judge_prompt_tokens": 393, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor? [Doc 84838]", "latency_ms": 5674.258470535278, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly gives the movie title 'Won't You Be My Neighbor?', which matches the gold answer exactly. However, it appends an extraneous bracketed reference '[Doc 84838]', which is not part of the true title and suggests some minor unfaithfulness to the expected plain-title output, so faithfulness is slightly reduced.", "main_total_tokens": 4237, "main_prompt_tokens": 3153, "main_completion_tokens": 1084, "judge_total_tokens": 516, "judge_prompt_tokens": 414, "judge_completion_tokens": 102}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The release years for the titles mentioned in the context are:\n\n*   **2018** for *Won't You Be My Neighbor?* [Doc 84838]\n*   **1981** for *The Neighborhood* [Doc 1669] and *Neighbors* [Doc 1423]\n*   **2007** for *The Neighbor* [Doc 40654]\n*   **2001** for *The Neighborhood* [Doc 26789]\n*   **1993** for *The Neighbor* [Doc 14860]\n*   **2019** for *The Neighbor* (TV) [Doc 127540]\n*   **2020** for *The Neighbor* [Doc 94297] and *The Neighbor in the Window* [Doc 93292]\n*   **2005** for *The Neighborly Thing* [Doc 3", "latency_ms": 7396.566867828369, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a single release year, with the gold answer being 2018. The generated response lists many different titles and their release years instead of directly answering with a single year. It is therefore not correct. It is also not faithful to the task of giving the specific release year requested, as it introduces multiple other works and years beyond the needed answer.", "main_total_tokens": 4238, "main_prompt_tokens": 2931, "main_completion_tokens": 1307, "judge_total_tokens": 661, "judge_prompt_tokens": 557, "judge_completion_tokens": 104}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy [Doc 112591]", "latency_ms": 7929.948806762695, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 'Astro Boy' as the animated TV series about a young boy robot in 2030 AD and matches the gold answer exactly. It does not introduce any contradictions or extraneous claims beyond a harmless document reference tag.", "main_total_tokens": 4626, "main_prompt_tokens": 3196, "main_completion_tokens": 1430, "judge_total_tokens": 475, "judge_prompt_tokens": 395, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "The woman and her young son were held captive for 7 years [Doc 67513].", "latency_ms": 4423.454761505127, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9463946303571863, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states they were held captive for 7 years, matching the gold answer with no contradictions or unsupported additions.", "main_total_tokens": 3462, "main_prompt_tokens": 2684, "main_completion_tokens": 778, "judge_total_tokens": 469, "judge_prompt_tokens": 413, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26m [Doc 133469].", "latency_ms": 2656.926155090332, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The answer correctly states that the Brink's-Mat security depot was holding gold worth £26m before the break-in, and it faithfully reflects the provided gold answer without adding unsupported details.", "main_total_tokens": 2827, "main_prompt_tokens": 2370, "main_completion_tokens": 457, "judge_total_tokens": 484, "judge_prompt_tokens": 416, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 560.5707168579102, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model said it did not know the answer, while a correct numeric value (80.5772) exists. This makes the answer incorrect. However, the response is faithful to the model’s uncertainty and does not fabricate an answer, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 396, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 263.16189765930176, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer states that there was 1 Russian language TV show released in 1997, while the generated answer claims it cannot find any such shows. This is factually incorrect relative to the gold, so correctness is 0. The model does stay within its claimed inability to find information (it does not hallucinate a number), so it is somewhat faithful to its own stated limitations, but unfaithful to the ground truth, yielding a middling faithfulness score.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 522, "judge_prompt_tokens": 394, "judge_completion_tokens": 128}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes [Doc 121928].", "latency_ms": 6138.011693954468, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the TV show as 'Angel Eyes', which matches the gold answer. However, it unnecessarily appends '[Doc 121928]', which is not part of the TV show title and suggests reference to an external document, reducing strict faithfulness slightly.", "main_total_tokens": 3990, "main_prompt_tokens": 2880, "main_completion_tokens": 1110, "judge_total_tokens": 490, "judge_prompt_tokens": 403, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The context provides several titles with different languages:\n\n*   **en** (English) for *Angel Eyes* (1993, 2001), *Eyes of an Angel* (1991), *Angel from Hell* (2016), *Lost Angel* (2013), and *Angel Heart* (1987) [Doc 14842, Doc 25900, Doc 11997, Doc 124271, Doc 62745, Doc 7049].\n*   **ko** (Korean) for *Angel Eyes* (2014) and *Angel's Last Mission: Love* (2019) [Doc 121928, Doc 127211].\n*   **pl** (Polish) for *Angel in Love* (2005) [Doc 34006].\n*   **fr** (French) for *Angel-A* (2005) [Doc 34844].\n*   **zh** (Chinese) for *Angel Heart* (1995) [Doc 16597].", "latency_ms": 9188.196659088135, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question asks for the language of a specific movie whose gold answer is Korean. The generated answer does not select a single language or identify which movie the question refers to; instead, it lists multiple possible languages for various similarly titled movies. Thus it fails to correctly answer the question (correctness 0). It is partially faithful in that it accurately restates the contextual information about multiple titles and their languages, but it does not commit to the required single answer, so faithfulness is only partial (0.5).", "main_total_tokens": 4938, "main_prompt_tokens": 3151, "main_completion_tokens": 1787, "judge_total_tokens": 722, "judge_prompt_tokens": 586, "judge_completion_tokens": 136}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 619.6200847625732, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 2 such movies, while the generated answer says there are none, which is factually incorrect. However, the model does not hallucinate specific incorrect movies or data; it simply states inability to find matches, so it remains faithful to its (albeit limited) internal retrieval.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 496, "judge_prompt_tokens": 403, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 3138.7219429016113, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that there are no movies from 1979 in the database. The generated answer says simply \"I don't know,\" which contradicts the specific database-based reason provided in the gold. It is thus not faithful. It is also not considered correct relative to the gold, since the expected response includes the database limitation, not generic ignorance.", "main_total_tokens": 1196, "main_prompt_tokens": 605, "main_completion_tokens": 591, "judge_total_tokens": 500, "judge_prompt_tokens": 399, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "I am sorry, but the provided context does not contain information about a movie titled \"Back to the Future part IV.\" The context includes information for \"Back to the Future\" [Doc 4834], \"Back to the Future Part II\" [Doc 9405], and \"Back to the Future Part III\" [Doc 10621].", "latency_ms": 5148.674249649048, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The correct answer is that there is no Back to the Future Part IV in the database, i.e., it does not exist as a known movie. The generated answer states that the provided context does not contain information about a movie titled Back to the Future part IV, and lists the existing parts I–III. This is effectively equivalent and does not assert any incorrect release year, so it is correct. It is also faithful to the referenced context (mentioning only the three existing movies), so both correctness and faithfulness merit full scores.", "main_total_tokens": 4189, "main_prompt_tokens": 3323, "main_completion_tokens": 866, "judge_total_tokens": 608, "judge_prompt_tokens": 470, "judge_completion_tokens": 138}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there are no movies from 1980 about the Chernobyl disaster, which did not occur until 1986. The generated answer is faithful to the question because it clearly indicates that no such movies or shows matching the criteria can be found, matching the gold answer's substance. It does not hallucinate a movie or a popularity score and even hints that the event and date may not align.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 553, "judge_prompt_tokens": 434, "judge_completion_tokens": 119}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 216.24755859375, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The gold answer says there are 4 titles, while the generated answer claims there are none, so the correctness is 0. The model does not hallucinate a specific incorrect number or list; it only states it cannot find any, which is broadly consistent with a failure to retrieve rather than inventing facts, so faithfulness is relatively high though not perfect.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 502, "judge_prompt_tokens": 397, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 5871.121644973755, "retrieval_recall": 0.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.17723928678404774, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The correct answer is 2, but the generated answer says there are none, so it is factually incorrect. It is somewhat faithful in that it does not fabricate a specific incorrect number or titles, but instead claims not to find any, which reflects a kind of uncertainty rather than hallucinating details; however, it still conflicts directly with the known gold answer.", "main_total_tokens": 4737, "main_prompt_tokens": 3714, "main_completion_tokens": 1023, "judge_total_tokens": 500, "judge_prompt_tokens": 395, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 278.8970470428467, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show, but the generated answer claims there were none, which is factually incorrect relative to the gold. However, the generated answer does not introduce fabricated details; it simply states it cannot find any, so it is faithful to its own (albeit limited) retrieval and does not hallucinate specifics.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 498, "judge_prompt_tokens": 398, "judge_completion_tokens": 100}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 184.43799018859863, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0, and the generated answer states that it cannot find any movies or shows matching those criteria, which semantically implies there are none in that period. This matches the gold answer, so it is correct. The response is fully grounded in the given QA pair, so it is faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 493, "judge_prompt_tokens": 400, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 184.922456741333, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto', a specific and correct response. The generated answer says 'I don't know', which is incorrect given the gold. However, the generated answer does not introduce any hallucinated facts and is internally consistent, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 470, "judge_prompt_tokens": 386, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "Based strictly on the context provided, there is no information about a Jurassic Park/Power Ranger crossover movie.", "latency_ms": 4865.482568740845, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.0, "faithfulness_score": 0.0, "justification": "The gold answer clarifies that the movie was not released. The generated answer instead claims there is no information about such a movie in the provided context, which fails to give the correct fact (that it was not released). This contradicts the gold answer, so it is neither correct nor faithful to the reference.", "main_total_tokens": 3818, "main_prompt_tokens": 2982, "main_completion_tokens": 836, "judge_total_tokens": 503, "judge_prompt_tokens": 406, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "", "latency_ms": 9451.703786849976, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer correctly states that no such movie exists and thus was not released, which matches reality. The evaluation faithfully reflects the relationship between the question and the gold answer.", "main_total_tokens": 4894, "main_prompt_tokens": 3195, "main_completion_tokens": 1699, "judge_total_tokens": 448, "judge_prompt_tokens": 383, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The 4th oldest Star Wars movie listed in the context is When Star Wars Ruled the World (2004) [Doc 32997].\n\nThe movies listed, sorted by year, are:\n1. The Empire Strikes Back (1980) [Doc 3]\n2. Return of the Jedi (1983) [Doc 2858]\n3. The Story of Star Wars (2004) [Doc 32101]\n4. When Star Wars Ruled the World (2004) [Doc 32997]", "latency_ms": 7025.744438171387, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'Star Wars: Droids - The Great Heep', but the generated answer gives 'When Star Wars Ruled the World (2004)' as the 4th oldest movie, so it is incorrect. It also fabricates a sorted list of movies inconsistent with the gold, so it is not faithful to the expected answer or context.", "main_total_tokens": 4556, "main_prompt_tokens": 3176, "main_completion_tokens": 1380, "judge_total_tokens": 600, "judge_prompt_tokens": 497, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The release years found in the context are: 1983 [Doc 3568], 1999 [Doc 23388], 2004 [Doc 32997, Doc 32101, Doc 31821, Doc 32312], 2005 [Doc 117312, Doc 35893], 2006 [Doc 37990], 2007 [Doc 41419, Doc 39908], 2014 [Doc 65897], and 2015 [Doc 67472].", "latency_ms": 6788.132667541504, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly specifies the release year as 1986. The generated answer does not mention 1986 at all and lists several other years instead. It is therefore incorrect. However, it appears to accurately reflect the years found in the given (though unseen) context documents, so it is faithful to its cited context even though it does not match the gold answer.", "main_total_tokens": 4360, "main_prompt_tokens": 3009, "main_completion_tokens": 1351, "judge_total_tokens": 589, "judge_prompt_tokens": 484, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 263.2484436035156, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct rating is 8.466, but the generated answer says 'I don't know', which is incorrect. However, it does not fabricate any information, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 472, "judge_prompt_tokens": 398, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med [Doc 121125]", "latency_ms": 5442.9426193237305, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the correct title of the TV show, 'Mighty Med'. It is correct and faithful to the gold answer, with no additional incorrect information.", "main_total_tokens": 3792, "main_prompt_tokens": 2830, "main_completion_tokens": 962, "judge_total_tokens": 465, "judge_prompt_tokens": 400, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina [Doc 53582].", "latency_ms": 3491.7423725128174, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Argentina as the country, matching the gold answer exactly and not adding unsupported information.", "main_total_tokens": 3823, "main_prompt_tokens": 3256, "main_completion_tokens": 567, "judge_total_tokens": 450, "judge_prompt_tokens": 398, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The apprentice falls into a trap designed by Jiang Jie [Doc 135231].", "latency_ms": 4234.115362167358, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the apprentice falls into Jiang Jie's trap, which matches the gold answer. It does not introduce unsupported information, so it is fully faithful to the provided data.", "main_total_tokens": 3548, "main_prompt_tokens": 2927, "main_completion_tokens": 621, "judge_total_tokens": 469, "judge_prompt_tokens": 400, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 4812.021493911743, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.19519002499605084, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.2, "ndcg_at_10": 0.3080351663449861, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 4, but the generated response claims there are no movies or shows matching the criteria, which directly conflicts with the gold. This makes the answer factually incorrect. However, the model does not fabricate specific movie titles or counts; it simply states its inability to find such movies, which is consistent with its own claimed limitations, so it is faithful to its internal reasoning.", "main_total_tokens": 4822, "main_prompt_tokens": 3799, "main_completion_tokens": 1023, "judge_total_tokens": 503, "judge_prompt_tokens": 394, "judge_completion_tokens": 109}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 4215.9388065338135, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there are no known movies (or shows) featuring both Bruce Lee and Steven Seagal. The generated answer is fully consistent with the gold answer and does not add incorrect details, so it is both correct and faithful.", "main_total_tokens": 3174, "main_prompt_tokens": 2413, "main_completion_tokens": 761, "judge_total_tokens": 489, "judge_prompt_tokens": 406, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 1103.7864685058594, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (6.3) and does not introduce any contradictions or extraneous claims, so it is both correct and faithful.", "main_total_tokens": 2541, "main_prompt_tokens": 2516, "main_completion_tokens": 25, "judge_total_tokens": 466, "judge_prompt_tokens": 403, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The movie *Tokyo Mater* features a drift-style race [Doc 42750].", "latency_ms": 5164.088010787964, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'Drift'. The generated answer states that the movie features a drift-style race, which matches the gold label exactly, so it is correct. It is also faithful, as it directly answers the question without adding contradictory or unsupported details.", "main_total_tokens": 4049, "main_prompt_tokens": 3225, "main_completion_tokens": 824, "judge_total_tokens": 477, "judge_prompt_tokens": 395, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer [Doc 131298]", "latency_ms": 5198.219299316406, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, 'Confronting a Serial Killer', and does not introduce any contradictory or extraneous information beyond a harmless document reference, so it is both correct and faithful.", "main_total_tokens": 3922, "main_prompt_tokens": 3077, "main_completion_tokens": 845, "judge_total_tokens": 490, "judge_prompt_tokens": 418, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838 [Doc 20603]", "latency_ms": 3126.7778873443604, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold value exactly (7.838) and does not introduce unsupported claims or deviations.", "main_total_tokens": 3051, "main_prompt_tokens": 2492, "main_completion_tokens": 559, "judge_total_tokens": 443, "judge_prompt_tokens": 395, "judge_completion_tokens": 48}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense [Doc 22098]", "latency_ms": 5319.246292114258, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6895405204413555, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title 'The Sixth Sense' and does not introduce any contradictions or unsupported content apart from a harmless document reference tag.", "main_total_tokens": 3634, "main_prompt_tokens": 2700, "main_completion_tokens": 934, "judge_total_tokens": 467, "judge_prompt_tokens": 400, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002 [Doc 27318]", "latency_ms": 2671.6461181640625, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that The Transporter was released in 2002, which matches the gold answer exactly and does not introduce any contradictions or extraneous claims.", "main_total_tokens": 3383, "main_prompt_tokens": 3039, "main_completion_tokens": 344, "judge_total_tokens": 452, "judge_prompt_tokens": 389, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is The Transporter Refueled [Doc 67500].", "latency_ms": 4378.703594207764, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, giving the correct title 'The Transporter Refueled'. It is fully supported by the provided information and does not introduce contradictions, so it is both correct and faithful.", "main_total_tokens": 3448, "main_prompt_tokens": 3061, "main_completion_tokens": 387, "judge_total_tokens": 492, "judge_prompt_tokens": 418, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015 [Doc 67500]", "latency_ms": 4802.305459976196, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer gives the correct year, 2015, matching the gold. However, it introduces an unsupported citation tag '[Doc 67500]' that is not grounded in the provided information, so it is not fully faithful to the context/instructions.", "main_total_tokens": 3794, "main_prompt_tokens": 3083, "main_completion_tokens": 711, "judge_total_tokens": 475, "judge_prompt_tokens": 393, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "Based strictly on the context provided, there is no romantic comedy listed in which Abraham Lincoln comes back from the dead. The documents mention Lincoln in action, horror, fantasy, history, and drama genres [Doc 54943, Doc 59279, Doc 58110, Doc 57213, Doc 55023], and they mention a movie titled *A Romantic Comedy* that does not feature Abraham Lincoln [Doc 47970].", "latency_ms": 6425.1158237457275, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a romantic comedy where Abraham Lincoln comes back from the dead; the gold answer says no such movie description can be found. The generated answer correctly states that, based on the context, there is no romantic comedy in which Lincoln comes back from the dead, and it further notes that Lincoln appears only in other genres while a movie titled 'A Romantic Comedy' does not feature him. This matches the gold answer’s conclusion and stays grounded in the cited documents, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 4004, "main_prompt_tokens": 2890, "main_completion_tokens": 1114, "judge_total_tokens": 613, "judge_prompt_tokens": 478, "judge_completion_tokens": 135}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Havana Darkness [Doc 90923]\nLupin the Third: The Hemingway Papers [Doc 10975]", "latency_ms": 6023.368358612061, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0.5, "faithfulness_score": 0.5, "justification": "The question asks for the title of a movie that alludes to a manuscript written by Ernest Hemingway. The gold answer is 'Havana Darkness.' The generated answer lists two items: 'Havana Darkness [Doc 90923]' and 'Lupin the Third: The Hemingway Papers [Doc 10975].' Including 'Havana Darkness' is correct, but adding a second, unsupported movie title is incorrect and unfaithful to the gold answer. Therefore, the response is only partially correct and partially faithful.", "main_total_tokens": 3894, "main_prompt_tokens": 2819, "main_completion_tokens": 1075, "judge_total_tokens": 550, "judge_prompt_tokens": 411, "judge_completion_tokens": 139}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 4032.6099395751953, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Doll & Em'), correctly identifying the TV show and faithfully using the provided information.", "main_total_tokens": 3495, "main_prompt_tokens": 2996, "main_completion_tokens": 499, "judge_total_tokens": 449, "judge_prompt_tokens": 392, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam Foe [Doc 39389]", "latency_ms": 5893.266916275024, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the 2007 movie where Hallam spies on people to find the cause of his mother’s death. The correct answer is 'Hallam Foe', and the generated answer matches this exactly, with only an extraneous citation tag that does not alter the content. Thus it is fully correct and faithful to the intent and facts.", "main_total_tokens": 4076, "main_prompt_tokens": 2929, "main_completion_tokens": 1147, "judge_total_tokens": 510, "judge_prompt_tokens": 409, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Based strictly on the context provided, there is no information about a specific person whose last name is mentioned and sounds like \"friend\" or \"foe.\"", "latency_ms": 8780.070066452026, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'A foe', but the generated answer says there is no information, so it is incorrect. However, the answer is faithful to the provided context, as no additional or contradictory details are invented.", "main_total_tokens": 4601, "main_prompt_tokens": 3102, "main_completion_tokens": 1499, "judge_total_tokens": 491, "judge_prompt_tokens": 417, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "Sam is an 18-year-old [Doc 124583].", "latency_ms": 5517.013072967529, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old, which matches the gold answer, and it does not introduce any contradictory or extraneous information beyond a citation placeholder.", "main_total_tokens": 3802, "main_prompt_tokens": 2811, "main_completion_tokens": 991, "judge_total_tokens": 466, "judge_prompt_tokens": 400, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam, an 18-year-old in the 2017 TV show *Atypical*, is on the autism spectrum [Doc 124583].", "latency_ms": 3928.778886795044, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which matches the gold answer. It does not introduce contradictions or unsupported claims, so it is both correct and faithful to the reference.", "main_total_tokens": 3688, "main_prompt_tokens": 3037, "main_completion_tokens": 651, "judge_total_tokens": 487, "judge_prompt_tokens": 417, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "I am unable to answer this question because the provided context does not contain information about the movie *À corde tendue* [Doc 97714, Doc 52064, Doc 3719, Doc 126675, Doc 119043, Doc 112707, Doc 39996, Doc 79440, Doc 24646, Doc 98883, Doc 59525, Doc 93882, Doc 68013, Doc 108876, Doc 108130].", "latency_ms": 6476.107597351074, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is Chamonix, but the generated answer says it cannot answer, so it is incorrect. However, the model honestly reflects not finding the information in the provided context, so it is faithful to the given documents.", "main_total_tokens": 4312, "main_prompt_tokens": 3082, "main_completion_tokens": 1230, "judge_total_tokens": 574, "judge_prompt_tokens": 497, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481]", "latency_ms": 5798.813819885254, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and includes only that content plus a citation marker, so it is both correct and faithful to the given information.", "main_total_tokens": 3981, "main_prompt_tokens": 2916, "main_completion_tokens": 1065, "judge_total_tokens": 463, "judge_prompt_tokens": 398, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland [Doc 114810].", "latency_ms": 5103.694438934326, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The answer 'Greenland' matches the gold answer exactly and is fully supported by the provided information, with no contradictions or fabrications.", "main_total_tokens": 4147, "main_prompt_tokens": 3214, "main_completion_tokens": 933, "judge_total_tokens": 459, "judge_prompt_tokens": 401, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon [Doc 96838]", "latency_ms": 7180.781126022339, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('One Afternoon'), so it is correct. It is also faithful because it does not add or contradict any information relative to the provided gold, aside from an irrelevant document ID tag.", "main_total_tokens": 4458, "main_prompt_tokens": 2979, "main_completion_tokens": 1479, "judge_total_tokens": 487, "judge_prompt_tokens": 412, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The Life Ahead [Doc 93260]", "latency_ms": 7469.7840213775635, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('The Life Ahead') and correctly identifies the movie described. It is fully faithful to the provided information with no additions or contradictions.", "main_total_tokens": 5057, "main_prompt_tokens": 3402, "main_completion_tokens": 1655, "judge_total_tokens": 472, "judge_prompt_tokens": 407, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary “McLaren” is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 3465.4195308685303, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary, matching the gold answer exactly and staying faithful to it, only adding accurate contextual detail.", "main_total_tokens": 3299, "main_prompt_tokens": 2839, "main_completion_tokens": 460, "judge_total_tokens": 483, "judge_prompt_tokens": 417, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The name of the racing team founded by Bruce McLaren is the McLaren Motor Racing team [Doc 73859]. It is also referred to simply as McLaren [Doc 126697].", "latency_ms": 5937.781810760498, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly gives the name of the racing team as 'The McLaren Motor Racing team,' which matches the gold answer. The extra note that it is also referred to simply as McLaren does not contradict the gold and is grounded in the cited documents, so it is both correct and faithful to the provided information.", "main_total_tokens": 4212, "main_prompt_tokens": 3271, "main_completion_tokens": 941, "judge_total_tokens": 517, "judge_prompt_tokens": 422, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "", "latency_ms": 9367.13457107544, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "No answer was generated to compare against the gold label or the question, so correctness and faithfulness cannot be established and are treated as 0.", "main_total_tokens": 5006, "main_prompt_tokens": 3290, "main_completion_tokens": 1716, "judge_total_tokens": 437, "judge_prompt_tokens": 377, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based strictly on the provided context, there is no movie listed from 2013 that describes John McClane blowing up the Great Wall of China.\n\nThe 2013 movie featuring John McClane is *A Good Day to Die Hard*, but its plot summary details action in Moscow related to a nuclear weapons heist [Doc 58791].", "latency_ms": 8215.006589889526, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a specific movie where John McClane blows up the Great Wall of China. The gold answer correctly states that no such movie exists in the database. The generated answer instead introduces external knowledge, naming 'A Good Day to Die Hard' and referencing a document ID not present in the given context, which violates the constraint of basing the answer strictly on the provided information. Therefore, it is not faithful to the gold/reference behavior and does not correctly adhere to the requirement of admitting the absence of such a movie in the context.", "main_total_tokens": 4461, "main_prompt_tokens": 2949, "main_completion_tokens": 1512, "judge_total_tokens": 608, "judge_prompt_tokens": 468, "judge_completion_tokens": 140}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit [Doc 28589]", "latency_ms": 5816.07723236084, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2002 Japanese horror film as 'Tomie: Forbidden Fruit'. It is fully correct and faithful to the query and gold reference.", "main_total_tokens": 4302, "main_prompt_tokens": 3121, "main_completion_tokens": 1181, "judge_total_tokens": 498, "judge_prompt_tokens": 428, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 years [Doc 98566]", "latency_ms": 4732.77735710144, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, including the correct number of years (65). There is no additional content that conflicts with the provided information, so it is both correct and faithful to the source.", "main_total_tokens": 3957, "main_prompt_tokens": 3137, "main_completion_tokens": 820, "judge_total_tokens": 475, "judge_prompt_tokens": 403, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "a legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 4839.418172836304, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies that they are searching for the Book of Souls and remains faithful to the gold answer, only adding a descriptive phrase without contradicting it.", "main_total_tokens": 4154, "main_prompt_tokens": 3378, "main_completion_tokens": 776, "judge_total_tokens": 472, "judge_prompt_tokens": 409, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I am unable to answer this question as the title \"Zombies on the Moon\" is not present in the provided context.", "latency_ms": 4996.28758430481, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database. The generated answer says the title is not present in the provided context, effectively also declining to answer. Both correctly refrain from fabricating a release year and are consistent with the given information, so correctness and faithfulness are both high.", "main_total_tokens": 3685, "main_prompt_tokens": 2751, "main_completion_tokens": 934, "judge_total_tokens": 505, "judge_prompt_tokens": 416, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I am unable to answer this question as the title \"The Galactic baker\" is not mentioned in the provided context.", "latency_ms": 4570.696115493774, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database. The generated answer essentially says it cannot answer because the title is not in the provided context. Both convey inability to answer, which is correct and faithful to the absence of information. There is no hallucinated year or extra fabricated details.", "main_total_tokens": 3937, "main_prompt_tokens": 3189, "main_completion_tokens": 748, "judge_total_tokens": 501, "judge_prompt_tokens": 412, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I am unable to answer this question as the title \"Return of the Null Pointer\" is not mentioned in the provided context.", "latency_ms": 4099.677324295044, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot answer the question and notes the title is not in the provided context, which aligns with the gold response that the movie is not in the database. It makes no unsupported claims and remains faithful to the information given.", "main_total_tokens": 3550, "main_prompt_tokens": 2805, "main_completion_tokens": 745, "judge_total_tokens": 497, "judge_prompt_tokens": 416, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The context provided does not contain information about a movie titled \"Data Science: The Movie.\"", "latency_ms": 5768.60499382019, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information about a movie titled 'Data Science: The Movie' in the given context, which aligns with the gold response that the movie is not in the database. It does not introduce any unsupported details and remains faithful to the absence of information.", "main_total_tokens": 3962, "main_prompt_tokens": 2944, "main_completion_tokens": 1018, "judge_total_tokens": 497, "judge_prompt_tokens": 409, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for \"Python's Revenge 2\" is not available.", "latency_ms": 6308.113813400269, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about the release year of a movie that the gold answer states is not in the database. The generated answer correctly states that the release year is not available based on the provided context, without inventing a year. This matches the gold answer in substance and does not introduce any unsupported information.", "main_total_tokens": 4420, "main_prompt_tokens": 3191, "main_completion_tokens": 1229, "judge_total_tokens": 504, "judge_prompt_tokens": 413, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Based strictly on the context provided, the release year for \"The Last Pixel\" is not available.", "latency_ms": 4531.304359436035, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year / information is not available in the given context. The generated answer does not fabricate a year and is consistent with the gold response, so it is both correct and faithful to the provided context.", "main_total_tokens": 3457, "main_prompt_tokens": 2585, "main_completion_tokens": 872, "judge_total_tokens": 489, "judge_prompt_tokens": 409, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I am unable to answer this question as the title \"The man who ate the sun\" is not mentioned in the provided context.", "latency_ms": 4779.73198890686, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states inability to answer and aligns with the gold response that the movie is not in the database. It does not invent a year or contradict the given information, so it is both correct and faithful to the context.", "main_total_tokens": 4197, "main_prompt_tokens": 3387, "main_completion_tokens": 810, "judge_total_tokens": 495, "judge_prompt_tokens": 418, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not mention who directed the movie \"The Last Days of Lehman Brothers\" [Doc 46069].", "latency_ms": 4620.136022567749, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the director is not mentioned/provided in the context or database. The generated response matches the limitation of the available information and does not hallucinate a director, so it is both correct and faithful to the given context.", "main_total_tokens": 4157, "main_prompt_tokens": 3374, "main_completion_tokens": 783, "judge_total_tokens": 505, "judge_prompt_tokens": 422, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not contain information about the movie \"A Love to Die For\" or its director.", "latency_ms": 5411.640405654907, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information for the movie is not available in the provided context/database. The generated answer correctly refrains from hallucinating a director and is fully consistent with the gold answer, so it is both correct and faithful.", "main_total_tokens": 4125, "main_prompt_tokens": 3211, "main_completion_tokens": 914, "judge_total_tokens": 498, "judge_prompt_tokens": 416, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Based strictly on the context provided, the director of the movie \"Journey into Gaza\" is not mentioned [Doc 111360].", "latency_ms": 4517.488718032837, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director of \"Journey into Gaza\" is not mentioned, which matches the gold response that the director information is not provided. It does not introduce unsupported details and explicitly grounds the limitation in the provided context, so it is both correct and faithful.", "main_total_tokens": 3929, "main_prompt_tokens": 3218, "main_completion_tokens": 711, "judge_total_tokens": 505, "judge_prompt_tokens": 419, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not contain information about who directed \"The Last Night at Tremore Beach\" [Doc 135098].", "latency_ms": 4133.736610412598, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not contain information about the director, which aligns with the gold answer indicating the database lacks that information. It is both correct (does not invent a director) and faithful to the given context.", "main_total_tokens": 3493, "main_prompt_tokens": 2857, "main_completion_tokens": 636, "judge_total_tokens": 500, "judge_prompt_tokens": 422, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 3979.3167114257812, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Rise of the Mummy'), so it is both correct and faithful to the provided reference.", "main_total_tokens": 3492, "main_prompt_tokens": 2954, "main_completion_tokens": 538, "judge_total_tokens": 454, "judge_prompt_tokens": 396, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 5072.245836257935, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Stranger in My Bed') and correctly identifies the movie. It is both correct and fully faithful to the provided reference.", "main_total_tokens": 3757, "main_prompt_tokens": 2992, "main_completion_tokens": 765, "judge_total_tokens": 464, "judge_prompt_tokens": 400, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 6694.019794464111, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question actually refers to a TV series, not a movie, but both the gold answer and the generated answer match each other and the described plot: 'Game of Silence'. Therefore the generated answer is correct relative to the task and fully faithful to the gold.", "main_total_tokens": 3785, "main_prompt_tokens": 2677, "main_completion_tokens": 1108, "judge_total_tokens": 483, "judge_prompt_tokens": 400, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer.", "latency_ms": 3808.173179626465, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'The Craigslist Killer,' based on the given description.", "main_total_tokens": 3399, "main_prompt_tokens": 2893, "main_completion_tokens": 506, "judge_total_tokens": 452, "judge_prompt_tokens": 396, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job! [Doc 66094]", "latency_ms": 3280.712366104126, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Wood Job!'. It is both correct and faithful to the query and gold reference.", "main_total_tokens": 4166, "main_prompt_tokens": 3716, "main_completion_tokens": 450, "judge_total_tokens": 472, "judge_prompt_tokens": 409, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 6454.848051071167, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'Sesame Street: The Magical Wand Chase.'", "main_total_tokens": 4353, "main_prompt_tokens": 3265, "main_completion_tokens": 1088, "judge_total_tokens": 472, "judge_prompt_tokens": 417, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond [Doc 74597]", "latency_ms": 6257.197618484497, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Beyond') and does not introduce any unsupported information. It is both correct and faithful.", "main_total_tokens": 3919, "main_prompt_tokens": 2782, "main_completion_tokens": 1137, "judge_total_tokens": 472, "judge_prompt_tokens": 415, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9 [Doc 22478]", "latency_ms": 7396.4080810546875, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as '6ixtynin9'. It is correct and directly faithful to the given gold reference without adding extra information.", "main_total_tokens": 4277, "main_prompt_tokens": 2888, "main_completion_tokens": 1389, "judge_total_tokens": 483, "judge_prompt_tokens": 415, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread [Doc 32410]", "latency_ms": 7806.785583496094, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Club Dread') and does not introduce any contradictions or hallucinations. It is both correct and faithful to the provided information.", "main_total_tokens": 4768, "main_prompt_tokens": 3254, "main_completion_tokens": 1514, "judge_total_tokens": 498, "judge_prompt_tokens": 433, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet [Doc 48873]", "latency_ms": 5018.838405609131, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches a specific plot description. Both the gold answer and the generated answer identify the movie as \"Toilet\". This matches exactly, so the generated answer is correct and faithful to the gold reference.", "main_total_tokens": 4350, "main_prompt_tokens": 3461, "main_completion_tokens": 889, "judge_total_tokens": 497, "judge_prompt_tokens": 421, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel [Doc 1024]", "latency_ms": 5498.5551834106445, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Electric Angel', which matches the gold answer. It is faithful to the user's query and does not introduce extraneous or conflicting information; the added '[Doc 1024]' tag does not alter the correctness of the title.", "main_total_tokens": 4206, "main_prompt_tokens": 3180, "main_completion_tokens": 1026, "judge_total_tokens": 508, "judge_prompt_tokens": 424, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "Bottom of the Water [Doc 106899]", "latency_ms": 5227.12779045105, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer does not match the gold answer 'Mega Ape' and instead gives a different movie title. There is no indication or citation supporting the generated title, so it is both incorrect and unfaithful to the reference.", "main_total_tokens": 4026, "main_prompt_tokens": 2758, "main_completion_tokens": 1268, "judge_total_tokens": 467, "judge_prompt_tokens": 391, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 249.80616569519043, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Tea House', indicating there is a specific correct answer. The generated answer, 'I don't know.', is incorrect, so correctness is 0. However, it does not invent or misrepresent any facts and honestly indicates lack of knowledge, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 476, "judge_prompt_tokens": 386, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 276.60560607910156, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct movie title ('The Uncut Story of Mewtwo's Origin'), so it is incorrect. However, it does not introduce any hallucinated or false information beyond admitting uncertainty, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 482, "judge_prompt_tokens": 395, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "cn [Doc 354]", "latency_ms": 5559.236288070679, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says the language is English, but the generated answer says 'cn'. This is factually incorrect with respect to the gold label. It is also unfaithful because there is no supporting context indicating the answer should be 'cn' instead of English.", "main_total_tokens": 3867, "main_prompt_tokens": 2793, "main_completion_tokens": 1074, "judge_total_tokens": 473, "judge_prompt_tokens": 389, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was *Popular* (4.5113) [Doc 115196].", "latency_ms": 8248.57497215271, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect; the gold label says the TV show is 'Law & Order: Special Victims Unit', but the model answered 'Popular'. It also fabricates a specific popularity score and document reference that are not supported by the given gold answer, so it is not faithful to the source.", "main_total_tokens": 5132, "main_prompt_tokens": 3475, "main_completion_tokens": 1657, "judge_total_tokens": 509, "judge_prompt_tokens": 417, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 1420.8476543426514, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and cites a document as the source, indicating it is both correct and faithful to the provided information.", "main_total_tokens": 2556, "main_prompt_tokens": 2365, "main_completion_tokens": 191, "judge_total_tokens": 464, "judge_prompt_tokens": 400, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in 1999 [Doc 22481], 2019 [Doc 88811], and 2025 [Doc 112077].", "latency_ms": 5172.741889953613, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the movie was released in 2019, but the generated answer lists three different years (1999, 2019, 2025) instead of giving a single correct year. Therefore the generated answer is factually incorrect relative to the gold. It also fabricates multiple document references and release years not supported by the gold, so it is not faithful to the provided ground truth.", "main_total_tokens": 3990, "main_prompt_tokens": 2925, "main_completion_tokens": 1065, "judge_total_tokens": 535, "judge_prompt_tokens": 423, "judge_completion_tokens": 112}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 740.1347160339355, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 279 votes, but the generated answer is 3, which is numerically incorrect. The model’s answer does not match the required value, so correctness is 0. It is, however, directly answering the question without contradicting its own reasoning, so faithfulness is 1.", "main_total_tokens": 2407, "main_prompt_tokens": 2402, "main_completion_tokens": 5, "judge_total_tokens": 476, "judge_prompt_tokens": 383, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 4801.905632019043, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the number of votes received by 'Oh My God!', with the gold answer being 12. The generated response claims it cannot find any movies or shows matching that criteria, which does not provide the correct numeric answer. Therefore, correctness is 0. However, the response does not contradict any provided context; it simply states a failure to find the item, which is internally consistent with its claim of lacking information, so faithfulness is 1.", "main_total_tokens": 3488, "main_prompt_tokens": 2465, "main_completion_tokens": 1023, "judge_total_tokens": 516, "judge_prompt_tokens": 392, "judge_completion_tokens": 124}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "2009 [Doc 46069].", "latency_ms": 4029.3705463409424, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer gives the correct release year, 2009, which matches the gold answer, and does not introduce any contradictory or extraneous claims beyond a harmless citation marker.", "main_total_tokens": 3657, "main_prompt_tokens": 3020, "main_completion_tokens": 637, "judge_total_tokens": 462, "judge_prompt_tokens": 396, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-small_gemini-flash-latest", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English (en) [Doc 48035].", "latency_ms": 2671.3082790374756, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the original language of 'The King's Speech' is English, matching the gold answer 'en'. It does not introduce contradictions or unsupported claims beyond citing a document reference, so it is both correct and faithful.", "main_total_tokens": 3673, "main_prompt_tokens": 3284, "main_completion_tokens": 389, "judge_total_tokens": 483, "judge_prompt_tokens": 405, "judge_completion_tokens": 78}
