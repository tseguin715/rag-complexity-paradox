{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "According to [Doc 6993], the movie \"Wall Street\" was released in 1987.", "latency_ms": 13195.053577423096, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer, and it stays within the information suggested by the reference doc without adding contradictory or extraneous claims.", "main_total_tokens": 2907, "main_prompt_tokens": 2885, "main_completion_tokens": 22, "judge_total_tokens": 469, "judge_prompt_tokens": 399, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "According to [Doc 103513], \"Perfect Strangers\" (2022) features a plot where friends decide to play a game with their cell phones, but it does not explicitly state that the game is a survival game.\n\nHowever, according to [Doc 90746], \"47 Hours to Live\" (2019) features a plot where two girls find a game on a creepy website that claims you can use your phone to summon the supernatural...and if you lose the game...you lose your life.", "latency_ms": 13748.1210231781, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question asks for the movie where classmates have to play a survival game using their cell phones, with the gold answer being 'Brain Fluid Explosion Girl'. The generated answer instead references 'Perfect Strangers' and '47 Hours to Live', and never identifies 'Brain Fluid Explosion Girl', so it is incorrect. In terms of faithfulness, the answer paraphrases and relies on the cited documents about 'Perfect Strangers' and '47 Hours to Live' and does not fabricate details beyond those descriptions, but it also never addresses the actual target movie. Since it is partially grounded in its cited sources but not aligned with the gold, a mid-range faithfulness score is appropriate.", "main_total_tokens": 2803, "main_prompt_tokens": 2699, "main_completion_tokens": 104, "judge_total_tokens": 658, "judge_prompt_tokens": 491, "judge_completion_tokens": 167}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "According to [Doc 112849], the TV show \"Dungeons & Dragons\" (1983) features six friends who are transported into the Dungeons & Dragons realm and must try to find a way home with the help of their guide 'Dungeon Master'.", "latency_ms": 9140.267133712769, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons' and accurately reflects the description in the provided document, including the detail about six friends and their guide, the Dungeon Master. It matches the gold answer and stays faithful to the cited source.", "main_total_tokens": 2795, "main_prompt_tokens": 2742, "main_completion_tokens": 53, "judge_total_tokens": 525, "judge_prompt_tokens": 442, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe (1982) [Doc 2590]", "latency_ms": 6574.064493179321, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold title exactly and does not introduce unsupported information. It is correct and faithful to the reference.", "main_total_tokens": 3161, "main_prompt_tokens": 3144, "main_completion_tokens": 17, "judge_total_tokens": 468, "judge_prompt_tokens": 413, "judge_completion_tokens": 55}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "According to [Doc 126191], the events of Picnic at Hanging Rock take place in the year 1900.", "latency_ms": 6328.846216201782, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of Picnic at Hanging Rock take place in 1900, which matches the gold answer, and it remains faithful to the cited document without adding unsupported claims.", "main_total_tokens": 2877, "main_prompt_tokens": 2851, "main_completion_tokens": 26, "judge_total_tokens": 479, "judge_prompt_tokens": 409, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June (2004) [Doc 32483]", "latency_ms": 5530.851602554321, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The question asks for the movie title; the correct answer is simply 'June'. The generated answer 'June (2004) [Doc 32483]' includes the correct title but adds year and a document reference not present in the gold answer. So correctness is 1.0, as the movie is correctly identified, but faithfulness is slightly reduced because of extra, unsupported metadata.", "main_total_tokens": 3124, "main_prompt_tokens": 3112, "main_completion_tokens": 12, "judge_total_tokens": 553, "judge_prompt_tokens": 443, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "According to the provided context, the highest rated Russian-language movie released in 1980 is \"Moscow Does Not Believe in Tears\" with a rating of 7.6 [Doc 61].", "latency_ms": 8057.132005691528, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger' as the highest rated Russian-language movie released in 1980, but the generated answer claims it is 'Moscow Does Not Believe in Tears' with a rating of 7.6. This directly contradicts the gold, so the answer is incorrect. It also references 'the provided context' and Doc 61 to justify a different movie, which is not faithful to the gold reference for this evaluation, so faithfulness is also scored as 0.", "main_total_tokens": 3143, "main_prompt_tokens": 3102, "main_completion_tokens": 41, "judge_total_tokens": 576, "judge_prompt_tokens": 435, "judge_completion_tokens": 141}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "According to [Doc 22144], \"Features of the Russian Bath\" (1999) has a rating of 4.571. However, I also found another document that might be relevant.\n\nAccording to [Doc 31331], \"It's Russian\" is not from 1999 but according to [Doc 104570] and [Doc 104619] there are no movies with higher ratings in the list for 1999", "latency_ms": 12134.193420410156, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says the highest Russian-language movie rating in 1999 is 7.6, but the generated answer claims 4.571, so it is factually incorrect. It also references specific documents and a film title not supported by the provided gold answer, and asserts there are no higher-rated movies without evidence, so it is not faithful to the supposed ground truth.", "main_total_tokens": 2631, "main_prompt_tokens": 2541, "main_completion_tokens": 90, "judge_total_tokens": 575, "judge_prompt_tokens": 473, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "According to the provided context, there are three movies that meet the criteria:\n\n1. A Chinese Ghost Story (Rating: 7.3)\n2. Last Eunuch In China (Rating: 7.8)\n3. China White is not released in 1987 but in 1989", "latency_ms": 10040.842771530151, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer says there are 4 movies, while the generated answer concludes there are three and incorrectly includes a movie that does not meet the year criterion, so the final result is wrong. It is partially faithful in that it tries to use the given context to enumerate movies, but it contradicts the gold count and misapplies the criteria, so faithfulness is limited.", "main_total_tokens": 3046, "main_prompt_tokens": 2984, "main_completion_tokens": 62, "judge_total_tokens": 560, "judge_prompt_tokens": 452, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "Based on the provided context, there is one TV show that meets the criteria:\n\n* [Doc 113169] Title: Journey to the West\nType: TV\nYear: 1986 (not 1987)\nRating: 8.4\nPopularity: 4.4866\nLanguage: zh\n\nHowever, this show was released in 1986, not 1987.\n\nThere is another TV show that meets some of the criteria:\n\n* [Doc 113393] Title: China Beach\nType: TV\nYear: 1988 (not 1987)\nRating: 7.4\nPopularity: 6.6421\nLanguage: en\n\nHowever, this show was released in 1988 and is not in Chinese.\n\nThere is another TV show that meets some of the criteria:\n\n* [Doc 113169] Title: Journey to the West\nType: TV\nYear: 1986 (not 1987)\nRating: 8.4\nPopularity: 4.4866\nLanguage: zh\n\nHowever, this show was released in 1986, not 1987.\n\nThere is another TV show that meets some of the criteria:\n\n* [Doc 113046] Title: 227", "latency_ms": 28212.466716766357, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.7, "justification": "The question asks specifically for the count of TV shows released in 1987, rating over 7, and in Chinese, with the gold answer being 2. The generated answer concludes there is one TV show that meets the criteria, which is incorrect numerically, and in fact identifies a show from 1986, so the final answer is wrong (score 0 for correctness). In terms of faithfulness, the answer largely describes what is in the provided documents (e.g., Journey to the West: 1986, Chinese, rating 8.4; China Beach: 1988, English, rating 7.4) and notes they do not meet the year/ language constraints, which is consistent with the cited data. However, it fails to identify or count the actual 1987 Chinese TV shows implied by the gold answer, and it repeats one document unnecessarily. There is no obvious fabrication about the cited items, but the reasoning is incomplete relative to the true answer, so faithfulness is partial rather than full.", "main_total_tokens": 2971, "main_prompt_tokens": 2715, "main_completion_tokens": 256, "judge_total_tokens": 887, "judge_prompt_tokens": 646, "judge_completion_tokens": 241}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "According to the provided context, there are multiple French language TV shows and movies released in 1980, but one of them is \"Loulou\" (Doc 689), which is a movie. However, I couldn't find any information about a French language TV show specifically released in 1980.\n\nHowever, there is another document (not explicitly mentioned as the answer) that mentions a TV show with a release year close to 1980: \"Champs-Elysées\" (Doc 112747), which was released in 1982.", "latency_ms": 15106.013059616089, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'Fantômas', but the generated answer says it cannot find such a show and mentions 'Loulou' (a movie) and 'Champs-Elysées' (1982), so it is factually incorrect. It is also unfaithful to the gold/reference answer since it does not mention 'Fantômas' at all and instead asserts there is no such show.", "main_total_tokens": 3201, "main_prompt_tokens": 3088, "main_completion_tokens": 113, "judge_total_tokens": 609, "judge_prompt_tokens": 496, "judge_completion_tokens": 113}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "According to [Doc 11205], the movie \"Kill Cruise\" (1990) is about two British beauties going to Barbados with a yacht captain who does not know what he's in for.", "latency_ms": 8264.244794845581, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise' and accurately reflects the description in the gold answer. It does not introduce contradictions or unsupported details beyond minor phrasing, so it is both correct and faithful.", "main_total_tokens": 2999, "main_prompt_tokens": 2956, "main_completion_tokens": 43, "judge_total_tokens": 498, "judge_prompt_tokens": 424, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 5120.105504989624, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct movie title about Fred Rogers and his TV series. It is both correct and fully faithful to the provided gold response.", "main_total_tokens": 3029, "main_prompt_tokens": 3021, "main_completion_tokens": 8, "judge_total_tokens": 475, "judge_prompt_tokens": 408, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "According to [Doc 107925], the movie \"YEAR\" was released in 2023.\n\nHowever, there are multiple movies mentioned in the context, so I'll provide a list of release years for each:\n\n* YEAR (Documentary): 2023 ([Doc 107925])\n* The Disco Years: 1994 ([Doc 15797])\n* 663114: 2012 ([Doc 58592])\n* Mine: 2013 ([Doc 62815])\n* My Favorite Year: 1982 ([Doc 2118])\n* 1: 2008 ([Doc 44750])\n* Paper Year: 2018 ([Doc 85553])\n* Year of Grace: 2011 ([Doc 53000])\n* A Year and Change: 2015 ([Doc 69571])\n* The Year of the Deer: 1995 ([Doc 17784])\n* Year of the Gun: 1991 ([Doc 11881])\n* Another Year: 2014 ([Doc 62952])\n* What It Was: 2014 ([Doc 65131])\n* Let It Be: 2005 ([Doc 35791]) or 2010 ([Doc 50373])\n* Year of the Horse: ", "latency_ms": 29046.48208618164, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question refers to a single item with the correct release year 2018, but the generated answer claims a 2023 release for the main movie and lists many unrelated titles and years. It neither gives the correct year (2018) nor adheres to the gold answer, so it is both incorrect and unfaithful.", "main_total_tokens": 3359, "main_prompt_tokens": 3103, "main_completion_tokens": 256, "judge_total_tokens": 729, "judge_prompt_tokens": 632, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy (1980) [Doc 112591]", "latency_ms": 5858.855247497559, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The question asks for the TV series that is an animation about a young boy robot in 2030 AD, whose correct answer is simply 'Astro Boy.' The generated answer 'Astro Boy (1980) [Doc 112591]' correctly identifies the series, so it is correct. However, the addition of '(1980) [Doc 112591]' is extra metadata not present in the gold answer and likely not in the original question, so it is not fully faithful to the source query/answer format.", "main_total_tokens": 3234, "main_prompt_tokens": 3220, "main_completion_tokens": 14, "judge_total_tokens": 535, "judge_prompt_tokens": 399, "judge_completion_tokens": 136}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "According to [Doc 67513], the woman and her young son were held captive for 7 years in an enclosed space.", "latency_ms": 6160.230398178101, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9463946303571863, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer. It does not introduce contradictory or extraneous information and remains consistent with the referenced document, so it is both correct and faithful.", "main_total_tokens": 2572, "main_prompt_tokens": 2545, "main_completion_tokens": 27, "judge_total_tokens": 501, "judge_prompt_tokens": 420, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "According to [Doc 133469], the Brink's-Mat security depot was holding £26m worth of gold bullion when it was broken into on 26 November 1983.", "latency_ms": 7685.6701374053955, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding £26 million worth of gold bullion before the 26 November 1983 robbery, and it aligns with the cited document without adding unsupported claims.", "main_total_tokens": 2836, "main_prompt_tokens": 2796, "main_completion_tokens": 40, "judge_total_tokens": 507, "judge_prompt_tokens": 432, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "According to the provided context, the most popular English-speaking TV show released in 1997 is \"Buffy the Vampire Slayer\" with a popularity score of 25.9735.", "latency_ms": 8584.37967300415, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the popularity score is 80.5772, but the generated answer claims it is 25.9735 and specifies a particular show. This is factually incorrect relative to the gold. It also fabricates details about the context (naming a specific show and score) that are not supported by the gold answer, so it is not faithful.", "main_total_tokens": 3640, "main_prompt_tokens": 3602, "main_completion_tokens": 38, "judge_total_tokens": 534, "judge_prompt_tokens": 429, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "Based on the provided context, there is no information about any Russian language TV shows being released in 1997. However, there are several non-Russian language TV shows that were released in 1997, such as \"13 posterunek\", \"Daria\", \"Schimanski\", and \"Viva Variety\".", "latency_ms": 9662.285804748535, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1, but the generated answer incorrectly states there is no information and does not provide the number, so it is factually wrong. It is faithful to the (incorrect) reasoning it presents and does not hallucinate beyond that context.", "main_total_tokens": 2441, "main_prompt_tokens": 2375, "main_completion_tokens": 66, "judge_total_tokens": 524, "judge_prompt_tokens": 447, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes (2014) [Doc 121928]", "latency_ms": 5233.670711517334, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Angel Eyes', which matches the gold answer. Adding '(2014)' does not change the correctness and remains faithful to the question and the gold label.", "main_total_tokens": 2861, "main_prompt_tokens": 2848, "main_completion_tokens": 13, "judge_total_tokens": 478, "judge_prompt_tokens": 407, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "According to [Doc 100184], the language of the movie \"Lala\" is Spanish (\"es\").", "latency_ms": 6097.088098526001, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states the language is Spanish, while the gold label is Korean, so it is factually incorrect. It also references an external document rather than the provided gold answer, so it is not faithful to the given data.", "main_total_tokens": 2823, "main_prompt_tokens": 2800, "main_completion_tokens": 23, "judge_total_tokens": 475, "judge_prompt_tokens": 398, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "Based on the provided context, there are no English-speaking Horror movies released in 2009 with a popularity score greater than 7. However, I can provide you with a list of English-speaking Horror movies released in 2009 with their corresponding popularity scores:\n\n* Perkins' 14 (Popularity: 1.1125)\n* Scare Zone (Popularity: 0.3206)\n* Vampiro (Popularity: 0.3315)\n\nThese are the only English-speaking Horror movies from 2009 mentioned in the context with their corresponding popularity scores.", "latency_ms": 16059.606075286865, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there are 2 such movies, but the generated answer claims there are none, so it is factually incorrect. It is also unfaithful to the ground truth by asserting zero and inventing a rationale and list that do not match the labeled answer.", "main_total_tokens": 2730, "main_prompt_tokens": 2612, "main_completion_tokens": 118, "judge_total_tokens": 591, "judge_prompt_tokens": 505, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Not found.", "latency_ms": 4360.7776165008545, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer explicitly states there are no movies from 1979 in the database, while the generated answer only says 'Not found' without specifying that this is due to the limits of the database. This changes the meaning compared to the gold. Therefore it is not fully faithful, and it also does not correctly reflect the gold response.", "main_total_tokens": 2879, "main_prompt_tokens": 2875, "main_completion_tokens": 4, "judge_total_tokens": 496, "judge_prompt_tokens": 398, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "There is no information about a movie titled \"Back to the Future Part IV\" in the provided context. The trilogy consists of three movies: Back to the Future (1985), Back to the Future Part II (1989), and Back to the Future Part III (1990).", "latency_ms": 10156.229972839355, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information about a movie titled 'Back to the Future Part IV' and lists only the three actual films in the trilogy, which is factually accurate. It is faithful to the gold answer's core claim that such a movie does not exist in the database/context, even though it is phrased differently and adds extra correct details about the trilogy.", "main_total_tokens": 3196, "main_prompt_tokens": 3138, "main_completion_tokens": 58, "judge_total_tokens": 565, "judge_prompt_tokens": 457, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "There is no information about movies from 1980 specifically about the Chernobyl disaster, as the disaster occurred on April 26, 1986. However, there is a movie titled \"Nuclear Shelter\" released in 1981 with a popularity score of 0.2861 [Doc 1818].", "latency_ms": 10579.118251800537, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question asks for the popularity score of the least popular movie in 1980 about the Chernobyl disaster. The gold answer correctly states that no such movies exist. The generated answer introduces a specific movie, \"Nuclear Shelter\" (1981) with a popularity score, which is irrelevant to the 1980 Chernobyl constraint and thus incorrect. It also fabricates a specific popularity score and citation from Doc 1818 not supported by the gold. While it correctly notes that the Chernobyl disaster occurred in 1986 and that there is no information about 1980 movies about it, it then violates faithfulness by hallucinating a different movie and numeric score, so correctness is 0 and faithfulness is partial at best (0.5).", "main_total_tokens": 3185, "main_prompt_tokens": 3120, "main_completion_tokens": 65, "judge_total_tokens": 657, "judge_prompt_tokens": 470, "judge_completion_tokens": 187}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "According to the provided context, there are at least 4 titles with \"Astro Boy\" in them:\n\n1. Astro Boy (Movie, 2009) [Doc 44947]\n2. Astro Boy vs The Junkyard Pirates (Movie, 2010) [Doc 49121]\n3. Astro Boy (TV, 1980) [Doc 112591]\n4. Astro Boy (TV, 2003) [Doc 116265]\n\nNote that there may be more titles with \"Astro Boy\" in them, but these are the ones mentioned in the provided context.", "latency_ms": 15861.494541168213, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 1.1510196182278052, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.5330476224182168, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 4, and the generated answer lists exactly four titles with 'Astro Boy' in the title, explicitly stating that there are at least 4 according to the provided context. This matches the gold answer. The reasoning is consistent with the context and does not introduce unsupported claims beyond a cautious note that more might exist outside the provided context, which does not affect the correctness regarding the question as framed. Thus, both correctness and faithfulness are 1.", "main_total_tokens": 3087, "main_prompt_tokens": 2964, "main_completion_tokens": 123, "judge_total_tokens": 634, "judge_prompt_tokens": 508, "judge_completion_tokens": 126}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "Based on the provided context, there are several movies from 2025 that allude to the Holocaust:\n\n1. \"Everyone Is Going to Die\" (Doc 111564) - Although not directly about the Holocaust, it mentions a father's reconciliation attempt being hijacked by two female burglars, which might be an allegory for the chaos and violence of the Holocaust.\n2. \"What Happens After the Massacre?\" (Doc 112547) - The title suggests a connection to the Holocaust, but the plot description doesn't explicitly mention it.\n3. \"We're All Gonna Die\" (Doc 111988) - This movie's plot involves an alien tentacle materializing in Earth's upper atmosphere, which might be seen as a metaphor for the sudden and catastrophic events of the Holocaust.\n\nHowever, there are also movies that more directly address the Holocaust:\n\n1. \"Mazel Tov\" (Doc 112087) - The plot description mentions a family gathering for a wedding and Bat Mitzvah in Argentina, but it doesn't explicitly mention the Holocaust.\n2. \"Shadow of God\" (Doc 111520) - This movie's plot involves an exorcist battling an entity that might be a holy being – perhaps even God.", "latency_ms": 29554.643392562866, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there are 2 movies, but the generated answer does not give a clear numeric answer and instead speculates about several films with incorrect or metaphorical connections to the Holocaust. This is factually incorrect relative to the gold label. It is also unfaithful to the (implied) source context: it invents Holocaust allusions from titles and brief descriptions without evidence, and over-interprets metaphorical connections that are not present in the provided data.", "main_total_tokens": 3512, "main_prompt_tokens": 3256, "main_completion_tokens": 256, "judge_total_tokens": 759, "judge_prompt_tokens": 633, "judge_completion_tokens": 126}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "According to the provided context, there are two TV shows related to Super Mario Bros that were released in the 1990s:\n\n1. The Adventures of Super Mario Bros. 3 (1990) [Doc 113661]\n2. Super Mario World (1991) [Doc 113814]\n\nNote that these are the only two TV shows mentioned in the context that meet the specified criteria.", "latency_ms": 12448.324203491211, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.8010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there is 1 TV show, but the generated answer claims there are 2, so the main numerical answer is incorrect. It also explicitly references a context and documents (Doc 113661, Doc 113814) that are not present in the user message, indicating it is not faithful to the given information.", "main_total_tokens": 3220, "main_prompt_tokens": 3138, "main_completion_tokens": 82, "judge_total_tokens": 566, "judge_prompt_tokens": 468, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "According to the provided context, there are no TV shows specifically about Super Mario Bros that were released between 2000 and 2010. However, there is a mention of \"The Adventures of Super Mario Bros. 3\" which was released in 1990, and another show called \"Super Mario World\" which was released in 1991.\n\nHowever, it's worth noting that the context does mention an Italian sitcom titled simply \"Mario\" that aired on MTV Italy in 2013, but it's unclear if this is related to Super Mario Bros.", "latency_ms": 14402.34637260437, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold answer is 0 and the generated answer explicitly states that there are no TV shows about Super Mario Bros released between 2000 and 2010. Additional information about earlier shows and a 2013 Italian sitcom is extra but does not contradict the core numerical answer, so it remains correct and faithful to the gold.", "main_total_tokens": 2933, "main_prompt_tokens": 2819, "main_completion_tokens": 114, "judge_total_tokens": 595, "judge_prompt_tokens": 500, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "According to the provided context, the most popular TV show or movie about ninjas is \"House of Ninjas\" (2024) with a popularity rating of 4.4317. However, it's worth noting that this rating might be specific to the Japanese audience, as the language of the show is listed as \"ja\".\n\nIf we consider the overall ratings and popularity across all languages, \"Ninja Warrior\" (1997) has a high rating of 6.9 and a relatively high popularity rating of 0.7818, making it a strong contender for the most popular TV show about ninjas.\n\nIt's also worth mentioning that \"American Ninja\" (1985) has a high rating of 5.967 and a relatively high popularity rating of 2.4014, making it another popular movie about ninjas.", "latency_ms": 20932.45244026184, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'Naruto', but the generated answer instead claims 'House of Ninjas', 'Ninja Warrior', and 'American Ninja' as the most popular, which contradicts the gold. It also incorrectly cites a 'provided context' and specific popularity ratings that are not present in the user prompt here, so it is not faithful to the given data in this evaluation setting.", "main_total_tokens": 3375, "main_prompt_tokens": 3204, "main_completion_tokens": 171, "judge_total_tokens": 659, "judge_prompt_tokens": 550, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "There is no information about a Jurassic Park/Power Rangers crossover movie in the provided context. The documents mention separate movies and TV shows for Jurassic Park and Power Rangers, but there is no indication of a crossover between the two franchises.", "latency_ms": 8609.243631362915, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly states that the movie was not released. The generated answer instead says there is no information about such a crossover in the provided context, which leaves open the possibility that it could exist but just isn’t mentioned. This is factually incorrect relative to the gold, so correctness is 0. However, the generated answer accurately reflects the (implied) content of the context it describes—that the documents only mention separate Jurassic Park and Power Rangers media and no crossover—so faithfulness to the given context is high, thus faithfulness is 1.", "main_total_tokens": 3052, "main_prompt_tokens": 3005, "main_completion_tokens": 47, "judge_total_tokens": 574, "judge_prompt_tokens": 431, "judge_completion_tokens": 143}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "There is no information in the provided context that suggests Barbie fights the Predator in any of the listed movies. The Predator is mentioned in [Doc 6958] as a separate entity, and there is no connection to Barbie in that document.\n\nHowever, it's worth noting that [Doc 82000] mentions \"The Predator\" as a movie title, but it does not feature Barbie as a character.", "latency_ms": 12074.110269546509, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question is essentially a trick: the correct answer is that no such movie exists or it was not released, as in the gold answer. The generated answer instead claims there is no information in the provided context and then discusses specific documents (Doc 6958, Doc 82000), which are hallucinated because the prompt does not actually provide any such documents. It also fails to directly state that the movie does not exist or was not released, so it does not match the gold. Therefore, it is both incorrect with respect to the task and unfaithful to the given conversation.", "main_total_tokens": 3077, "main_prompt_tokens": 2995, "main_completion_tokens": 82, "judge_total_tokens": 610, "judge_prompt_tokens": 462, "judge_completion_tokens": 148}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "According to the provided context, here are the Star Wars movies in chronological order by release year:\n\n1. [Doc 3] Title: The Empire Strikes Back (1980)\n2. [Doc 2858] Title: Return of the Jedi (1983)\n3. [Doc 3568] Title: Classic Creatures: Return of the Jedi (1983)\n\nSince \"Classic Creatures: Return of the Jedi\" is a documentary, I'll consider it as a separate entity.\n\nThe next oldest Star Wars movie would be:\n\n4. [Doc 32101] Title: The Story of Star Wars (2004)\n\nSo, the 4th oldest Star Wars movie is \"The Story of Star Wars\".", "latency_ms": 18039.63804244995, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer is 'Star Wars: Droids - The Great Heep', while the generated answer is 'The Story of Star Wars', so the final result is incorrect (correctness 0). The reasoning is partially grounded in the provided document list (it cites specific docs and their years) but it fabricates the ordering and ignores the gold-correct title, and also jumps from 1983 to a 2004 title without evidence that it is truly the 4th oldest overall. Thus, the answer is only partially faithful to the context (faithfulness 0.5).", "main_total_tokens": 3166, "main_prompt_tokens": 3021, "main_completion_tokens": 145, "judge_total_tokens": 686, "judge_prompt_tokens": 535, "judge_completion_tokens": 151}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "According to [Doc 107925], the movie \"YEAR\" was released in 2023.\n\nHowever, there are multiple movies mentioned in the context, so I'll provide a list of release years for each:\n\n* YEAR (Documentary): 2023 ([Doc 107925])\n* The Disco Years: 1994 ([Doc 15797])\n* 663114: 2012 ([Doc 58592])\n* Mine: 2013 ([Doc 62815])\n* My Favorite Year: 1982 ([Doc 2118])\n* 1: 2008 ([Doc 44750])\n* Paper Year: 2018 ([Doc 85553])\n* Year of Grace: 2011 ([Doc 53000])\n* A Year and Change: 2015 ([Doc 69571])\n* The Year of the Deer: 1995 ([Doc 17784])\n* Year of the Gun: 1991 ([Doc 11881])\n* Another Year: 2014 ([Doc 62952])\n* What It Was: 2014 ([Doc 65131])\n* Let It Be: 2005 ([Doc 35791]) or 2010 ([Doc 50373])\n* Year of the Horse: ", "latency_ms": 28948.376655578613, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer specifies 1986, but the generated answer states 2023 for the main question, so it is incorrect. The model also lists several other movies and years not matching the gold, and the primary asked item is wrong, so correctness is 0. For faithfulness, the answer references specific documents (e.g., [Doc 107925]) and appears to base its claims on that context, but it does not align with the gold label and seems to conflate multiple items. It is partially grounded in the provided context but ultimately not faithful to the task target, so an intermediate faithfulness score (0.5) is appropriate.", "main_total_tokens": 3359, "main_prompt_tokens": 3103, "main_completion_tokens": 256, "judge_total_tokens": 796, "judge_prompt_tokens": 632, "judge_completion_tokens": 164}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "According to [Doc 15236], the rating of the movie \"Forrest Gump\" is 8.466.", "latency_ms": 6255.552530288696, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating value 8.466 and attributes it to a document, staying consistent with the provided information without adding unsupported claims.", "main_total_tokens": 2734, "main_prompt_tokens": 2708, "main_completion_tokens": 26, "judge_total_tokens": 481, "judge_prompt_tokens": 419, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med [Doc 121125]", "latency_ms": 5077.700614929199, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Mighty Med'. The added bracketed note does not alter the correctness or contradict any known facts, so the response is both correct and faithful.", "main_total_tokens": 2879, "main_prompt_tokens": 2868, "main_completion_tokens": 11, "judge_total_tokens": 469, "judge_prompt_tokens": 400, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina", "latency_ms": 4577.836990356445, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: both state that the prison was in Argentina. This is correct and faithful.", "main_total_tokens": 3164, "main_prompt_tokens": 3162, "main_completion_tokens": 2, "judge_total_tokens": 447, "judge_prompt_tokens": 392, "judge_completion_tokens": 55}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "According to [Doc 135231], in Sweet Trap (2024), the apprentice Li Nai falls into a trap designed by Jiang Jie, also known as the \"Chef Assassin\".", "latency_ms": 7903.483152389526, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies that, in Sweet Trap (2024), the apprentice falls into Jiang Jie's trap, matching the gold answer. The added detail naming the apprentice as Li Nai and calling Jiang Jie the 'Chef Assassin' is plausible but not supported by the gold reference and may be hallucinated, so faithfulness is slightly reduced.", "main_total_tokens": 2913, "main_prompt_tokens": 2874, "main_completion_tokens": 39, "judge_total_tokens": 521, "judge_prompt_tokens": 420, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "Based on the provided context, I found the following Korean-language movies that mention assassins and were released after 2023:\n\n1. [Doc 106219] Title: The Assassin (2023)\n2. [Doc 67739] Title: Assassination (2015) - does not meet the criteria since it was released before 2023\n3. [Doc 77277] Title: V.I.P. (2017) - does not meet the criteria since it was released before 2023\n\nThere is only one Korean-language movie that mentions assassins and was released after 2023, but it's actually from 2023:\n\n1. [Doc 106219] Title: The Assassin", "latency_ms": 17806.191205978394, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says there are 4 movies, but the generated answer concludes there is only 1 that meets the criteria and even misaligns on the year constraint (using 2023 instead of strictly after 2023). This is incorrect. It is also unfaithful to the given gold label, so both correctness and faithfulness are 0.", "main_total_tokens": 2896, "main_prompt_tokens": 2750, "main_completion_tokens": 146, "judge_total_tokens": 631, "judge_prompt_tokens": 528, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "According to the provided context, there is no mention of Steven Seagal's filmography or any collaboration with Bruce Lee.\n\nHowever, I can provide information on Bruce Lee's filmography based on the given context:\n\n* Bruce Lee has appeared in several films, including:\n\t+ \"The Legend of Bruce Lee\" (2009)\n\t+ \"Bruce Lee: The Immortal Dragon\" (1994) - a documentary\n\t+ \"The Life of Bruce Lee\" (1994) - a documentary\n\t+ \"Dragon: The Bruce Lee Story\" (1993)\n\t+ \"Bruce, King of Kung Fu\" (1980)\n\nPlease note that this list is not exhaustive and only based on the provided context.", "latency_ms": 18931.913375854492, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.7, "faithfulness_score": 0.4, "justification": "The question asks how many movies feature both Steven Seagal and Bruce Lee together. The gold answer correctly states that there are none. The generated answer correctly conveys that there is no mention of Steven Seagal or a collaboration with Bruce Lee, which loosely aligns with the idea that such movies are not found, but it does not explicitly answer the numeric question (i.e., 0 movies). It then adds an unrelated partial filmography of Bruce Lee based on the context, which is unnecessary and not tied to the query. Moreover, some listed titles are not actual Bruce Lee films but later biopics or documentaries, so the added details may be factually questionable given general knowledge and are not grounded in the original gold/context. Thus correctness is partial and faithfulness to the gold answer and the user’s specific query is limited.", "main_total_tokens": 3455, "main_prompt_tokens": 3307, "main_completion_tokens": 148, "judge_total_tokens": 738, "judge_prompt_tokens": 539, "judge_completion_tokens": 199}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3 [Doc 42750]", "latency_ms": 5293.468236923218, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (6.3) and stays consistent with the provided information, so it is both correct and faithful.", "main_total_tokens": 3087, "main_prompt_tokens": 3077, "main_completion_tokens": 10, "judge_total_tokens": 453, "judge_prompt_tokens": 393, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The movie \"Need for Speed\" features street racing and high-performance driving, with a focus on high-speed racing on public roads.", "latency_ms": 6891.592025756836, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'Drift', but the generated answer describes general street and high-speed racing instead of drift, so it is incorrect. It also does not reflect the labeled gold answer, so it is unfaithful to the provided reference.", "main_total_tokens": 3147, "main_prompt_tokens": 3120, "main_completion_tokens": 27, "judge_total_tokens": 484, "judge_prompt_tokens": 404, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer", "latency_ms": 4842.317819595337, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie title as 'Confronting a Serial Killer', which is faithful to the question.", "main_total_tokens": 3044, "main_prompt_tokens": 3037, "main_completion_tokens": 7, "judge_total_tokens": 473, "judge_prompt_tokens": 412, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "According to [Doc 20603], the popularity score of Armageddon (1998) is 7.838.", "latency_ms": 6920.705556869507, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold value (7.838) and explicitly attributes it to the referenced document, so it is both correct and faithful to the provided source.", "main_total_tokens": 3109, "main_prompt_tokens": 3083, "main_completion_tokens": 26, "judge_total_tokens": 475, "judge_prompt_tokens": 410, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense [Doc 22098]", "latency_ms": 4686.433792114258, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6895405204413555, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer, 'The Sixth Sense', exactly matches the gold answer and correctly identifies the 1999 psychological thriller alluding to an extra sense. The added bracketed note does not change the factual answer and appears to be a citation marker, so the response is both correct and faithful.", "main_total_tokens": 2707, "main_prompt_tokens": 2697, "main_completion_tokens": 10, "judge_total_tokens": 489, "judge_prompt_tokens": 400, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "[Doc 27318] The Transporter was released in 2002.", "latency_ms": 5746.407032012939, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer. It is both correct and fully faithful to the provided information.", "main_total_tokens": 3006, "main_prompt_tokens": 2989, "main_completion_tokens": 17, "judge_total_tokens": 460, "judge_prompt_tokens": 396, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "According to [Doc 67500], the title of the 2015 movie is \"The Transporter Refueled\".", "latency_ms": 6300.135612487793, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2015 reboot title as 'The Transporter Refueled' and aligns with the gold answer. It does not introduce contradictions or unsupported information and remains faithful to the implied source citation.", "main_total_tokens": 2927, "main_prompt_tokens": 2901, "main_completion_tokens": 26, "judge_total_tokens": 486, "judge_prompt_tokens": 416, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "[Doc 67500] states that \"The Transporter Refueled\" was released in 2015.", "latency_ms": 6304.356575012207, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, which matches the gold answer. It explicitly attributes this information to Doc 67500, so it is faithful to the cited source.", "main_total_tokens": 3001, "main_prompt_tokens": 2977, "main_completion_tokens": 24, "judge_total_tokens": 485, "judge_prompt_tokens": 407, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "Not found [Doc 54943], [Doc 59279], [Doc 58110], [Doc 57213], [Doc 60175], [Doc 61551], [Doc 113428], [Doc 55023]", "latency_ms": 8635.395765304565, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that no such movie description can be found, which correctly reflects the apparent absence of a romantic comedy where Abraham Lincoln comes back from the dead. The generated answer mentions specific document IDs but does not invent any contradictory information, remaining faithful to the underlying claim of non-existence.", "main_total_tokens": 2771, "main_prompt_tokens": 2720, "main_completion_tokens": 51, "judge_total_tokens": 532, "judge_prompt_tokens": 440, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Havana Darkness", "latency_ms": 4412.841081619263, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Havana Darkness' and correctly identifies the movie, so it is both correct and faithful.", "main_total_tokens": 2898, "main_prompt_tokens": 2894, "main_completion_tokens": 4, "judge_total_tokens": 448, "judge_prompt_tokens": 390, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "According to [Doc 122259], the answer is \"Doll & Em\".", "latency_ms": 5674.687147140503, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Doll & Em' and correctly attributes the information to the cited document, remaining faithful to the provided evidence.", "main_total_tokens": 2912, "main_prompt_tokens": 2894, "main_completion_tokens": 18, "judge_total_tokens": 468, "judge_prompt_tokens": 406, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "According to [Doc 39389], Hallam's talent for spying on people reveals his darkest fears-and his most peculiar desires, as he is driven to expose the true cause of his mother's death. The title of this 2007 movie is \"Hallam Foe\".", "latency_ms": 9487.24627494812, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the film as 'Hallam Foe', which matches the gold answer. The explanation stays consistent with the question and does not introduce contradictions, so it is faithful to the provided information.", "main_total_tokens": 2958, "main_prompt_tokens": 2901, "main_completion_tokens": 57, "judge_total_tokens": 527, "judge_prompt_tokens": 454, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "According to [Doc 7499], the character's last name is \"Jolly Devil\", which might initially suggest that he is a foe. However, the title of the movie is \"Friend of the Jolly Devil\", implying that despite his potentially ominous-sounding name, he might be a friend after all.", "latency_ms": 9952.913045883179, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks whether the last name sounds at first like friend or foe; the gold answer is simply \"A foe.\" The generated answer equivocates and suggests mixed implications, not clearly affirming it sounds like a foe, so it does not match the gold. It also fabricates a document reference ([Doc 7499]) and movie title context not present in the question or gold, making it unfaithful to the provided information.", "main_total_tokens": 2815, "main_prompt_tokens": 2751, "main_completion_tokens": 64, "judge_total_tokens": 568, "judge_prompt_tokens": 450, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "According to [Doc 124583], Sam is an 18-year-old on the autism spectrum in the 2017 TV show Atypical.", "latency_ms": 6583.650827407837, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Sam as 18 years old, which matches the gold answer. It does not introduce contradictions or unsupported claims beyond restating that he is on the autism spectrum, which is accurate for the show, so it is faithful to the provided information.", "main_total_tokens": 2753, "main_prompt_tokens": 2723, "main_completion_tokens": 30, "judge_total_tokens": 501, "judge_prompt_tokens": 417, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "According to [Doc 124583], in the 2017 TV show \"Atypical\", Sam is on the autism spectrum.", "latency_ms": 6479.823589324951, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the TV show 'Atypical', Sam is on the autism spectrum, matching the gold answer exactly. It also remains fully faithful to the provided information without adding unsupported details.", "main_total_tokens": 2933, "main_prompt_tokens": 2906, "main_completion_tokens": 27, "judge_total_tokens": 487, "judge_prompt_tokens": 414, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "There is no information about a movie titled \"À corde tendue\" or its release year being 2013 in the provided context. However, there is a movie titled \"Landes\" released in 2013, and it mentions a young widow who wants to carry on with her husband's work managing a pine plantation where they extract resin.", "latency_ms": 11071.259260177612, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The generated answer fails to provide the correct birthplace of Sandra (Chamonix) and instead claims there is no information about the film, discussing a different movie (Landes). This is incorrect. It is somewhat faithful to its own stated lack of information, but it does not address the actual question or gold answer, and introduces unrelated details, so faithfulness is only partial.", "main_total_tokens": 3225, "main_prompt_tokens": 3154, "main_completion_tokens": 71, "judge_total_tokens": 573, "judge_prompt_tokens": 465, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "According to [Doc 100481], the answer is \"rains\". The sentence should read:\n\nHazuki clips her nails every time it rains.", "latency_ms": 6997.6465702056885, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer matches the gold answer exactly ('rains'), so it is fully correct. However, it fabricates a citation to 'Doc 100481' and claims the document states the answer, which is not grounded in the provided context, so faithfulness is low.", "main_total_tokens": 2973, "main_prompt_tokens": 2942, "main_completion_tokens": 31, "judge_total_tokens": 507, "judge_prompt_tokens": 421, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland", "latency_ms": 4595.606327056885, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Greenland'), so it is both correct and faithful.", "main_total_tokens": 3068, "main_prompt_tokens": 3065, "main_completion_tokens": 3, "judge_total_tokens": 446, "judge_prompt_tokens": 395, "judge_completion_tokens": 51}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "According to [Doc 96838], the movie is \"One Afternoon\".", "latency_ms": 5476.994037628174, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold title 'One Afternoon' exactly and does not introduce unsupported details, so it is both correct and faithful.", "main_total_tokens": 2804, "main_prompt_tokens": 2787, "main_completion_tokens": 17, "judge_total_tokens": 477, "judge_prompt_tokens": 419, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "According to the context provided, the answer is \"The Life Ahead\" ([Doc 93260]).", "latency_ms": 11597.22089767456, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer, 'The Life Ahead', correctly matches the gold answer and accurately reflects the question. It is fully correct and faithful to the given context.", "main_total_tokens": 3470, "main_prompt_tokens": 3449, "main_completion_tokens": 21, "judge_total_tokens": 480, "judge_prompt_tokens": 418, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "According to [Doc 73859], the 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team.", "latency_ms": 7673.182964324951, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and matches the gold answer without adding unsupported claims. It is both factually correct and faithful to the provided information.", "main_total_tokens": 3134, "main_prompt_tokens": 3098, "main_completion_tokens": 36, "judge_total_tokens": 491, "judge_prompt_tokens": 420, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "[Doc 52333] does not mention the name of the racing team.", "latency_ms": 6273.553371429443, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is faithful to the provided document snippet (it states that the document does not mention the team name), but it is incorrect with respect to the gold answer, which specifies the team as the McLaren Motor Racing team.", "main_total_tokens": 3208, "main_prompt_tokens": 3191, "main_completion_tokens": 17, "judge_total_tokens": 476, "judge_prompt_tokens": 399, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The text does not specify which character's country of origin is being asked about. However, based on the title \"Country of the Deaf\", it can be inferred that the main character Yaya is likely from Russia or a Russian-speaking country, given that the movie title is in Russian (\"Strana Glukhikh\").", "latency_ms": 10358.084440231323, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer is New Zealand, while the generated answer speculates Russia or a Russian-speaking country. This is incorrect. It also incorrectly claims the text does not specify which character’s country of origin is being asked about, instead of simply not having enough context to answer. The reasoning is speculative and not grounded in the provided gold answer, so correctness is 0. Some limited faithfulness is present because it at least tries to reason from the movie title, but it diverges from the labeled truth, so faithfulness is low.", "main_total_tokens": 3028, "main_prompt_tokens": 2962, "main_completion_tokens": 66, "judge_total_tokens": 573, "judge_prompt_tokens": 439, "judge_completion_tokens": 134}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "According to the provided context, there is no mention of John McClane blowing up the Great Wall of China in any 2013 movie. However, I can suggest that you might be thinking of \"A Good Day to Die Hard\" (2013), which features John McClane as the main character, but it does not involve him blowing up the Great Wall of China.\n\nIt's possible that you're confusing this with another movie, such as \"The Great Wall\" (2016), which features a battle on the Great Wall of China, but it does not feature John McClane.", "latency_ms": 15479.931116104126, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.6, "faithfulness_score": 0.7, "justification": "The gold answer simply states that there is no such movie in the database, which is correct relative to the dataset. The generated answer adds speculation and suggestions about possible movies, which go beyond the gold and the provided context. While it correctly notes that there is no 2013 movie where John McClane blows up the Great Wall of China, it then introduces additional movie titles and conjecture not present in the context. This lowers faithfulness. Correctness is partially reduced because the task is to match the gold answer (\"I do not have a movie like this in my database\"), and the extra speculation is unnecessary and not supported by the gold.", "main_total_tokens": 3049, "main_prompt_tokens": 2930, "main_completion_tokens": 119, "judge_total_tokens": 683, "judge_prompt_tokens": 517, "judge_completion_tokens": 166}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "According to [Doc 28589], the answer is \"Tomie: Forbidden Fruit\".", "latency_ms": 5911.622524261475, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Tomie: Forbidden Fruit' and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 2985, "main_prompt_tokens": 2966, "main_completion_tokens": 19, "judge_total_tokens": 491, "judge_prompt_tokens": 435, "judge_completion_tokens": 56}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 years [Doc 98566]", "latency_ms": 5235.099792480469, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years) and does not introduce unsupported information, so it is both correct and faithful.", "main_total_tokens": 3233, "main_prompt_tokens": 3224, "main_completion_tokens": 9, "judge_total_tokens": 461, "judge_prompt_tokens": 403, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King teams up with a female warrior named Tala, who is the sister of The Nubian King. Together they search for a legendary relic known as The Book of Souls, which will allow them to put an end to an evil warlord. [Doc 82582]", "latency_ms": 10635.588645935059, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks what the Scorpion King and Tala search for. The gold answer is 'The Book of Souls.' The generated answer states they search for a legendary relic known as The Book of Souls, matching the gold answer and providing consistent context. Therefore it is both correct and faithful to the source.", "main_total_tokens": 3629, "main_prompt_tokens": 3569, "main_completion_tokens": 60, "judge_total_tokens": 542, "judge_prompt_tokens": 451, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Not found [Doc].", "latency_ms": 4257.6704025268555, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is not in the database. The generated answer conveys the same lack of information (“Not found”), which is correct and faithful to the gold response. There is no hallucinated year or extra details.", "main_total_tokens": 2655, "main_prompt_tokens": 2649, "main_completion_tokens": 6, "judge_total_tokens": 472, "judge_prompt_tokens": 396, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Not found [Doc 7967], [Doc 109654], [Doc 110414], [Doc 130930], [Doc 65897], [Doc 130152], [Doc 70740], [Doc 102239], [Doc 130930], [Doc 17000], [Doc 33333], [Doc 70740], [Doc 102239], [Doc 130930], [Doc 2192]", "latency_ms": 13399.614572525024, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "Gold answer says the movie is not in the database; the generated answer effectively conveys that it is not found, so it is correct in outcome. However, it adds extraneous document ID references that are not supported by the gold and are somewhat opaque, so faithfulness is slightly reduced.", "main_total_tokens": 3353, "main_prompt_tokens": 3260, "main_completion_tokens": 93, "judge_total_tokens": 571, "judge_prompt_tokens": 481, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Not found [Doc 58699], [Doc 697], [Doc 11800], [Doc 13807], [Doc 296], [Doc 14200], [Doc 1943], [Doc 112569], [Doc 8853], [Doc 197], [Doc 296], [Doc 36911], [Doc 70781], [Doc 76993], [Doc 81169], [Doc 76625], [Doc 89836], [Doc 100189], and [Doc 121172]", "latency_ms": 15026.778936386108, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The gold answer indicates the movie is not in the database and does not provide a release year. The generated answer also does not provide a year and effectively says it was not found, so it is correct relative to the question. However, it references specific document IDs that are not mentioned in the prompt or gold answer, introducing unsupported details, so it is not fully faithful.", "main_total_tokens": 3073, "main_prompt_tokens": 2959, "main_completion_tokens": 114, "judge_total_tokens": 609, "judge_prompt_tokens": 504, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no information in the provided context about a movie titled \"Data Science: The Movie\".", "latency_ms": 5944.664716720581, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie that is not known. The gold response correctly indicates lack of information. The generated response also correctly states that there is no information in the provided context about the movie, and does not fabricate a year. This is correct and faithful to the (absent) context and the gold answer’s stance of not having that movie in the database.", "main_total_tokens": 2876, "main_prompt_tokens": 2856, "main_completion_tokens": 20, "judge_total_tokens": 518, "judge_prompt_tokens": 410, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "Not found", "latency_ms": 4756.163835525513, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer conveys that the movie is not present/known, matching the gold answer’s meaning. It does not introduce incorrect facts and remains faithful to the provided gold response.", "main_total_tokens": 3251, "main_prompt_tokens": 3248, "main_completion_tokens": 3, "judge_total_tokens": 459, "judge_prompt_tokens": 393, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no movie titled \"The Last Pixel\". However, there are movies titled \"Pixels\", with two different releases in 2010 and 2015.", "latency_ms": 6702.150583267212, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct behavior per the gold is to state that the movie is not in the database, not to assert that it does not exist. The generated answer incorrectly claims there is no movie titled \"The Last Pixel,\" which could be false, and introduces extra information about other movies not mentioned in the gold. This contradicts the gold response and goes beyond it, so it is neither correct nor faithful.", "main_total_tokens": 2607, "main_prompt_tokens": 2574, "main_completion_tokens": 33, "judge_total_tokens": 531, "judge_prompt_tokens": 421, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no mention of \"The man who ate the sun\" in the provided context.", "latency_ms": 6552.858114242554, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database; the generated answer says there is no mention in the provided context. Both convey lack of information without fabricating a year, so the content is factually safe (correct) and aligns with the absence of data (faithful). The slight wording difference does not introduce incorrect information.", "main_total_tokens": 3481, "main_prompt_tokens": 3462, "main_completion_tokens": 19, "judge_total_tokens": 508, "judge_prompt_tokens": 410, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "[Doc 46069] does not mention the director of the movie \"The Last Days of Lehman Brothers\".", "latency_ms": 6954.041481018066, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the director, and the gold answer correctly states that this information is not available in the database. The generated answer states that the cited document does not mention the director, which is consistent with the gold rationale of missing information. It does not hallucinate a director and stays within the evidence. Therefore it is both correct and faithful to the given context.", "main_total_tokens": 3257, "main_prompt_tokens": 3233, "main_completion_tokens": 24, "judge_total_tokens": 524, "judge_prompt_tokens": 420, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Not found.", "latency_ms": 4687.966346740723, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the system cannot answer because the director information is not in the database. The generated answer says 'Not found,' which is equivalent in meaning: it indicates the director is unknown/unavailable. Thus it is correct relative to the gold and faithful to the constraint that the database lacks the information.", "main_total_tokens": 3052, "main_prompt_tokens": 3048, "main_completion_tokens": 4, "judge_total_tokens": 490, "judge_prompt_tokens": 398, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context does not provide information about who directed the movie \"Journey into Gaza\".", "latency_ms": 6439.4121170043945, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not provide information about the director, which matches the gold answer that the director information is not in the database. It is both correct and faithful to the provided context.", "main_total_tokens": 3161, "main_prompt_tokens": 3143, "main_completion_tokens": 18, "judge_total_tokens": 482, "judge_prompt_tokens": 409, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Not found [Doc 135098].", "latency_ms": 4608.28709602356, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer correctly states that the system cannot answer because the director information is not in the database. The generated answer instead asserts 'Not found [Doc 135098]' without grounding this in the stated database limitation and introduces a specific document reference that is not mentioned in the gold. This is not an equivalent or fully correct response with respect to the task, and it is not faithful to the gold rationale about missing database information.", "main_total_tokens": 2716, "main_prompt_tokens": 2707, "main_completion_tokens": 9, "judge_total_tokens": 521, "judge_prompt_tokens": 405, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "According to [Doc 98375], this happens in the movie \"Rise of the Mummy\" (2021).", "latency_ms": 6606.101751327515, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'Rise of the Mummy,' matching the gold answer, so correctness is 1. However, it fabricates a source citation ('Doc 98375') and a specific year (2021) not given in the prompt, so faithfulness to the provided information is 0.", "main_total_tokens": 2858, "main_prompt_tokens": 2832, "main_completion_tokens": 26, "judge_total_tokens": 511, "judge_prompt_tokens": 415, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "According to [Doc 35237], this happens in the movie \"Stranger in My Bed\" (2005).", "latency_ms": 6438.589572906494, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Stranger in My Bed,\" which matches the gold answer. It is faithful to the provided document reference and does not introduce unsupported claims beyond specifying the year, which is consistent with common knowledge about the film.", "main_total_tokens": 2864, "main_prompt_tokens": 2839, "main_completion_tokens": 25, "judge_total_tokens": 501, "judge_prompt_tokens": 420, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "According to [Doc 123938], this happens in \"Game of Silence\".", "latency_ms": 5183.053731918335, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The answer 'Game of Silence' is correct, as this plot describes that TV series. However, the justification references a non-existent source '[Doc 123938]', which is fabricated, so the reasoning is not faithful to the provided information or tools.", "main_total_tokens": 2496, "main_prompt_tokens": 2479, "main_completion_tokens": 17, "judge_total_tokens": 494, "judge_prompt_tokens": 413, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "According to [Doc 53039], the movie \"The Craigslist Killer\" (2011) is based on a true story about a killer who used Craigslist to find victims.", "latency_ms": 7204.033613204956, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as \"The Craigslist Killer,\" so the correctness is 1. However, it references an external source \"[Doc 53039]\" that is not in the provided context, making that citation fabricated; thus faithfulness is 0.", "main_total_tokens": 2720, "main_prompt_tokens": 2684, "main_completion_tokens": 36, "judge_total_tokens": 513, "judge_prompt_tokens": 427, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "According to [Doc 66094], this happens in the movie \"Wood Job!\".", "latency_ms": 7484.012842178345, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Wood Job!', which matches the gold answer. It does not introduce contradictions or fabrications about the content; the reference to a document does not alter the factual answer, so the response is both correct and faithful.", "main_total_tokens": 4157, "main_prompt_tokens": 4139, "main_completion_tokens": 18, "judge_total_tokens": 499, "judge_prompt_tokens": 417, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase (2017) [Doc 78373]", "latency_ms": 6240.679740905762, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'Sesame Street: The Magical Wand Chase', matching the gold answer, so correctness is 1. However, it adds extra metadata '[Doc 78373]' that is not present in the gold answer or the question context, so it is not fully faithful to the given information, resulting in a faithfulness score of 0.", "main_total_tokens": 3233, "main_prompt_tokens": 3213, "main_completion_tokens": 20, "judge_total_tokens": 533, "judge_prompt_tokens": 427, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "According to [Doc 74597], this happens in the movie \"Beyond\" (2016).", "latency_ms": 5692.013263702393, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Beyond,\" which matches the gold answer. The added year (2016) is consistent with the known film and does not contradict the source, so the response remains faithful to the given information.", "main_total_tokens": 2674, "main_prompt_tokens": 2653, "main_completion_tokens": 21, "judge_total_tokens": 506, "judge_prompt_tokens": 428, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "According to [Doc 22478], this happens in the movie \"6ixtynin9\".", "latency_ms": 5985.687494277954, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as '6ixtynin9' and does not introduce any contradictions or unsupported claims beyond a generic citation placeholder.", "main_total_tokens": 2805, "main_prompt_tokens": 2783, "main_completion_tokens": 22, "judge_total_tokens": 490, "judge_prompt_tokens": 423, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "According to [Doc 32410], this happens in the movie \"Club Dread\".", "latency_ms": 5908.29062461853, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'Club Dread', so correctness is 1. However, it cites a specific document ([Doc 32410]) that was not mentioned in the user prompt or gold answer, making the explanation unfaithful to the provided context, so faithfulness is 0.", "main_total_tokens": 3108, "main_prompt_tokens": 3090, "main_completion_tokens": 18, "judge_total_tokens": 535, "judge_prompt_tokens": 442, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet (2010) [Doc 48873]", "latency_ms": 5980.138063430786, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot. The gold answer is 'Toilet', and the generated answer is 'Toilet (2010) [Doc 48873]', which correctly identifies the same film with additional year and document reference. This is correct and faithful to the gold.", "main_total_tokens": 3451, "main_prompt_tokens": 3438, "main_completion_tokens": 13, "judge_total_tokens": 514, "judge_prompt_tokens": 425, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel (1981) [Doc 1024]", "latency_ms": 5703.23634147644, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Electric Angel', which matches the gold answer. It does not introduce any contradictions or unsupported claims and remains faithful to the provided information.", "main_total_tokens": 3272, "main_prompt_tokens": 3259, "main_completion_tokens": 13, "judge_total_tokens": 494, "judge_prompt_tokens": 428, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "According to the provided context, the lowest rated movie from 2023 is \"Dark\" with a rating of 4.5 [Doc 108151].", "latency_ms": 5373.146057128906, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the gold label indicates that the lowest rated movie from 2023 is 'Mega Ape', whereas the generated response claims it is 'Dark' with a rating of 4.5. However, the answer is faithful to its cited context, as it explicitly refers to the provided document and does not introduce information beyond that context.", "main_total_tokens": 2675, "main_prompt_tokens": 2642, "main_completion_tokens": 33, "judge_total_tokens": 510, "judge_prompt_tokens": 413, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "According to the provided context, the highest rated TV show from 2010 is \"Game of Thrones\" with a rating of 8.457, but it was actually released in 2011.\n\nAmong the TV shows that were actually released in 2010, the highest rated one is \"Justified\" with a rating of 8.0, followed by \"Sherlock\" and \"The Walking Dead\", both with ratings of 8.094 and 8.511 respectively", "latency_ms": 9803.618431091309, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says the correct response is 'Tea House'. The generated answer instead lists several different TV shows (Game of Thrones, Justified, Sherlock, The Walking Dead) and associated ratings, none of which match the gold answer. It therefore gives an incorrect result. It also introduces ratings and details that are not grounded in the gold/reference, so it is unfaithful to the provided answer as well.", "main_total_tokens": 3075, "main_prompt_tokens": 2977, "main_completion_tokens": 98, "judge_total_tokens": 587, "judge_prompt_tokens": 479, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "According to the provided context, the highest rated Japanese language movie from 1999 is \"Kikujiro\" with a rating of 7.5 [Doc 22213].", "latency_ms": 8347.22876548767, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the highest rated Japanese language movie from 1999 is 'The Uncut Story of Mewtwo's Origin', whereas the generated answer claims it is 'Kikujiro' with a rating of 7.5. This directly contradicts the gold answer, so the generated answer is incorrect. However, the generated answer is faithful to its cited context [Doc 22213], as it reports exactly what that document states. Therefore, correctness is 0, and faithfulness to the given context is 1.", "main_total_tokens": 3178, "main_prompt_tokens": 3140, "main_completion_tokens": 38, "judge_total_tokens": 568, "judge_prompt_tokens": 428, "judge_completion_tokens": 140}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "According to [Doc 268], \"The Enemy\" has a rating of 5.0 and its language is Turkish (tr).", "latency_ms": 6557.498455047607, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer claims the lowest-rated 1980 movie is in Turkish, but the gold answer is English, so it is factually incorrect. However, the answer is consistent with the provided document citation and does not invent extra details beyond that, so it is faithful to the referenced source.", "main_total_tokens": 2705, "main_prompt_tokens": 2677, "main_completion_tokens": 28, "judge_total_tokens": 499, "judge_prompt_tokens": 410, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "According to the provided context, the TV show \"Teen Choice Awards\" had a rating of 9.1 and a popularity of 0.4589, but it's not clear if this is the highest popularity among all TV shows in 1999.\n\nHowever, another document mentions \"Who Wants to Be a Millionaire?\" with a popularity of 2.3756, which is higher than the Teen Choice Awards.\n\nBut then there's \"Hollywood Squares\" with a popularity of 7.0597, which seems to be the highest among all mentioned TV shows in 1999.\n\nSo, based on the provided context, it appears that \"Hollywood Squares\" was the TV show in 1999 with the highest popularity.", "latency_ms": 20160.387754440308, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'Law & Order: Special Victims Unit', but the generated answer concludes that 'Hollywood Squares' had the highest popularity, which is incorrect. It also fabricates and compares specific popularity numbers and rankings that are not supported by the gold answer, so it is not faithful to the required outcome.", "main_total_tokens": 4039, "main_prompt_tokens": 3887, "main_completion_tokens": 152, "judge_total_tokens": 629, "judge_prompt_tokens": 535, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2 [Doc 79355]", "latency_ms": 4942.626953125, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and does not introduce unsupported information. It is both correct and faithful.", "main_total_tokens": 2916, "main_prompt_tokens": 2906, "main_completion_tokens": 10, "judge_total_tokens": 450, "judge_prompt_tokens": 392, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "According to [Doc 112077], the movie \"The Story of Us\" was released in 2025.\n\nHowever, there are also other movies with the same title released in different years:\n\n* [Doc 88811] mentions a movie titled \"The Story of Us\" released in 2019.\n* [Doc 22481] mentions a movie titled \"The Story of Us\" released in 1999.\n\nSo, the answer depends on which specific movie you are referring to.", "latency_ms": 13388.198375701904, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly identifies 2019 as the correct year. The generated answer highlights multiple possible years and fails to select 2019 as the definitive answer, instead saying the answer depends on which movie is meant. This makes the final answer incorrect with respect to the gold. However, the generated answer is faithful to the provided documents, accurately reporting the years they mention and not fabricating information beyond them.", "main_total_tokens": 2942, "main_prompt_tokens": 2842, "main_completion_tokens": 100, "judge_total_tokens": 595, "judge_prompt_tokens": 483, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The number of votes is not mentioned in the provided context. However, it does mention a rating of 6.7 and a popularity score of 1.7749.", "latency_ms": 8058.924913406372, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that 'My Dog Skip' received 279 votes, but the generated answer incorrectly states that the number of votes is not mentioned. This is factually wrong relative to the gold. However, the generated answer is internally consistent with its stated assumption about the provided context and does not invent a specific vote count, so it remains faithful to its own reasoning about the (purported) context.", "main_total_tokens": 3138, "main_prompt_tokens": 3102, "main_completion_tokens": 36, "judge_total_tokens": 528, "judge_prompt_tokens": 416, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "Not found [Doc 43976], [Doc 85434].", "latency_ms": 5428.969383239746, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 12 votes, but the generated answer says the information was not found, which is incorrect. The model is faithful to the (incorrect) retrieval result it reports, but the final answer does not match the gold.", "main_total_tokens": 2955, "main_prompt_tokens": 2940, "main_completion_tokens": 15, "judge_total_tokens": 473, "judge_prompt_tokens": 395, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "According to [Doc 46069], the movie \"The Last Days of Lehman Brothers\" was released in 2009.", "latency_ms": 7184.456348419189, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie \"The Last Days of Lehman Brothers\" was released in 2009, which matches the gold answer. It also attributes the information to the provided document, aligning with the given context, so it is both correct and faithful.", "main_total_tokens": 3247, "main_prompt_tokens": 3220, "main_completion_tokens": 27, "judge_total_tokens": 498, "judge_prompt_tokens": 413, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en", "latency_ms": 4713.071346282959, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the original language is English (\"en\"), and the generated answer also states \"en.\" This matches the factual truth, so the response is both correct and faithful to the gold.", "main_total_tokens": 3229, "main_prompt_tokens": 3227, "main_completion_tokens": 2, "judge_total_tokens": 451, "judge_prompt_tokens": 384, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "Wall Street was released in the year 1987 [Doc 6993].", "latency_ms": 4931.986570358276, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly gives 1987, matching the gold answer, and it does not introduce contradictions or unsupported claims beyond a plausible document reference.", "main_total_tokens": 2317, "main_prompt_tokens": 2300, "main_completion_tokens": 17, "judge_total_tokens": 454, "judge_prompt_tokens": 394, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie in which classmates have to play a survival game using their cell phones is \"47 Hours to Live\" [Doc 90746].", "latency_ms": 5648.293495178223, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'Brain Fluid Explosion Girl', but the generated answer claims it is '47 Hours to Live', which is factually incorrect. There is no support in the provided QA pair for this incorrect movie title, so the response is also unfaithful to the given information.", "main_total_tokens": 2053, "main_prompt_tokens": 2024, "main_completion_tokens": 29, "judge_total_tokens": 504, "judge_prompt_tokens": 416, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show where six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" (1983) [Doc 112849].", "latency_ms": 6253.539085388184, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons', which matches the gold answer exactly. It stays faithful to the provided information and does not introduce contradictions, only adding the year and a reference, which are consistent and not misleading.", "main_total_tokens": 2094, "main_prompt_tokens": 2060, "main_completion_tokens": 34, "judge_total_tokens": 504, "judge_prompt_tokens": 423, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 3912.9371643066406, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'Nacht der Wölfe' as the movie about two rival gangs in Munich. It is fully correct and faithful to the reference.", "main_total_tokens": 2300, "main_prompt_tokens": 2293, "main_completion_tokens": 7, "judge_total_tokens": 473, "judge_prompt_tokens": 403, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of Picnic at Hanging Rock take place in 1900. [Doc 126191]", "latency_ms": 5050.081968307495, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 1900 as the year in which the events of Picnic at Hanging Rock take place, matching the gold answer. It is fully supported by the cited document, so both correctness and faithfulness are complete.", "main_total_tokens": 2117, "main_prompt_tokens": 2095, "main_completion_tokens": 22, "judge_total_tokens": 482, "judge_prompt_tokens": 405, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The movie that fits the description is \"June\" (2004), as it is about a young violinist named June who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her.", "latency_ms": 9370.092153549194, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'June' (matching the gold answer) and accurately restates the description from the question without adding unsupported details. It is both correct and faithful.", "main_total_tokens": 2453, "main_prompt_tokens": 2390, "main_completion_tokens": 63, "judge_total_tokens": 562, "judge_prompt_tokens": 493, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1381.7720413208008, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a particular film, but the generated answer says 'I don't know,' which does not match the correct response and is therefore incorrect. However, the model is honest about its uncertainty and does not hallucinate an alternative answer, so it is fully faithful to its own stated knowledge limits.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 490, "judge_prompt_tokens": 399, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 401.0038375854492, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer admits lack of knowledge instead of providing an incorrect value. This is faithful to the model's uncertainty but does not supply the correct numeric rating (7.6), so correctness is 0 while faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 467, "judge_prompt_tokens": 389, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1160.7158184051514, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.5143371794949736, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.013881288441812, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is factually incorrect for this evaluation. However, the assistant directly answered the question without adding unrelated or hallucinated reasoning, so it is faithful to the user query itself.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 476, "judge_prompt_tokens": 394, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 231.6763401031494, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2 but the generated answer is 0, so the response is incorrect. The explanation is implicitly faithful to the given gold and generated values; there is no fabricated information, only a direct comparison.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 470, "judge_prompt_tokens": 395, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1620.408058166504, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct specific show 'Fantômas,' so correctness is 0. However, it does not introduce any hallucinated information and accurately reflects uncertainty, so faithfulness to the model's knowledge is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 389, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise", "latency_ms": 3877.6917457580566, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Kill Cruise'), which is the correct movie for the described plot. It is both correct and fully faithful to the reference.", "main_total_tokens": 2308, "main_prompt_tokens": 2305, "main_completion_tokens": 3, "judge_total_tokens": 451, "judge_prompt_tokens": 387, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life's most important issues is not explicitly mentioned in the provided context. However, based on the information given, it seems that \"Won't You Be My Neighbor?\" [Doc 84838] might be related to Fred Rogers and his television program, as it mentions him speaking directly to young children about important issues.\n\nAdditionally, another movie titled \"A Beautiful Day in the Neighborhood\" [Doc 87937] is mentioned, which features a journalist writing an Esquire profile piece on Fred Rogers. This could potentially be the movie you are looking for.\n\nPlease note that without more specific information or clarification, it's difficult to provide a definitive answer.", "latency_ms": 18405.78579902649, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks directly for the movie title; the gold answer is 'Won't You Be My Neighbor?'. The generated answer equivocates and says the title is not explicitly mentioned, offering both 'Won't You Be My Neighbor?' and 'A Beautiful Day in the Neighborhood' as possibilities and refusing to commit. Thus it does not correctly answer the question. It is also unfaithful to the provided context, which clearly identifies 'Won't You Be My Neighbor?' as the relevant film, so faithfulness is low.", "main_total_tokens": 2478, "main_prompt_tokens": 2329, "main_completion_tokens": 149, "judge_total_tokens": 682, "judge_prompt_tokens": 547, "judge_completion_tokens": 135}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The year it was released is 2023 [Doc 107925].", "latency_ms": 4996.929883956909, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states the release year as 2023, while the correct year is 2018, so it is factually incorrect. However, it clearly bases its answer on the cited document [Doc 107925], indicating it is faithful to its stated source even though that source (or its use) is wrong.", "main_total_tokens": 2414, "main_prompt_tokens": 2398, "main_completion_tokens": 16, "judge_total_tokens": 488, "judge_prompt_tokens": 392, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is Astro Boy, according to [Doc 112591].", "latency_ms": 6200.012445449829, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as Astro Boy, matching the gold answer exactly. It also remains faithful to the cited document context without adding unsupported information.", "main_total_tokens": 2384, "main_prompt_tokens": 2354, "main_completion_tokens": 30, "judge_total_tokens": 479, "judge_prompt_tokens": 416, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "According to [Doc 67513], the woman and her young son were held captive for 7 years in an enclosed space before gaining their freedom.", "latency_ms": 5622.094631195068, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9463946303571863, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer. It does not introduce contradictions or unsupported claims and remains consistent with the referenced document.", "main_total_tokens": 1936, "main_prompt_tokens": 1905, "main_completion_tokens": 31, "judge_total_tokens": 496, "judge_prompt_tokens": 424, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "According to [Doc 133469], on 26 November 1983, six armed men broke into the Brink's-Mat security depot and found gold bullion worth £26m.", "latency_ms": 6855.193853378296, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks how much gold was being held at the Brink's-Mat security depot before the 26 November 1983 break-in. The provided answer says the depot held gold bullion worth £26m, matching the figure in the reference ([Doc 133469]) that states six armed men broke in and found gold bullion worth £26m. This directly answers the question and is consistent with the cited document, so it is both correct and faithful.", "main_total_tokens": 2191, "main_prompt_tokens": 2151, "main_completion_tokens": 40, "judge_total_tokens": 554, "judge_prompt_tokens": 432, "judge_completion_tokens": 122}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6361.419677734375, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered 'I don't know,' which is incorrect because the gold answer is 80.5772. However, it did not hallucinate any facts and simply admitted uncertainty, so the response is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 470, "judge_prompt_tokens": 396, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 330.8374881744385, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there was 1 Russian language TV show released in 1997, while the generated answer is 0, so the generated answer is factually incorrect relative to the gold. However, there is no evidence that the model contradicted or ignored any information from the prompt beyond providing the (incorrect) count, so it is fully faithful to the user query and given context.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 494, "judge_prompt_tokens": 385, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" (2014). [Doc 121928]", "latency_ms": 6411.348581314087, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Angel Eyes,' which matches the gold answer exactly. It remains faithful to the information implied by the question and does not introduce any contradictory or unsupported claims; adding the year 2014 is accurate context and does not detract from faithfulness.", "main_total_tokens": 2140, "main_prompt_tokens": 2107, "main_completion_tokens": 33, "judge_total_tokens": 515, "judge_prompt_tokens": 426, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of this movie is French (fr).", "latency_ms": 3990.8430576324463, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because it states the language is French, while the gold label says Korean. However, it is fully faithful to the user-provided generated text, accurately summarizing what it said, so faithfulness is high.", "main_total_tokens": 2018, "main_prompt_tokens": 2007, "main_completion_tokens": 11, "judge_total_tokens": 466, "judge_prompt_tokens": 386, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 661.9570255279541, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the count of English-speaking horror movies released in 2009 with popularity > 7. The gold answer is 2, but the generated answer is 0, which is incorrect with respect to the ground truth. However, the generated answer directly responds to the question without adding unsupported information, so it is faithful but incorrect.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 494, "judge_prompt_tokens": 394, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The best-rated movie released in 1979 is not found in the provided context.", "latency_ms": 4983.106851577759, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Given the provided gold answer, the correct response is that no such movie exists in the database/context. The generated answer states that the best-rated movie released in 1979 is not found in the provided context, which is logically equivalent to the gold answer. It does not introduce any unsupported information and stays within the given context. Therefore, it is both correct relative to the task and faithful to the available information.", "main_total_tokens": 2230, "main_prompt_tokens": 2212, "main_completion_tokens": 18, "judge_total_tokens": 525, "judge_prompt_tokens": 412, "judge_completion_tokens": 113}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "There is no mention of a movie titled \"Back to the Future Part IV\" in the provided context. The highest numbered title mentioned is \"Back to the Future Part III\", which is listed as [Doc 10621] and has a release year of 1990.", "latency_ms": 8778.629779815674, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.5, "faithfulness_score": 1, "justification": "The question asks for the release year of Back to the Future Part IV. The correct gold answer is that such a movie does not exist (in the database), i.e., there is no release year. The generated answer instead talks about the highest numbered title mentioned (Part III, 1990) and never explicitly states that Part IV does not exist or that there is no release year. Thus it does not correctly answer the question, but it is fully faithful to the given context it references (it accurately describes that only Part III, 1990, is mentioned).", "main_total_tokens": 2536, "main_prompt_tokens": 2480, "main_completion_tokens": 56, "judge_total_tokens": 602, "judge_prompt_tokens": 455, "judge_completion_tokens": 147}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that there are no movies fitting the criteria, which is correct because the Chernobyl disaster occurred in 1986, so there were no 1980 movies about it. The generated answer is faithful to the question and does not introduce fabricated details; it simply suggests reconsidering the temporal premise.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 531, "judge_prompt_tokens": 434, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 285.0940227508545, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer ('0') is numerically incorrect compared to the gold answer of 4, so correctness is 0. However, there is no reasoning or additional content that contradicts the question; it simply provides a wrong number, so with respect to staying on-topic and not hallucinating extra details, faithfulness is maximal.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 388, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 369.78626251220703, "retrieval_recall": 0.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.17723928678404774, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies 2, but the generated answer is 1, so it is factually incorrect relative to the given gold. The explanation is otherwise consistent with the given numbers (no extra fabricated details), so faithfulness to the provided comparison is intact.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 386, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 231.66418075561523, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states that there were 0 TV shows released in the 1990s about Super Mario Bros, while the gold answer indicates there was 1. This makes the generated answer factually incorrect, so correctness is 0. The model directly answers the question without contradicting or ignoring the prompt, so faithfulness to the user query is high, yielding a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 501, "judge_prompt_tokens": 389, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 231.70804977416992, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (0) and correctly reflects that there were no TV shows about Super Mario Bros released between 2000 and 2010. It is fully correct and faithful to the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 391, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 216.16315841674805, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The user question has a clear expected answer: 'Naruto.' The generated answer 'I don't know.' is incorrect because it fails to provide the known popular ninja-related show. However, it is faithful in that it does not fabricate information or contradict the model's stated uncertainty, so faithfulness is high while correctness is zero.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 482, "judge_prompt_tokens": 386, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "There is no Jurassic Park/Power Ranger crossover movie mentioned in the provided context.", "latency_ms": 4779.572248458862, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie was not released. The generated answer states there is no such crossover movie mentioned, which aligns in meaning with the gold (i.e., it does not exist/was not released). It is both correct and faithful to the gold.", "main_total_tokens": 2235, "main_prompt_tokens": 2218, "main_completion_tokens": 17, "judge_total_tokens": 484, "judge_prompt_tokens": 401, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "There is no movie where Barbie fights the Predator.", "latency_ms": 4394.522428512573, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no such movie, which is equivalent in meaning to the gold answer that the movie was not released. It is fully correct and faithful to the gold response.", "main_total_tokens": 2273, "main_prompt_tokens": 2262, "main_completion_tokens": 11, "judge_total_tokens": 462, "judge_prompt_tokens": 392, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The 4th oldest Star Wars movie is not found in the provided context. However, based on the information given, we can determine that \"Return of the Jedi\" (1983) [Doc 2858] is the third oldest Star Wars movie mentioned.\n\nTo find the fourth oldest, we need to look at the other movies and their release years:\n\n* The Empire Strikes Back (1980) [Doc 3]\n* Return of the Jedi (1983) [Doc 2858]\n\nSince there are no other Star Wars movies from before 1980 in the provided context, we can conclude that \"The Story of Star Wars\" is not a movie but rather a documentary about Star Wars.\n\nHowever, if we consider documentaries and TV series as well, then:\n\n* When Star Wars Ruled the World (2004) [Doc 32997]\n* The Characters of Star Wars (2004) [Doc 31821]\n* Science of Star Wars (2005) [Doc 117312]\n\nThe fourth oldest would be \"When Star Wars Ruled the World\" or \"The Characters of Star Wars\", both from 2004, but since we are looking for a movie and not a documentary, we can consider \"Return of the Jedi\" as", "latency_ms": 27765.69175720215, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'Star Wars: Droids - The Great Heep', but the generated answer does not mention this title and instead rambles about other films and documentaries, ending incoherently. It is therefore incorrect. It is also unfaithful to the gold answer, as it neither aligns with nor approximates the correct title or reasoning.", "main_total_tokens": 2647, "main_prompt_tokens": 2391, "main_completion_tokens": 256, "judge_total_tokens": 747, "judge_prompt_tokens": 646, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The year it was released is 2023 [Doc 107925].", "latency_ms": 4921.809911727905, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states the release year as 2023, while the correct year is 1986, so it is factually incorrect. There is no indication that the incorrect year comes from the provided gold answer; instead it appears to be hallucinated or misread from an external document reference, so the response is not faithful to the given information.", "main_total_tokens": 2414, "main_prompt_tokens": 2398, "main_completion_tokens": 16, "judge_total_tokens": 488, "judge_prompt_tokens": 392, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 231.68587684631348, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a concrete rating (8.466), but the model responded with 'I don't know.' While this is honest about its uncertainty and thus faithful, it is incorrect because it fails to provide the known correct rating.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 398, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is Mighty Med [Doc 121125].", "latency_ms": 5594.247579574585, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Mighty Med' and does not introduce any contradictions or extraneous claims.", "main_total_tokens": 2085, "main_prompt_tokens": 2056, "main_completion_tokens": 29, "judge_total_tokens": 482, "judge_prompt_tokens": 420, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "The seven prisoners escaped from a prison in Argentina, specifically from the Villa Devoto prison in Buenos Aires. [Doc 53582]", "latency_ms": 6148.356914520264, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the country of the prison; the generated answer correctly states Argentina, which matches the gold. The added detail about Villa Devoto prison is plausible and does not contradict the question, and is presented as coming from the cited document, so it is faithful to the provided information.", "main_total_tokens": 2498, "main_prompt_tokens": 2470, "main_completion_tokens": 28, "judge_total_tokens": 507, "judge_prompt_tokens": 418, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The apprentice falls into Jiang Jie's trap in Sweet Trap (2024). [Doc 135231]", "latency_ms": 5312.407970428467, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, stating that the apprentice falls into Jiang Jie's trap. It is correct and fully supported by the provided information, with no hallucinations or deviations.", "main_total_tokens": 2202, "main_prompt_tokens": 2179, "main_completion_tokens": 23, "judge_total_tokens": 474, "judge_prompt_tokens": 405, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 592.7448272705078, "retrieval_recall": 0.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.19519002499605084, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.2, "ndcg_at_10": 0.3080351663449861, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer '0' is incorrect because the gold answer is '4'. The evaluation correctly identifies the mismatch without adding unsupported claims, so it is faithful to the provided information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 452, "judge_prompt_tokens": 385, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "46", "latency_ms": 177.75678634643555, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks how many movies feature both Steven Seagal and Bruce Lee. The gold answer correctly states that there are no movies with both of them. The generated answer of 46 is factually wrong. It is also unfaithful to the gold/reference answer, which explicitly indicates zero such movies.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 486, "judge_prompt_tokens": 395, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 5316.2572383880615, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3 and clearly attributes the value to the document, so it is both correct and faithful.", "main_total_tokens": 2352, "main_prompt_tokens": 2331, "main_completion_tokens": 21, "judge_total_tokens": 464, "judge_prompt_tokens": 403, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in this movie is street racing.", "latency_ms": 4417.095184326172, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the style of racing is drift, but the generated answer says street racing, which is incorrect. The evaluation correctly compares the two, so it is faithful to the task, but the generated answer itself is factually wrong relative to the gold.", "main_total_tokens": 2355, "main_prompt_tokens": 2343, "main_completion_tokens": 12, "judge_total_tokens": 473, "judge_prompt_tokens": 389, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little is \"Confronting a Serial Killer\". [Doc 131298]", "latency_ms": 7602.649688720703, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Confronting a Serial Killer.' It is fully correct and consistent with the provided information, with no hallucinated details.", "main_total_tokens": 2335, "main_prompt_tokens": 2290, "main_completion_tokens": 45, "judge_total_tokens": 521, "judge_prompt_tokens": 450, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 5480.525970458984, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (7.838) and does not introduce any unsupported information. It correctly attributes the value to the cited document.", "main_total_tokens": 2355, "main_prompt_tokens": 2332, "main_completion_tokens": 23, "judge_total_tokens": 469, "judge_prompt_tokens": 407, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\". [Doc 22098]", "latency_ms": 5686.068296432495, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6895405204413555, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the film as 'The Sixth Sense' and does not introduce any contradictions or unsupported content beyond a harmless citation tag.", "main_total_tokens": 1997, "main_prompt_tokens": 1967, "main_completion_tokens": 30, "judge_total_tokens": 485, "judge_prompt_tokens": 420, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in 2002 [Doc 27318].", "latency_ms": 4838.484287261963, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer and staying faithful to the provided information.", "main_total_tokens": 2271, "main_prompt_tokens": 2255, "main_completion_tokens": 16, "judge_total_tokens": 454, "judge_prompt_tokens": 395, "judge_completion_tokens": 59}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\". [Doc 67500]", "latency_ms": 6051.4514446258545, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2015 reboot title as 'The Transporter Refueled'. It does not introduce contradictions or unsupported claims; the bracketed doc reference does not affect factuality.", "main_total_tokens": 2297, "main_prompt_tokens": 2267, "main_completion_tokens": 30, "judge_total_tokens": 497, "judge_prompt_tokens": 420, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The Transporter Refueled was released in 2015 [Doc 67500].", "latency_ms": 5034.9180698394775, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer exactly. It stays fully faithful to the provided information without adding unsupported details.", "main_total_tokens": 2317, "main_prompt_tokens": 2298, "main_completion_tokens": 19, "judge_total_tokens": 473, "judge_prompt_tokens": 402, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "The romantic comedy where Abraham Lincoln comes back from the dead is not found in the provided context.", "latency_ms": 4851.6881465911865, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that there is no identified romantic comedy in which Abraham Lincoln comes back from the dead. This matches the information given and does not invent a specific movie, so it is correct. The generated answer is also faithful to the (implicit) context that no such film description is available.", "main_total_tokens": 2099, "main_prompt_tokens": 2079, "main_completion_tokens": 20, "judge_total_tokens": 502, "judge_prompt_tokens": 409, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by Ernest Hemingway is \"Havana Darkness\". [Doc 90923]", "latency_ms": 6031.733512878418, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'Havana Darkness' as the movie title and remaining faithful to the provided information.", "main_total_tokens": 2177, "main_prompt_tokens": 2147, "main_completion_tokens": 30, "judge_total_tokens": 474, "judge_prompt_tokens": 415, "judge_completion_tokens": 59}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show \"Doll & Em\". [Doc 122259]", "latency_ms": 5928.458213806152, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Doll & Em,\" which matches the gold answer exactly. It is faithful to the provided information and does not introduce any unsupported details beyond repeating the question context.", "main_total_tokens": 2234, "main_prompt_tokens": 2204, "main_completion_tokens": 30, "judge_total_tokens": 490, "judge_prompt_tokens": 417, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in the 2007 movie \"Hallam Foe\". [Doc 39389]", "latency_ms": 7056.565523147583, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2007 movie as \"Hallam Foe,\" matching the gold answer exactly. It is fully supported by the provided question and aligns with the source citation, so it is both correct and faithful.", "main_total_tokens": 2158, "main_prompt_tokens": 2119, "main_completion_tokens": 39, "judge_total_tokens": 514, "judge_prompt_tokens": 437, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The question asks if someone's last name sounds like they're a friend or foe based on their name alone.\n\nIn this context, we can look for clues from the provided documents that mention names related to friendship or enmity. \n\nFrom [Doc 46423], we have \"20th Century Boys 3: Redemption\" which mentions \"Friend\" as part of its plot. However, it does not provide a last name.\n\nFrom [Doc 22549], we have \"He is My Enemy, Partner and Father-In-Law\", but no specific last name related to friendship or enmity.\n\nFrom [Doc 40999], we have \"My Enemy's Enemy\" which implies that the person might be a friend due to being an enemy of their enemy. However, it does not provide a last name.\n\nFrom [Doc 49408], we have \"My Good Enemy\", but no specific last name related to friendship or enmity.\n\nFrom [Doc 56432], we have \"Frenemies\" which implies that the person might be both a friend and an enemy. However, it does not provide a last name.\n\nFrom [Doc 66883], we have \"Vanmam\" which mentions two friends turning into bitter foes. However, it does", "latency_ms": 27191.2739276886, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question clearly expects the categorical answer \"A foe\" (as in the gold). The generated answer never commits to either \"friend\" or \"foe\" and instead rambles about documents, so it is incorrect. It is also unfaithful to the provided gold answer because it does not reflect or align with it in any way.", "main_total_tokens": 2281, "main_prompt_tokens": 2025, "main_completion_tokens": 256, "judge_total_tokens": 739, "judge_prompt_tokens": 640, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "Sam's age is 18, as stated in the description of the TV show \"Atypical\" (2017) [Doc 124583].", "latency_ms": 5838.807821273804, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18, matching the gold answer. It also attributes this to a specific document, indicating it is grounded in the provided source. No hallucinated or conflicting information is present.", "main_total_tokens": 2029, "main_prompt_tokens": 1998, "main_completion_tokens": 31, "judge_total_tokens": 492, "judge_prompt_tokens": 418, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam is on the autism spectrum. [Doc 124583]", "latency_ms": 4504.784107208252, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which exactly matches the gold answer. It is fully accurate and faithful to the provided gold reference.", "main_total_tokens": 2244, "main_prompt_tokens": 2230, "main_completion_tokens": 14, "judge_total_tokens": 463, "judge_prompt_tokens": 400, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "The name of the place that Sandra was born is not found in the provided context.", "latency_ms": 4966.0069942474365, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that Sandra was born in Chamonix, while the generated answer claims that the birthplace is not found in the context. This directly contradicts the gold, so the generated answer is incorrect. However, the generated answer stays within the bounds of the given (unspecified) context and does not hallucinate a different location; it honestly states that the information is not found, so it is faithful to the context provided to it.", "main_total_tokens": 2345, "main_prompt_tokens": 2327, "main_completion_tokens": 18, "judge_total_tokens": 529, "judge_prompt_tokens": 414, "judge_completion_tokens": 115}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "it rains", "latency_ms": 3462.141513824463, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer 'it rains' is semantically correct and matches the gold answer 'rains', so correctness is full. However, the instruction is to fill in the blank, which should be just the missing word 'rains'. The generated output repeats extra context ('it rains'), so it does not faithfully adhere to the specified format for the task.", "main_total_tokens": 2191, "main_prompt_tokens": 2188, "main_completion_tokens": 3, "judge_total_tokens": 495, "judge_prompt_tokens": 393, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "According to [Doc 114810], Tabaluga is the last of the dragons and the crown-prince of Greenland.", "latency_ms": 5802.947044372559, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Greenland') and correctly attributes it to the referenced document. Both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2444, "main_prompt_tokens": 2418, "main_completion_tokens": 26, "judge_total_tokens": 480, "judge_prompt_tokens": 419, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers\" is not found among the provided documents.\n\nHowever, there are some movies that have similar descriptions or elements:\n\n* [Doc 37491] Title: Beautiful Ohio - This movie has a description about two brothers growing up in an unconventional household with their parents.\n* [Doc 36558] Title: How the Lack of Love Affects Two Men - This movie has a description about a miserly father and his clever yet immature son living emotionally impoverished lives together.\n\nBut none of these movies exactly match the given description.", "latency_ms": 16201.103925704956, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer, as given by the gold label, is 'One Afternoon'. The generated answer incorrectly claims the movie is not found and does not identify 'One Afternoon', so it is factually incorrect. However, the response remains faithful to the (incorrect) retrieval context it describes and does not hallucinate a specific wrong title, instead explicitly stating no exact match was found and only listing similar movies from documents, so it is faithful to its stated evidence.", "main_total_tokens": 2353, "main_prompt_tokens": 2215, "main_completion_tokens": 138, "judge_total_tokens": 663, "judge_prompt_tokens": 541, "judge_completion_tokens": 122}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The movie is \"The Life Ahead\" (2020) [Doc 93260].", "latency_ms": 4327.727794647217, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'The Life Ahead' and the generated answer is also 'The Life Ahead', matching exactly. The explanation remains aligned with the given gold answer, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2639, "main_prompt_tokens": 2620, "main_completion_tokens": 19, "judge_total_tokens": 490, "judge_prompt_tokens": 416, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary “McLaren” is about Bruce McLaren, a New Zealander who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 6277.233362197876, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and does not introduce any contradictions. The additional detail that he was a New Zealander who founded the McLaren Motor Racing team is accurate and consistent with the gold answer, making the response both correct and faithful.", "main_total_tokens": 2127, "main_prompt_tokens": 2093, "main_completion_tokens": 34, "judge_total_tokens": 509, "judge_prompt_tokens": 418, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The name of the racing team is not mentioned in any of the provided documents.", "latency_ms": 5983.705043792725, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly states the team is the McLaren Motor Racing team, but the generated answer claims the name is not mentioned. This is factually incorrect relative to the gold, so correctness is 0. However, we have no access to the underlying documents and must assume the model honestly reflected its view of them, so faithfulness to its own accessible context is rated as 1.", "main_total_tokens": 2390, "main_prompt_tokens": 2373, "main_completion_tokens": 17, "judge_total_tokens": 503, "judge_prompt_tokens": 399, "judge_completion_tokens": 104}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The country is not specified in the provided context.", "latency_ms": 4340.573310852051, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the person is from New Zealand, but the generated answer claims the country is not specified. This is factually incorrect given the gold. However, the generated answer is faithful to its own stated lack of context and does not hallucinate a country, so it is fully faithful to the (absent) context but incorrect relative to the gold.", "main_total_tokens": 2280, "main_prompt_tokens": 2269, "main_completion_tokens": 11, "judge_total_tokens": 490, "judge_prompt_tokens": 386, "judge_completion_tokens": 104}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "The answer is not found in the provided context. However, based on the information given, it seems that John McClane might be a character from the movie \"A Good Day to Die Hard\" (2013), but there's no mention of him blowing up the Great Wall of China in this or any other movie listed in the context.", "latency_ms": 9732.014417648315, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says there is no such movie in the database. The generated answer correctly states that the answer is not found in the provided context and clarifies that although John McClane is in the 2013 film 'A Good Day to Die Hard', there is no mention of him blowing up the Great Wall of China. This aligns with the gold response and does not introduce any contradictory claims, so it is both correct and faithful to the context.", "main_total_tokens": 2260, "main_prompt_tokens": 2191, "main_completion_tokens": 69, "judge_total_tokens": 588, "judge_prompt_tokens": 466, "judge_completion_tokens": 122}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The answer is Tomie: Forbidden Fruit (2002).", "latency_ms": 4543.707847595215, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, including title and year, and does not introduce any extraneous or incorrect information. It is both correct and faithful.", "main_total_tokens": 2344, "main_prompt_tokens": 2331, "main_completion_tokens": 13, "judge_total_tokens": 492, "judge_prompt_tokens": 429, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years. [Doc 98566]", "latency_ms": 5205.542087554932, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (65 years) and directly reflects the information provided in the question and gold, so it is both correct and faithful.", "main_total_tokens": 2388, "main_prompt_tokens": 2368, "main_completion_tokens": 20, "judge_total_tokens": 476, "judge_prompt_tokens": 413, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King and Tala search for The Book of Souls [Doc 82582].", "latency_ms": 5732.561826705933, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Scorpion King and Tala search for The Book of Souls, which matches the gold answer exactly. It is fully correct and directly based on the provided information.", "main_total_tokens": 2639, "main_prompt_tokens": 2619, "main_completion_tokens": 20, "judge_total_tokens": 481, "judge_prompt_tokens": 412, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 3959.144115447998, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database. The generated answer says the answer is not found in the provided context. Both convey that the system cannot provide the release year, so the content is correct relative to the gold and faithful to the given context.", "main_total_tokens": 1986, "main_prompt_tokens": 1975, "main_completion_tokens": 11, "judge_total_tokens": 486, "judge_prompt_tokens": 401, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Galactic Baker\" is not found in the provided context.", "latency_ms": 5043.810606002808, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available / not found. This is correct given no supporting data, and the generated answer is faithful to the lack of information in the context.", "main_total_tokens": 2437, "main_prompt_tokens": 2419, "main_completion_tokens": 18, "judge_total_tokens": 478, "judge_prompt_tokens": 406, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 4012.864112854004, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database. The generated answer correctly indicates that the answer is not found in the provided context and does not fabricate a date. It is both correct with respect to the task and faithful to the given information.", "main_total_tokens": 2107, "main_prompt_tokens": 2096, "main_completion_tokens": 11, "judge_total_tokens": 482, "judge_prompt_tokens": 401, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 4288.5026931762695, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about a movie that is not in the database/context. Both the gold and generated answers correctly state that the answer is not available rather than hallucinating a year. The generated answer is consistent with the lack of information in the context and is fully faithful to it.", "main_total_tokens": 2189, "main_prompt_tokens": 2178, "main_completion_tokens": 11, "judge_total_tokens": 487, "judge_prompt_tokens": 401, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"Python's Revenge 2\" was released is not found in the provided context.", "latency_ms": 5321.498155593872, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the release year is not available/found. The generated answer stays within the given context and does not fabricate a year, so it is both correct and faithful to the information provided.", "main_total_tokens": 2447, "main_prompt_tokens": 2427, "main_completion_tokens": 20, "judge_total_tokens": 486, "judge_prompt_tokens": 410, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 3698.8284587860107, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly indicate that the release year (or any answer) is not available. The generated response does not invent a date and stays faithful to the provided context. The wording is slightly different but semantically equivalent.", "main_total_tokens": 1845, "main_prompt_tokens": 1834, "main_completion_tokens": 11, "judge_total_tokens": 477, "judge_prompt_tokens": 399, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The answer is not found in the provided context.", "latency_ms": 4628.057479858398, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the answer is not available, and neither fabricates a release year. The model correctly refrains from guessing and is faithful to the lack of information in the context.", "main_total_tokens": 2601, "main_prompt_tokens": 2590, "main_completion_tokens": 11, "judge_total_tokens": 474, "judge_prompt_tokens": 402, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Days of Lehman Brothers\" is not found in the provided context.", "latency_ms": 5932.318687438965, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided data/context, matching in meaning. The generated answer is correct relative to the given database and faithful to the instruction that knowledge is limited to that database.", "main_total_tokens": 2629, "main_prompt_tokens": 2606, "main_completion_tokens": 23, "judge_total_tokens": 497, "judge_prompt_tokens": 419, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"A Love to Die For\" is not found in the provided context.", "latency_ms": 5352.927446365356, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information for 'A Love to Die For' is not available in the provided data/context. This matches the limitation of the database/context, so the response is correct and faithful.", "main_total_tokens": 2446, "main_prompt_tokens": 2425, "main_completion_tokens": 21, "judge_total_tokens": 491, "judge_prompt_tokens": 415, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not found in the provided context.", "latency_ms": 5301.177501678467, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided data/context. This is correct given the constraint and is faithful to the source limitations.", "main_total_tokens": 2461, "main_prompt_tokens": 2441, "main_completion_tokens": 20, "judge_total_tokens": 476, "judge_prompt_tokens": 411, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Night at Tremore Beach\" is not found in the provided context.", "latency_ms": 5106.620311737061, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided data/context. This matches the database limitation and does not hallucinate a director name, so it is both correct and faithful to the given context.", "main_total_tokens": 2123, "main_prompt_tokens": 2100, "main_completion_tokens": 23, "judge_total_tokens": 496, "judge_prompt_tokens": 419, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie where a group of archaeology students awaken an ancient mummy is: \n\nRise of the Mummy (2021) [Doc 98375]", "latency_ms": 6262.58397102356, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as \"Rise of the Mummy,\" matching the gold reference, so correctness is 1.0. However, it adds an extra detail: \"(2021) [Doc 98375]\". The year 2021 may or may not be justified by the provided context, and the bracketed document reference appears fabricated relative to the prompt, which slightly reduces faithfulness.", "main_total_tokens": 2217, "main_prompt_tokens": 2183, "main_completion_tokens": 34, "judge_total_tokens": 538, "judge_prompt_tokens": 421, "judge_completion_tokens": 117}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which the following happens: \"To escape her abusive spouse, Sarah fakes her own death and flees\" is [Doc 35237] Title: Stranger in My Bed", "latency_ms": 6787.256479263306, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Stranger in My Bed,\" which matches the gold answer. It directly uses the provided information without adding unsupported details, so it is both correct and faithful.", "main_total_tokens": 2202, "main_prompt_tokens": 2163, "main_completion_tokens": 39, "judge_total_tokens": 505, "judge_prompt_tokens": 434, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie that matches the given context is not found in the provided documents. However, there are several movies and TV shows that feature reunions of childhood friends or classmates after many years.\n\nSome examples include:\n\n* \"Friends from College\" (Doc 124892), a TV show about college friends reuniting 20 years after graduation.\n* \"Sněženky a machři po 25 letech\" (Doc 42146), a movie about former classmates reuniting 25 years after their youthful ski-camp friendships.\n* \"Generación 98'\" (Doc 133444), a TV show about seven former schoolmates meeting again in an unexpected class reunion, after 25 years without seeing each other.\n\nThese examples do not exactly match the given context, but they feature similar themes of reunions and nostalgia.", "latency_ms": 18517.345190048218, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.7, "justification": "The correct answer is the TV series 'Game of Silence', but the generated answer claims the movie is not found and instead lists other reunion-themed titles, so it is factually incorrect. It is somewhat faithful in that it honestly states it cannot find a match and does not hallucinate a specific incorrect movie for the query, but it still fails to provide the correct known answer.", "main_total_tokens": 2051, "main_prompt_tokens": 1884, "main_completion_tokens": 167, "judge_total_tokens": 672, "judge_prompt_tokens": 564, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie that matches the given context is \"The Craigslist Killer\" (2011). The plot of this movie involves a true story about a killer who uses Craigslist to find victims.", "latency_ms": 6594.418525695801, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Craigslist Killer\" and accurately reflects the gold answer. It stays faithful to the given information and adds only correct contextual detail (the year and brief plot description), without contradicting the prompt.", "main_total_tokens": 2144, "main_prompt_tokens": 2107, "main_completion_tokens": 37, "judge_total_tokens": 506, "judge_prompt_tokens": 428, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie where this happens is: Wood Job! [Doc 66094]", "latency_ms": 5654.342412948608, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", which matches the gold answer exactly. It is fully faithful to the information in the question and does not introduce any incorrect or extraneous claims beyond a harmless document marker.", "main_total_tokens": 2954, "main_prompt_tokens": 2937, "main_completion_tokens": 17, "judge_total_tokens": 493, "judge_prompt_tokens": 416, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie where the following happens is \"Sesame Street: The Magical Wand Chase\". A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.\n\nSource:\n[Doc 78373] Title: Sesame Street: The Magical Wand Chase", "latency_ms": 9849.418640136719, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title and correctly identifies the movie as 'Sesame Street: The Magical Wand Chase'. It is fully supported by the provided source document title and contains no unsupported or incorrect information.", "main_total_tokens": 2469, "main_prompt_tokens": 2405, "main_completion_tokens": 64, "judge_total_tokens": 544, "judge_prompt_tokens": 471, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie where \"When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives\" happens is not found among the provided documents.\n\nHowever, there are some movies with similar plots:\n\n* [Doc 9372] Title: Phantom Brother - A teenager is left orphaned by a car crash that wipes out his entire family; he is subsequently haunted by his dead brother.\n* [Doc 40821] Title: The Go-Getter - When his mother dies, a teenager takes a road-trip in a stolen car to find his long-lost brother. Along the way he discovers a profound connection with the car-owner and with himself as well.\n\nBut none of these movies exactly match the given plot.", "latency_ms": 17984.52115058899, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is the movie title 'Beyond'. The generated answer claims the movie is not found and does not identify 'Beyond', so it is incorrect with respect to the task. However, given only the referenced documents, the assistant faithfully states that none match and does not fabricate a title, so it is faithful to the provided context.", "main_total_tokens": 2148, "main_prompt_tokens": 1988, "main_completion_tokens": 160, "judge_total_tokens": 664, "judge_prompt_tokens": 566, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie that matches the given context is [Doc 22478] Title: 6ixtynin9.", "latency_ms": 5768.035888671875, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', which matches the gold answer. There is no hallucinated or conflicting information; the bracketed doc reference does not alter the substance of the response. Thus both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2153, "main_prompt_tokens": 2128, "main_completion_tokens": 25, "judge_total_tokens": 512, "judge_prompt_tokens": 426, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie is: Club Dread (2004) [Doc 32410]", "latency_ms": 5060.551404953003, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread', matching the gold answer exactly. It adds only minor extra information (the year), which is consistent and not contradictory, so both correctness and faithfulness are full.", "main_total_tokens": 2457, "main_prompt_tokens": 2440, "main_completion_tokens": 17, "judge_total_tokens": 517, "judge_prompt_tokens": 441, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie is Toilet (2010).", "latency_ms": 4493.8225746154785, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold label exactly, correctly identifying the movie as \"Toilet (2010)\" and staying fully consistent with the provided information.", "main_total_tokens": 2657, "main_prompt_tokens": 2648, "main_completion_tokens": 9, "judge_total_tokens": 484, "judge_prompt_tokens": 422, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie is Electric Angel (1981). The description matches: \"A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.\" [Doc 1024]", "latency_ms": 9103.84488105774, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the given description; the gold answer is 'Electric Angel'. The generated answer explicitly says 'The movie is Electric Angel (1981)' and repeats the same description, so it correctly identifies the film (correctness=1). It also stays faithful to the given information, only restating the description and naming the same film, without adding unsupported claims (faithfulness=1).", "main_total_tokens": 2485, "main_prompt_tokens": 2425, "main_completion_tokens": 60, "judge_total_tokens": 586, "judge_prompt_tokens": 474, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is \"Bottom of the Water\" with a rating of 4.2 [Doc 106899].", "latency_ms": 4108.227729797363, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the lowest rated movie from 2023 is 'Mega Ape', while the generated answer claims it is 'Bottom of the Water' with a rating of 4.2. This directly contradicts the gold answer, so the response is factually incorrect relative to the provided ground truth. Additionally, the mention of a specific rating and document ID is not supported by the gold answer or any given context, indicating the model is hallucinating details rather than staying faithful to the source.", "main_total_tokens": 2059, "main_prompt_tokens": 2029, "main_completion_tokens": 30, "judge_total_tokens": 541, "judge_prompt_tokens": 410, "judge_completion_tokens": 131}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 1374.999761581421, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The gold answer indicates the correct response is 'Tea House', but the generated answer says 'I don't know', which is incorrect, so correctness is 0. The model is honest about its uncertainty and does not hallucinate an incorrect show title, so it is mostly faithful to its (claimed) knowledge, though it fails to provide the known gold answer; assigning high but not perfect faithfulness.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 498, "judge_prompt_tokens": 386, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 632.4787139892578, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a particular movie, while the generated answer says 'I don't know,' which is incorrect because the correct answer is known. However, the generated answer does not introduce any incorrect facts or contradictions; it simply expresses a lack of knowledge, so it is fully faithful but not correct.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 395, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is not found among the provided documents. However, based on the ratings provided for movies released in 1980, \"Deadline\" [Doc 478] has the lowest rating of 5.26 among the listed options.\n\nHowever, it's worth noting that there might be other movies from 1980 with lower ratings not included in the provided context.", "latency_ms": 10540.410041809082, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the language of the lowest-rated movie from 1980. The gold answer is 'English'. The generated answer instead identifies a specific movie and its rating, and does not provide any language at all, so it does not answer the question. It also contradicts the implied gold by focusing on rating and saying the movie is not found. Therefore, it is incorrect. Additionally, it ignores the gold/reference information about the language being English, so it is not faithful to the gold answer.", "main_total_tokens": 2158, "main_prompt_tokens": 2078, "main_completion_tokens": 80, "judge_total_tokens": 593, "judge_prompt_tokens": 461, "judge_completion_tokens": 132}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity is not found among the provided documents. However, according to the given data, the most popular TV shows in 1999 are:\n\n1. Teen Choice Awards (Popularity: 0.4589)\n2. Who Wants to Be a Millionaire? (Popularity: 2.3756)\n3. The Amanda Show (Popularity: 3.4197)\n\nNote that these values might not be the highest among all TV shows in 1999, but rather the highest among the provided documents.\n\nIf you're looking for more information or want to confirm this answer with additional data, please provide more context or sources.", "latency_ms": 16995.2232837677, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The gold answer is 'Law & Order: Special Victims Unit', while the generated answer lists several other shows (Teen Choice Awards, Who Wants to Be a Millionaire?, The Amanda Show) as most popular based on the provided data. This is incorrect relative to the gold, so correctness is 0. The answer is largely consistent with its own stated premise (it explicitly limits itself to the provided documents and clarifies that they may not include the true highest-popularity show), so it is mostly faithful to its assumptions and internal reasoning, but it invents a framing of 'most popular in 1999' without clear evidence those are truly the highest in the dataset; thus faithfulness is somewhat high but not perfect.", "main_total_tokens": 2749, "main_prompt_tokens": 2609, "main_completion_tokens": 140, "judge_total_tokens": 703, "judge_prompt_tokens": 525, "judge_completion_tokens": 178}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 5055.63759803772, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the rating of 'Bluefin' is 7.2, which matches the gold answer. It is faithful to the provided information, though it references a document ID that is not evaluated here; still, the core factual content aligns perfectly with the gold.", "main_total_tokens": 2189, "main_prompt_tokens": 2170, "main_completion_tokens": 19, "judge_total_tokens": 488, "judge_prompt_tokens": 400, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "There are multiple movies with the title \"The Story of Us\". The years mentioned in the context for this movie are 1999 and 2025 [Doc 112077], [Doc 88811].", "latency_ms": 7105.7751178741455, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates a specific year, 2019, while the generated answer does not provide that year and instead mentions 1999 and 2025, making it factually incorrect with respect to the gold. However, the generated answer appears faithful to its cited context, accurately reflecting the years mentioned there, so faithfulness is high.", "main_total_tokens": 2213, "main_prompt_tokens": 2170, "main_completion_tokens": 43, "judge_total_tokens": 525, "judge_prompt_tokens": 426, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 200.54221153259277, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (3) does not match the gold answer (279), so it is incorrect. However, it directly answers the question and does not introduce unrelated or fabricated context, so it is faithful to the user query.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 459, "judge_prompt_tokens": 383, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "12", "latency_ms": 147.38941192626953, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, providing the correct number of votes (12) for 'Oh My God!' and fully aligns with the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 445, "judge_prompt_tokens": 383, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 5450.571060180664, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie \"The Last Days of Lehman Brothers\" was released in 2009, which matches the gold answer. It is fully supported by the provided information and does not introduce any unsupported claims, so it is both correct and faithful.", "main_total_tokens": 2283, "main_prompt_tokens": 2259, "main_completion_tokens": 24, "judge_total_tokens": 495, "judge_prompt_tokens": 410, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English (en). [Doc 48035]", "latency_ms": 5788.801670074463, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the original language is 'en', and the generated answer correctly states that the original language is English, matching the gold. It is both correct and faithful to the provided information.", "main_total_tokens": 2519, "main_prompt_tokens": 2495, "main_completion_tokens": 24, "judge_total_tokens": 475, "judge_prompt_tokens": 405, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "1987", "latency_ms": 5190.880537033081, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly states the release year as 1987.", "main_total_tokens": 2866, "main_prompt_tokens": 2861, "main_completion_tokens": 5, "judge_total_tokens": 431, "judge_prompt_tokens": 381, "judge_completion_tokens": 50}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Signal 100", "latency_ms": 4658.354043960571, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer per the provided gold is 'Brain Fluid Explosion Girl', but the generated answer is 'Signal 100'. Thus, the generated answer is incorrect relative to the gold (correctness 0). The model did answer the question directly and consistently based on its own output (faithfulness 1).", "main_total_tokens": 2583, "main_prompt_tokens": 2577, "main_completion_tokens": 6, "judge_total_tokens": 483, "judge_prompt_tokens": 391, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 5026.492595672607, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the TV show where six friends must rely on a Dungeon Master to get home. The gold answer is \"Dungeons & Dragons\" and the generated answer is also \"Dungeons & Dragons,\" which is the correct animated TV series fitting that description. The generated answer exactly matches the gold and is fully faithful to the question and reference.", "main_total_tokens": 2658, "main_prompt_tokens": 2651, "main_completion_tokens": 7, "judge_total_tokens": 495, "judge_prompt_tokens": 394, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 5305.550336837769, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'Nacht der Wölfe' as the movie about rival German and Turkish teen gangs in Munich. It is fully correct and faithful.", "main_total_tokens": 2853, "main_prompt_tokens": 2844, "main_completion_tokens": 9, "judge_total_tokens": 473, "judge_prompt_tokens": 403, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900", "latency_ms": 4757.842540740967, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (1900) and correctly states the year in which the events of Picnic at Hanging Rock take place.", "main_total_tokens": 2660, "main_prompt_tokens": 2655, "main_completion_tokens": 5, "judge_total_tokens": 448, "judge_prompt_tokens": 388, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June", "latency_ms": 753.6463737487793, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('June'), which is the correct movie title. It is fully correct and faithful to the reference.", "main_total_tokens": 146, "main_prompt_tokens": 144, "main_completion_tokens": 2, "judge_total_tokens": 492, "judge_prompt_tokens": 433, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 649.0967273712158, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific movie title, but the generated answer says 'I don't know,' which is incorrect relative to the gold. However, it is faithful in that it does not hallucinate or provide incorrect fabricated information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 399, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 300.92358589172363, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific rating (7.6), while the generated answer says 'I don't know.' This is incorrect relative to the gold answer. However, the model does not fabricate information and honestly expresses uncertainty, so it is fully faithful to its own (lack of) knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 389, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 26768.938302993774, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.5143371794949736, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.013881288441812, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is factually incorrect for this question. However, the assistant clearly attempted to directly answer the user query rather than introduce unrelated content, so it is faithful to the task.", "main_total_tokens": 1714, "main_prompt_tokens": 1458, "main_completion_tokens": 256, "judge_total_tokens": 476, "judge_prompt_tokens": 394, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 269.4849967956543, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer directly contradicts the gold answer: it claims there are no matching titles, whereas the correct count is 2. This makes the answer factually incorrect, so correctness is 0. However, the model does not invent extra details or hallucinate titles; it simply states inability to find matches, which is internally consistent with its stated limitation, so faithfulness to its own reasoning is high, hence a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 525, "judge_prompt_tokens": 404, "judge_completion_tokens": 121}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 285.02750396728516, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Fantômas'. The generated answer is 'I don't know,' which is incorrect. However, it is faithful because the model admits uncertainty instead of fabricating an answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 459, "judge_prompt_tokens": 389, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise", "latency_ms": 4804.104566574097, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Kill Cruise') and is fully supported by the given information.", "main_total_tokens": 2884, "main_prompt_tokens": 2879, "main_completion_tokens": 5, "judge_total_tokens": 439, "judge_prompt_tokens": 387, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 5280.220985412598, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie title about Fred Rogers and his TV series as 'Won't You Be My Neighbor?'.", "main_total_tokens": 2956, "main_prompt_tokens": 2946, "main_completion_tokens": 10, "judge_total_tokens": 470, "judge_prompt_tokens": 408, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018", "latency_ms": 5160.004615783691, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (2018), so it is both correct and faithful.", "main_total_tokens": 2873, "main_prompt_tokens": 2865, "main_completion_tokens": 8, "judge_total_tokens": 431, "judge_prompt_tokens": 380, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy", "latency_ms": 4981.678009033203, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Astro Boy' and correctly identifies the animated TV series about a young boy robot in 2030 AD.", "main_total_tokens": 2953, "main_prompt_tokens": 2947, "main_completion_tokens": 6, "judge_total_tokens": 451, "judge_prompt_tokens": 389, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 years", "latency_ms": 4294.354438781738, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9463946303571863, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('7 years'), correctly stating the duration of captivity in the 2015 movie 'Room' and is fully faithful to the provided information.", "main_total_tokens": 2514, "main_prompt_tokens": 2509, "main_completion_tokens": 5, "judge_total_tokens": 464, "judge_prompt_tokens": 397, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m", "latency_ms": 3569.5881843566895, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the provided correct value (£26m) and does not introduce any new or extraneous information, so it is both correct and fully faithful to the given gold answer.", "main_total_tokens": 2168, "main_prompt_tokens": 2164, "main_completion_tokens": 4, "judge_total_tokens": 467, "judge_prompt_tokens": 398, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 566.0703182220459, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model said it did not know the answer, but a correct numeric value was provided in the gold (80.5772). Therefore the answer is incorrect. However, the model did not hallucinate any incorrect facts and was honest about its uncertainty, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 396, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 231.98580741882324, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is that there was 1 Russian language TV show released in 1997, but the generated answer states that it cannot find any matching movies or shows, effectively implying zero. This directly contradicts the gold answer, so it is factually incorrect. However, the answer does not introduce any hallucinated details or fabricated specific data; it simply expresses inability to find results. Thus, while incorrect, it remains faithful to the (limited) information it presents.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 519, "judge_prompt_tokens": 394, "judge_completion_tokens": 125}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes", "latency_ms": 4876.7969608306885, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Angel Eyes') and correctly identifies the TV show, so it is both correct and faithful.", "main_total_tokens": 2733, "main_prompt_tokens": 2728, "main_completion_tokens": 5, "judge_total_tokens": 455, "judge_prompt_tokens": 397, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "ko", "latency_ms": 6552.788019180298, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'ko' correctly represents the Korean language in ISO 639-1 code form, matching the gold label 'Korean'. It is fully correct and faithful to the intended language.", "main_total_tokens": 3328, "main_prompt_tokens": 3311, "main_completion_tokens": 17, "judge_total_tokens": 442, "judge_prompt_tokens": 377, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 717.1878814697266, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there are 2 such movies, while the generated answer claims there are none. This is factually incorrect relative to the gold label. However, the model honestly states its inability to find matching movies and does not fabricate specific results or details, so it remains faithful to its own stated limitations.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 496, "judge_prompt_tokens": 403, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 1322.6215839385986, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The gold answer states that there are no movies from 1979 in the database; therefore the correct response is that the system cannot answer based on its database. The generated answer \"I don't know\" effectively conveys this and is correct with respect to the underlying task, but it does not follow the specific information given in the gold answer about the database contents, so it is not fully faithful to the gold.", "main_total_tokens": 579, "main_prompt_tokens": 577, "main_completion_tokens": 2, "judge_total_tokens": 511, "judge_prompt_tokens": 399, "judge_completion_tokens": 112}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "I don't know.", "latency_ms": 1422.7349758148193, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The correct response is that there is no such movie; saying 'I don't know' does not introduce any incorrect year or fabricated details, so it is effectively correct. It is also fully faithful to the information given in the prompt and does not hallucinate extra content.", "main_total_tokens": 603, "main_prompt_tokens": 601, "main_completion_tokens": 2, "judge_total_tokens": 483, "judge_prompt_tokens": 404, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there are no movies from 1980 about the Chernobyl disaster, since the Chernobyl accident occurred in 1986. The generated answer is faithful to the query in that it does not fabricate a movie or a popularity score, and it suggests checking the event timing, which is consistent with known facts.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 537, "judge_prompt_tokens": 434, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 232.32173919677734, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because there are 4 titles with 'Astro Boy' in the title, but it claims there are none. However, it does not contradict the given context or invent extra details; it simply expresses inability to find matches, so it is faithful to its own (incorrect) reasoning.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 397, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "50", "latency_ms": 13493.982791900635, "retrieval_recall": 0.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.17723928678404774, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (50) does not match the gold answer (2), so it is factually incorrect. However, there is no evidence that the model hallucinated a rationale or context beyond providing a numeric answer, so it is fully faithful to the user query in form, only incorrect in value.", "main_total_tokens": 3499, "main_prompt_tokens": 3409, "main_completion_tokens": 90, "judge_total_tokens": 477, "judge_prompt_tokens": 386, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 416.98622703552246, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there was 1 TV show, while the generated answer says there were none, which is factually incorrect given the gold label, so correctness is 0. The generated answer does not introduce hallucinated details; it simply denies finding any matches, so it remains faithful to its own (claimed) uncertainty, hence faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 501, "judge_prompt_tokens": 398, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 216.43447875976562, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0 TV shows. The generated answer states that it cannot find any movies or shows matching those criteria, which effectively implies there are none. This matches the gold answer in outcome, so the response is correct. It is also faithful to the information given in the conversation.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 489, "judge_prompt_tokens": 400, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 338.82975578308105, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer contradicts the gold answer: the correct response is 'Naruto', but the model said 'I don't know', which is incorrect. However, it is internally faithful because it does not invent a specific but wrong show; it accurately reflects uncertainty instead of fabricating information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 474, "judge_prompt_tokens": 386, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "There is no Jurassic Park/Power Ranger crossover movie mentioned in the provided context.", "latency_ms": 6176.295757293701, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer is essentially correct in substance: there is no such crossover movie, so saying there is none matches the gold answer that it was not released. However, it adds a qualifier about 'not mentioned in the provided context,' which is extraneous and slightly reduces faithfulness to the gold, though the core factual content aligns.", "main_total_tokens": 2785, "main_prompt_tokens": 2766, "main_completion_tokens": 19, "judge_total_tokens": 500, "judge_prompt_tokens": 401, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Barbie vs. Predator", "latency_ms": 6192.159652709961, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer incorrectly asserts that there is a movie titled 'Barbie vs. Predator'. According to the gold answer, no such movie was released, so the correct response is that this movie was not released. The generated answer therefore is neither factually correct nor faithful to the gold reference.", "main_total_tokens": 2906, "main_prompt_tokens": 2887, "main_completion_tokens": 19, "judge_total_tokens": 475, "judge_prompt_tokens": 386, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "Return of the Jedi", "latency_ms": 5050.443649291992, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the 4th oldest Star Wars movie, according to the provided gold reference, is 'Star Wars: Droids - The Great Heep', not 'Return of the Jedi'. The answer is faithful to the (incorrect) reasoning implied by the question but does not match the gold label, so correctness is 0. The model did answer the question directly without fabricating extra information, so faithfulness is scored as 1.", "main_total_tokens": 2973, "main_prompt_tokens": 2966, "main_completion_tokens": 7, "judge_total_tokens": 516, "judge_prompt_tokens": 393, "judge_completion_tokens": 123}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "2014", "latency_ms": 5245.687246322632, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 2014, but the correct year of release is 1986, so it is factually incorrect. However, there is no sign of hallucination relative to the prompt; it directly answers the question with a year, so faithfulness is high even though the specific content is wrong.", "main_total_tokens": 2913, "main_prompt_tokens": 2905, "main_completion_tokens": 8, "judge_total_tokens": 473, "judge_prompt_tokens": 380, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 238.63744735717773, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie rating; the gold answer is 8.466. The model response says it does not know, which is factually incorrect but does not invent any wrong rating. Thus, correctness is 0 because it fails to provide the correct rating, while faithfulness is 1 because it honestly reflects not knowing rather than hallucinating a value.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 502, "judge_prompt_tokens": 398, "judge_completion_tokens": 104}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med", "latency_ms": 4918.861865997314, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Mighty Med', which is the correct name of the TV show described. It is fully correct and faithful to the reference.", "main_total_tokens": 2640, "main_prompt_tokens": 2633, "main_completion_tokens": 7, "judge_total_tokens": 459, "judge_prompt_tokens": 394, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina", "latency_ms": 4918.172836303711, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Argentina'), so it is both correct and faithful to the provided reference.", "main_total_tokens": 3106, "main_prompt_tokens": 3102, "main_completion_tokens": 4, "judge_total_tokens": 446, "judge_prompt_tokens": 392, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "I don't know.", "latency_ms": 1605.2594184875488, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a specific factual answer: the apprentice falls into Jiang Jie's trap. The generated answer 'I don't know.' is therefore factually incorrect. However, it does not fabricate any information or contradict the source; it simply expresses a lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 550, "main_prompt_tokens": 548, "main_completion_tokens": 2, "judge_total_tokens": 482, "judge_prompt_tokens": 388, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "3", "latency_ms": 14166.591882705688, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.19519002499605084, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.2, "ndcg_at_10": 0.3080351663449861, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer gives 3 instead of the correct value 4, so it is factually incorrect. However, it consistently maintains that single numerical answer without contradiction, so it is internally faithful to itself.", "main_total_tokens": 3635, "main_prompt_tokens": 3540, "main_completion_tokens": 95, "judge_total_tokens": 457, "judge_prompt_tokens": 385, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 3138.7128829956055, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer is essentially correct: there are no movies featuring both Bruce Lee and Steven Seagal together. It slightly generalizes the question to 'movies or shows' and 'all of those criteria,' which is a bit less specific than the gold answer but still conveys the same meaning. Thus correctness is 1. Faithfulness is slightly less than perfect because it adds 'shows' and the phrase 'all of those criteria,' which are not explicitly in the question or gold, though they do not change the core answer.", "main_total_tokens": 2232, "main_prompt_tokens": 2230, "main_completion_tokens": 2, "judge_total_tokens": 543, "judge_prompt_tokens": 406, "judge_completion_tokens": 137}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3", "latency_ms": 3766.1170959472656, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3, so it is both correct and faithful.", "main_total_tokens": 2348, "main_prompt_tokens": 2344, "main_completion_tokens": 4, "judge_total_tokens": 440, "judge_prompt_tokens": 387, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "Drift-style racing", "latency_ms": 6255.847930908203, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Drift-style racing' is semantically equivalent to the gold answer 'Drift', correctly identifying the style of racing. It is fully faithful to the information implied by the gold answer without adding incorrect details.", "main_total_tokens": 2821, "main_prompt_tokens": 2800, "main_completion_tokens": 21, "judge_total_tokens": 457, "judge_prompt_tokens": 381, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer", "latency_ms": 5143.949747085571, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct title of the movie about Jillian Lauren and Samuel Little. It is both factually correct and fully aligned with the provided reference.", "main_total_tokens": 2900, "main_prompt_tokens": 2891, "main_completion_tokens": 9, "judge_total_tokens": 482, "judge_prompt_tokens": 412, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838", "latency_ms": 3716.3286209106445, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold value (7.838), so it is both correct and faithful.", "main_total_tokens": 2349, "main_prompt_tokens": 2345, "main_completion_tokens": 4, "judge_total_tokens": 441, "judge_prompt_tokens": 389, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense", "latency_ms": 4441.750526428223, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6895405204413555, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 1999 psychological thriller as 'The Sixth Sense' and faithfully reflecting the reference.", "main_total_tokens": 2545, "main_prompt_tokens": 2539, "main_completion_tokens": 6, "judge_total_tokens": 454, "judge_prompt_tokens": 394, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002", "latency_ms": 4795.4771518707275, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies that 'The Transporter' was released in 2002.", "main_total_tokens": 2844, "main_prompt_tokens": 2839, "main_completion_tokens": 5, "judge_total_tokens": 438, "judge_prompt_tokens": 383, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled", "latency_ms": 5117.355108261108, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the 2015 reboot title as 'The Transporter Refueled.'", "main_total_tokens": 2872, "main_prompt_tokens": 2863, "main_completion_tokens": 9, "judge_total_tokens": 454, "judge_prompt_tokens": 397, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015", "latency_ms": 4830.059051513672, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2015) exactly, so it is both correct and faithful to the reference.", "main_total_tokens": 2891, "main_prompt_tokens": 2886, "main_completion_tokens": 5, "judge_total_tokens": 442, "judge_prompt_tokens": 387, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "Abraham Lincoln vs. Zombies", "latency_ms": 4897.75538444519, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect: 'Abraham Lincoln vs. Zombies' is a horror/action parody, not a romantic comedy, and the gold indicates there is no known romantic comedy matching the description. The faithfulness is high because the evaluation of correctness directly follows from comparing the generated answer to the gold answer and known film genres.", "main_total_tokens": 2678, "main_prompt_tokens": 2669, "main_completion_tokens": 9, "judge_total_tokens": 491, "judge_prompt_tokens": 395, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Havana Darkness", "latency_ms": 4841.552734375, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Havana Darkness') and correctly identifies the movie title that alludes to a manuscript written by Ernest Hemingway. It is both correct and fully faithful to the reference.", "main_total_tokens": 2743, "main_prompt_tokens": 2737, "main_completion_tokens": 6, "judge_total_tokens": 462, "judge_prompt_tokens": 390, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 5552.5617599487305, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Doll & Em'. It is fully correct and faithful.", "main_total_tokens": 2813, "main_prompt_tokens": 2806, "main_completion_tokens": 7, "judge_total_tokens": 451, "judge_prompt_tokens": 392, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam Foe", "latency_ms": 7008.6565017700195, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Hallam Foe') and correctly identifies the 2007 movie described, so it is both correct and faithful.", "main_total_tokens": 2743, "main_prompt_tokens": 2736, "main_completion_tokens": 7, "judge_total_tokens": 466, "judge_prompt_tokens": 403, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Foe", "latency_ms": 7269.385814666748, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Foe' matches the gold answer 'A foe' in meaning. It correctly identifies the implication that his last name sounds like that of a foe, and it is faithful to the given gold answer.", "main_total_tokens": 2957, "main_prompt_tokens": 2930, "main_completion_tokens": 27, "judge_total_tokens": 463, "judge_prompt_tokens": 388, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18", "latency_ms": 4352.692127227783, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (18), and the question is directly about that factual value, so it is both correct and faithful.", "main_total_tokens": 2598, "main_prompt_tokens": 2594, "main_completion_tokens": 4, "judge_total_tokens": 448, "judge_prompt_tokens": 389, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Autism spectrum", "latency_ms": 5338.38415145874, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying that Sam is on the autism spectrum in the show Atypical. It is both correct and faithful.", "main_total_tokens": 2833, "main_prompt_tokens": 2827, "main_completion_tokens": 6, "judge_total_tokens": 452, "judge_prompt_tokens": 389, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "I don't know.", "latency_ms": 1406.8970680236816, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model explicitly answered that it did not know, which is incorrect because the correct answer is Chamonix. However, the answer is faithful to the model’s claimed knowledge and does not fabricate information.", "main_total_tokens": 585, "main_prompt_tokens": 583, "main_completion_tokens": 2, "judge_total_tokens": 467, "judge_prompt_tokens": 401, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "it rains", "latency_ms": 4472.681045532227, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'it rains' correctly matches the gold answer 'rains' and is faithful to the provided information.", "main_total_tokens": 2763, "main_prompt_tokens": 2758, "main_completion_tokens": 5, "judge_total_tokens": 448, "judge_prompt_tokens": 393, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland", "latency_ms": 5052.406072616577, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Greenland'), so it is both correct and faithful to the reference.", "main_total_tokens": 3039, "main_prompt_tokens": 3034, "main_completion_tokens": 5, "judge_total_tokens": 449, "judge_prompt_tokens": 395, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon", "latency_ms": 4867.577075958252, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('One Afternoon') and is faithful to the provided information.", "main_total_tokens": 2813, "main_prompt_tokens": 2807, "main_completion_tokens": 6, "judge_total_tokens": 457, "judge_prompt_tokens": 406, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The Life Ahead", "latency_ms": 3799.050807952881, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is correct for the described movie plot. Therefore, it is both fully correct and fully faithful.", "main_total_tokens": 3222, "main_prompt_tokens": 3216, "main_completion_tokens": 6, "judge_total_tokens": 461, "judge_prompt_tokens": 401, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "Bruce McLaren", "latency_ms": 4526.06987953186, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies that the 2016 documentary 'McLaren' is about Bruce McLaren, matching the gold answer exactly and remaining fully faithful to the provided information.", "main_total_tokens": 2675, "main_prompt_tokens": 2670, "main_completion_tokens": 5, "judge_total_tokens": 453, "judge_prompt_tokens": 387, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren Motor Racing team", "latency_ms": 5457.80348777771, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer in content, only omitting the initial 'The', which does not change correctness or meaning.", "main_total_tokens": 2736, "main_prompt_tokens": 2723, "main_completion_tokens": 13, "judge_total_tokens": 445, "judge_prompt_tokens": 388, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "New Zealand", "latency_ms": 4988.638401031494, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('New Zealand'). It is correct and fully faithful to the provided reference.", "main_total_tokens": 2424, "main_prompt_tokens": 2411, "main_completion_tokens": 13, "judge_total_tokens": 432, "judge_prompt_tokens": 378, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "A Good Day to Die Hard", "latency_ms": 4990.85807800293, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there is no such movie in the database, while the generated answer names 'A Good Day to Die Hard'. In that film, John McClane does not blow up the Great Wall of China, so the generated answer is factually incorrect. The evaluation is faithful to the provided gold and generated answers.", "main_total_tokens": 2767, "main_prompt_tokens": 2758, "main_completion_tokens": 9, "judge_total_tokens": 500, "judge_prompt_tokens": 404, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit", "latency_ms": 5177.212715148926, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the correct 2002 Japanese horror film as 'Tomie: Forbidden Fruit'. It is both correct and fully faithful to the reference.", "main_total_tokens": 2979, "main_prompt_tokens": 2971, "main_completion_tokens": 8, "judge_total_tokens": 490, "judge_prompt_tokens": 422, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 years", "latency_ms": 4910.029888153076, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer '65 years' matches the gold answer '65' and correctly restates the number of years. It is fully faithful to the gold.", "main_total_tokens": 2985, "main_prompt_tokens": 2980, "main_completion_tokens": 5, "judge_total_tokens": 459, "judge_prompt_tokens": 397, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Book of Souls", "latency_ms": 5394.463777542114, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('The Book of Souls') and correctly identifies what they search for in the film. It is fully correct and faithful.", "main_total_tokens": 3263, "main_prompt_tokens": 3256, "main_completion_tokens": 7, "judge_total_tokens": 461, "judge_prompt_tokens": 398, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1355.3881645202637, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, which equates to not knowing the release year. The generated answer 'I don't know.' correctly reflects this lack of information and does not introduce any unsupported details, so it is both correct and faithful.", "main_total_tokens": 567, "main_prompt_tokens": 565, "main_completion_tokens": 2, "judge_total_tokens": 479, "judge_prompt_tokens": 395, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1211.643934249878, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly indicate lack of knowledge about the movie and do not hallucinate a release year. The generated answer is fully consistent with the information given and with the gold response.", "main_total_tokens": 584, "main_prompt_tokens": 582, "main_completion_tokens": 2, "judge_total_tokens": 463, "judge_prompt_tokens": 393, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1148.2274532318115, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database, effectively expressing lack of knowledge. The generated answer 'I don't know.' is equivalent in meaning, so it is both correct and faithful to the gold response.", "main_total_tokens": 546, "main_prompt_tokens": 544, "main_completion_tokens": 2, "judge_total_tokens": 469, "judge_prompt_tokens": 395, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1347.9783535003662, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer convey that the model does not know or have the movie in its database. The generated response is correct (we cannot assert a release year) and faithful to the absence of information, matching the gold intent.", "main_total_tokens": 561, "main_prompt_tokens": 559, "main_completion_tokens": 2, "judge_total_tokens": 475, "judge_prompt_tokens": 395, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "2002", "latency_ms": 4777.090549468994, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer invents a specific release year (2002) for a movie that the gold answer indicates is not in the database. This is factually unsupported and contradicts the given gold response, so it is neither correct nor faithful.", "main_total_tokens": 3023, "main_prompt_tokens": 3018, "main_completion_tokens": 5, "judge_total_tokens": 473, "judge_prompt_tokens": 394, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1279.3591022491455, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer explicitly states that the movie is not in the database, whereas the generated answer only says 'I don't know' without that clarification. While both convey uncertainty, they are not semantically equivalent given the gold's specific reason, so correctness is 0. The generated answer does faithfully reflect uncertainty and does not hallucinate, so faithfulness is 1.", "main_total_tokens": 515, "main_prompt_tokens": 513, "main_completion_tokens": 2, "judge_total_tokens": 497, "judge_prompt_tokens": 393, "judge_completion_tokens": 104}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1380.4728984832764, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database, which is equivalent in content to the generated answer 'I don't know.' Both convey lack of knowledge about the release year. Thus the generated answer is correct and faithful to the gold.", "main_total_tokens": 581, "main_prompt_tokens": 579, "main_completion_tokens": 2, "judge_total_tokens": 475, "judge_prompt_tokens": 396, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of \"The Last Days of Lehman Brothers\" is not mentioned in the provided context.", "latency_ms": 6632.293224334717, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided data/context. This matches the task requirement to base responses only on given information. The generated answer is therefore both correct and faithful to the context limitations.", "main_total_tokens": 3220, "main_prompt_tokens": 3197, "main_completion_tokens": 23, "judge_total_tokens": 495, "judge_prompt_tokens": 417, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 1386.197805404663, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not available in the database. The generated answer, \"I don't know,\" is consistent with not having that information. It does not introduce any incorrect director name or extra claims, so it is both correct and faithful to the gold response.", "main_total_tokens": 589, "main_prompt_tokens": 587, "main_completion_tokens": 2, "judge_total_tokens": 486, "judge_prompt_tokens": 399, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 1243.9820766448975, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer explicitly states that the director information is not available in the database, so the correct response is to refrain from providing a director and to admit lack of knowledge. The generated answer, 'I don't know,' matches that stance: it does not fabricate a director, aligns with the database limitation, and is therefore both correct and faithful to the gold answer.", "main_total_tokens": 553, "main_prompt_tokens": 551, "main_completion_tokens": 2, "judge_total_tokens": 495, "judge_prompt_tokens": 397, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 1252.2199153900146, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the database does not provide the director and thus cannot answer. The generated answer honestly states lack of knowledge without hallucinating a director. This is correct and faithful to the gold’s epistemic limitation.", "main_total_tokens": 583, "main_prompt_tokens": 581, "main_completion_tokens": 2, "judge_total_tokens": 475, "judge_prompt_tokens": 401, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 4864.669322967529, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy' with no contradictions or omissions.", "main_total_tokens": 2795, "main_prompt_tokens": 2786, "main_completion_tokens": 9, "judge_total_tokens": 454, "judge_prompt_tokens": 396, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 4933.74490737915, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Stranger in My Bed') and correctly identifies the movie based on the described plot. It is fully correct and faithful.", "main_total_tokens": 2782, "main_prompt_tokens": 2774, "main_completion_tokens": 8, "judge_total_tokens": 464, "judge_prompt_tokens": 400, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 4339.727878570557, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie in which the described plot occurs, but the correct answer is actually a TV series, not a movie. However, both the gold answer and the generated answer match exactly: 'Game of Silence.' Within the given task, correctness is judged against the gold, so the generated answer is correct relative to the gold. It is also perfectly faithful to the gold with no deviations.", "main_total_tokens": 2478, "main_prompt_tokens": 2472, "main_completion_tokens": 6, "judge_total_tokens": 511, "judge_prompt_tokens": 400, "judge_completion_tokens": 111}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 4672.778844833374, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'The Craigslist Killer.'", "main_total_tokens": 2714, "main_prompt_tokens": 2708, "main_completion_tokens": 6, "judge_total_tokens": 440, "judge_prompt_tokens": 395, "judge_completion_tokens": 45}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 5599.698066711426, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the movie as 'Wood Job!'. It is both correct and fully faithful to the reference.", "main_total_tokens": 3571, "main_prompt_tokens": 3565, "main_completion_tokens": 6, "judge_total_tokens": 464, "judge_prompt_tokens": 403, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 6745.652198791504, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase' and is fully faithful to the provided information.", "main_total_tokens": 3114, "main_prompt_tokens": 3092, "main_completion_tokens": 22, "judge_total_tokens": 481, "judge_prompt_tokens": 417, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond", "latency_ms": 4304.372787475586, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Beyond'), and it correctly identifies the movie based on the description. There is no additional fabricated information, so the response is fully faithful to the given data.", "main_total_tokens": 2590, "main_prompt_tokens": 2586, "main_completion_tokens": 4, "judge_total_tokens": 480, "judge_prompt_tokens": 409, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9", "latency_ms": 5080.934524536133, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as '6ixtynin9' with no contradictions or omissions.", "main_total_tokens": 2738, "main_prompt_tokens": 2729, "main_completion_tokens": 9, "judge_total_tokens": 468, "judge_prompt_tokens": 409, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread", "latency_ms": 5106.165170669556, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Club Dread' exactly matches the gold answer and correctly identifies the movie from the description. It is both factually correct and fully faithful to the gold reference.", "main_total_tokens": 3089, "main_prompt_tokens": 3084, "main_completion_tokens": 5, "judge_total_tokens": 494, "judge_prompt_tokens": 427, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet", "latency_ms": 5267.6379680633545, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title; the gold answer is 'Toilet' and the generated answer is also 'Toilet'. This is correct and fully consistent with the gold, so both correctness and faithfulness are 1.", "main_total_tokens": 3285, "main_prompt_tokens": 3280, "main_completion_tokens": 5, "judge_total_tokens": 492, "judge_prompt_tokens": 415, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel", "latency_ms": 4947.554588317871, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Electric Angel'), so it is fully correct. The explanation accurately reflects the comparison, so the response is fully faithful to the given information.", "main_total_tokens": 3021, "main_prompt_tokens": 3016, "main_completion_tokens": 5, "judge_total_tokens": 485, "judge_prompt_tokens": 418, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "4.2", "latency_ms": 2944.948196411133, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a movie title (the lowest rated movie from 2023), and the gold answer is the title 'Mega Ape'. The generated answer '4.2' is a numerical rating, not a movie title, so it does not correctly answer the question. It is also not faithful to the gold answer, which specifies a particular movie, not a score.", "main_total_tokens": 2560, "main_prompt_tokens": 2554, "main_completion_tokens": 6, "judge_total_tokens": 486, "judge_prompt_tokens": 385, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 312.4997615814209, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Tea House', but the model said 'I don't know', which is not correct. However, it did not introduce any incorrect alternative answer and honestly acknowledged uncertainty, so it is fully faithful to its own (lack of) knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 468, "judge_prompt_tokens": 386, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 347.78594970703125, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which is incorrect because the gold answer provides a specific movie title. However, the model does not introduce any contradictory or fabricated information; it simply expresses lack of knowledge, so it is fully faithful to its own output.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 395, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The language of the lowest-rated movie from 1980 is en", "latency_ms": 5512.977123260498, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer is 'English' while the generated answer is 'en'. Although 'en' is the ISO code for English and likely refers to the same language, it does not exactly match the expected natural language form. Thus, the answer is not strictly correct relative to the gold. Faithfulness is partial because the model is answering the right question and referring to the correct underlying fact but with a mismatched format.", "main_total_tokens": 2628, "main_prompt_tokens": 2612, "main_completion_tokens": 16, "judge_total_tokens": 512, "judge_prompt_tokens": 396, "judge_completion_tokens": 116}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity is Celebrity Deathmatch with a Popularity of 2.3247", "latency_ms": 7907.672643661499, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the gold label identifies 'Law & Order: Special Victims Unit' as the TV show with the highest popularity in 1999, while the model predicts 'Celebrity Deathmatch'. The response is, however, internally consistent and directly answers the question without contradicting its own reasoning, so it is faithful to the (incorrect) underlying retrieval or calculation.", "main_total_tokens": 3219, "main_prompt_tokens": 3191, "main_completion_tokens": 28, "judge_total_tokens": 522, "judge_prompt_tokens": 415, "judge_completion_tokens": 107}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2", "latency_ms": 3490.7355308532715, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2, so it is both correct and faithful.", "main_total_tokens": 2187, "main_prompt_tokens": 2183, "main_completion_tokens": 4, "judge_total_tokens": 439, "judge_prompt_tokens": 386, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "2025", "latency_ms": 4796.4794635772705, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer gives the release year as 2025, while the gold label indicates 2019. Since the years do not match, the answer is incorrect. However, it is internally consistent with the question and does not contradict any information it itself stated, so it is fully faithful to its own content.", "main_total_tokens": 2740, "main_prompt_tokens": 2735, "main_completion_tokens": 5, "judge_total_tokens": 480, "judge_prompt_tokens": 387, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "1", "latency_ms": 5228.735446929932, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer (1) does not match the gold answer (279), so it is incorrect. There is no evidence or reasoning provided to support the generated answer, so it is also unfaithful.", "main_total_tokens": 2228, "main_prompt_tokens": 2226, "main_completion_tokens": 2, "judge_total_tokens": 455, "judge_prompt_tokens": 383, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "10", "latency_ms": 27564.23807144165, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (10) does not match the gold answer (12), so it is factually incorrect for this QA pair. However, there is no evidence that the answer was derived in an unfaithful way from its (implicit) context; it is simply the wrong number, so faithfulness is scored as intact.", "main_total_tokens": 2533, "main_prompt_tokens": 2277, "main_completion_tokens": 256, "judge_total_tokens": 479, "judge_prompt_tokens": 383, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "2009", "latency_ms": 4787.195920944214, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, giving the correct release year 2009 and staying fully consistent with the provided information.", "main_total_tokens": 2852, "main_prompt_tokens": 2847, "main_completion_tokens": 5, "judge_total_tokens": 447, "judge_prompt_tokens": 390, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-small_llama3.1-70b-iq2_xs", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en", "latency_ms": 5018.754005432129, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The original language of 'The King’s Speech' is indeed English, and the generated answer matches the gold label exactly, so it is both correct and faithful.", "main_total_tokens": 3096, "main_prompt_tokens": 3092, "main_completion_tokens": 4, "judge_total_tokens": 447, "judge_prompt_tokens": 384, "judge_completion_tokens": 63}
