{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The document set does not provide information about a movie or TV show titled \"Wall Street.\"", "latency_ms": 5456.356763839722, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the release year of the film 'Wall Street' (1987). The generated answer claims the documents do not provide information about a movie or show titled 'Wall Street' instead of giving the correct year, so it is factually incorrect. However, it does not contradict the given document context and stays within what it claims is (not) in the document set, so it is faithful to that context.", "main_total_tokens": 2936, "main_prompt_tokens": 2918, "main_completion_tokens": 18, "judge_total_tokens": 512, "judge_prompt_tokens": 396, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie where classmates have to play a survival game using their cell phones is \"Future Diary Another World.\" In this series, a cell phone diary contains entries about future events, and a game of survival begins between seven people who possess the Future Diary [Doc 120683].", "latency_ms": 1451.6503810882568, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'Brain Fluid Explosion Girl', but the generated answer gives 'Future Diary Another World', which is incorrect. It also introduces extra, unsupported details about seven people and future diaries, which are not grounded in the provided gold answer or question context.", "main_total_tokens": 2681, "main_prompt_tokens": 2626, "main_completion_tokens": 55, "judge_total_tokens": 526, "judge_prompt_tokens": 443, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "In the TV show \"Dungeons & Dragons,\" six friends are transported into the Dungeons & Dragons realm and must try to find a way home with the help of their guide, the Dungeon Master [Doc 112849].", "latency_ms": 972.353458404541, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Dungeons & Dragons,\" matching the gold answer. The added description about six friends being transported and guided by the Dungeon Master is accurate to the show and remains consistent with the provided information, so it is both correct and faithful.", "main_total_tokens": 2465, "main_prompt_tokens": 2420, "main_completion_tokens": 45, "judge_total_tokens": 521, "judge_prompt_tokens": 435, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The context provided does not mention a movie about two rival gangs in Munich, one composed of German teens and the other of Turkish ones.", "latency_ms": 655.8637619018555, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Nacht der Wölfe', so the generated answer is factually incorrect; such a movie does exist. However, the generated response is faithful to the (stated) context it refers to, since it honestly claims the context does not mention that movie.", "main_total_tokens": 2965, "main_prompt_tokens": 2938, "main_completion_tokens": 27, "judge_total_tokens": 513, "judge_prompt_tokens": 425, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of \"Picnic at Hanging Rock\" take place on Valentine's Day, 1900 [Doc 126191].", "latency_ms": 586.1198902130127, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of 'Picnic at Hanging Rock' take place in 1900, matching the gold answer. It also remains faithful to the given information without adding unsupported claims.", "main_total_tokens": 2669, "main_prompt_tokens": 2644, "main_completion_tokens": 25, "judge_total_tokens": 482, "judge_prompt_tokens": 410, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The provided documents do not contain information about a 2004 movie with the described plot.", "latency_ms": 1134.432077407837, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'June', but the generated answer claims the information is not present instead of providing the correct movie title. It stays faithful to the (assumed) provided documents but is factually incorrect with respect to the gold answer.", "main_total_tokens": 2928, "main_prompt_tokens": 2910, "main_completion_tokens": 18, "judge_total_tokens": 529, "judge_prompt_tokens": 450, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "The highest rated Russian-language movie released in 1980 is \"Moscow Does Not Believe in Tears\" with a rating of 7.6 [Doc 61].", "latency_ms": 501.6295909881592, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect with respect to the gold answer: the highest rated Russian-language movie released in 1980 is given as 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger', but the model responded with 'Moscow Does Not Believe in Tears'. However, the model does faithfully reflect the (incorrect) information from the cited document, so it is faithful to its source but not correct relative to the gold.", "main_total_tokens": 2659, "main_prompt_tokens": 2625, "main_completion_tokens": 34, "judge_total_tokens": 549, "judge_prompt_tokens": 429, "judge_completion_tokens": 120}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest Russian-language movie rating in 1999 is 7.117 for \"The Rifleman of the Voroshilov Regiment\" [Doc 22123].", "latency_ms": 718.0171012878418, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states 7.6 as the highest rating, but the generated answer claims 7.117, which is incorrect. It also fabricates a specific rating and movie title that contradict the gold, so it is not faithful to the reference.", "main_total_tokens": 2750, "main_prompt_tokens": 2716, "main_completion_tokens": 34, "judge_total_tokens": 496, "judge_prompt_tokens": 419, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "Two movies released in 1987 have a rating over 7 and are in Chinese:\n\n1. \"A Chinese Ghost Story\" with a rating of 7.3 [Doc 7064].\n2. \"Last Eunuch In China\" with a rating of 7.8 [Doc 7754].", "latency_ms": 957.3016166687012, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.75, "justification": "The gold answer says there are 4 such movies, but the generated answer says there are 2, so the main numeric result is incorrect. It is partially faithful because it names two real movies with correct details from the (implied) documents, but it omits at least two additional qualifying movies, so overall faithfulness is limited.", "main_total_tokens": 2640, "main_prompt_tokens": 2577, "main_completion_tokens": 63, "judge_total_tokens": 556, "judge_prompt_tokens": 455, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "None of the TV shows released in 1987 with a rating over 7 are in Chinese. The available TV shows in 1987 with a rating over 7 are in other languages [Doc 113344, Doc 113317, Doc 113251].", "latency_ms": 950.2224922180176, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The question asks for a count (2), but the generated answer instead asserts there are none, so the numerical result is incorrect. The answer is broadly consistent with its cited documents (no obvious contradiction between statement and references), so it is relatively faithful to the provided docs, but because the docs are not shown here, there is slight uncertainty, so faithfulness is not perfect.", "main_total_tokens": 3019, "main_prompt_tokens": 2965, "main_completion_tokens": 54, "judge_total_tokens": 555, "judge_prompt_tokens": 447, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "There is no French language TV show released in 1980 mentioned in the provided documents.", "latency_ms": 501.5699863433838, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the only French language TV show released in 1980 is 'Fantômas'. The generated answer claims that there is no such show mentioned, which directly contradicts the gold answer and is therefore incorrect. However, the generated answer appears to be a faithful statement about the (presumably) provided documents, indicating that within those documents, no French language TV show from 1980 was mentioned. Thus, it is faithful to the given context but factually incorrect relative to the gold answer.", "main_total_tokens": 2491, "main_prompt_tokens": 2473, "main_completion_tokens": 18, "judge_total_tokens": 537, "judge_prompt_tokens": 403, "judge_completion_tokens": 134}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" [Doc 11205].", "latency_ms": 717.8692817687988, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Kill Cruise,\" which matches the gold answer exactly. It is fully supported by the provided information and does not introduce any unsupported claims.", "main_total_tokens": 2624, "main_prompt_tokens": 2600, "main_completion_tokens": 24, "judge_total_tokens": 475, "judge_prompt_tokens": 409, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 572.2033977508545, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, providing the correct movie title \"Won't You Be My Neighbor?\". It does not introduce contradictions or unsupported claims beyond citing a document id, so it is both correct and faithful to the given information.", "main_total_tokens": 2930, "main_prompt_tokens": 2891, "main_completion_tokens": 39, "judge_total_tokens": 521, "judge_prompt_tokens": 440, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "Please specify which title you are referring to, as there are multiple titles mentioned in the provided documents.", "latency_ms": 1156.3606262207031, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates that the correct response is the specific year 2018. The generated answer instead asks for clarification and does not provide the correct year, so it is incorrect. However, it remains faithful to the (implied) context by noting multiple titles and not hallucinating a year, so it is fully faithful.", "main_total_tokens": 2707, "main_prompt_tokens": 2687, "main_completion_tokens": 20, "judge_total_tokens": 493, "judge_prompt_tokens": 397, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The provided documents do not contain information about a TV series that is an animation about a young boy robot in 2030 AD.", "latency_ms": 662.9531383514404, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the TV series that is an animation about a young boy robot in 2030 AD, whose known correct answer is 'Astro Boy'. The generated answer incorrectly claims the information is not in the documents instead of giving 'Astro Boy', so it is factually incorrect. However, it does not invent any contradictory facts about the documents; it simply states absence of information, so it remains faithful to the (assumed) context, even though it fails the task.", "main_total_tokens": 2509, "main_prompt_tokens": 2483, "main_completion_tokens": 26, "judge_total_tokens": 537, "judge_prompt_tokens": 413, "judge_completion_tokens": 124}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 783.0750942230225, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer, and it does not introduce any contradictions or unsupported claims beyond that fact.", "main_total_tokens": 2301, "main_prompt_tokens": 2271, "main_completion_tokens": 30, "judge_total_tokens": 494, "judge_prompt_tokens": 424, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 720.8878993988037, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before the 26 November 1983 break-in, which matches the provided gold answer. It closely follows the given information without adding unsupported details, so it is both correct and faithful.", "main_total_tokens": 3029, "main_prompt_tokens": 2996, "main_completion_tokens": 33, "judge_total_tokens": 517, "judge_prompt_tokens": 428, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "The most popular English-speaking TV show released in 1997 is \"Oz,\" with a popularity score of 14.7527 [Doc 114714].", "latency_ms": 623.0213642120361, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the popularity score of the most popular English-speaking TV show released in 1997. The gold answer is 80.5772, but the generated answer gives 14.7527, which is incorrect. The generated answer is consistent with its own cited document, so it is faithful to the provided evidence but factually wrong relative to the gold.", "main_total_tokens": 2532, "main_prompt_tokens": 2500, "main_completion_tokens": 32, "judge_total_tokens": 524, "judge_prompt_tokens": 424, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "The provided documents do not mention any Russian language TV shows released in 1997.", "latency_ms": 628.2689571380615, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that there was 1 Russian language TV show released in 1997. The generated answer claims there were none mentioned in the documents, which contradicts the gold answer, so it is incorrect. However, relative to the provided documents (which the model states do not mention any), the answer is faithful to that evidence, so faithfulness is high.", "main_total_tokens": 2578, "main_prompt_tokens": 2561, "main_completion_tokens": 17, "judge_total_tokens": 500, "judge_prompt_tokens": 400, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 1275.12526512146, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Angel Eyes', which matches the gold answer. It stays faithful to the provided information and does not introduce contradictions or unsupported claims.", "main_total_tokens": 3422, "main_prompt_tokens": 3394, "main_completion_tokens": 28, "judge_total_tokens": 488, "judge_prompt_tokens": 422, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"What Is This Film Called Love?\" is English [Doc 58692].", "latency_ms": 621.9174861907959, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer incorrectly states that the language is English, while the gold label says Korean. It also references an external document contradicting the gold, so it is neither correct nor faithful to the provided answer.", "main_total_tokens": 2503, "main_prompt_tokens": 2482, "main_completion_tokens": 21, "judge_total_tokens": 469, "judge_prompt_tokens": 397, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "There are no English-speaking Horror movies released in 2009 with a popularity score greater than 7 in the provided documents.", "latency_ms": 593.3818817138672, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer claims there are none, so it is factually incorrect. It is, however, internally consistent and faithful to its own stated reasoning (no contradiction within the answer itself or claims beyond the question context).", "main_total_tokens": 2760, "main_prompt_tokens": 2735, "main_completion_tokens": 25, "judge_total_tokens": 499, "judge_prompt_tokens": 417, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The context does not provide information about movies released in 1979.", "latency_ms": 948.3282566070557, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Given the provided gold answer, the system has no information about movies released in 1979. The generated answer correctly states that the context does not provide this information, which aligns with the gold response. Therefore it is both correct and faithful to the given context.", "main_total_tokens": 2829, "main_prompt_tokens": 2815, "main_completion_tokens": 14, "judge_total_tokens": 492, "judge_prompt_tokens": 409, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The context provided does not mention a movie titled \"Back to the Future Part IV.\" Therefore, there is no information available about its release year.", "latency_ms": 627.0458698272705, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information about a movie titled 'Back to the Future Part IV' or its release year, which aligns with the gold answer noting that such a movie does not exist in the database. It does not invent a release year or contradict the given context, so it is both correct and faithful.", "main_total_tokens": 3300, "main_prompt_tokens": 3271, "main_completion_tokens": 29, "judge_total_tokens": 526, "judge_prompt_tokens": 429, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "The provided documents do not mention any movie from 1980 about the nuclear Chernobyl disaster. Therefore, there is no popularity score available for such a movie.", "latency_ms": 1176.1021614074707, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the popularity score of the least popular 1980 movie about the Chernobyl disaster. The gold answer notes that no such movies are found. The generated answer correctly states that the documents do not mention any such movie and thus no popularity score is available. This matches the gold intent: no movie -> no score. It does not invent data and stays within the provided information, so it is both correct and faithful.", "main_total_tokens": 2907, "main_prompt_tokens": 2874, "main_completion_tokens": 33, "judge_total_tokens": 557, "judge_prompt_tokens": 439, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are three TV shows or movies with 'Astro Boy' in the title:\n\n1. Astro Boy (TV, 2003) [Doc 116265]\n2. Astro Boy (TV, 1980) [Doc 112591]\n3. Astro Boy (Movie, 2009) [Doc 44947]", "latency_ms": 1220.5150127410889, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.8318724637288826, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.8318724637288826, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 4 titles, but the generated answer claims there are 3 and lists only three entries. This is directly contradicted by the gold, so the numerical answer is incorrect. However, the model is internally consistent and accurately reports the count that matches its own listed items, so it is faithful to its own reasoning and evidence.", "main_total_tokens": 2793, "main_prompt_tokens": 2725, "main_completion_tokens": 68, "judge_total_tokens": 556, "judge_prompt_tokens": 454, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "None of the movies listed from 2025 allude to the Holocaust.", "latency_ms": 586.8539810180664, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that there are 2 such movies, but the generated answer claims none of the listed movies allude to the Holocaust, which contradicts the gold answer, so it is incorrect. However, the generated answer is internally consistent with itself and does not introduce unsupported information beyond what it asserts, so it is fully faithful to its own reasoning/context.", "main_total_tokens": 2671, "main_prompt_tokens": 2656, "main_completion_tokens": 15, "judge_total_tokens": 502, "judge_prompt_tokens": 399, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "There were two TV shows released in the 1990s about Super Mario Bros:\n\n1. \"The Adventures of Super Mario Bros. 3\" released in 1990 [Doc 113661].\n2. \"Super Mario World\" released in 1991 [Doc 113814].", "latency_ms": 1208.655595779419, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer specifies 1 show, while the generated answer claims 2 shows. This directly contradicts the gold label, making the generated answer incorrect. The model also confidently cites two series as distinct 1990s shows when only one should count per the gold, so the reasoning is not faithful to the required answer.", "main_total_tokens": 3289, "main_prompt_tokens": 3230, "main_completion_tokens": 59, "judge_total_tokens": 542, "judge_prompt_tokens": 446, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "The provided documents do not mention any TV shows released between 2000 and 2010 about Super Mario Bros.", "latency_ms": 585.2887630462646, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the number of TV shows between 2000 and 2010 about Super Mario Bros, and the gold answer is 0. The generated answer states that the documents do not mention any such TV shows, which implies there are none known from the context, aligning with the gold answer of 0. It accurately reflects the lack of evidence in the provided documents, so it is both correct and faithful to the sources.", "main_total_tokens": 2900, "main_prompt_tokens": 2877, "main_completion_tokens": 23, "judge_total_tokens": 530, "judge_prompt_tokens": 412, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular TV show or movie about ninjas, based on the provided data, is \"NARUTO Spin-Off: Rock Lee & His Ninja Pals\" with a popularity score of 10.7994 [Doc 120321].", "latency_ms": 717.8747653961182, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer is 'Naruto', which is the main and most popular ninja series. The generated answer instead gives 'NARUTO Spin-Off: Rock Lee & His Ninja Pals', a related but different spin-off, so it is incorrect with respect to the gold. It does, however, remain partially faithful to the given document snippet by citing a show and popularity score from that data, but it overstates it as 'the most popular' in general rather than in the limited context of the provided data, so faithfulness is only partial.", "main_total_tokens": 2983, "main_prompt_tokens": 2934, "main_completion_tokens": 49, "judge_total_tokens": 574, "judge_prompt_tokens": 431, "judge_completion_tokens": 143}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "There is no mention of a Jurassic Park/Power Ranger crossover movie in the provided documents.", "latency_ms": 603.4977436065674, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks when a Jurassic Park/Power Ranger crossover movie was released; the gold answer clarifies that such a movie was never released. The generated answer states that there is no mention of such a crossover movie in the provided documents, which is consistent with the gold’s implication that it does not exist. It does not hallucinate a date or contradict the gold. Thus it is both correct relative to the gold and faithful to the provided information.", "main_total_tokens": 2794, "main_prompt_tokens": 2776, "main_completion_tokens": 18, "judge_total_tokens": 523, "judge_prompt_tokens": 403, "judge_completion_tokens": 120}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "There is no movie in the provided documents where Barbie fights the Predator. The documents mention a movie titled \"Killer Barbys vs. Dracula,\" but it does not involve the Predator [Doc 28703].", "latency_ms": 1389.8124694824219, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates that such a movie was not released, meaning effectively that no such movie exists. The generated answer correctly states that there is no movie where Barbie fights the Predator in the provided documents and clarifies that the mentioned film involves Dracula, not Predator. This aligns with the gold answer in substance. The mention of a specific document is consistent with using the given context and does not contradict any facts, so both correctness and faithfulness are satisfied.", "main_total_tokens": 3034, "main_prompt_tokens": 2992, "main_completion_tokens": 42, "judge_total_tokens": 545, "judge_prompt_tokens": 424, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The 4th oldest Star Wars movie mentioned in the documents is \"Star Wars: The Last Jedi,\" which was released in 2017 [Doc 76818].", "latency_ms": 649.9674320220947, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect: the 4th oldest Star Wars movie, based on release chronology, is not 'The Last Jedi' (2017) but matches the gold answer 'Star Wars: Droids - The Great Heep.' It is also unfaithful to the gold/reference, as it contradicts the specified correct title.", "main_total_tokens": 3128, "main_prompt_tokens": 3094, "main_completion_tokens": 34, "judge_total_tokens": 522, "judge_prompt_tokens": 423, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "Please specify which title you are referring to, as there are multiple titles mentioned in the provided documents.", "latency_ms": 1151.0331630706787, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer expects the specific year 1986, but the generated answer does not provide any year and instead asks for clarification. Therefore it is incorrect. However, given only the question text, the assistant does not know the referenced title and is honest about this uncertainty, so the response is faithful to the available information.", "main_total_tokens": 2707, "main_prompt_tokens": 2687, "main_completion_tokens": 20, "judge_total_tokens": 492, "judge_prompt_tokens": 397, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The rating of the movie \"Forrest Gump\" is not provided in the given documents.", "latency_ms": 1105.926275253296, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the rating is 8.466, but the generated answer claims the rating is not provided. This is incorrect relative to the gold, so correctness is 0. However, the generated answer is faithful to the (assumed) cited documents it refers to, stating that the rating is not in them, so faithfulness is 1.", "main_total_tokens": 2850, "main_prompt_tokens": 2831, "main_completion_tokens": 19, "judge_total_tokens": 517, "judge_prompt_tokens": 413, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show is \"Mighty Med\" [Doc 121125].", "latency_ms": 543.7517166137695, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the show as 'Mighty Med' and matches the gold answer exactly. It remains faithful to the provided information without adding unsupported details.", "main_total_tokens": 2181, "main_prompt_tokens": 2163, "main_completion_tokens": 18, "judge_total_tokens": 473, "judge_prompt_tokens": 410, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In \"Túnel de los huesos\" (2011), seven prisoners escaped from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 622.1487522125244, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Argentina as the country where the prison is located, matching the gold answer. The additional detail about Villa Devoto prison in Buenos Aires is plausible and does not contradict the gold answer. The response is faithful to the provided information and aligns with it.", "main_total_tokens": 2778, "main_prompt_tokens": 2744, "main_completion_tokens": 34, "judge_total_tokens": 505, "judge_prompt_tokens": 425, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The answer is not found in the provided documents.", "latency_ms": 438.70043754577637, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Jiang Jie', but the generated answer says the answer is not found in the documents, which is factually incorrect relative to the gold. However, the model does not invent an alternative answer and accurately reflects its (claimed) lack of information, so it remains faithful to its own stated context.", "main_total_tokens": 2684, "main_prompt_tokens": 2674, "main_completion_tokens": 10, "judge_total_tokens": 484, "judge_prompt_tokens": 394, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "There are two Korean-language movies from 2023 that mention assassins:\n\n1. \"The Assassin\" - It is an action historical drama depicting the story of Joseon's best swordsman Yi Nan [Doc 106219].\n2. \"Kill Boksoon\" - The movie features a renowned assassin who is also a single mom [Doc 105198].", "latency_ms": 1285.2108478546143, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 4, but the generated answer says there are two movies, so the numerical result is incorrect. Additionally, the question asks for Korean-language movies that mention assassins after 2023, whereas the generated answer lists two movies from 2023, so it does not faithfully follow the temporal constraint ('after 2023') or the gold answer.", "main_total_tokens": 2581, "main_prompt_tokens": 2510, "main_completion_tokens": 71, "judge_total_tokens": 559, "judge_prompt_tokens": 454, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "The provided documents do not contain any information about movies featuring Steven Seagal or a collaboration between Steven Seagal and Bruce Lee. Therefore, I cannot provide an answer based on the given context.", "latency_ms": 633.864164352417, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks how many movies feature both Steven Seagal and Bruce Lee. The gold answer says it cannot find any such movies. The generated answer correctly states that the provided documents do not contain information about movies featuring Steven Seagal or a collaboration between them and therefore cannot answer based on the context. This is correct given the constraints and faithful to the given context.", "main_total_tokens": 3299, "main_prompt_tokens": 3261, "main_completion_tokens": 38, "judge_total_tokens": 534, "judge_prompt_tokens": 431, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 572.6191997528076, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3 and does not introduce any unsupported information beyond citing a document reference.", "main_total_tokens": 2785, "main_prompt_tokens": 2765, "main_completion_tokens": 20, "judge_total_tokens": 460, "judge_prompt_tokens": 403, "judge_completion_tokens": 57}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The document does not specify the style of racing in the movie \"Oram Po\" [Doc 39729].", "latency_ms": 658.0028533935547, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the racing style is drift, but the generated answer claims the document does not specify the style of racing. This contradicts the gold, so it is factually incorrect. However, the generated answer is internally consistent with its own stated lack of information, so it is fully faithful to what it claims about the document.", "main_total_tokens": 2960, "main_prompt_tokens": 2937, "main_completion_tokens": 23, "judge_total_tokens": 495, "judge_prompt_tokens": 401, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 707.6261043548584, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Confronting a Serial Killer' and attributes it consistently to the referenced document, indicating both correctness and faithfulness.", "main_total_tokens": 2642, "main_prompt_tokens": 2622, "main_completion_tokens": 20, "judge_total_tokens": 489, "judge_prompt_tokens": 426, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 646.3067531585693, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and correctly attributes it. It is both correct and faithful to the provided information.", "main_total_tokens": 3017, "main_prompt_tokens": 2995, "main_completion_tokens": 22, "judge_total_tokens": 468, "judge_prompt_tokens": 407, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 538.4407043457031, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the movie title as 'The Sixth Sense'. It does not introduce contradictions or hallucinations relative to the given information, so it is both correct and faithful.", "main_total_tokens": 2753, "main_prompt_tokens": 2724, "main_completion_tokens": 29, "judge_total_tokens": 493, "judge_prompt_tokens": 420, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 841.2096500396729, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold year 2002 exactly and does not introduce unsupported information beyond a citation placeholder, so it is both correct and faithful.", "main_total_tokens": 2956, "main_prompt_tokens": 2939, "main_completion_tokens": 17, "judge_total_tokens": 458, "judge_prompt_tokens": 397, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 727.5934219360352, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly stating that the 2015 reboot is titled 'The Transporter Refueled.' It is fully correct and consistent with the provided information.", "main_total_tokens": 3018, "main_prompt_tokens": 2989, "main_completion_tokens": 29, "judge_total_tokens": 489, "judge_prompt_tokens": 420, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The movie 'The Transporter Refueled' was released in the year 2015 [Doc 67500].", "latency_ms": 723.6697673797607, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It is fully supported by the cited document reference, so it is both correct and faithful.", "main_total_tokens": 3006, "main_prompt_tokens": 2982, "main_completion_tokens": 24, "judge_total_tokens": 483, "judge_prompt_tokens": 408, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "The context provided does not mention any romantic comedy where Abraham Lincoln comes back from the dead.", "latency_ms": 514.7650241851807, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no such movie description is found, and the generated answer correctly and faithfully echoes that the context does not mention any romantic comedy with Abraham Lincoln coming back from the dead. There is no contradiction or hallucination relative to the given information.", "main_total_tokens": 2871, "main_prompt_tokens": 2853, "main_completion_tokens": 18, "judge_total_tokens": 490, "judge_prompt_tokens": 408, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The context does not provide information about a movie that alludes to a manuscript written by Ernest Hemingway.", "latency_ms": 488.39879035949707, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the title of a movie that alludes to a manuscript written by Ernest Hemingway, and the correct answer is 'Havana Darkness.' The generated answer incorrectly claims the context does not provide this information and fails to supply the title, so it is factually incorrect. However, it does not invent any new facts beyond the given context; it simply states that the information is not present, so it remains faithful to the (implied) context.", "main_total_tokens": 2677, "main_prompt_tokens": 2657, "main_completion_tokens": 20, "judge_total_tokens": 531, "judge_prompt_tokens": 408, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "In the TV show \"Doll & Em,\" Doll heads to Hollywood to be with her childhood best friend Em, who is now a successful actress [Doc 122259].", "latency_ms": 671.0515022277832, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Doll & Em\" and matches the gold answer exactly. It does not introduce contradictions or unsupported content; the additional detail about Em being a successful actress is consistent with typical descriptions and is cited from the provided document, so it is faithful to the given source.", "main_total_tokens": 2660, "main_prompt_tokens": 2625, "main_completion_tokens": 35, "judge_total_tokens": 517, "judge_prompt_tokens": 424, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The answer is not found in the provided documents.", "latency_ms": 680.0310611724854, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The gold answer is 'Hallam Foe', a specific movie title. The generated answer claims the answer is not found, which is incorrect; it should have given the movie title. Thus correctness is 0. The model is faithful to what it (incorrectly) stated about the documents, not hallucinating a different movie title, so faithfulness is relatively high, though it incorrectly asserts absence, so not perfect.", "main_total_tokens": 2666, "main_prompt_tokens": 2656, "main_completion_tokens": 10, "judge_total_tokens": 521, "judge_prompt_tokens": 409, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The context does not provide information about a character's last name that sounds like \"friend\" or \"foe.\" Therefore, the answer is not found.", "latency_ms": 702.0304203033447, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'A foe', indicating the question is answerable from the context and the correct choice is that his last name sounds like a foe. The generated answer claims the information is not provided and that the answer is not found, which contradicts the gold answer, so it is incorrect. However, it is faithful to the model’s stated interpretation of the (limited) context given in the prompt and does not hallucinate details beyond that, so faithfulness is high.", "main_total_tokens": 2341, "main_prompt_tokens": 2310, "main_completion_tokens": 31, "judge_total_tokens": 544, "judge_prompt_tokens": 417, "judge_completion_tokens": 127}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "Sam is 18 years old in the 2017 TV show \"Atypical\" [Doc 124583].", "latency_ms": 664.4935607910156, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, which matches the gold answer. It does not introduce conflicting or extraneous information beyond a benign document reference, so it is faithful to the given gold answer.", "main_total_tokens": 2856, "main_prompt_tokens": 2832, "main_completion_tokens": 24, "judge_total_tokens": 496, "judge_prompt_tokens": 411, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show \"Atypical,\" Sam is on the autism spectrum [Doc 124583].", "latency_ms": 649.0914821624756, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the TV show 'Atypical,' Sam is on the autism spectrum, which matches the gold answer exactly. It is also faithful, directly answering the question without introducing unsupported information (the bracketed citation does not conflict with the content).", "main_total_tokens": 2759, "main_prompt_tokens": 2735, "main_completion_tokens": 24, "judge_total_tokens": 496, "judge_prompt_tokens": 411, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Sandra was born in Chamonix [Doc 59659].", "latency_ms": 639.5821571350098, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, which matches the gold answer exactly. It is faithful to the provided information and does not introduce unsupported details.", "main_total_tokens": 3215, "main_prompt_tokens": 3202, "main_completion_tokens": 13, "judge_total_tokens": 476, "judge_prompt_tokens": 410, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481].", "latency_ms": 1119.2398071289062, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly fills in the blank with 'rains,' matching the gold answer exactly aside from the extraneous reference '[Doc 100481]'. This makes the core prediction fully correct. Faithfulness is slightly reduced due to the unexplained document citation appended to the otherwise correct answer.", "main_total_tokens": 3789, "main_prompt_tokens": 3781, "main_completion_tokens": 8, "judge_total_tokens": 489, "judge_prompt_tokens": 398, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the German language animation \"Tabaluga,\" Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 702.4955749511719, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Greenland as the place where Tabaluga is crown-prince, matching the gold answer exactly and staying faithful to the information provided.", "main_total_tokens": 3078, "main_prompt_tokens": 3046, "main_completion_tokens": 32, "judge_total_tokens": 488, "judge_prompt_tokens": 426, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is \"One Afternoon\" [Doc 96838].", "latency_ms": 1057.0626258850098, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'One Afternoon' and stays within the information given, including the bracketed document reference, so it is both correct and faithful.", "main_total_tokens": 2436, "main_prompt_tokens": 2392, "main_completion_tokens": 44, "judge_total_tokens": 513, "judge_prompt_tokens": 448, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The context provided does not contain information about a low-rated Italian language movie involving a Holocaust survivor taking care of a 12-year-old street kid who recently robbed her.", "latency_ms": 5250.555753707886, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "recall_at_10": 0.0, "precision_at_5": 0.0, "precision_at_10": 0.0, "ndcg_at_5": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_5": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'The Life Ahead'. The generated answer incorrectly claims the context lacks this information and does not provide the correct movie title, so it is factually incorrect. However, it remains faithful to the (assumed) given context by not fabricating a movie title, so its faithfulness to the provided context is high.", "main_total_tokens": 2754, "main_prompt_tokens": 2721, "main_completion_tokens": 33, "judge_total_tokens": 529, "judge_prompt_tokens": 431, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 938.1608963012695, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary, matching the gold answer exactly. The additional details about him being a New Zealander and founder of the McLaren racing team are accurate and do not conflict with the source, so the response is both correct and faithful.", "main_total_tokens": 2922, "main_prompt_tokens": 2889, "main_completion_tokens": 33, "judge_total_tokens": 510, "judge_prompt_tokens": 417, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The name of the racing team is the Newman-Haas (Andretti) racing team [Doc 20456].", "latency_ms": 1090.5437469482422, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer gives a different team name (Newman-Haas (Andretti)) from the gold answer (The McLaren Motor Racing team), so it is factually incorrect for this question. It also appears to conflict with the referenced document, so it is not faithful to the provided evidence.", "main_total_tokens": 2582, "main_prompt_tokens": 2559, "main_completion_tokens": 23, "judge_total_tokens": 496, "judge_prompt_tokens": 406, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context does not provide information about the country of origin for any specific individual.", "latency_ms": 622.523307800293, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies 'New Zealand' as the country, but the generated answer claims the context does not provide this information. This contradicts the gold answer, so it is incorrect. However, it remains faithful to the (assumed) provided context by not hallucinating an answer, so faithfulness is high.", "main_total_tokens": 2521, "main_prompt_tokens": 2505, "main_completion_tokens": 16, "judge_total_tokens": 486, "judge_prompt_tokens": 392, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "The context provided does not mention any 2013 movie where John McClane blows up the Great Wall of China.", "latency_ms": 498.5642433166504, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer incorrectly implies reliance on a given context rather than clearly stating that there is no such movie, whereas the gold answer correctly says the movie does not exist in the database. Thus the generated answer is not fully correct, but it is faithful to the (unstated) context it references.", "main_total_tokens": 2610, "main_prompt_tokens": 2586, "main_completion_tokens": 24, "judge_total_tokens": 512, "judge_prompt_tokens": 422, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with that description is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 517.9300308227539, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Tomie: Forbidden Fruit' and does not introduce any contradictions or fabrications relative to the provided information.", "main_total_tokens": 2766, "main_prompt_tokens": 2741, "main_completion_tokens": 25, "judge_total_tokens": 509, "judge_prompt_tokens": 442, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie \"Lei mi parla ancora,\" Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 509.7184181213379, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years) and does not introduce unsupported claims beyond citing a document, so it is both correct and faithful.", "main_total_tokens": 3191, "main_prompt_tokens": 3161, "main_completion_tokens": 30, "judge_total_tokens": 486, "judge_prompt_tokens": 424, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The answer is not found in the provided documents.", "latency_ms": 851.003885269165, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the question is directly answerable: in the film they search for the Book of Souls, which matches the gold answer. However, the response does not contradict any provided context; it simply claims the answer is not found, so it remains faithful to what it asserts about the documents.", "main_total_tokens": 2501, "main_prompt_tokens": 2491, "main_completion_tokens": 10, "judge_total_tokens": 497, "judge_prompt_tokens": 404, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Zombies on the Moon\" is not found in the provided documents.", "latency_ms": 554.6472072601318, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year or information about 'Zombies on the Moon' is not available in the given sources, which is correct given the context. The generated answer is faithful to the limitation of the provided documents and does not hallucinate a year.", "main_total_tokens": 3077, "main_prompt_tokens": 3058, "main_completion_tokens": 19, "judge_total_tokens": 497, "judge_prompt_tokens": 410, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Galactic baker\" is not found in the provided documents.", "latency_ms": 1243.5202598571777, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer and the generated answer are semantically equivalent: both state that the release year / movie information is not available. This is correct given the lack of data and faithful to the prompt that the model should rely on provided documents.", "main_total_tokens": 3364, "main_prompt_tokens": 3347, "main_completion_tokens": 17, "judge_total_tokens": 484, "judge_prompt_tokens": 406, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Return of the Null Pointer\" is not found in the provided documents.", "latency_ms": 727.025032043457, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both answers correctly state that the release year/information is not available, which matches the gold response. The generated answer is faithful to the implied context (no data in the provided documents) and does not fabricate a year.", "main_total_tokens": 2606, "main_prompt_tokens": 2587, "main_completion_tokens": 19, "judge_total_tokens": 485, "judge_prompt_tokens": 410, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Data Science: The Movie\" is not found in the provided documents.", "latency_ms": 887.3820304870605, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available / not found. This is correct given the context and faithful to the provided documents, which do not contain the movie or its release year.", "main_total_tokens": 2734, "main_prompt_tokens": 2715, "main_completion_tokens": 19, "judge_total_tokens": 483, "judge_prompt_tokens": 410, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document does not contain information about a movie titled \"Python's Revenge 2.\"", "latency_ms": 809.5715045928955, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie that is not in the referenced data. The gold answer correctly states that the movie is not in the database. The generated answer similarly states that the document does not contain information about that movie. This is correct (no year is fabricated) and faithful to the absence of information in the underlying document.", "main_total_tokens": 2897, "main_prompt_tokens": 2880, "main_completion_tokens": 17, "judge_total_tokens": 508, "judge_prompt_tokens": 408, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Last Pixel\" is not found in the provided documents.", "latency_ms": 649.4626998901367, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database. The generated answer says the release year is not found in the provided documents. Both convey that the information is unavailable, so the content is correct and faithful to the gold.", "main_total_tokens": 2361, "main_prompt_tokens": 2344, "main_completion_tokens": 17, "judge_total_tokens": 483, "judge_prompt_tokens": 406, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The man who ate the sun\" is not found in the provided documents.", "latency_ms": 806.3805103302002, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie that appears not to be in the database. The gold answer states that the movie is not in the database, and the generated answer states that the release year is not found in the provided documents. Both convey that the information is unavailable, so the generated answer is correct and faithful to the gold.", "main_total_tokens": 2741, "main_prompt_tokens": 2721, "main_completion_tokens": 20, "judge_total_tokens": 512, "judge_prompt_tokens": 412, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Days of Lehman Brothers\" is not mentioned in the provided documents.", "latency_ms": 889.9214267730713, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned in the provided documents, which aligns with the gold answer that the director information is not in the database. It is both correct and faithful to the given context.", "main_total_tokens": 2692, "main_prompt_tokens": 2670, "main_completion_tokens": 22, "judge_total_tokens": 493, "judge_prompt_tokens": 419, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"A Love to Die For\" is not mentioned in the provided documents.", "latency_ms": 522.1168994903564, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned, matching the gold response that the database does not provide the director information. It does not introduce unsupported facts and stays faithful to the given context.", "main_total_tokens": 2964, "main_prompt_tokens": 2944, "main_completion_tokens": 20, "judge_total_tokens": 486, "judge_prompt_tokens": 415, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not mentioned in the provided documents. [Doc 111360]", "latency_ms": 675.7631301879883, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the director of 'Journey into Gaza'. The gold answer explicitly states that this information is not available in the database. The generated answer states that the director is not mentioned in the provided documents, which is equivalent in meaning. It does not hallucinate a director and is consistent with the source limitations. Therefore it is both correct with respect to the task and faithful to the described evidence.", "main_total_tokens": 3403, "main_prompt_tokens": 3379, "main_completion_tokens": 24, "judge_total_tokens": 528, "judge_prompt_tokens": 417, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Night at Tremore Beach\" is not mentioned in the provided documents. [Doc 135098]", "latency_ms": 887.5679969787598, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director is not mentioned / not provided in the available information. This matches the database constraint and does not introduce hallucinated details. Therefore the answer is correct and faithful to the source.", "main_total_tokens": 2392, "main_prompt_tokens": 2364, "main_completion_tokens": 28, "judge_total_tokens": 501, "judge_prompt_tokens": 425, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 1583.803653717041, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly matches the gold answer: it identifies the movie as 'Rise of the Mummy,' so it is factually correct for this evaluation. However, it introduces an unsupported reference \"[Doc 98375]\" that is not grounded in the provided information, so it is not fully faithful to the given context or instructions.", "main_total_tokens": 2557, "main_prompt_tokens": 2530, "main_completion_tokens": 27, "judge_total_tokens": 517, "judge_prompt_tokens": 418, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse is \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 686.8908405303955, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Stranger in My Bed' and does not introduce unsupported information beyond a harmless document reference.", "main_total_tokens": 2601, "main_prompt_tokens": 2572, "main_completion_tokens": 29, "judge_total_tokens": 489, "judge_prompt_tokens": 425, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie you are referring to is not a movie, but a TV show titled \"Game of Silence\" [Doc 123938].", "latency_ms": 902.8315544128418, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question clearly refers to 'Game of Silence' as a movie, consistent with the gold answer. The generated answer incorrectly claims it is not a movie but a TV show, contradicting the gold reference and the user’s framing. This makes the response factually incorrect relative to the task and unfaithful to the provided gold answer.", "main_total_tokens": 2199, "main_prompt_tokens": 2172, "main_completion_tokens": 27, "judge_total_tokens": 522, "judge_prompt_tokens": 424, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie you are referring to is \"The Craigslist Killer\" from 2011. It is based on a true story about Philip Markoff, a med student who leads a double life as a brutal and cruel sexual deviant, abusing prostitutes he finds via Craigslist [Doc 53039].", "latency_ms": 1002.8805732727051, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.85, "justification": "The question asks for the movie title, which is correctly identified as 'The Craigslist Killer.' The additional description about the 2011 film and Philip Markoff is accurate in general, but the citation '[Doc 53039]' is fabricated and not grounded in any provided source, reducing faithfulness slightly.", "main_total_tokens": 2636, "main_prompt_tokens": 2578, "main_completion_tokens": 58, "judge_total_tokens": 543, "judge_prompt_tokens": 450, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which this happens is \"Wood Job!\" [Doc 66094].", "latency_ms": 771.345853805542, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", which matches the gold answer exactly. It is faithful to the provided information and does not add unsupported details.", "main_total_tokens": 3694, "main_prompt_tokens": 3677, "main_completion_tokens": 17, "judge_total_tokens": 482, "judge_prompt_tokens": 417, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure is \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 1923.593282699585, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as \"Sesame Street: The Magical Wand Chase\" and stays faithful to the provided information.", "main_total_tokens": 3417, "main_prompt_tokens": 3368, "main_completion_tokens": 49, "judge_total_tokens": 522, "judge_prompt_tokens": 459, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a rock fall traps a boy's parents inside a burning car, and the 7-year-old boy must find the hero within him to save their lives, is \"Beyond\" [Doc 74597].", "latency_ms": 1166.4817333221436, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Beyond,\" which matches the gold label. It stays faithful to the question and does not introduce unsupported information, aside from a citation placeholder that does not alter the core answer.", "main_total_tokens": 2473, "main_prompt_tokens": 2428, "main_completion_tokens": 45, "judge_total_tokens": 527, "judge_prompt_tokens": 453, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman, fired from a financial corporation during the Asia crisis, returns home with no money is \"6ixtynin9\" [Doc 22478].", "latency_ms": 649.4479179382324, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"6ixtynin9,\" which matches the gold answer exactly. The response is fully supported by the provided information and does not introduce unsupported claims, so it is both correct and faithful.", "main_total_tokens": 2434, "main_prompt_tokens": 2397, "main_completion_tokens": 37, "judge_total_tokens": 517, "judge_prompt_tokens": 439, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie in which a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort is \"Club Dread\" [Doc 32410].", "latency_ms": 802.9258251190186, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Club Dread,\" which matches the gold answer exactly. It is faithful to the provided information and does not introduce any contradictions or unsupported details.", "main_total_tokens": 2817, "main_prompt_tokens": 2785, "main_completion_tokens": 32, "judge_total_tokens": 524, "judge_prompt_tokens": 456, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is \"Toilet\" [Doc 48873].", "latency_ms": 787.0340347290039, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" which matches the gold answer exactly. It stays faithful to the question, restating the premise and providing the correct title without adding extraneous or contradictory information.", "main_total_tokens": 3071, "main_prompt_tokens": 3020, "main_completion_tokens": 51, "judge_total_tokens": 539, "judge_prompt_tokens": 465, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie you are referring to is \"Electric Angel\" [Doc 1024].", "latency_ms": 1042.4175262451172, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Electric Angel', which matches the gold answer exactly. It does not introduce any contradictions or unsupported details; the bracketed reference does not change the core answer. Therefore, both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3004, "main_prompt_tokens": 2987, "main_completion_tokens": 17, "judge_total_tokens": 515, "judge_prompt_tokens": 433, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is \"Fall,\" with a rating of 4.0 [Doc 106810].", "latency_ms": 603.9469242095947, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the lowest rated 2023 movie is 'Mega Ape', while the generated answer claims it is 'Fall' with a 4.0 rating. This is factually incorrect relative to the gold, so correctness is 0. The generated answer also introduces a specific rating and citation from a document that contradicts the given gold label, indicating it is not faithful to the provided information, so faithfulness is 0.", "main_total_tokens": 2843, "main_prompt_tokens": 2817, "main_completion_tokens": 26, "judge_total_tokens": 527, "judge_prompt_tokens": 407, "judge_completion_tokens": 120}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "The highest rated TV show from 2010 is \"Tea House\" with a rating of 9.2 [Doc 119448].", "latency_ms": 773.1227874755859, "retrieval_recall": 1.0, "retrieval_mrr": 0.08333333333333333, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 'Tea House' as the highest rated TV show from 2010, matching the gold answer exactly. It does not introduce contradictions or unsupported claims, and the added rating detail appears to be sourced from the referenced document, so it remains faithful to the given context.", "main_total_tokens": 3137, "main_prompt_tokens": 3109, "main_completion_tokens": 28, "judge_total_tokens": 499, "judge_prompt_tokens": 410, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest rated Japanese language movie from 1999 is \"The Geisha House,\" with a rating of 7.4 [Doc 23076].", "latency_ms": 719.8488712310791, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer cites 'The Geisha House' as the highest rated Japanese language movie from 1999, which contradicts the gold answer 'The Uncut Story of Mewtwo's Origin'. Thus, it is factually incorrect relative to the gold. It also introduces a specific rating and document reference without support from the provided gold information, so it is not faithful to the source.", "main_total_tokens": 2774, "main_prompt_tokens": 2743, "main_completion_tokens": 31, "judge_total_tokens": 531, "judge_prompt_tokens": 422, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is \"Cataclysm,\" with a rating of 3.889. The language of this movie is English [Doc 744].", "latency_ms": 709.7635269165039, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980. The gold answer is 'English'. The generated answer identifies the movie ('Cataclysm') and states its language is English. Even though it adds extra detail about the title and rating, it correctly answers that the language is English and remains consistent with the gold. Thus it is both correct and faithful to the provided information.", "main_total_tokens": 3273, "main_prompt_tokens": 3237, "main_completion_tokens": 36, "judge_total_tokens": 531, "judge_prompt_tokens": 419, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was \"The Amanda Show,\" with a popularity score of 3.4197 [Doc 115219].", "latency_ms": 821.9699859619141, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the gold label states that the correct show is 'Law & Order: Special Victims Unit,' whereas the model answered 'The Amanda Show.' However, the model did not contradict its own cited document and seems faithful to the provided citation, so faithfulness is high even though the answer is factually wrong relative to the gold.", "main_total_tokens": 3363, "main_prompt_tokens": 3330, "main_completion_tokens": 33, "judge_total_tokens": 525, "judge_prompt_tokens": 423, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 1016.1492824554443, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 and attributes it to a document, indicating it is grounded in the provided source.", "main_total_tokens": 2823, "main_prompt_tokens": 2805, "main_completion_tokens": 18, "judge_total_tokens": 460, "judge_prompt_tokens": 400, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "There are two movies titled \"The Story of Us\" mentioned in the documents. One was released in 1999 [Doc 22481], and the other was released in 2019 [Doc 88811].", "latency_ms": 780.6010246276855, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question expects a single year (2019), but the generated answer lists both 1999 and 2019 and does not clearly identify which one answers the question, so it is incorrect. However, it accurately reflects the documents by stating there are two films and giving both years, so it is faithful to the provided information.", "main_total_tokens": 2862, "main_prompt_tokens": 2818, "main_completion_tokens": 44, "judge_total_tokens": 525, "judge_prompt_tokens": 428, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The number of votes \"My Dog Skip\" received is not provided in the document. [Doc 24114]", "latency_ms": 791.945219039917, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the film received 279 votes, but the generated answer claims the number of votes is not provided. This directly contradicts the gold, so the answer is factually incorrect. However, the generated answer is internally consistent and does not invent a specific number; it faithfully reflects a (mistaken) reading of the referenced document, so faithfulness is high.", "main_total_tokens": 2832, "main_prompt_tokens": 2809, "main_completion_tokens": 23, "judge_total_tokens": 506, "judge_prompt_tokens": 404, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The documents provided do not contain information about the number of votes received by \"Oh My God!\" in any of its versions.", "latency_ms": 667.3405170440674, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that \"Oh My God!\" received 12 votes. The generated answer claims that the documents do not contain information about the number of votes, which contradicts the gold. Thus, it is incorrect. However, assuming the model only had the provided documents and those indeed lacked the vote count, the statement is faithful to its cited context, so faithfulness is high.", "main_total_tokens": 2889, "main_prompt_tokens": 2864, "main_completion_tokens": 25, "judge_total_tokens": 515, "judge_prompt_tokens": 406, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The release year for the movie \"The Last Days of Lehman Brothers\" is not found in the provided documents.", "latency_ms": 772.5741863250732, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates the movie was released in 2009, but the generated answer claims the release year is not found, which is factually incorrect. However, the generated response is faithful to the (assumed) provided documents by explicitly stating that the information is not present, so faithfulness is high while correctness is low.", "main_total_tokens": 2600, "main_prompt_tokens": 2577, "main_completion_tokens": 23, "judge_total_tokens": 507, "judge_prompt_tokens": 410, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_bge-m3_gpt-4o", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 633.5761547088623, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the original language of 'The King’s Speech' is English, matching the gold answer 'en'. It is fully supported by the provided information and adds only a harmless citation-style note, remaining faithful to the question and answer.", "main_total_tokens": 2882, "main_prompt_tokens": 2863, "main_completion_tokens": 19, "judge_total_tokens": 487, "judge_prompt_tokens": 402, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The year of release for \"Wall Street\" is not provided in the given documents.", "latency_ms": 603.0795574188232, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1987, but the generated answer claims the release year is not provided, which is factually incorrect. However, the answer is faithful to the (incorrect) reasoning it presents, as it does not fabricate a year and accurately reflects its own (mistaken) assessment of the given documents.", "main_total_tokens": 2093, "main_prompt_tokens": 2076, "main_completion_tokens": 17, "judge_total_tokens": 489, "judge_prompt_tokens": 395, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie where classmates have to play a survival game using their cell phones is \"Tokyo Gore School\" [Doc 45096].", "latency_ms": 538.8109683990479, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Brain Fluid Explosion Girl', but the generated answer gives 'Tokyo Gore School', which is incorrect. The model clearly used the provided document faithfully, but that document did not match the gold answer, so the response is faithful to its source but factually incorrect relative to the gold.", "main_total_tokens": 1944, "main_prompt_tokens": 1918, "main_completion_tokens": 26, "judge_total_tokens": 505, "judge_prompt_tokens": 414, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show in which six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" [Doc 112849].", "latency_ms": 655.663013458252, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the TV show as 'Dungeons & Dragons'. It is correct and faithful to the provided information.", "main_total_tokens": 1695, "main_prompt_tokens": 1665, "main_completion_tokens": 30, "judge_total_tokens": 481, "judge_prompt_tokens": 420, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The document set does not contain information about a movie involving two rival gangs in Munich, one composed of German teens and the other of Turkish ones.", "latency_ms": 918.9329147338867, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is faithful to the (implied) document set by honestly stating that it lacks the information, so faithfulness is high. However, it is factually incorrect relative to the gold answer because the correct movie is 'Nacht der Wölfe', which the system did not provide.", "main_total_tokens": 2042, "main_prompt_tokens": 2013, "main_completion_tokens": 29, "judge_total_tokens": 518, "judge_prompt_tokens": 427, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of \"Picnic at Hanging Rock\" take place on Valentine's Day, 1900 [Doc 126191].", "latency_ms": 1076.9884586334229, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the year as 1900 and aligns with the provided gold answer without adding unsupported information beyond the referenced document citation.", "main_total_tokens": 1935, "main_prompt_tokens": 1910, "main_completion_tokens": 25, "judge_total_tokens": 469, "judge_prompt_tokens": 410, "judge_completion_tokens": 59}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie you are referring to is \"April's Shower.\" The main character's name, April, is also the name of a calendar month. However, the provided documents do not contain information about this movie or its plot. Therefore, based on the context given, the answer is not found.", "latency_ms": 1235.6011867523193, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.4, "justification": "The correct answer from the gold label is 'June', not 'April's Shower', so the generated answer is factually incorrect. The assistant also incorrectly asserts that the answer is not found. It partially references the question context (month-name heroine) but fabricates a specific movie title without grounding in the provided information, so faithfulness is low.", "main_total_tokens": 2318, "main_prompt_tokens": 2256, "main_completion_tokens": 62, "judge_total_tokens": 596, "judge_prompt_tokens": 494, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1202.9030323028564, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answer is factually incorrect because the gold answer provides a specific movie while the model says it does not know. However, the answer is faithful to the model’s own claimed uncertainty and does not fabricate information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 476, "judge_prompt_tokens": 399, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 284.8620414733887, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a concrete rating (7.6), while the generated answer says 'I don't know,' which does not match the correct value. It is therefore incorrect. However, it is faithful in that it does not fabricate an answer and accurately reflects uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 473, "judge_prompt_tokens": 389, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 995.6016540527344, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3535124404757273, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is incorrect. The assistant directly answered the question, so it is faithful to the user query, but the content is wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 467, "judge_prompt_tokens": 394, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 163.6674404144287, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer of 0 is incorrect because the gold answer is 2. The explanation of the answer aligns with the comparison given (gold vs. generated) and does not introduce any contradictions or hallucinations, so it is faithful to the provided information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 472, "judge_prompt_tokens": 395, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1482.0537567138672, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Fantômas'. The generated answer says 'I don't know', which does not provide the correct title, so correctness is 0. However, the model honestly indicates lack of knowledge rather than fabricating an answer, so it is fully faithful to its own uncertainty and thus gets a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 488, "judge_prompt_tokens": 389, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" [Doc 11205].", "latency_ms": 628.1464099884033, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' which matches the gold answer. It is fully consistent with the provided information and does not introduce unsupported details beyond citing the document, so it is both correct and faithful.", "main_total_tokens": 1899, "main_prompt_tokens": 1875, "main_completion_tokens": 24, "judge_total_tokens": 485, "judge_prompt_tokens": 409, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 849.2889404296875, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title \"Won't You Be My Neighbor?\" and stays faithful to the information given without adding incorrect details.", "main_total_tokens": 2137, "main_prompt_tokens": 2098, "main_completion_tokens": 39, "judge_total_tokens": 500, "judge_prompt_tokens": 440, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The movie \"It\" was released in the year 2017 [Doc 76823].", "latency_ms": 849.7517108917236, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states the movie was released in 2017, but the gold answer specifies 2018, so the content is factually incorrect. However, the model faithfully reflects the content it cites from Doc 76823, so it is internally consistent with its stated source.", "main_total_tokens": 1971, "main_prompt_tokens": 1952, "main_completion_tokens": 19, "judge_total_tokens": 483, "judge_prompt_tokens": 396, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The context provided does not mention a TV series that is an animation about a young boy robot in 2030 AD.", "latency_ms": 630.3768157958984, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question clearly refers to the animated TV series 'Astro Boy', but the generated answer incorrectly claims the context does not mention such a series, instead of providing the correct title. Thus, the response is factually incorrect. However, it stays within the bounds of the given context and does not fabricate additional information, so it is faithful to the (mistaken) interpretation of the context.", "main_total_tokens": 1960, "main_prompt_tokens": 1936, "main_completion_tokens": 24, "judge_total_tokens": 520, "judge_prompt_tokens": 411, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 878.8073062896729, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the 2015 film 'Room,' the woman and her son were held captive for 7 years, which matches the gold answer exactly. The reference tag does not contradict anything, so the answer is both correct and faithful.", "main_total_tokens": 1670, "main_prompt_tokens": 1640, "main_completion_tokens": 30, "judge_total_tokens": 507, "judge_prompt_tokens": 424, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 799.081563949585, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983, matching the provided gold answer exactly and staying faithful to it.", "main_total_tokens": 2089, "main_prompt_tokens": 2056, "main_completion_tokens": 33, "judge_total_tokens": 505, "judge_prompt_tokens": 428, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6393.253564834595, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model says it does not know rather than providing an incorrect value. This is faithful to its uncertainty but does not answer the question correctly, since the gold answer is a specific numeric score (80.5772).", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 470, "judge_prompt_tokens": 396, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 160.09521484375, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1, but the generated answer is 0, so it is incorrect. However, the evaluation accurately compares the generated answer to the gold answer without introducing new facts, so it is fully faithful to the given information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 464, "judge_prompt_tokens": 385, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 707.5433731079102, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, correctly identifying the TV show as 'Angel Eyes' and remains faithful to the information provided without adding unsupported details.", "main_total_tokens": 2500, "main_prompt_tokens": 2465, "main_completion_tokens": 35, "judge_total_tokens": 491, "judge_prompt_tokens": 429, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"What Is This Film Called Love?\" is English [Doc 58692].", "latency_ms": 503.34811210632324, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the movie language is Korean, while the generated answer states it is English. This is factually incorrect relative to the gold, so correctness is 0. The generated answer also contradicts the provided gold label and is thus not faithful, so faithfulness is 0.", "main_total_tokens": 1845, "main_prompt_tokens": 1824, "main_completion_tokens": 21, "judge_total_tokens": 486, "judge_prompt_tokens": 397, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 548.3474731445312, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. However, the model did provide a concrete numeric answer to the question without contradicting its own reasoning or adding unrelated information, so it is internally faithful even though factually wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 394, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The context does not provide information about the best-rated movie released in 1979. The documents provided only contain information about movies from 1980 and 1981. Therefore, the best-rated movie from 1979 is not found in the given context.", "latency_ms": 950.5994319915771, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that there are no movies from 1979 in the database. The generated answer says that the best-rated movie from 1979 is not found in the given context and refers to documents from 1980 and 1981, contradicting the gold assumption about the database contents. It also incorrectly frames the limitation as lack of context rather than explicitly acknowledging the database limitation. Therefore it is not correct relative to the task and not faithful to the gold answer.", "main_total_tokens": 2207, "main_prompt_tokens": 2155, "main_completion_tokens": 52, "judge_total_tokens": 573, "judge_prompt_tokens": 447, "judge_completion_tokens": 126}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The context provided does not mention the release of a movie titled \"Back to the Future Part IV.\" The documents only reference the original trilogy: \"Back to the Future\" (1985) [Doc 4834], \"Back to the Future Part II\" (1989) [Doc 9405], and \"Back to the Future Part III\" (1990) [Doc 10621]. There is no information about a fourth installment in the series.", "latency_ms": 1003.9262771606445, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information about a movie titled 'Back to the Future Part IV' and that only the original trilogy is referenced. This aligns with reality (there is no official Part IV film) and is faithful to the described context documents. The wording differs slightly from the gold answer but is semantically equivalent in conveying that such a movie is not present in the database/context.", "main_total_tokens": 2745, "main_prompt_tokens": 2652, "main_completion_tokens": 93, "judge_total_tokens": 604, "judge_prompt_tokens": 493, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot find any movies or shows matching the criteria, which aligns with the gold answer that there are no movies from 1980 about the Chernobyl disaster. It is faithful to the gold response and does not introduce contradictory information, only a mild suggestion to recheck the question, which is consistent with the fact that Chernobyl occurred in 1986.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 544, "judge_prompt_tokens": 434, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 131.1938762664795, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is numerically incorrect because the correct number is 4 but it answered 0. It is, however, internally consistent and does not contradict itself or the question, so it is faithful to its own reasoning even though the result is wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 470, "judge_prompt_tokens": 388, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 184.61132049560547, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer per the provided gold label is 2, but the generated answer is 0, so it is incorrect. The evaluation (0) is fully based on the given Gold vs. Generated pair, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 466, "judge_prompt_tokens": 386, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 120.66864967346191, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show about Super Mario Bros released in the 1990s, while the generated answer says 0. This directly contradicts the gold, so correctness is 0. The model consistently maintains its (incorrect) claim without introducing unrelated or fabricated details beyond that incorrect count, so it is internally faithful, but still factually wrong relative to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 499, "judge_prompt_tokens": 389, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 115.89264869689941, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (0) exactly and responds directly to the question, so it is both correct and faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 448, "judge_prompt_tokens": 391, "judge_completion_tokens": 57}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 115.89431762695312, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the most popular TV show or movie about ninjas, and the gold answer is 'Naruto'. The generated answer is 'I don't know,' which is incorrect because it fails to provide the correct show. However, the model did not fabricate any incorrect information; it simply admitted not knowing, so it remains fully faithful to its own (claimed) uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 386, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The context provided does not mention a Jurassic Park/Power Ranger crossover movie or its release date. Therefore, the release date for such a movie is not found in the documents provided.", "latency_ms": 1093.8241481781006, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer indicates that the correct response is that the movie does not exist/was not released. The generated answer instead frames it as \"not found in the documents provided,\" which implies the information might exist elsewhere and fails to state that the movie was not released, so it is incorrect. It is partially faithful to the (unstated) context by saying the documents do not mention it, but it does not follow the gold answer's requirement, so the faithfulness to the task/gold is also low.", "main_total_tokens": 2083, "main_prompt_tokens": 2047, "main_completion_tokens": 36, "judge_total_tokens": 556, "judge_prompt_tokens": 421, "judge_completion_tokens": 135}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "The context provided does not mention any movie where Barbie fights the Predator. None of the documents list a film with such a plot.", "latency_ms": 902.7807712554932, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about a fictional crossover movie that does not exist. The gold answer correctly states that the movie was not released. The generated answer, while phrased differently, conveys the same substantive point: there is no such movie mentioned or known. It does not invent a title or contradict the gold, and it stays within the limits of the context, so both correctness and faithfulness are preserved.", "main_total_tokens": 2206, "main_prompt_tokens": 2180, "main_completion_tokens": 26, "judge_total_tokens": 518, "judge_prompt_tokens": 408, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The context does not provide a list of Star Wars movies in chronological order of release. Therefore, it is not possible to determine the 4th oldest Star Wars movie based on the provided documents.", "latency_ms": 821.6073513031006, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the specific 4th oldest Star Wars movie, and a correct answer exists (per the gold label: 'Star Wars: Droids - The Great Heep'). The generated answer claims it is not possible to determine this from the context, which is factually incorrect relative to the task. However, the response is faithful to the (stated) limitation of the provided context, as it does not hallucinate an answer and instead notes missing information. Thus, correctness is 0, while faithfulness to the given context is 1.", "main_total_tokens": 2400, "main_prompt_tokens": 2361, "main_completion_tokens": 39, "judge_total_tokens": 571, "judge_prompt_tokens": 428, "judge_completion_tokens": 143}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The movie \"It\" was released in the year 2017 [Doc 76823].", "latency_ms": 559.4124794006348, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states 1986, while the generated answer incorrectly gives 2017, so it is factually wrong w.r.t. the gold. It also references a document ID and introduces a specific movie interpretation not supported by the gold, so it is not faithful to the provided reference answer.", "main_total_tokens": 1971, "main_prompt_tokens": 1952, "main_completion_tokens": 19, "judge_total_tokens": 487, "judge_prompt_tokens": 396, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 116.02497100830078, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific numerical rating for the movie Forrest Gump, while the generated answer says 'I don't know,' which is incorrect. However, the response is faithful in that it does not invent or fabricate an answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 398, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 593.4529304504395, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med,' which matches the gold answer exactly. It is fully faithful to the user's query and does not introduce any unsupported information beyond the citation-like bracket, which does not alter the factual content.", "main_total_tokens": 1697, "main_prompt_tokens": 1666, "main_completion_tokens": 31, "judge_total_tokens": 504, "judge_prompt_tokens": 423, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In \"Túnel de los huesos\" (2011), seven prisoners escaped from a prison in Argentina [Doc 53582].", "latency_ms": 988.5702133178711, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Argentina as the country where the prison break occurred in 'Túnel de los huesos' (2011), matching the gold answer. It is also faithful, directly using and aligning with the provided document reference without adding unsupported information.", "main_total_tokens": 2084, "main_prompt_tokens": 2056, "main_completion_tokens": 28, "judge_total_tokens": 502, "judge_prompt_tokens": 419, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The context provided does not contain any information about a movie titled \"Sweet Trap\" or details about an apprentice falling into a trap. Therefore, I cannot determine whose trap the apprentice falls into based on the given documents.", "latency_ms": 847.2332954406738, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the apprentice falls into Jiang Jie's trap. The generated answer claims the information is not in the context and refuses to answer, which conflicts with the gold. This makes the answer factually incorrect. However, it accurately reflects the assistant’s stated lack of information about the context, so it is faithful to the (assumed) provided documents.", "main_total_tokens": 2148, "main_prompt_tokens": 2105, "main_completion_tokens": 43, "judge_total_tokens": 526, "judge_prompt_tokens": 427, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 455.62219619750977, "retrieval_recall": 0.0, "retrieval_mrr": 0.25, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.16812753627111746, "hit_rate_at_5": 1.0, "recall_at_10": 0.25, "precision_at_10": 0.1, "ndcg_at_10": 0.16812753627111746, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (4), so it is incorrect. However, the model directly provided a numeric answer to the question without adding unsupported details, so it is faithful to its own (albeit wrong) prediction.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 467, "judge_prompt_tokens": 385, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "35", "latency_ms": 76.01714134216309, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '35' is factually incorrect because there are no known movies featuring both Bruce Lee and Steven Seagal together, matching the gold response. It is also unfaithful to the gold answer, which explicitly states that no such movies can be found.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 479, "judge_prompt_tokens": 395, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of \"Tokyo Mater\" (2008) is 6.3 [Doc 42750].", "latency_ms": 614.342212677002, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3 and does not introduce any unsupported information beyond citing a document. Both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2032, "main_prompt_tokens": 2010, "main_completion_tokens": 22, "judge_total_tokens": 470, "judge_prompt_tokens": 405, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in the movie \"Fast Track: No Limits\" is street racing [Doc 41929].", "latency_ms": 859.738826751709, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question asks specifically for the style of racing, with the gold answer being 'Drift'. The generated answer instead states 'street racing', which is a different style and therefore incorrect. It also fabricates a specific movie title ('Fast Track: No Limits') not mentioned in the prompt and attributes the answer to a document, indicating partial unfaithfulness to the given QA pair context.", "main_total_tokens": 2263, "main_prompt_tokens": 2240, "main_completion_tokens": 23, "judge_total_tokens": 511, "judge_prompt_tokens": 401, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 673.2330322265625, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as \"Confronting a Serial Killer,\" which matches the gold answer exactly. It stays faithful to the provided information without adding unsupported claims, only restating the context from the question.", "main_total_tokens": 2053, "main_prompt_tokens": 2007, "main_completion_tokens": 46, "judge_total_tokens": 528, "judge_prompt_tokens": 452, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of \"Armageddon\" (1998) is 7.838 [Doc 20603].", "latency_ms": 650.1610279083252, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and is directly grounded in the cited document, so it is both correct and faithful.", "main_total_tokens": 2325, "main_prompt_tokens": 2301, "main_completion_tokens": 24, "judge_total_tokens": 471, "judge_prompt_tokens": 409, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 588.3991718292236, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'The Sixth Sense' with no contradictions or hallucinations.", "main_total_tokens": 2104, "main_prompt_tokens": 2075, "main_completion_tokens": 29, "judge_total_tokens": 478, "judge_prompt_tokens": 420, "judge_completion_tokens": 58}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 461.2150192260742, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer, and it does not introduce any extraneous or conflicting information.", "main_total_tokens": 2235, "main_prompt_tokens": 2218, "main_completion_tokens": 17, "judge_total_tokens": 461, "judge_prompt_tokens": 397, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 504.03642654418945, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the title as 'The Transporter Refueled' and stays faithful to the question.", "main_total_tokens": 2291, "main_prompt_tokens": 2262, "main_completion_tokens": 29, "judge_total_tokens": 480, "judge_prompt_tokens": 420, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "\"The Transporter Refueled\" was released in the year 2015 [Doc 67500].", "latency_ms": 554.5973777770996, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It does not introduce any contradictions or extraneous claims, and the citation style does not affect faithfulness. Both correctness and faithfulness are therefore fully satisfied.", "main_total_tokens": 2225, "main_prompt_tokens": 2204, "main_completion_tokens": 21, "judge_total_tokens": 494, "judge_prompt_tokens": 406, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "The context provided does not mention any romantic comedy where Abraham Lincoln comes back from the dead. None of the movie descriptions in the documents reference Abraham Lincoln or a plot involving him returning from the dead.", "latency_ms": 855.9331893920898, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a specific romantic comedy where Abraham Lincoln comes back from the dead. The gold answer indicates that no such movie description can be found. The generated answer correctly states that the provided context does not mention any such romantic comedy and that none of the movie descriptions reference Abraham Lincoln or him returning from the dead. This directly aligns with the gold answer and stays within the given context, so it is both correct and faithful.", "main_total_tokens": 2151, "main_prompt_tokens": 2112, "main_completion_tokens": 39, "judge_total_tokens": 545, "judge_prompt_tokens": 429, "judge_completion_tokens": 116}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The context provided does not mention a movie title that alludes to a manuscript written by Ernest Hemingway.", "latency_ms": 738.715410232544, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Havana Darkness', but the generated answer incorrectly claims the context does not mention such a movie, so it is factually wrong relative to the gold. However, the answer is faithful to the (incorrect) reasoning based on the context it referenced, so it is internally consistent with the given context, warranting a high faithfulness score.", "main_total_tokens": 2085, "main_prompt_tokens": 2065, "main_completion_tokens": 20, "judge_total_tokens": 511, "judge_prompt_tokens": 408, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show \"Doll & Em\" [Doc 122259].", "latency_ms": 760.3766918182373, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em,' which matches the gold answer. It remains faithful to the provided gold information and does not introduce any contradictions, only adding minor context consistent with the gold.", "main_total_tokens": 1951, "main_prompt_tokens": 1922, "main_completion_tokens": 29, "judge_total_tokens": 493, "judge_prompt_tokens": 417, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The 2007 movie in which Hallam has a talent for spying on people and attempts to find the cause of his mother’s death is not mentioned in the provided documents.", "latency_ms": 999.8595714569092, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The correct answer is 'Hallam Foe.' The generated answer incorrectly claims the movie is not mentioned, so it is factually wrong. However, it does accurately reflect that, given only the provided documents (which are presumably limited), the model did not find the answer there, so it remains mostly faithful to its stated evidence constraint.", "main_total_tokens": 2050, "main_prompt_tokens": 2015, "main_completion_tokens": 35, "judge_total_tokens": 534, "judge_prompt_tokens": 434, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The last name \"Stallone\" in the context of [Doc 22549] might initially sound like a foe. This is because the character is described as a \"small-time crook,\" which typically implies a negative or adversarial role, especially in a crime-related plot. However, the context of the movie suggests that Stallone is actually a key witness, which could make him an ally in the pursuit of the serial killer. Nonetheless, the initial impression based on the description of being a crook leans towards foe.", "latency_ms": 1457.1011066436768, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a simple categorical answer (friend or foe) and the gold answer is just 'A foe'. The generated answer provides a long explanation involving 'Stallone', crook, witness, and serial killer, which introduces movie-specific context not present in the gold. It does not clearly and succinctly answer in the same categorical form, and it fabricates additional details (Doc 22549, crime plot, serial killer) that are unsupported by the given gold. Therefore it is neither correct in form nor faithful to the reference answer or context.", "main_total_tokens": 1880, "main_prompt_tokens": 1773, "main_completion_tokens": 107, "judge_total_tokens": 636, "judge_prompt_tokens": 493, "judge_completion_tokens": 143}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "In the 2017 TV show \"Atypical,\" Sam is 18 years old [Doc 124583].", "latency_ms": 585.9007835388184, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, matching the gold answer exactly. It does not introduce any contradictions or extra speculative information, so it is both correct and faithful.", "main_total_tokens": 2190, "main_prompt_tokens": 2166, "main_completion_tokens": 24, "judge_total_tokens": 490, "judge_prompt_tokens": 411, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show \"Atypical,\" Sam is on the autism spectrum [Doc 124583].", "latency_ms": 663.9719009399414, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the 2017 TV show 'Atypical,' Sam is on the autism spectrum, which matches the gold answer exactly and remains faithful to the provided information.", "main_total_tokens": 2130, "main_prompt_tokens": 2106, "main_completion_tokens": 24, "judge_total_tokens": 481, "judge_prompt_tokens": 411, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Sandra was born in Chamonix in the movie \"À corde tendue\" [Doc 59659].", "latency_ms": 672.7852821350098, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, matching the gold answer exactly. It is fully consistent with the provided information and does not introduce unsupported details.", "main_total_tokens": 2436, "main_prompt_tokens": 2414, "main_completion_tokens": 22, "judge_total_tokens": 485, "judge_prompt_tokens": 419, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In the Japanese language movie \"When the Rain Falls,\" Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 702.4505138397217, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly fills in the blank with 'rains,' matching the gold answer. However, it hallucinates an apparent movie title ('When the Rain Falls') and a document reference, which are not grounded in the provided information. This slightly reduces faithfulness despite the correct completion.", "main_total_tokens": 2850, "main_prompt_tokens": 2824, "main_completion_tokens": 26, "judge_total_tokens": 508, "judge_prompt_tokens": 417, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the animated series \"Tabaluga,\" Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 1032.963752746582, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Greenland as the place Tabaluga is crown-prince of, matching the gold answer exactly. It stays fully consistent with the provided information and does not introduce any contradictions or hallucinations.", "main_total_tokens": 2395, "main_prompt_tokens": 2364, "main_completion_tokens": 31, "judge_total_tokens": 498, "judge_prompt_tokens": 425, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers\" is \"One Afternoon\" [Doc 96838].", "latency_ms": 776.1881351470947, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'One Afternoon,' which matches the provided gold answer. The answer is directly grounded in the given description and citation, with no unsupported additions.", "main_total_tokens": 1792, "main_prompt_tokens": 1748, "main_completion_tokens": 44, "judge_total_tokens": 515, "judge_prompt_tokens": 448, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie where a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is not mentioned in the provided documents.", "latency_ms": 723.0587005615234, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "recall_at_10": 0.0, "precision_at_5": 0.0, "precision_at_10": 0.0, "ndcg_at_5": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_5": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the title of a specific movie, and the correct answer is 'The Life Ahead'. The generated answer incorrectly claims the movie is not mentioned, failing to provide the correct title, so it is factually incorrect (correctness 0). However, if we assume the model was constrained to the provided documents and the movie truly was not in them, then its statement about the documents is internally consistent and not hallucinated, giving full faithfulness (1).", "main_total_tokens": 2212, "main_prompt_tokens": 2180, "main_completion_tokens": 32, "judge_total_tokens": 555, "judge_prompt_tokens": 430, "judge_completion_tokens": 125}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about Bruce McLaren, a New Zealander who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 1210.3471755981445, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and adds accurate contextual information about him being a New Zealander who founded the McLaren motor racing team. This is consistent with the gold answer and does not contradict it, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2270, "main_prompt_tokens": 2236, "main_completion_tokens": 34, "judge_total_tokens": 511, "judge_prompt_tokens": 418, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The name of his racing team is not provided in the given documents.", "latency_ms": 939.9206638336182, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the team as 'The McLaren Motor Racing team,' while the generated answer claims the name is not provided. This is factually incorrect given the gold reference, so correctness is 0. However, assuming the model only had access to documents that indeed lacked the team name, its statement is internally consistent with that premise, so faithfulness is 1.", "main_total_tokens": 1911, "main_prompt_tokens": 1897, "main_completion_tokens": 14, "judge_total_tokens": 503, "judge_prompt_tokens": 397, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context provided does not specify the country he is from.", "latency_ms": 945.3713893890381, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states the person is from New Zealand, but the generated answer claims the country is not specified. This is incorrect relative to the gold label, but it is faithful to its own (implied) context where the country was not given.", "main_total_tokens": 1812, "main_prompt_tokens": 1800, "main_completion_tokens": 12, "judge_total_tokens": 469, "judge_prompt_tokens": 388, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "The context provided does not mention a 2013 movie in which John McClane blows up the Great Wall of China. None of the listed movies feature this plot or character. [Doc 59033] [Doc 58877] [Doc 61641] [Doc 72056] [Doc 59142] [Doc 60068] [Doc 62231] [Doc 59431] [Doc 62869] [Doc 61554] [Doc 59572] [Doc 62500] [Doc 62035] [Doc 61560] [Doc 62034].", "latency_ms": 1229.0534973144531, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.5, "faithfulness_score": 0.5, "justification": "The gold answer states that there is no such movie in the database, which directly and clearly answers the question given the system's knowledge. The generated answer instead talks about the provided context and listed documents, which are irrelevant to the user's question and not mentioned in the gold answer. It fails to clearly state that no such movie exists in the database, and it fabricates references to documents. Thus it is only partially correct and only partially faithful to the gold answer.", "main_total_tokens": 2004, "main_prompt_tokens": 1878, "main_completion_tokens": 126, "judge_total_tokens": 651, "judge_prompt_tokens": 524, "judge_completion_tokens": 127}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with the description \"When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental\" is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 740.6058311462402, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as \"Tomie: Forbidden Fruit.\" It does not add unsupported information and remains faithful to the provided description and gold label.", "main_total_tokens": 2210, "main_prompt_tokens": 2152, "main_completion_tokens": 58, "judge_total_tokens": 544, "judge_prompt_tokens": 475, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years in the 2021 movie \"Lei mi parla ancora\" [Doc 98566].", "latency_ms": 802.3321628570557, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (65 years) exactly and is fully supported by the provided information.", "main_total_tokens": 2435, "main_prompt_tokens": 2405, "main_completion_tokens": 30, "judge_total_tokens": 476, "judge_prompt_tokens": 424, "judge_completion_tokens": 52}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The context provided does not contain information about \"Scorpion King: Book of Souls\" or the specific quest of The Scorpion King and Tala. Therefore, the answer to your question is not found in the documents provided.", "latency_ms": 903.8031101226807, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'The Book of Souls'. The generated answer claims the information is not in the context and does not provide the correct movie plot detail, so it is factually incorrect. However, given the instruction that the model should answer only from provided documents, its statement about the context is internally consistent and not hallucinated, so it is faithful to the (unstated) context assumption.", "main_total_tokens": 1948, "main_prompt_tokens": 1904, "main_completion_tokens": 44, "judge_total_tokens": 548, "judge_prompt_tokens": 438, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Zombies on the Moon\" is not provided in the given documents.", "latency_ms": 623.4123706817627, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer state that the release year is not available, which is correct given the information. The model does not hallucinate a year and stays faithful to the provided context that lacks this data.", "main_total_tokens": 2263, "main_prompt_tokens": 2244, "main_completion_tokens": 19, "judge_total_tokens": 485, "judge_prompt_tokens": 410, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Galactic Baker\" is not found in the provided documents.", "latency_ms": 675.2440929412842, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is not in the database, effectively saying the release year is unknown. The generated answer states that the release year is not found in the provided documents, which is equivalent in this context. It does not invent a year and stays within the information scope, so it is both correct with respect to the task and faithful to the given evidence.", "main_total_tokens": 2486, "main_prompt_tokens": 2469, "main_completion_tokens": 17, "judge_total_tokens": 510, "judge_prompt_tokens": 406, "judge_completion_tokens": 104}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Return of the Null Pointer\" is not found in the provided documents.", "latency_ms": 687.7169609069824, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available/found. This is correct because no information is provided about the film, and the generated answer accurately reflects that the documents do not contain the release year. It is therefore both correct and faithful to the given information.", "main_total_tokens": 2069, "main_prompt_tokens": 2050, "main_completion_tokens": 19, "judge_total_tokens": 498, "judge_prompt_tokens": 410, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Data Science: The Movie\" is not provided in the given documents.", "latency_ms": 526.9865989685059, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers indicate that the release year is unknown or not available from the given information. The generated answer is correct relative to the prompt and faithful to the (implied) context, stating only that the data is not provided, without fabricating a year.", "main_total_tokens": 2022, "main_prompt_tokens": 2003, "main_completion_tokens": 19, "judge_total_tokens": 496, "judge_prompt_tokens": 410, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Python's Revenge 2\" is not provided in the given documents.", "latency_ms": 737.0061874389648, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the release year is not available; the generated answer attributes this to the given documents, which is faithful to the prompt context.", "main_total_tokens": 2073, "main_prompt_tokens": 2054, "main_completion_tokens": 19, "judge_total_tokens": 475, "judge_prompt_tokens": 410, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Last Pixel\" is not found in the provided documents.", "latency_ms": 438.671350479126, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers indicate that the release year is not available/known. The generated answer is correct and faithful to the information provided (no release year in the documents).", "main_total_tokens": 1789, "main_prompt_tokens": 1772, "main_completion_tokens": 17, "judge_total_tokens": 473, "judge_prompt_tokens": 406, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year of release for \"The Man Who Ate the Sun\" is not found in the provided documents.", "latency_ms": 510.7128620147705, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database, and the generated answer states the release year is not found in the provided documents. Both convey the same outcome (information not available) and do not invent a year, so the response is correct and faithful.", "main_total_tokens": 1977, "main_prompt_tokens": 1956, "main_completion_tokens": 21, "judge_total_tokens": 497, "judge_prompt_tokens": 413, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Days of Lehman Brothers\" is not mentioned in the provided documents.", "latency_ms": 554.9192428588867, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director is not provided / not mentioned in the available information. This is correct relative to the database context and faithful to the constraint of using only the provided documents.", "main_total_tokens": 2164, "main_prompt_tokens": 2142, "main_completion_tokens": 22, "judge_total_tokens": 491, "judge_prompt_tokens": 419, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not contain information about a movie titled \"A Love to Die For\" or its director.", "latency_ms": 939.9960041046143, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the context does not contain information about a movie titled 'A Love to Die For' or its director, aligning with the gold answer that the director information is not provided in the database. It does not introduce unsupported facts and remains faithful to the given information.", "main_total_tokens": 2278, "main_prompt_tokens": 2256, "main_completion_tokens": 22, "judge_total_tokens": 505, "judge_prompt_tokens": 417, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not mentioned in the provided documents.", "latency_ms": 555.0336837768555, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned and aligns with the gold answer that the database does not provide this information. It does not add unsupported details or contradict the given data, so it is both correct and faithful.", "main_total_tokens": 2415, "main_prompt_tokens": 2397, "main_completion_tokens": 18, "judge_total_tokens": 488, "judge_prompt_tokens": 411, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Night at Tremore Beach\" is not mentioned in the provided documents. [Doc 135098]", "latency_ms": 571.035623550415, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer and the generated answer are semantically equivalent: both state that the director information is not available in the provided sources/database. The generated response correctly indicates that the director is not mentioned in the documents and does not introduce any unsupported facts. Therefore, it is both correct relative to the gold and faithful to the (described) context.", "main_total_tokens": 1814, "main_prompt_tokens": 1786, "main_completion_tokens": 28, "judge_total_tokens": 525, "judge_prompt_tokens": 425, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 584.7716331481934, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy' with no additions or contradictions. It is both correct and faithful to the provided information.", "main_total_tokens": 1924, "main_prompt_tokens": 1897, "main_completion_tokens": 27, "judge_total_tokens": 487, "judge_prompt_tokens": 418, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse is \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 624.0732669830322, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as \"Stranger in My Bed.\" It is correct and faithful to the given information.", "main_total_tokens": 1957, "main_prompt_tokens": 1928, "main_completion_tokens": 29, "judge_total_tokens": 486, "judge_prompt_tokens": 425, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The scenario described, where a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years, occurs in the TV show \"Game of Silence\" [Doc 123938].", "latency_ms": 781.2201976776123, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a movie, but both the gold answer and the generated answer identify 'Game of Silence,' which is a TV show, not a movie. The generated answer therefore does not correctly match the requested type of media, so correctness is 0. However, it is faithful to the provided document reference and to the gold answer, so faithfulness is 1.", "main_total_tokens": 1683, "main_prompt_tokens": 1644, "main_completion_tokens": 39, "judge_total_tokens": 542, "judge_prompt_tokens": 436, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie in which a killer from the site Craigslist is featured is \"The Craigslist Killer\" [Doc 53039].", "latency_ms": 664.6318435668945, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Craigslist Killer,\" which matches the gold answer exactly. It does not introduce any contradictory or extraneous information beyond referencing a document, so it is fully faithful to the given QA pair.", "main_total_tokens": 1959, "main_prompt_tokens": 1935, "main_completion_tokens": 24, "judge_total_tokens": 493, "judge_prompt_tokens": 416, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which Yuki Hirano sets his eyes on the role of a forestry trainee after failing his university entrance exam is \"Wood Job!\" [Doc 66094].", "latency_ms": 578.3214569091797, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", matching the gold answer exactly. It stays faithful to the question, only restating and slightly elaborating the given plot premise without adding unsupported details.", "main_total_tokens": 2830, "main_prompt_tokens": 2795, "main_completion_tokens": 35, "judge_total_tokens": 508, "judge_prompt_tokens": 435, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure is \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 606.2934398651123, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase' and does not introduce any contradictions or extraneous claims, so it is both correct and faithful.", "main_total_tokens": 2358, "main_prompt_tokens": 2309, "main_completion_tokens": 49, "judge_total_tokens": 533, "judge_prompt_tokens": 459, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a 7-year-old boy must find the hero within him to save his parents trapped inside a burning car due to a rock fall is \"Beyond\" [Doc 74597].", "latency_ms": 873.1775283813477, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Beyond,\" which matches the gold answer. It restates the plot description consistent with the prompt and does not introduce contradictions or unsupported claims, so it is both correct and faithful.", "main_total_tokens": 1877, "main_prompt_tokens": 1837, "main_completion_tokens": 40, "judge_total_tokens": 523, "judge_prompt_tokens": 448, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman, fired from a financial corporation during the Asia crisis, returns home with no money is \"6ixtynin9\" [Doc 22478].", "latency_ms": 533.5428714752197, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as '6ixtynin9'. It restates the question context correctly and does not introduce any unsupported information, so it is both correct and faithful.", "main_total_tokens": 1883, "main_prompt_tokens": 1846, "main_completion_tokens": 37, "judge_total_tokens": 514, "judge_prompt_tokens": 439, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie in which a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort, and it's up to the club's staff to stop the violence or at least hide it, is \"Club Dread\" [Doc 32410].", "latency_ms": 740.4711246490479, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Club Dread,\" which matches the gold answer exactly. It stays faithful to the provided information without adding unsupported details, only lightly rephrasing the question prompt.", "main_total_tokens": 2210, "main_prompt_tokens": 2159, "main_completion_tokens": 51, "judge_total_tokens": 547, "judge_prompt_tokens": 475, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is \"Toilet\" [Doc 48873].", "latency_ms": 741.8346405029297, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" matching the gold answer exactly. It is fully supported by the provided information, with no hallucinations or contradictions, so both correctness and faithfulness are maximal.", "main_total_tokens": 2411, "main_prompt_tokens": 2360, "main_completion_tokens": 51, "judge_total_tokens": 539, "judge_prompt_tokens": 465, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century is \"Electric Angel\" [Doc 1024].", "latency_ms": 981.2655448913574, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Electric Angel,\" which matches the gold label exactly. It remains faithful to the information given in the prompt and does not introduce contradictions or unsupported claims, aside from a harmless citation marker that does not affect correctness or faithfulness.", "main_total_tokens": 2240, "main_prompt_tokens": 2195, "main_completion_tokens": 45, "judge_total_tokens": 545, "judge_prompt_tokens": 461, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023, based on the provided context, is \"Fall\" with a rating of 4.0 [Doc 106810].", "latency_ms": 956.7744731903076, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'Mega Ape' as the lowest rated movie from 2023, while the generated answer claims it is 'Fall' with a rating of 4.0. This directly contradicts the gold answer, so it is factually incorrect in this evaluation context. It is also unfaithful to the provided gold information, hence both correctness and faithfulness are scored 0.", "main_total_tokens": 2132, "main_prompt_tokens": 2099, "main_completion_tokens": 33, "judge_total_tokens": 524, "judge_prompt_tokens": 414, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 1306.0250282287598, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the highest rated TV show from 2010, and the gold answer is 'Tea House'. The generated answer says 'I don't know,' which does not match the gold answer and is therefore incorrect. However, the response is faithful in that it does not hallucinate an incorrect show; it honestly states a lack of knowledge, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 386, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 565.3245449066162, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which is incorrect because the gold answer specifies 'The Uncut Story of Mewtwo's Origin' as the correct response. However, the model does not hallucinate an incorrect movie and honestly states lack of knowledge, so it is fully faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 488, "judge_prompt_tokens": 395, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The language of the lowest-rated movie from 1980 is English. The movie is \"Cataclysm,\" which has a rating of 3.889 [Doc 744].", "latency_ms": 760.0038051605225, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the language of the lowest-rated movie from 1980 is English, matching the gold answer. The additional detail about the movie title and rating does not contradict the query and is plausibly drawn from the referenced document, so it is faithful to the described source.", "main_total_tokens": 2332, "main_prompt_tokens": 2295, "main_completion_tokens": 37, "judge_total_tokens": 509, "judge_prompt_tokens": 420, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity, according to the provided documents, was \"Popular\" with a popularity score of 4.5113 [Doc 115196].", "latency_ms": 970.0160026550293, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer does not match the gold answer \"Law & Order: Special Victims Unit,\" so it is incorrect. It also appears to rely on a specific document score and contradicts the gold label, so with respect to the gold-standard truth it is not faithful.", "main_total_tokens": 2318, "main_prompt_tokens": 2280, "main_completion_tokens": 38, "judge_total_tokens": 513, "judge_prompt_tokens": 428, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 1044.2273616790771, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 and does not introduce conflicting or extraneous information, so it is both correct and faithful.", "main_total_tokens": 2213, "main_prompt_tokens": 2195, "main_completion_tokens": 18, "judge_total_tokens": 462, "judge_prompt_tokens": 400, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in the year 1999 [Doc 22481].", "latency_ms": 997.7364540100098, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states the film was released in 1999, while the gold label says 2019, so it is factually incorrect relative to the provided gold. However, the answer is faithful to the cited document [Doc 22481], so faithfulness is high even though correctness to the task gold is low.", "main_total_tokens": 2156, "main_prompt_tokens": 2134, "main_completion_tokens": 22, "judge_total_tokens": 502, "judge_prompt_tokens": 406, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "1", "latency_ms": 76.51424407958984, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 279 votes, but the generated answer is 1, which is incorrect. The model’s response is internally consistent with its own statement, so it is faithful but not correct relative to the gold answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 454, "judge_prompt_tokens": 383, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "7", "latency_ms": 75.68764686584473, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (7) does not match the gold answer (12), so it is incorrect. It is, however, a direct answer to the question without adding unsupported details, so it remains faithful to the user query format.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 460, "judge_prompt_tokens": 383, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The year of release for the movie \"The Last Days of Lehman Brothers\" is not provided in the given documents.", "latency_ms": 928.7400245666504, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer shows the movie was released in 2009, but the generated answer incorrectly claims the release year is not provided, making it factually wrong. However, it is faithful to its own (incorrect) reasoning based on the premise that it only uses the provided documents, so faithfulness is high.", "main_total_tokens": 2043, "main_prompt_tokens": 2019, "main_completion_tokens": 24, "judge_total_tokens": 504, "judge_prompt_tokens": 411, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_bge-m3_gpt-4o", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 673.5739707946777, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the language is English ('en'), and the generated answer states that the original language of 'The King's Speech' is English. This matches the gold and does not introduce extraneous or incorrect information, so it is both correct and faithful.", "main_total_tokens": 2189, "main_prompt_tokens": 2170, "main_completion_tokens": 19, "judge_total_tokens": 484, "judge_prompt_tokens": 402, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "I don't know.", "latency_ms": 775.5870819091797, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model response is factually incorrect because the film 'Wall Street' was released in 1987, but the answer given was 'I don't know.' However, the answer is faithful to the model's stated uncertainty and does not fabricate information.", "main_total_tokens": 546, "main_prompt_tokens": 545, "main_completion_tokens": 1, "judge_total_tokens": 462, "judge_prompt_tokens": 382, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Tokyo Gore School", "latency_ms": 863.0032539367676, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer 'Tokyo Gore School' is incorrect; the correct movie is 'Brain Fluid Explosion Girl'. The explanation is faithful to the provided gold answer.", "main_total_tokens": 2483, "main_prompt_tokens": 2479, "main_completion_tokens": 4, "judge_total_tokens": 453, "judge_prompt_tokens": 391, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 722.5697040557861, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show where six friends rely on a Dungeon Master to get home as 'Dungeons & Dragons.'", "main_total_tokens": 2178, "main_prompt_tokens": 2173, "main_completion_tokens": 5, "judge_total_tokens": 457, "judge_prompt_tokens": 394, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "I don't know.", "latency_ms": 300.2595901489258, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the correct movie title 'Nacht der Wölfe'. The generated answer says 'I don't know,' which is incorrect. However, the generated answer does not introduce any fabricated or incorrect information about the movie, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 545, "main_prompt_tokens": 544, "main_completion_tokens": 1, "judge_total_tokens": 490, "judge_prompt_tokens": 402, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900", "latency_ms": 1153.3567905426025, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (1900), so it is both correct and faithful.", "main_total_tokens": 2493, "main_prompt_tokens": 2490, "main_completion_tokens": 3, "judge_total_tokens": 439, "judge_prompt_tokens": 388, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "Playing Mona Lisa", "latency_ms": 416.6746139526367, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is the movie 'June', matching the gold label. The generated answer 'Playing Mona Lisa' is a different film and does not match the described plot or the gold answer, so it is incorrect and unfaithful.", "main_total_tokens": 159, "main_prompt_tokens": 156, "main_completion_tokens": 3, "judge_total_tokens": 513, "judge_prompt_tokens": 435, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 153.3658504486084, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific movie title. The gold answer provides that title, but the generated answer says 'I don't know,' which is incorrect. However, the generated answer does not fabricate information and honestly reflects uncertainty, so it is fully faithful to the model's stated knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 486, "judge_prompt_tokens": 399, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 170.58444023132324, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the rating as 7.6, while the generated answer says 'I don't know,' which does not provide the correct value. Therefore, the correctness is 0. However, the generated answer does not introduce any incorrect fabricated facts; it simply admits lack of knowledge, so it is fully faithful to the model's uncertainty, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 498, "judge_prompt_tokens": 389, "judge_completion_tokens": 109}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "11", "latency_ms": 1057.5895309448242, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3535124404757273, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (11) does not match the gold answer (4), so it is factually incorrect for this question. However, there is no indication it contradicts or ignores any prior reasoning steps or given constraints beyond giving the wrong count, so it is fully faithful to the (incorrect) computation it presents.", "main_total_tokens": 1463, "main_prompt_tokens": 1442, "main_completion_tokens": 21, "judge_total_tokens": 488, "judge_prompt_tokens": 394, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 153.68175506591797, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer says there are none, so it is factually incorrect. However, the model does not fabricate a number; it honestly states it cannot find any, so it remains faithful to its own (albeit limited) retrieval.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 490, "judge_prompt_tokens": 404, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 197.0536708831787, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the correct answer is 'Fantômas', while the model responded 'I don't know.' However, the model did not hallucinate an incorrect specific show and admitted uncertainty, so it remains fully faithful to its own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 471, "judge_prompt_tokens": 389, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise", "latency_ms": 972.3019599914551, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the movie as 'Kill Cruise.'", "main_total_tokens": 2444, "main_prompt_tokens": 2441, "main_completion_tokens": 3, "judge_total_tokens": 436, "judge_prompt_tokens": 387, "judge_completion_tokens": 49}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 632.7085494995117, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie title. It is fully correct and faithful to the provided information.", "main_total_tokens": 2706, "main_prompt_tokens": 2698, "main_completion_tokens": 8, "judge_total_tokens": 465, "judge_prompt_tokens": 408, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018 [Doc 84838].", "latency_ms": 1790.2660369873047, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer gives the correct year, 2018, matching the gold answer. However, it appends an extra bracketed reference \"[Doc 84838]\" that is not part of the expected answer. This slightly reduces faithfulness to the target style, but factual correctness is fully satisfied.", "main_total_tokens": 2866, "main_prompt_tokens": 2844, "main_completion_tokens": 22, "judge_total_tokens": 480, "judge_prompt_tokens": 386, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "I don't know.", "latency_ms": 379.06646728515625, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Astro Boy'. The generated answer says 'I don't know', which is incorrect but not hallucinated, so correctness is 0 and faithfulness is 1.", "main_total_tokens": 565, "main_prompt_tokens": 564, "main_completion_tokens": 1, "judge_total_tokens": 460, "judge_prompt_tokens": 391, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 years [Doc 67513].", "latency_ms": 1099.8437404632568, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer matches the gold answer and is factually correct. However, it references an external document identifier ([Doc 67513]), which is not grounded in the provided context and thus reduces faithfulness to the given information.", "main_total_tokens": 2185, "main_prompt_tokens": 2176, "main_completion_tokens": 9, "judge_total_tokens": 479, "judge_prompt_tokens": 403, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "26", "latency_ms": 360.88085174560547, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The model removed the currency symbol and unit, returning just '26' instead of '£26m'. This is numerically and semantically incorrect, and it does not faithfully preserve the original answer format or content.", "main_total_tokens": 2070, "main_prompt_tokens": 2069, "main_completion_tokens": 1, "judge_total_tokens": 471, "judge_prompt_tokens": 397, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 459.32507514953613, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric value (the popularity score) and the gold answer is 80.5772. The generated answer says only 'I don't know,' which does not provide the required value and is therefore incorrect. However, it does not invent or misrepresent any facts; it simply expresses a lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 504, "judge_prompt_tokens": 396, "judge_completion_tokens": 108}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 161.0255241394043, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The gold answer indicates there was 1 Russian language TV show released in 1997, while the generated answer claims there were none, so it is factually incorrect (score 0). The model does, however, refrain from hallucinating a specific incorrect number or title and instead states an inability to find matches, which is relatively faithful to a lack of information rather than inventing data, though it still contradicts the gold, so partial faithfulness credit (0.8).", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 523, "judge_prompt_tokens": 394, "judge_completion_tokens": 129}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes", "latency_ms": 812.8445148468018, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Angel Eyes' exactly matches the gold answer and correctly identifies the TV show where Soo-wan undergoes eye transplant surgery that restores her sight.", "main_total_tokens": 3078, "main_prompt_tokens": 3075, "main_completion_tokens": 3, "judge_total_tokens": 460, "judge_prompt_tokens": 397, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "en", "latency_ms": 1472.9576110839844, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer ('en') is incorrect because the gold label indicates the movie language is Korean. The answer directly contradicts the gold label, so it is neither correct nor faithful.", "main_total_tokens": 2916, "main_prompt_tokens": 2904, "main_completion_tokens": 12, "judge_total_tokens": 444, "judge_prompt_tokens": 377, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 483.2925796508789, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because the gold label indicates there are 2 such movies, while the answer claims there are none. However, the model does not fabricate specific movie data; it simply states inability to find matches, which is consistent with its stated limitations, so it is faithful to its own reasoning process.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 498, "judge_prompt_tokens": 403, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 384.8867416381836, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states explicitly that there are no movies released in 1979 in the database, while the model answer simply says 'I don't know.' This is not equivalent: the model fails to convey the specific database limitation and thus does not correctly answer the question as defined by the gold. It is also not faithful to the gold answer's content or reasoning.", "main_total_tokens": 545, "main_prompt_tokens": 544, "main_completion_tokens": 1, "judge_total_tokens": 502, "judge_prompt_tokens": 399, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "I don't know.", "latency_ms": 343.45269203186035, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer convey that the model does not have information about a movie titled 'Back to the Future Part IV,' which is correct because such a film has not been released. The generated answer is concise but still faithful to the underlying knowledge and does not hallucinate a release year.", "main_total_tokens": 587, "main_prompt_tokens": 586, "main_completion_tokens": 1, "judge_total_tokens": 496, "judge_prompt_tokens": 404, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no movies from 1980 about the Chernobyl disaster could be found. The generated answer similarly states that it cannot find any movies or shows matching the criteria and hints that the event timing might be incorrect, which is consistent with the fact that the Chernobyl disaster occurred in 1986. Therefore, the generated answer is both correct and faithful to the gold answer, differing only slightly in phrasing but not in substance.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 556, "judge_prompt_tokens": 434, "judge_completion_tokens": 122}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 112.39004135131836, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 titles, but the generated answer claims there are none, which is factually incorrect. The evaluation of the generated response against the gold is done correctly and without fabrication, so faithfulness to the comparison task is maintained, but the generated answer itself is not correct relative to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 397, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 478.1146049499512, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a known gold answer of 2, but the model responded that it could not find any movies or shows matching the criteria. This directly contradicts the gold answer, so correctness is 0. However, given the model's training cutoff and lack of access to future-specific or niche data, stating inability to find such movies is consistent with its knowledge limitations and does not fabricate information, so faithfulness is 1.", "main_total_tokens": 3295, "main_prompt_tokens": 3294, "main_completion_tokens": 1, "judge_total_tokens": 512, "judge_prompt_tokens": 395, "judge_completion_tokens": 117}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 111.70363426208496, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The correct answer is 1, but the generated answer says there are none, so it is factually incorrect. It is mostly faithful to its own (limited) reasoning, but it fails to reflect the known gold answer, so correctness is 0; faithfulness is slightly reduced for not acknowledging uncertainty or possible oversight.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 495, "judge_prompt_tokens": 398, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 112.02597618103027, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0 TV shows, and the generated answer effectively states that there are no movies or shows matching the criteria, which is consistent with the gold. It does not introduce unsupported details and remains within the information given.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 477, "judge_prompt_tokens": 400, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 100.20589828491211, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto', but the generated answer says 'I don't know,' which is incorrect with respect to the reference. It is, however, internally faithful because it does not hallucinate an alternative answer; it simply expresses lack of knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 468, "judge_prompt_tokens": 386, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 771.5187072753906, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the movie does not exist (thus no release date), whereas the generated answer says 'I don't know.' This is faithful to the lack of information in the prompt but is not factually correct given the gold, so correctness is 0 and faithfulness is 1.", "main_total_tokens": 536, "main_prompt_tokens": 535, "main_completion_tokens": 1, "judge_total_tokens": 479, "judge_prompt_tokens": 389, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 301.1467456817627, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the movie was not released, which is a specific fact. The generated answer says 'I don't know,' which does not assert that fact and is therefore not correct relative to the labeled answer. It is also unfaithful to the gold because it fails to convey the key information that no such released movie exists.", "main_total_tokens": 538, "main_prompt_tokens": 537, "main_completion_tokens": 1, "judge_total_tokens": 484, "judge_prompt_tokens": 386, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "I don't know.", "latency_ms": 452.87394523620605, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not identify the 4th oldest Star Wars movie and thus does not match the correct gold answer 'Star Wars: Droids - The Great Heep,' so it is incorrect. However, it does not introduce any fabricated information or contradictions; it simply admits lack of knowledge, so it is fully faithful to the model's own (claimed) uncertainty.", "main_total_tokens": 554, "main_prompt_tokens": 553, "main_completion_tokens": 1, "judge_total_tokens": 505, "judge_prompt_tokens": 393, "judge_completion_tokens": 112}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "I don't know.", "latency_ms": 787.480354309082, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1986. The model answered 'I don't know,' which is factually incorrect, so correctness is 0. However, it did not hallucinate a wrong year and instead admitted not knowing, so it is fully faithful to its own uncertainty, giving faithfulness a score of 1.", "main_total_tokens": 669, "main_prompt_tokens": 655, "main_completion_tokens": 14, "judge_total_tokens": 475, "judge_prompt_tokens": 381, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 115.92650413513184, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie rating; the gold answer is 8.466. The generated answer says 'I don't know,' which is incorrect relative to the gold label but is a faithful admission of lack of knowledge rather than hallucinating a value. Therefore correctness is 0, but faithfulness to the model's knowledge state is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 498, "judge_prompt_tokens": 398, "judge_completion_tokens": 100}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med", "latency_ms": 702.5361061096191, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Mighty Med', which is the correct name of the TV show described. It is both correct and fully faithful to the reference.", "main_total_tokens": 2200, "main_prompt_tokens": 2196, "main_completion_tokens": 4, "judge_total_tokens": 460, "judge_prompt_tokens": 394, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina [Doc 53582].", "latency_ms": 1265.8183574676514, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The answer correctly identifies Argentina as the country, matching the gold answer. The response is fully supported by the provided information and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 2640, "main_prompt_tokens": 2632, "main_completion_tokens": 8, "judge_total_tokens": 462, "judge_prompt_tokens": 398, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "I don't know.", "latency_ms": 316.73169136047363, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states the trap belongs to Jiang Jie, while the generated answer says 'I don't know,' which is incorrect. However, the model does not hallucinate; it honestly admits not knowing, so it is fully faithful to its own uncertainty.", "main_total_tokens": 569, "main_prompt_tokens": 568, "main_completion_tokens": 1, "judge_total_tokens": 469, "judge_prompt_tokens": 388, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "2", "latency_ms": 680.7265281677246, "retrieval_recall": 0.0, "retrieval_mrr": 0.25, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.16812753627111746, "hit_rate_at_5": 1.0, "recall_at_10": 0.25, "precision_at_10": 0.1, "ndcg_at_10": 0.16812753627111746, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (2) does not match the gold answer (4), so it is incorrect. The model appears to have given a specific numeric answer, indicating it followed the user’s request format, so it is faithful to the instruction but factually wrong with respect to the gold.", "main_total_tokens": 3389, "main_prompt_tokens": 3386, "main_completion_tokens": 3, "judge_total_tokens": 473, "judge_prompt_tokens": 385, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 370.3274726867676, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that no movies or shows match the criteria, which is consistent with the gold answer that there are no movies with both Bruce Lee and Steven Seagal. It is faithful to the user query and does not introduce contradictory or unsupported information.", "main_total_tokens": 2231, "main_prompt_tokens": 2230, "main_completion_tokens": 1, "judge_total_tokens": 488, "judge_prompt_tokens": 406, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3 [Doc 42750].", "latency_ms": 1050.443172454834, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (6.3) and does not introduce any unsupported information, so it is both correct and faithful.", "main_total_tokens": 2032, "main_prompt_tokens": 2023, "main_completion_tokens": 9, "judge_total_tokens": 453, "judge_prompt_tokens": 393, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "Drift-style racing is featured in the movie \"Tokyo Mater\" [Doc 42750].", "latency_ms": 2540.147542953491, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The question only asks for the style of racing, and the gold answer is simply 'Drift'. The generated answer correctly identifies the style as drift-style racing, so it is correct. However, it introduces an unsupported citation to 'Tokyo Mater' and a document reference that are not grounded in the provided gold answer or any given context, making it unfaithful to the source.", "main_total_tokens": 2612, "main_prompt_tokens": 2580, "main_completion_tokens": 32, "judge_total_tokens": 503, "judge_prompt_tokens": 396, "judge_completion_tokens": 107}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer", "latency_ms": 1154.2737483978271, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie title as 'Confronting a Serial Killer.'", "main_total_tokens": 2615, "main_prompt_tokens": 2608, "main_completion_tokens": 7, "judge_total_tokens": 466, "judge_prompt_tokens": 412, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838 [Doc 20603]", "latency_ms": 1022.850751876831, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838), and there is no addition or contradiction relative to the given information.", "main_total_tokens": 2323, "main_prompt_tokens": 2314, "main_completion_tokens": 9, "judge_total_tokens": 453, "judge_prompt_tokens": 395, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense", "latency_ms": 939.495325088501, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 1999 psychological thriller as 'The Sixth Sense' and staying fully faithful to the question.", "main_total_tokens": 2643, "main_prompt_tokens": 2639, "main_completion_tokens": 4, "judge_total_tokens": 456, "judge_prompt_tokens": 394, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002", "latency_ms": 1279.5171737670898, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the release year as 2002.", "main_total_tokens": 2782, "main_prompt_tokens": 2779, "main_completion_tokens": 3, "judge_total_tokens": 433, "judge_prompt_tokens": 383, "judge_completion_tokens": 50}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled", "latency_ms": 1289.3333435058594, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the title of the 2015 reboot of 'The Transporter' as 'The Transporter Refueled.'", "main_total_tokens": 2852, "main_prompt_tokens": 2845, "main_completion_tokens": 7, "judge_total_tokens": 462, "judge_prompt_tokens": 397, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015", "latency_ms": 897.5553512573242, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2015), correctly identifying the release year of 'The Transporter Refueled' and is fully supported by the provided information.", "main_total_tokens": 2790, "main_prompt_tokens": 2787, "main_completion_tokens": 3, "judge_total_tokens": 452, "judge_prompt_tokens": 387, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "I don't know.", "latency_ms": 724.8818874359131, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says no such movie description can be found, indicating the correct response is essentially that the answer is unknown. The generated answer, 'I don't know,' is therefore correct and aligns with the gold. It does not introduce any extra assumptions or hallucinations.", "main_total_tokens": 561, "main_prompt_tokens": 560, "main_completion_tokens": 1, "judge_total_tokens": 478, "judge_prompt_tokens": 394, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "I don't know.", "latency_ms": 338.2081985473633, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because there is a specific movie title that fits the description ('Havana Darkness'), but the model responded with 'I don't know.' However, the response is faithful in that it does not fabricate an answer and accurately reflects the model's stated uncertainty.", "main_total_tokens": 570, "main_prompt_tokens": 569, "main_completion_tokens": 1, "judge_total_tokens": 477, "judge_prompt_tokens": 392, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 1523.0743885040283, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the show as 'Doll & Em.'", "main_total_tokens": 2586, "main_prompt_tokens": 2552, "main_completion_tokens": 34, "judge_total_tokens": 443, "judge_prompt_tokens": 392, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "I don't know.", "latency_ms": 371.9305992126465, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered 'I don't know,' which is incorrect because the correct answer is 'Hallam Foe.' However, it did not fabricate an incorrect movie title, so the answer is faithful to the model's stated uncertainty.", "main_total_tokens": 574, "main_prompt_tokens": 573, "main_completion_tokens": 1, "judge_total_tokens": 474, "judge_prompt_tokens": 403, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "I don't know.", "latency_ms": 1000.847578048706, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'A foe', but the generated answer is 'I don't know.', which is incorrect. However, the model does not hallucinate a specific incorrect answer; it explicitly states uncertainty, so it is faithful to its own lack of knowledge.", "main_total_tokens": 730, "main_prompt_tokens": 705, "main_completion_tokens": 25, "judge_total_tokens": 472, "judge_prompt_tokens": 390, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18", "latency_ms": 932.1713447570801, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (18), so it is both correct and faithful to the reference.", "main_total_tokens": 2745, "main_prompt_tokens": 2743, "main_completion_tokens": 2, "judge_total_tokens": 442, "judge_prompt_tokens": 389, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Autism [Doc 124583].", "latency_ms": 967.698335647583, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer 'Autism' is essentially correct, as the question asks which spectrum Sam is on, and the gold answer is 'Autism spectrum'. The meaning matches. However, it is slightly less faithful in wording because it omits the word 'spectrum', so faithfulness is not perfect.", "main_total_tokens": 2703, "main_prompt_tokens": 2694, "main_completion_tokens": 9, "judge_total_tokens": 488, "judge_prompt_tokens": 394, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix [Doc 59659].", "latency_ms": 1527.6668071746826, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies Chamonix as the place where Sandra was born, so the correctness is 1. However, it cites a specific document ID ([Doc 59659]) that is not provided in the user prompt or context, which means it is referencing external, unverified information rather than sticking strictly to the given context. Therefore, the faithfulness score is 0.", "main_total_tokens": 3013, "main_prompt_tokens": 3003, "main_completion_tokens": 10, "judge_total_tokens": 514, "judge_prompt_tokens": 406, "judge_completion_tokens": 108}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481].", "latency_ms": 1589.418888092041, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer \"rains\" matches the gold answer exactly, so it is fully correct. The extra citation-like \"[Doc 100481]\" is irrelevant to the question and slightly detracts from faithfulness to the expected plain answer format, but the core content is faithful to the gold label.", "main_total_tokens": 3447, "main_prompt_tokens": 3438, "main_completion_tokens": 9, "judge_total_tokens": 492, "judge_prompt_tokens": 398, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland [Doc 114810].", "latency_ms": 2038.6593341827393, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the place where Tabaluga is crown-prince. The gold answer is Greenland. The generated answer is exactly 'Greenland', which matches the gold answer and is fully supported by the provided information. Therefore, it is both correct and faithful.", "main_total_tokens": 2940, "main_prompt_tokens": 2931, "main_completion_tokens": 9, "judge_total_tokens": 485, "judge_prompt_tokens": 401, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon", "latency_ms": 1104.0616035461426, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('One Afternoon') and correctly identifies the movie described. It is also fully faithful to the information given in the prompt.", "main_total_tokens": 2310, "main_prompt_tokens": 2307, "main_completion_tokens": 3, "judge_total_tokens": 469, "judge_prompt_tokens": 406, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "I don't know.", "latency_ms": 338.02008628845215, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "recall_at_10": 0.0, "precision_at_5": 0.0, "precision_at_10": 0.0, "ndcg_at_5": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_5": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered 'I don't know,' which is incorrect because the correct answer is 'The Life Ahead.' However, it did not fabricate any information and accurately reflected its uncertainty, so it is fully faithful to its own stated knowledge limitations.", "main_total_tokens": 580, "main_prompt_tokens": 579, "main_completion_tokens": 1, "judge_total_tokens": 475, "judge_prompt_tokens": 402, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "Bruce McLaren [Doc 73859].", "latency_ms": 872.2999095916748, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and exactly matches the gold answer, aside from an extraneous document reference tag. The core content is both correct and faithful to the provided information.", "main_total_tokens": 2834, "main_prompt_tokens": 2824, "main_completion_tokens": 10, "judge_total_tokens": 470, "judge_prompt_tokens": 393, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren Motor Racing team [Doc 73859].", "latency_ms": 2027.3375511169434, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the team as 'McLaren Motor Racing team,' which matches the gold answer except for the leading article. The added reference tag '[Doc 73859]' does not alter the factual content and remains faithful to the given information.", "main_total_tokens": 2767, "main_prompt_tokens": 2743, "main_completion_tokens": 24, "judge_total_tokens": 475, "judge_prompt_tokens": 394, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "New Zealand [Doc 73859].", "latency_ms": 1971.520185470581, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the country as New Zealand, matching the gold answer. The added document reference does not alter the factual content and is consistent with the gold, so the response is both correct and faithful.", "main_total_tokens": 2999, "main_prompt_tokens": 2979, "main_completion_tokens": 20, "judge_total_tokens": 457, "judge_prompt_tokens": 384, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "I don't know.", "latency_ms": 315.37675857543945, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says this movie is not in the database; the generated answer says 'I don't know.' Both convey lack of knowledge and do not introduce incorrect facts. Thus the generated answer is correct and faithful to the gold.", "main_total_tokens": 546, "main_prompt_tokens": 545, "main_completion_tokens": 1, "judge_total_tokens": 478, "judge_prompt_tokens": 402, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit", "latency_ms": 881.1748027801514, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct title of the 2002 Japanese horror film described.", "main_total_tokens": 2796, "main_prompt_tokens": 2790, "main_completion_tokens": 6, "judge_total_tokens": 477, "judge_prompt_tokens": 422, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 [Doc 98566].", "latency_ms": 1304.4219017028809, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer matches the gold answer (65 years), so it is correct. However, the inclusion of a fabricated citation format '[Doc 98566]' is not grounded in the provided context and constitutes an unfaithful hallucinated reference.", "main_total_tokens": 2996, "main_prompt_tokens": 2988, "main_completion_tokens": 8, "judge_total_tokens": 481, "judge_prompt_tokens": 402, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "I don't know.", "latency_ms": 395.8587646484375, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'The Book of Souls'. The generated answer says 'I don't know,' which does not match the correct answer, so it is incorrect. However, it is faithful to the model's stated uncertainty and does not hallucinate, so faithfulness is high.", "main_total_tokens": 581, "main_prompt_tokens": 580, "main_completion_tokens": 1, "judge_total_tokens": 484, "judge_prompt_tokens": 398, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 420.73988914489746, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, which is equivalent to not knowing the release year. The generated answer explicitly says 'I don't know,' which correctly reflects the absence of information and aligns with the gold response.", "main_total_tokens": 561, "main_prompt_tokens": 560, "main_completion_tokens": 1, "judge_total_tokens": 473, "judge_prompt_tokens": 395, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 415.8358573913574, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer explicitly states that the movie is not in the database, which is a more specific limitation than simply saying 'I don't know.' The generated answer could be interpreted as a generic lack of knowledge rather than reflecting the database constraint, so it is not fully faithful to the gold response.", "main_total_tokens": 569, "main_prompt_tokens": 568, "main_completion_tokens": 1, "judge_total_tokens": 482, "judge_prompt_tokens": 393, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 403.17392349243164, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, i.e., the system does not know the release year. The generated answer says, \"I don't know,\" which is equivalent. It is correct with respect to the gold and faithful to the prompt, without fabricating a release year.", "main_total_tokens": 558, "main_prompt_tokens": 557, "main_completion_tokens": 1, "judge_total_tokens": 486, "judge_prompt_tokens": 395, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 421.42796516418457, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, which is equivalent in substance to the generated answer 'I don't know.' Both convey lack of knowledge about the release year. The generated answer does not invent details and matches the gold answer in meaning, so it is both correct and faithful.", "main_total_tokens": 550, "main_prompt_tokens": 549, "main_completion_tokens": 1, "judge_total_tokens": 486, "judge_prompt_tokens": 395, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "2002", "latency_ms": 1028.6457538604736, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer asserts a specific year (2002) for a movie that the gold answer states is not in the database. There is no basis for this date in the provided information, so it is factually unsupported and not faithful to the gold response, which correctly expresses uncertainty/unavailability.", "main_total_tokens": 2637, "main_prompt_tokens": 2634, "main_completion_tokens": 3, "judge_total_tokens": 483, "judge_prompt_tokens": 394, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 399.80554580688477, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers appropriately indicate a lack of knowledge about the movie. The generated answer is correct because the release year is not known, and it is faithful to the prompt since it does not introduce any unsupported information.", "main_total_tokens": 521, "main_prompt_tokens": 520, "main_completion_tokens": 1, "judge_total_tokens": 469, "judge_prompt_tokens": 393, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 422.6386547088623, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer appropriately express lack of knowledge about the movie. The generated answer does not hallucinate a year and is consistent with the gold response. Therefore, it is correct and faithful.", "main_total_tokens": 558, "main_prompt_tokens": 557, "main_completion_tokens": 1, "judge_total_tokens": 469, "judge_prompt_tokens": 396, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Michael Samuels directed the movie \"The Last Days of Lehman Brothers\" [Doc 46069].", "latency_ms": 659.3084335327148, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer asserts that Michael Samuels directed 'The Last Days of Lehman Brothers,' contradicting the gold answer that this information is not available in the database. Regardless of real-world correctness, it is incorrect relative to the provided gold standard and is unfaithful because it fabricates information not supported by the database, even citing a specific document [Doc 46069] that is not evidenced.", "main_total_tokens": 2730, "main_prompt_tokens": 2708, "main_completion_tokens": 22, "judge_total_tokens": 529, "judge_prompt_tokens": 418, "judge_completion_tokens": 111}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 395.7369327545166, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates that the director information is not available in the database, so the correct response should acknowledge not knowing. The generated answer, 'I don't know.', correctly reflects this lack of information and does not introduce unsupported details. Therefore it is both correct and faithful to the gold.", "main_total_tokens": 565, "main_prompt_tokens": 564, "main_completion_tokens": 1, "judge_total_tokens": 487, "judge_prompt_tokens": 399, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not mentioned in the provided documents.", "latency_ms": 1618.7810897827148, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned, which aligns with the gold answer that the director information is not provided. It does not introduce unsupported information and remains faithful to the given database context.", "main_total_tokens": 2970, "main_prompt_tokens": 2951, "main_completion_tokens": 19, "judge_total_tokens": 483, "judge_prompt_tokens": 411, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 704.2965888977051, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the director of the movie, but the gold answer states that this information is not available in the database. The generated answer ('I don't know.') correctly reflects the lack of information and does not fabricate an answer, so it is both correct with respect to the task and faithful to the (unstated) source/database limits.", "main_total_tokens": 539, "main_prompt_tokens": 538, "main_completion_tokens": 1, "judge_total_tokens": 500, "judge_prompt_tokens": 401, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 1377.9876232147217, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy' and remaining fully faithful to the provided information.", "main_total_tokens": 2548, "main_prompt_tokens": 2531, "main_completion_tokens": 17, "judge_total_tokens": 457, "judge_prompt_tokens": 396, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 1449.3300914764404, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Stranger in My Bed' and fully aligns with the provided reference.", "main_total_tokens": 2521, "main_prompt_tokens": 2515, "main_completion_tokens": 6, "judge_total_tokens": 460, "judge_prompt_tokens": 400, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 1150.808572769165, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title in which that plot occurs; the gold answer is 'Game of Silence.' The generated answer matches exactly, so it is correct and faithful to the gold.", "main_total_tokens": 2194, "main_prompt_tokens": 2190, "main_completion_tokens": 4, "judge_total_tokens": 468, "judge_prompt_tokens": 400, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 1119.680404663086, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct movie title. It is both correct and fully faithful to the reference.", "main_total_tokens": 2494, "main_prompt_tokens": 2490, "main_completion_tokens": 4, "judge_total_tokens": 453, "judge_prompt_tokens": 395, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 1150.1669883728027, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Wood Job!') and correctly identifies the movie described. It is both correct and fully faithful to the reference.", "main_total_tokens": 3401, "main_prompt_tokens": 3397, "main_completion_tokens": 4, "judge_total_tokens": 465, "judge_prompt_tokens": 403, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 1806.488275527954, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase.' It is both factually correct and fully faithful to the reference.", "main_total_tokens": 2999, "main_prompt_tokens": 2976, "main_completion_tokens": 23, "judge_total_tokens": 485, "judge_prompt_tokens": 417, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond", "latency_ms": 670.9456443786621, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which indicates the correct movie title is 'Beyond.' There is no contradiction or fabrication relative to the provided information, so both correctness and faithfulness are maximal.", "main_total_tokens": 2424, "main_prompt_tokens": 2422, "main_completion_tokens": 2, "judge_total_tokens": 480, "judge_prompt_tokens": 409, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9", "latency_ms": 950.1857757568359, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('6ixtynin9') and correctly identifies the movie. It is both correct and faithful.", "main_total_tokens": 2432, "main_prompt_tokens": 2425, "main_completion_tokens": 7, "judge_total_tokens": 469, "judge_prompt_tokens": 409, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread [Doc 32410].", "latency_ms": 957.0910930633545, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the correct movie title 'Club Dread' given in the gold. No extraneous or incorrect information is introduced beyond a harmless document reference tag.", "main_total_tokens": 2774, "main_prompt_tokens": 2764, "main_completion_tokens": 10, "judge_total_tokens": 498, "judge_prompt_tokens": 433, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet", "latency_ms": 1304.5423030853271, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Toilet' exactly matches the gold answer 'Toilet' and correctly identifies the movie based on the description, so it is both correct and faithful.", "main_total_tokens": 2958, "main_prompt_tokens": 2955, "main_completion_tokens": 3, "judge_total_tokens": 480, "judge_prompt_tokens": 415, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel", "latency_ms": 856.0945987701416, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Electric Angel' exactly matches the gold answer, so it is correct. The explanation here accurately reflects that comparison, so it is faithful.", "main_total_tokens": 2735, "main_prompt_tokens": 2732, "main_completion_tokens": 3, "judge_total_tokens": 480, "judge_prompt_tokens": 418, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "Fall [Doc 106810].", "latency_ms": 1362.8697395324707, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates that the lowest-rated movie from 2023 is 'Mega Ape', while the generated answer is 'Fall', which is incorrect. There is no evidence or reference in the prompt to justify 'Fall' as the correct answer, so the response is both factually incorrect and unfaithful to the gold standard.", "main_total_tokens": 2639, "main_prompt_tokens": 2631, "main_completion_tokens": 8, "judge_total_tokens": 485, "judge_prompt_tokens": 388, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 173.99954795837402, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates a specific show (Tea House) as the highest rated TV show from 2010, while the generated answer says 'I don't know,' which does not match the gold and is therefore factually incorrect relative to the target. However, the generated answer is faithful to the question in that it does not fabricate an answer and correctly reflects ignorance instead of hallucinating.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 493, "judge_prompt_tokens": 386, "judge_completion_tokens": 107}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 200.2098560333252, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct movie ('The Uncut Story of Mewtwo's Origin') and thus is incorrect. However, it does not introduce any false information or contradictions relative to the question, so it is fully faithful to its own content.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 395, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "en", "latency_ms": 656.4009189605713, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer specifies the full language name 'English', while the generated answer is the language code 'en'. They are not string-equal, so under strict evaluation this is incorrect. The model also did not faithfully reproduce the expected format.", "main_total_tokens": 2882, "main_prompt_tokens": 2880, "main_completion_tokens": 2, "judge_total_tokens": 463, "judge_prompt_tokens": 384, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "Popular", "latency_ms": 605.9446334838867, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer \"Popular\" is incorrect; the correct show is \"Law & Order: Special Victims Unit.\" It is also unfaithful to the gold answer, providing a completely different TV show.", "main_total_tokens": 2855, "main_prompt_tokens": 2853, "main_completion_tokens": 2, "judge_total_tokens": 463, "judge_prompt_tokens": 391, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2 [Doc 79355].", "latency_ms": 530.0033092498779, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2). The added reference tag does not alter the correctness or contradict any given information, so the response is fully correct and faithful.", "main_total_tokens": 2217, "main_prompt_tokens": 2208, "main_completion_tokens": 9, "judge_total_tokens": 460, "judge_prompt_tokens": 392, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "1999 [Doc 22481].", "latency_ms": 1112.1671199798584, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1999) does not match the gold answer (2019), so it is factually incorrect relative to the specified gold. However, the model’s answer is internally consistent with its cited document, so faithfulness is high.", "main_total_tokens": 2718, "main_prompt_tokens": 2709, "main_completion_tokens": 9, "judge_total_tokens": 473, "judge_prompt_tokens": 393, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "1", "latency_ms": 439.2375946044922, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 279 votes, but the generated answer was 1, which is incorrect. The model clearly and directly answered with a number, just not the correct one, so it is faithful to its (incorrect) prediction but not correct relative to the gold.", "main_total_tokens": 2188, "main_prompt_tokens": 2187, "main_completion_tokens": 1, "judge_total_tokens": 463, "judge_prompt_tokens": 383, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 397.97139167785645, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 12 votes, but the generated answer claims it cannot find any matching movies or shows, which is factually incorrect. It also does not reflect the information given in the gold answer, so it is unfaithful to the reference.", "main_total_tokens": 2237, "main_prompt_tokens": 2236, "main_completion_tokens": 1, "judge_total_tokens": 469, "judge_prompt_tokens": 392, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "I don't know.", "latency_ms": 452.1176815032959, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answer is factually incorrect because the movie was released in 2009, but it responded with 'I don't know.' However, it does not introduce any fabricated or contradictory information beyond admitting uncertainty, so it is fully faithful to its own lack of knowledge.", "main_total_tokens": 526, "main_prompt_tokens": 525, "main_completion_tokens": 1, "judge_total_tokens": 475, "judge_prompt_tokens": 391, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_bge-m3_gpt-4o", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en", "latency_ms": 687.4322891235352, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The movie \"The King’s Speech\" is originally in English, and both the gold answer and the generated answer correctly state \"en\" as the language. The answer is correct and aligns fully with the gold.", "main_total_tokens": 2750, "main_prompt_tokens": 2748, "main_completion_tokens": 2, "judge_total_tokens": 457, "judge_prompt_tokens": 384, "judge_completion_tokens": 73}
