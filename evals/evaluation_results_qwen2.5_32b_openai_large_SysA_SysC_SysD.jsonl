{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "Wall Street was released in 1987 [Doc 6993].", "latency_ms": 32499.449968338013, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer, and it does not introduce contradictions or unsupported information aside from a benign document citation.", "main_total_tokens": 3201, "main_prompt_tokens": 3182, "main_completion_tokens": 19, "judge_total_tokens": 459, "judge_prompt_tokens": 392, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "In the movie \"Brain Fluid Explosion Girl\" (2015), Hana Ichii and her classmates are forced to take part in a survival game using their cellphones [Doc 69929].", "latency_ms": 6239.674091339111, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie features classmates forced into a survival game using their cell phones. The gold answer is 'Brain Fluid Explosion Girl.' The generated answer explicitly states that this occurs in the movie 'Brain Fluid Explosion Girl' (2015) and matches the gold answer. There are no contradictions or hallucinations relevant to the question, and it directly cites the correct film. Therefore, it is fully correct and faithful.", "main_total_tokens": 3147, "main_prompt_tokens": 3102, "main_completion_tokens": 45, "judge_total_tokens": 540, "judge_prompt_tokens": 426, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show where six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" from 1983 [Doc 112849].", "latency_ms": 6229.123115539551, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.4306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.719741384391281, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' which matches the gold answer. It does not introduce conflicting or extraneous information beyond specifying the year, so it is faithful to the provided gold response.", "main_total_tokens": 3323, "main_prompt_tokens": 3283, "main_completion_tokens": 40, "judge_total_tokens": 499, "judge_prompt_tokens": 423, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie that is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is \"Nacht der Wölfe\" [Doc 2590].", "latency_ms": 6390.95139503479, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'Nacht der Wölfe' as the movie about two rival gangs in Munich. It is fully supported by the referenced document and does not introduce any extra or incorrect information.", "main_total_tokens": 3108, "main_prompt_tokens": 3064, "main_completion_tokens": 44, "judge_total_tokens": 517, "judge_prompt_tokens": 439, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of Picnic at Hanging Rock take place in the year 1900 [Doc 126191].", "latency_ms": 4821.134090423584, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of Picnic at Hanging Rock take place in 1900, which matches the gold answer exactly and aligns with the referenced document, so it is both correct and faithful.", "main_total_tokens": 2943, "main_prompt_tokens": 2914, "main_completion_tokens": 29, "judge_total_tokens": 478, "judge_prompt_tokens": 406, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie you are referring to is \"June\" [Doc 32483]. It tells the story of June, a young violinist who works in an oddball Venice coffee shop during the day and rehearses for her professional music career at night. She spends much of her time daydreaming about a beautiful man destined for her.", "latency_ms": 9594.89917755127, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'June,' matching the gold answer exactly and accurately restating the plot details given in the question. It remains faithful to the implied source ([Doc 32483]) without adding unsupported claims.", "main_total_tokens": 3448, "main_prompt_tokens": 3372, "main_completion_tokens": 76, "judge_total_tokens": 579, "judge_prompt_tokens": 502, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "Based on the provided documents, the highest rated Russian-language movie released in 1980 is \"Adventures of Petrov and Vasechkin, Both Usual and Unbelieveable\" with a rating of 7.8 [Doc 2862]. However, it's important to note that this title is not listed as being from 1980 but rather 1983 in the provided context. Among the movies actually released in 1980, \"Moscow Does Not Believe in Tears\" has a rating of 7.6 [Doc 61], making it the highest rated Russian-language movie from that year according to the given documents.", "latency_ms": 14313.613176345825, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer specifies 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger' as the highest rated 1980 Russian-language movie, but the generated answer claims 'Moscow Does Not Believe in Tears' holds that position. This directly contradicts the gold, so correctness is 0. Additionally, the generated answer relies on supposed document ratings and even notes that its first candidate is misdated, then still picks another title; this shows speculative reasoning about the documents rather than faithfully reflecting the gold truth, so faithfulness is also 0.", "main_total_tokens": 3170, "main_prompt_tokens": 3026, "main_completion_tokens": 144, "judge_total_tokens": 666, "judge_prompt_tokens": 524, "judge_completion_tokens": 142}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest-rated Russian-language movie from 1999 based on the provided documents is \"The Rifleman of the Voroshilov Regiment\" with a rating of 7.117 [Doc 22123].", "latency_ms": 6992.846965789795, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the highest rating is 7.6, while the generated answer states 7.117 and names a specific film, which is numerically incorrect. Additionally, the generated answer references a specific document and film not mentioned in the prompt, so it is not faithful to the provided information.", "main_total_tokens": 3017, "main_prompt_tokens": 2967, "main_completion_tokens": 50, "judge_total_tokens": 519, "judge_prompt_tokens": 427, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "Based on the provided documents, three movies meet the criteria of being released in 1987, having a rating over 7, and being in Chinese:\n\n1. \"A Chinese Ghost Story\" with a rating of 7.3 [Doc 7064]\n2. \"The Last Emperor\" with a rating of 7.599 [Doc 6989] - Note: Although this movie is listed as being in English (en), it is about the last emperor of China and could be considered relevant to Chinese cinema.\n3. \"A Better Tomorrow II\" with a rating of 7.0 [Doc 7262]\n\nHowever, if we strictly adhere to the language being 'cn' or 'zh', then two movies fit the criteria:\n\n1. \"A Chinese Ghost Story\" with a rating of 7.3 [Doc 7064]\n2. \"A Better Tomorrow II\" with a rating of 7.0 [Doc 7262] \n\nTherefore, strictly speaking, there are 2 movies that were released in 1987, have a rating over 7, and are in Chinese (cn or zh).", "latency_ms": 21667.521715164185, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The gold answer is 4, but the generated answer concludes 2 (or conditionally 3) movies, so the final numeric answer is incorrect. The reasoning is mostly grounded in the described documents, but it introduces a looser interpretation by counting an English-language film as \"relevant to Chinese cinema,\" which is not faithful to the explicit question constraint about language being Chinese. Overall, partially grounded but incorrect and somewhat speculative regarding the language criterion.", "main_total_tokens": 3301, "main_prompt_tokens": 3053, "main_completion_tokens": 248, "judge_total_tokens": 746, "judge_prompt_tokens": 623, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "Based on the provided documents, there is one TV show that meets all the criteria: \"Dream of the Red Chamber\" [Doc 113285], which was released in 1987, has a rating of 9.4, and is in Chinese (zh).", "latency_ms": 7348.376035690308, "retrieval_recall": 0.0, "retrieval_mrr": 0.07142857142857142, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 2 such TV shows, but the generated answer claims there is only 1, so it is numerically incorrect. However, the generated answer appears to accurately reflect what is stated in the provided documents, so it is faithful to the given context.", "main_total_tokens": 3055, "main_prompt_tokens": 2994, "main_completion_tokens": 61, "judge_total_tokens": 534, "judge_prompt_tokens": 447, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "Based on the provided documents, there is no French language TV show released in 1980. The closest entry is a movie titled \"La Femme flic\" from 1980, but it is not a TV show [Doc 290].", "latency_ms": 6861.550807952881, "retrieval_recall": 1.0, "retrieval_mrr": 0.2, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.38685280723454163, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.38685280723454163, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the only French language TV show released in 1980, with the correct answer being 'Fantômas'. The generated answer incorrectly claims that there is no such TV show and instead mentions a movie, so it is factually wrong relative to the gold answer (correctness = 0). However, the generated response is consistent with the information it cites from the provided documents, stating that the closest entry is a movie, so it remains faithful to those documents (faithfulness = 1).", "main_total_tokens": 3023, "main_prompt_tokens": 2967, "main_completion_tokens": 56, "judge_total_tokens": 562, "judge_prompt_tokens": 434, "judge_completion_tokens": 128}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie that features two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" [Doc 11205].", "latency_ms": 5435.446739196777, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise' and aligns with the gold answer. It does not invent or contradict any information, so it is both correct and faithful.", "main_total_tokens": 3219, "main_prompt_tokens": 3188, "main_completion_tokens": 31, "judge_total_tokens": 477, "judge_prompt_tokens": 410, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 6567.108869552612, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Won't You Be My Neighbor?' and does not introduce any contradictions or unsupported claims beyond restating the question context, so it is both correct and faithful.", "main_total_tokens": 3581, "main_prompt_tokens": 3538, "main_completion_tokens": 43, "judge_total_tokens": 511, "judge_prompt_tokens": 440, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The document does not specify a single release year for \"it,\" as multiple movies with different release years are listed. Please specify which movie you are asking about.", "latency_ms": 5517.670392990112, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly expects the specific year 2018, so the correct response should simply be '2018'. The generated answer instead claims the document does not specify a single release year and asks for clarification, which is incorrect given the gold label. However, the generated answer is faithful to its own reasoning and does not contradict the provided text; it just fails to provide the required year. Therefore, correctness is 0, while faithfulness to its stated reasoning is 1.", "main_total_tokens": 3337, "main_prompt_tokens": 3304, "main_completion_tokens": 33, "judge_total_tokens": 536, "judge_prompt_tokens": 409, "judge_completion_tokens": 127}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is \"Astro Boy\" from 1980 [Doc 112591].", "latency_ms": 6423.18320274353, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy', matching the gold answer. However, it adds extra detail about it being 'from 1980 [Doc 112591]' which is not supported by the provided context in the prompt; thus, there is a small faithfulness deduction for introducing an ungrounded specific year and citation.", "main_total_tokens": 3568, "main_prompt_tokens": 3526, "main_completion_tokens": 42, "judge_total_tokens": 524, "judge_prompt_tokens": 420, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her young son were held captive for 7 years [Doc 67513].", "latency_ms": 5304.492712020874, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer exactly. It is fully supported by the provided information and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 2781, "main_prompt_tokens": 2744, "main_completion_tokens": 37, "judge_total_tokens": 500, "judge_prompt_tokens": 425, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26m before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 6687.183380126953, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26m before the 26 November 1983 break-in, matching the provided gold answer exactly and remaining consistent with the source reference.", "main_total_tokens": 3121, "main_prompt_tokens": 3077, "main_completion_tokens": 44, "judge_total_tokens": 505, "judge_prompt_tokens": 428, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "To determine the popularity score of the most popular English-speaking TV show released in 1997, we need to compare the popularity scores of all English-speaking TV shows from that year listed in the provided context.\n\nHere are the English-speaking TV shows from 1997 with their respective popularity scores:\n\n- **USA High**: Popularity score = 3.535 [Doc 114769]\n- **Just Shoot Me!**: Popularity score = 7.771 [Doc 114728]\n- **The Practice**: Popularity score = 18.4612 [Doc 114712]\n- **Oz**: Popularity score = 14.7527 [Doc 114714]\n- **Switch**: Popularity score = 1.4456 [Doc 114851] (Note: This show is in German, not English, so it's excluded from the comparison.)\n\nAmong these, the highest popularity score is for **The Practice**, which has a score of 18.4612 [Doc 114712].\n\nTherefore, the popularity score of the most popular English-speaking TV show released in 1997 is 18.4612.", "latency_ms": 23761.59119606018, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.7, "justification": "The correct answer according to the gold label is 80.5772, but the generated answer claims 18.4612, so it is incorrect (correctness 0). The reasoning process is mostly grounded in the provided (albeit incomplete) list of shows and their scores, and it explicitly excludes a non-English show; however, it incorrectly assumes that this subset is exhaustive and that The Practice is the most popular, ignoring the actual gold value. This unjustified assumption reduces faithfulness, since the model overclaims based on partial evidence, so faithfulness is partial rather than full.", "main_total_tokens": 3150, "main_prompt_tokens": 2870, "main_completion_tokens": 280, "judge_total_tokens": 772, "judge_prompt_tokens": 626, "judge_completion_tokens": 146}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "Based on the provided documents, there is no information about any Russian language TV shows that were released specifically in 1997. [Doc 19935] mentions a movie titled \"The Red and the Black\" from 1997, but it is not a TV show and its language is French, not Russian.", "latency_ms": 7889.588356018066, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates that one Russian language TV show was released in 1997, but the generated answer claims there is no information about any Russian language TV shows released in that year. This contradicts the gold answer, so the response is incorrect. However, the generated answer is internally consistent and accurately reflects the (stated) lack of information in the provided document, so it is faithful to its own cited source.", "main_total_tokens": 2909, "main_prompt_tokens": 2838, "main_completion_tokens": 71, "judge_total_tokens": 561, "judge_prompt_tokens": 446, "judge_completion_tokens": 115}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 6135.678768157959, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Angel Eyes'. It is fully correct and faithful to the provided information.", "main_total_tokens": 3429, "main_prompt_tokens": 3389, "main_completion_tokens": 40, "judge_total_tokens": 490, "judge_prompt_tokens": 429, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French [Doc 110681].", "latency_ms": 4702.645063400269, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states the movie language is French, while the gold label says Korean, so the answer is factually incorrect. It also appears to contradict the referenced document by misidentifying the language, so it is not faithful to the source.", "main_total_tokens": 3283, "main_prompt_tokens": 3261, "main_completion_tokens": 22, "judge_total_tokens": 473, "judge_prompt_tokens": 393, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "None of the English-speaking Horror movies listed from 2009 have a popularity score greater than 7. The most popular English-speaking Horror movie from 2009 is \"Perkins' 14\" with a popularity score of 1.1125 [Doc 46432].", "latency_ms": 7667.9441928863525, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 2 such movies, while the generated answer claims there are none, so it is factually incorrect relative to the gold. However, the generated answer accurately reflects the information it cites from the provided document (about 'Perkins' 14' and its popularity), and does not introduce contradictions to that source, so it is faithful to the given evidence.", "main_total_tokens": 2998, "main_prompt_tokens": 2931, "main_completion_tokens": 67, "judge_total_tokens": 556, "judge_prompt_tokens": 448, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The provided documents do not contain information about any movies released in 1979. Therefore, I cannot determine the best-rated movie from that year based on the given context.", "latency_ms": 5538.529872894287, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that there is no information about movies released in 1979 in the available data, and thus the best-rated movie from that year cannot be determined. This is consistent with the given context, making the generated response both correct and faithful to the source information.", "main_total_tokens": 3084, "main_prompt_tokens": 3047, "main_completion_tokens": 37, "judge_total_tokens": 518, "judge_prompt_tokens": 429, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The provided documents do not mention a \"Back to the Future Part IV\" movie release. The latest film mentioned in the context is \"Back to the Future Part III,\" which was released in 1990 [Doc 10621]. There is no information about a fourth part of the Back to the Future film series being made or released in any of the documents provided.", "latency_ms": 9335.595607757568, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information about a 'Back to the Future Part IV' movie and notes that only Part III (1990) is mentioned. This aligns with the gold answer indicating that Part IV does not exist in the database. It does not fabricate a release year and stays grounded in the provided documents, so it is both correct and faithful.", "main_total_tokens": 3700, "main_prompt_tokens": 3620, "main_completion_tokens": 80, "judge_total_tokens": 579, "judge_prompt_tokens": 474, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "The provided documents do not contain information about any movies from 1980 that are specifically about the Chernobyl disaster. The earliest document related to Chernobyl is \"Radioactive Dreams\" from 1986, which is a science fiction movie and not a documentary or drama about the actual Chernobyl disaster [Doc 6625]. Therefore, based on the given context, there is no answer to this question.", "latency_ms": 9991.868257522583, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "Gold answer states that no such movies from 1980 about the Chernobyl disaster were found, which is appropriate since the Chernobyl disaster occurred in 1986. The generated answer instead references a 1986 film \"Radioactive Dreams\" and claims it is the earliest related document, then concludes there is no answer based on given context. This diverges from the gold rationale and introduces an incorrect, unnecessary specific movie reference. While it is somewhat faithful to the (hypothetical) documents it cites, it is not correct relative to the gold: it should simply state that no such movies exist or were found without adding incorrect detail. Hence correctness is 0. Faithfulness is not perfect because it fabricates a specific connection and extra information beyond what is needed, but it still largely aligns with the idea that the documents do not contain a relevant 1980 Chernobyl movie, so it is partially faithful.", "main_total_tokens": 3481, "main_prompt_tokens": 3390, "main_completion_tokens": 91, "judge_total_tokens": 708, "judge_prompt_tokens": 490, "judge_completion_tokens": 218}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are four TV shows or movies with 'Astro Boy' in the title:\n\n1. Astro Boy (Movie, 2009) [Doc 44947]\n2. Astro Boy (TV, 2003) [Doc 116265]\n3. Astro Boy (TV, 1980) [Doc 112591]\n4. Astro Boy vs The Junkyard Pirates (Movie, 2010) [Doc 49121]", "latency_ms": 11415.158033370972, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 1.1510196182278052, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.773714921545009, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the number of TV shows or movies with 'Astro Boy' in the title, and the gold answer is 4. The generated response correctly states there are four such titles and lists four specific entries. This matches the gold answer exactly, so it is both correct and faithful to the question and gold.", "main_total_tokens": 3224, "main_prompt_tokens": 3111, "main_completion_tokens": 113, "judge_total_tokens": 572, "judge_prompt_tokens": 476, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "Based on the provided documents, there are no movies from 2025 that directly allude to the Holocaust. The only movie listed for 2025 is \"Gaza: How to Survive a Warzone,\" which focuses on life in Gaza during the Israel-Hamas war and does not mention the Holocaust [Doc 112486].", "latency_ms": 8908.177614212036, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 2 such movies, while the generated answer claims there are none, so it is factually incorrect relative to the gold. However, the generated answer is consistent with and grounded in the cited document, so it is faithful to the provided source.", "main_total_tokens": 3488, "main_prompt_tokens": 3412, "main_completion_tokens": 76, "judge_total_tokens": 537, "judge_prompt_tokens": 451, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "Based on the provided documents, there are three TV shows about Super Mario Bros that were released in the 1990s:\n\n1. The Adventures of Super Mario Bros. 3, which was released in 1990 [Doc 113661].\n2. Super Mario World, which was released in 1991 [Doc 113814].\n\nThese two shows are explicitly about the Super Mario Bros and were aired during the 1990s. No other TV shows about the Super Mario Bros from the provided context fall within this time frame.", "latency_ms": 12611.069440841675, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there was 1 TV show, but the generated answer concludes there are three, and then only lists two (1990 and 1991), which contradicts both the gold label and itself. This makes the numerical answer incorrect. It is also unfaithful to the given gold answer and even internally inconsistent with its own claim of three shows, so it scores low on faithfulness as well.", "main_total_tokens": 3469, "main_prompt_tokens": 3344, "main_completion_tokens": 125, "judge_total_tokens": 609, "judge_prompt_tokens": 495, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "Based on the provided documents, there are no TV shows about Super Mario Bros that were released between the years 2000 and 2010. The closest entries are \"The Adventures of Super Mario Bros. 3\" from 1990 and \"Super Mario World\" from 1991, both falling outside the specified time frame [Doc 113661] [Doc 113814].", "latency_ms": 9492.291688919067, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0, meaning no TV shows were released between 2000 and 2010 about Super Mario Bros. The generated answer states that there are no such TV shows in that time frame and cites examples from 1990 and 1991 as closest entries, which is fully consistent with the gold. It does not introduce contradictions or unsupported claims, only elaborates using the documents. Therefore, it is both correct and faithful.", "main_total_tokens": 2970, "main_prompt_tokens": 2875, "main_completion_tokens": 95, "judge_total_tokens": 587, "judge_prompt_tokens": 467, "judge_completion_tokens": 120}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular TV show or movie about ninjas based on the provided data is \"Supah Ninjas\" with a popularity score of 2.8397 [Doc 119858].", "latency_ms": 6414.770603179932, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates that 'Naruto' is the most popular ninja TV show, but the generated answer states 'Supah Ninjas' instead, which is incorrect relative to the gold. However, the generated answer is faithful to its cited document and does not contradict its own provided context, so faithfulness is high while correctness relative to the gold is low.", "main_total_tokens": 3320, "main_prompt_tokens": 3275, "main_completion_tokens": 45, "judge_total_tokens": 523, "judge_prompt_tokens": 420, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The provided documents do not mention a Jurassic Park/Power Rangers crossover movie. Therefore, the release date for such a crossover cannot be determined from the given information.", "latency_ms": 5409.01255607605, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the movie was not released, while the generated answer only says that the release date cannot be determined from the provided documents, implying uncertainty rather than affirming non-existence or non-release. This is factually incorrect relative to the gold and not faithful to it.", "main_total_tokens": 3146, "main_prompt_tokens": 3113, "main_completion_tokens": 33, "judge_total_tokens": 505, "judge_prompt_tokens": 417, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Based on the provided documents, there is no movie where Barbie fights a Predator.", "latency_ms": 4453.125238418579, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.9, "faithfulness_score": 1, "justification": "The gold answer specifies that the movie was not released, implying such a movie exists but was unreleased. The generated answer states there is no movie where Barbie fights a Predator, which slightly differs in nuance but leads to the same practical conclusion that no such released movie exists. It is mostly correct but not perfectly aligned with the gold. Faithfulness is full, as the evaluation matches the given gold and generated content without adding external assumptions.", "main_total_tokens": 3511, "main_prompt_tokens": 3494, "main_completion_tokens": 17, "judge_total_tokens": 517, "judge_prompt_tokens": 398, "judge_completion_tokens": 119}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The provided documents do not contain information about the Star Wars movie series or their release order. Therefore, I cannot determine the 4th oldest Star Wars movie based on the given context. [Fact not found in provided sources]", "latency_ms": 6810.984134674072, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer claims it cannot determine the 4th oldest Star Wars movie from the given documents. This is faithful to the (implied) constraint of relying only on provided sources, but it is factually incorrect relative to the gold answer, which specifies a definite movie: 'Star Wars: Droids - The Great Heep.' Therefore, correctness is 0, while faithfulness to the cited limitation is 1.", "main_total_tokens": 3507, "main_prompt_tokens": 3461, "main_completion_tokens": 46, "judge_total_tokens": 550, "judge_prompt_tokens": 434, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The document does not specify a single release year for \"it,\" as multiple movies with different release years are listed. Please specify which movie you are asking about.", "latency_ms": 5497.492074966431, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly expects the specific year 1986, so the correct response should have been '1986.' The generated answer instead claims the document does not specify a single release year and asks for clarification, which is incorrect for the labeled question. However, within the assistant’s own reasoning and visible context (treating 'it' as ambiguous among multiple movies), the answer is internally consistent and does not invent facts, so it is faithful but incorrect relative to the gold.", "main_total_tokens": 3337, "main_prompt_tokens": 3304, "main_completion_tokens": 33, "judge_total_tokens": 536, "judge_prompt_tokens": 409, "judge_completion_tokens": 127}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The rating of the movie Forrest Gump is 8.466 [Doc 15236].", "latency_ms": 4864.008665084839, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating value 8.466 and does not introduce any unsupported information. It is both correct and faithful to the provided gold answer.", "main_total_tokens": 3234, "main_prompt_tokens": 3209, "main_completion_tokens": 25, "judge_total_tokens": 477, "judge_prompt_tokens": 413, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is Mighty Med [Doc 121125].", "latency_ms": 5097.780227661133, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Mighty Med') and stays entirely consistent with the provided information, including the show description.", "main_total_tokens": 2910, "main_prompt_tokens": 2877, "main_completion_tokens": 33, "judge_total_tokens": 478, "judge_prompt_tokens": 420, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "The seven prisoners escaped from a prison in Argentina in the movie \"Túnel de los huesos\" (2011) [Doc 53582].", "latency_ms": 6145.512819290161, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the seven prisoners escaped from a prison in Argentina, which matches the gold answer. It does not introduce any contradictory or unsupported information, staying faithful to the referenced document.", "main_total_tokens": 3667, "main_prompt_tokens": 3630, "main_completion_tokens": 37, "judge_total_tokens": 492, "judge_prompt_tokens": 422, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice falls into the trap designed by Jiang Jie [Doc 135231].", "latency_ms": 5482.783317565918, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies that in Sweet Trap (2024), the apprentice falls into Jiang Jie's trap, which matches the gold answer. However, it adds a fabricated citation/reference tag [Doc 135231] that is not grounded in the provided information, so it is not fully faithful.", "main_total_tokens": 3198, "main_prompt_tokens": 3165, "main_completion_tokens": 33, "judge_total_tokens": 499, "judge_prompt_tokens": 409, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "Based on the provided documents, there is one Korean-language movie mentioning assassins that was released after 2023:\n\n\"Kill Boksoon\" [Doc 105198], a movie from 2023 with the language ko (Korean). \n\nHowever, since the question asks for movies after 2023 and \"Kill Boksoon\" was released in 2023, strictly speaking, there are no Korean-language movies mentioning assassins that were released after 2023 in the given context.", "latency_ms": 11550.002574920654, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer says there are 4 such movies, but the generated answer concludes that there are none after 2023, so the final numerical answer is incorrect, leading to a correctness score of 0. Regarding faithfulness, the generated answer claims there is one relevant movie in 2023 (\"Kill Boksoon\") and then infers that there are none after 2023 based on the provided documents. Since we don't see the documents here, we cannot verify whether the document-based count is accurate, but the reasoning is consistent with what it describes from the context. However, the model also initially states there is one Korean-language movie mentioning assassins after 2023, then immediately contradicts that by saying strictly speaking there are none after 2023. This internal inconsistency and misinterpretation of the time condition reduces faithfulness to the prompt and the described context, so faithfulness is partial rather than full.", "main_total_tokens": 3213, "main_prompt_tokens": 3098, "main_completion_tokens": 115, "judge_total_tokens": 699, "judge_prompt_tokens": 481, "judge_completion_tokens": 218}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "The provided documents do not contain any information about Steven Seagal and Bruce Lee appearing together in movies. [Doc X]", "latency_ms": 5028.709888458252, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information about Steven Seagal and Bruce Lee appearing together in movies, which matches the gold answer. It is faithful to the provided context, explicitly attributing the lack of information to the documents and not hallucinating any movies.", "main_total_tokens": 3307, "main_prompt_tokens": 3282, "main_completion_tokens": 25, "judge_total_tokens": 501, "judge_prompt_tokens": 417, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 4676.756381988525, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3 and does not introduce any contradictory or unsupported information beyond referencing a document. It is both correct and faithful.", "main_total_tokens": 3102, "main_prompt_tokens": 3076, "main_completion_tokens": 26, "judge_total_tokens": 468, "judge_prompt_tokens": 403, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The provided context does not specify a particular style of racing for \"The Last Race\" [Doc 86410]. However, it mentions that the movie is a cinematic portrait of a small town stock car track and its drivers. Therefore, the style of racing in this movie is stock car racing.", "latency_ms": 8060.112953186035, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer indicates the racing style is Drift, but the generated answer says stock car racing, which is incorrect. The assistant’s reasoning about the provided context might be faithful to that context (it infers stock car based on the mention of a stock car track), but relative to the actual labeled answer it is both incorrect and not faithful to the ground truth. Thus correctness is 0. Faithfulness to the given context is partial because it uses the context but draws an incorrect conclusion, so scored at 0.5.", "main_total_tokens": 3611, "main_prompt_tokens": 3548, "main_completion_tokens": 63, "judge_total_tokens": 575, "judge_prompt_tokens": 437, "judge_completion_tokens": 138}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 6694.3933963775635, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, providing the correct movie title 'Confronting a Serial Killer' and staying faithful to the information given, with no added or incorrect details.", "main_total_tokens": 3162, "main_prompt_tokens": 3111, "main_completion_tokens": 51, "judge_total_tokens": 520, "judge_prompt_tokens": 452, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 5272.47428894043, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and correctly attributes it, so it is both correct and faithful to the given information.", "main_total_tokens": 3369, "main_prompt_tokens": 3339, "main_completion_tokens": 30, "judge_total_tokens": 469, "judge_prompt_tokens": 407, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 5464.787721633911, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'The Sixth Sense,' matching the gold answer exactly. It remains faithful to the question, simply restating the correct title with no unsupported details.", "main_total_tokens": 3142, "main_prompt_tokens": 3107, "main_completion_tokens": 35, "judge_total_tokens": 489, "judge_prompt_tokens": 420, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 4696.0608959198, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold year 2002 exactly and does not introduce any contradictions or extraneous unsupported claims beyond a benign citation marker. It is correct and faithful.", "main_total_tokens": 3312, "main_prompt_tokens": 3289, "main_completion_tokens": 23, "judge_total_tokens": 462, "judge_prompt_tokens": 397, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 5769.351482391357, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, providing the correct title 'The Transporter Refueled' without adding unsupported information. It is both correct and faithful.", "main_total_tokens": 3469, "main_prompt_tokens": 3434, "main_completion_tokens": 35, "judge_total_tokens": 484, "judge_prompt_tokens": 420, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in the year 2015 [Doc 67500].", "latency_ms": 5050.823211669922, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It stays faithful to the provided information without adding unsupported details.", "main_total_tokens": 3347, "main_prompt_tokens": 3319, "main_completion_tokens": 28, "judge_total_tokens": 475, "judge_prompt_tokens": 406, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "None of the provided documents mention a romantic comedy where Abraham Lincoln comes back from the dead. The documents describe movies featuring Abraham Lincoln, but they are either in genres of horror and action like \"Abraham Lincoln vs. Zombies\" [Doc 57213] and \"Abraham Lincoln: Vampire Hunter\" [Doc 54943], or a drama/history genre in \"Saving Lincoln\" [Doc 59279]. None of these are romantic comedies where Lincoln comes back from the dead.", "latency_ms": 10773.923397064209, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a romantic comedy where Abraham Lincoln comes back from the dead, and the gold answer correctly states that no such movie description can be found. The generated answer accurately reflects the contents of the provided documents, noting that they cover horror/action/drama films involving Lincoln but no romantic comedy with him returning from the dead. This matches the gold conclusion and does not introduce unsupported claims. Therefore, it is both correct and faithful to the documents.", "main_total_tokens": 3116, "main_prompt_tokens": 3009, "main_completion_tokens": 107, "judge_total_tokens": 607, "judge_prompt_tokens": 487, "judge_completion_tokens": 120}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by Ernest Hemingway is \"Genius\" [Doc 73141]. In this movie, an unnamed writer submits a sprawling, chaotic 1,000-page manuscript which a literary editor, Max Perkins, believes to be the work of a literary genius. Although the manuscript is by an unknown writer in the movie, the plot revolves around the discovery and recognition of talent reminiscent of how Ernest Hemingway's work was initially received.", "latency_ms": 10852.661609649658, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct movie title is 'Havana Darkness', but the generated answer gives 'Genius', which is factually incorrect. It also fabricates a connection between the film 'Genius' and an Ernest Hemingway manuscript that is not supported by the provided gold answer, so it is unfaithful to the reference.", "main_total_tokens": 3225, "main_prompt_tokens": 3120, "main_completion_tokens": 105, "judge_total_tokens": 577, "judge_prompt_tokens": 483, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "In the TV show \"Doll & Em,\" Doll heads to Hollywood to be with her childhood best friend Em [Doc 122259].", "latency_ms": 5157.444000244141, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em,' matching the gold answer exactly. It remains faithful to the provided information and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 2977, "main_prompt_tokens": 2944, "main_completion_tokens": 33, "judge_total_tokens": 486, "judge_prompt_tokens": 417, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The 2007 movie in which Hallam has a talent for spying on people and attempts to find the cause of his mother's death is titled \"Hallam Foe\" [Doc 39389].", "latency_ms": 6529.865264892578, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Hallam Foe' and does not introduce unsupported information beyond a generic citation placeholder.", "main_total_tokens": 3229, "main_prompt_tokens": 3182, "main_completion_tokens": 47, "judge_total_tokens": 502, "judge_prompt_tokens": 439, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The provided context does not contain information about a character whose last name sounds like they could be either a friend or a foe. Therefore, based strictly on the given documents, there is no answer to this question.", "latency_ms": 6262.146949768066, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'A foe', so the question is answerable and has a specific expected response. The generated answer claims there is no answer based on the context, which contradicts the labeled gold answer, so it is factually incorrect with respect to the task. However, it does faithfully describe its (incorrect) reasoning about the provided context, so it is internally faithful to the evidence it cites. Thus correctness is 0, faithfulness 1.", "main_total_tokens": 3285, "main_prompt_tokens": 3242, "main_completion_tokens": 43, "judge_total_tokens": 551, "judge_prompt_tokens": 428, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "The age of Sam in the 2017 TV show Atypical is 18. [Doc 124583]", "latency_ms": 5020.293235778809, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: Sam is 18. It is fully correct and directly supported by the provided gold information, so both correctness and faithfulness are maximal.", "main_total_tokens": 3022, "main_prompt_tokens": 2991, "main_completion_tokens": 31, "judge_total_tokens": 478, "judge_prompt_tokens": 411, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 4553.108930587769, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the TV show Atypical, Sam is on the autism spectrum, which matches the gold answer exactly and does not introduce any contradictions or hallucinations.", "main_total_tokens": 2753, "main_prompt_tokens": 2724, "main_completion_tokens": 29, "judge_total_tokens": 478, "judge_prompt_tokens": 410, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Sandra was born in Chamonix according to the plot description of the movie \"À corde tendue\" [Doc 59659].", "latency_ms": 6884.827375411987, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, which matches the gold answer. It also attributes this information to the plot description of the specified movie without adding unsupported details, so it is both correct and faithful to the provided information.", "main_total_tokens": 3698, "main_prompt_tokens": 3665, "main_completion_tokens": 33, "judge_total_tokens": 505, "judge_prompt_tokens": 424, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "in this japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 4968.266487121582, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly fills in the blank with 'rains' and matches the gold answer exactly, preserving both meaning and wording.", "main_total_tokens": 3347, "main_prompt_tokens": 3321, "main_completion_tokens": 26, "judge_total_tokens": 469, "judge_prompt_tokens": 412, "judge_completion_tokens": 57}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the German language animation, Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 5629.35733795166, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which matches the gold answer exactly. It is fully faithful to the provided information without adding unsupported details.", "main_total_tokens": 3465, "main_prompt_tokens": 3432, "main_completion_tokens": 33, "judge_total_tokens": 490, "judge_prompt_tokens": 422, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is titled \"One Afternoon\" [Doc 96838].", "latency_ms": 6417.265892028809, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('One Afternoon') and correctly identifies it as the movie with the given description, with no additional unsupported claims.", "main_total_tokens": 2861, "main_prompt_tokens": 2810, "main_completion_tokens": 51, "judge_total_tokens": 510, "judge_prompt_tokens": 449, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie where a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" [Doc 93260].", "latency_ms": 25482.797861099243, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title of the low-rated Italian language movie whose plot involves a Holocaust survivor caring for a 12-year-old street kid who robbed her. The gold answer is \"The Life Ahead.\" The generated answer explicitly identifies the movie as \"The Life Ahead,\" matching the gold exactly. There are no contradictions or hallucinations beyond restating the description, so the response is both correct and faithful to the gold answer and the prompt.", "main_total_tokens": 3213, "main_prompt_tokens": 3172, "main_completion_tokens": 41, "judge_total_tokens": 553, "judge_prompt_tokens": 434, "judge_completion_tokens": 119}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary \"McLaren\" is about Bruce McLaren, a New Zealander who founded the McLaren Motor Racing team. [Doc 73859]", "latency_ms": 6057.051420211792, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and adds accurate contextual information about him founding the McLaren Motor Racing team. It is fully consistent with the gold answer and does not introduce unsupported claims.", "main_total_tokens": 3376, "main_prompt_tokens": 3336, "main_completion_tokens": 40, "judge_total_tokens": 497, "judge_prompt_tokens": 419, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The provided documents do not specify the name of Paul Newman's racing team. [Doc 68207]", "latency_ms": 5180.843114852905, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer contradicts the gold answer; the correct team name is the McLaren Motor Racing team, but the model claims the documents do not specify it, so correctness is 0. However, if the provided document indeed omitted the team name, then the answer is faithful to the given context, so faithfulness is 1.", "main_total_tokens": 3581, "main_prompt_tokens": 3556, "main_completion_tokens": 25, "judge_total_tokens": 502, "judge_prompt_tokens": 404, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The question \"What country is he from?\" does not have a specific answer based on the provided documents as there is no clear subject (\"he\") referred to in the context that is associated with a particular country. Each document describes different movies and their plots, but none of them specify a character simply as \"he\" without additional context that would indicate his country of origin.", "latency_ms": 8685.91022491455, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies 'New Zealand', so there is a correct, specific answer. The generated answer incorrectly claims that there is no specific answer, making it factually wrong relative to the gold. However, the reasoning in the generated answer is consistent with its own stated lack of context and does not introduce contradictions or hallucinations about the documents, so it is faithful to its own interpretation, though ultimately incorrect.", "main_total_tokens": 3373, "main_prompt_tokens": 3298, "main_completion_tokens": 75, "judge_total_tokens": 562, "judge_prompt_tokens": 450, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based on the provided documents, there is no information about John McClane blowing up the Great Wall of China in any 2013 movie. The 2013 movie featuring John McClane is \"A Good Day to Die Hard,\" but the plot summary does not mention anything about the Great Wall of China. [Doc 58791]", "latency_ms": 8892.939805984497, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there is no such movie in the database, a clear and direct response. The generated answer adds specific claims about provided documents and cites a document ID, which are hallucinated in this evaluation context, and adds unnecessary detail instead of simply matching the gold. Thus it is not fully correct relative to the gold and not faithful to the given information.", "main_total_tokens": 3586, "main_prompt_tokens": 3511, "main_completion_tokens": 75, "judge_total_tokens": 570, "judge_prompt_tokens": 467, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with the given description is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 5567.066431045532, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Tomie: Forbidden Fruit' and does not introduce any unsupported information. It is both correct and faithful to the provided gold answer.", "main_total_tokens": 3411, "main_prompt_tokens": 3379, "main_completion_tokens": 32, "judge_total_tokens": 509, "judge_prompt_tokens": 443, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years in the 2021 movie \"Lei mi parla ancora\" [Doc 98566].", "latency_ms": 6439.689159393311, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years) and does not introduce unsupported details. It is both correct and faithful to the reference.", "main_total_tokens": 3817, "main_prompt_tokens": 3778, "main_completion_tokens": 39, "judge_total_tokens": 484, "judge_prompt_tokens": 424, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In \"Scorpion King: Book of Souls\" (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 6688.969850540161, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in 'Scorpion King: Book of Souls' the Scorpion King and Tala are searching for the Book of Souls, matching the gold answer. It is faithful to the provided information and does not introduce contradictory or extraneous claims.", "main_total_tokens": 3570, "main_prompt_tokens": 3526, "main_completion_tokens": 44, "judge_total_tokens": 514, "judge_prompt_tokens": 431, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"Zombies on the Moon\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 5516.2458419799805, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the information about the movie is not available in the provided data, and therefore a release year cannot be given. The generated answer is correct (does not hallucinate a year) and faithful to the constraint of using only the provided documents.", "main_total_tokens": 2990, "main_prompt_tokens": 2953, "main_completion_tokens": 37, "judge_total_tokens": 513, "judge_prompt_tokens": 427, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain any information about a movie titled \"The Galactic baker\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 5740.00883102417, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database and thus no release year is provided. The generated answer similarly states that the provided documents do not contain information about the movie and therefore it cannot give the release year. This is effectively equivalent in correctness, as it does not hallucinate a year and correctly indicates lack of information. It is also faithful to the given context and does not introduce unsupported facts.", "main_total_tokens": 3303, "main_prompt_tokens": 3267, "main_completion_tokens": 36, "judge_total_tokens": 537, "judge_prompt_tokens": 424, "judge_completion_tokens": 113}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"Return of the Null Pointer\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 5891.968250274658, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there is no information about a movie titled 'Return of the Null Pointer' in the available sources, and thus do not fabricate a release year. The generated answer is consistent with the context limitation and does not introduce unsupported details.", "main_total_tokens": 3280, "main_prompt_tokens": 3243, "main_completion_tokens": 37, "judge_total_tokens": 512, "judge_prompt_tokens": 427, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"Data Science: The Movie\". Therefore, the release year for this specific title cannot be determined from the given context.", "latency_ms": 5898.892879486084, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the movie is not in the database, while the generated answer specifically claims that the provided documents do not contain information and limits the inability to determine the release year to the given context. This changes the scope of the claim and is not faithful to the gold response. Therefore, both correctness and faithfulness are scored as 0.", "main_total_tokens": 3239, "main_prompt_tokens": 3203, "main_completion_tokens": 36, "judge_total_tokens": 527, "judge_prompt_tokens": 426, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"Python's Revenge 2\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 5723.024606704712, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the provided documents do not contain information about 'Python's Revenge 2' and does not fabricate a release year. This aligns with the gold response that the movie is not in the database. The answer is both correct and faithful to the given context.", "main_total_tokens": 3226, "main_prompt_tokens": 3189, "main_completion_tokens": 37, "judge_total_tokens": 514, "judge_prompt_tokens": 427, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"The Last Pixel\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 5257.901430130005, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database, i.e., the system cannot provide the year. The generated answer states that the documents do not contain information about the movie and therefore it cannot provide the release year. This matches the gold behavior, does not hallucinate a year, and is consistent with the context, so it is both correct and faithful.", "main_total_tokens": 2929, "main_prompt_tokens": 2894, "main_completion_tokens": 35, "judge_total_tokens": 527, "judge_prompt_tokens": 423, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"The man who ate the sun\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 6159.578561782837, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the information about the movie is not available / not in the provided documents, and therefore do not fabricate a release year. This is correct given the context and faithful to the absence of supporting information.", "main_total_tokens": 3628, "main_prompt_tokens": 3590, "main_completion_tokens": 38, "judge_total_tokens": 507, "judge_prompt_tokens": 429, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not specify who directed the movie \"The Last Days of Lehman Brothers\". [Doc 46069]", "latency_ms": 5593.857526779175, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not specify the director, which matches the gold answer that the director information is not provided in the database. It does not introduce any unsupported information and is fully consistent with the given context.", "main_total_tokens": 3722, "main_prompt_tokens": 3693, "main_completion_tokens": 29, "judge_total_tokens": 499, "judge_prompt_tokens": 422, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about a movie titled \"A Love to Die For\" or its director. Therefore, I cannot answer your question based on the given context.", "latency_ms": 5531.699180603027, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided sources and thus the question cannot be answered from the given context. The generated answer is correct and faithful to the limitation of the database/documents.", "main_total_tokens": 3136, "main_prompt_tokens": 3100, "main_completion_tokens": 36, "judge_total_tokens": 507, "judge_prompt_tokens": 430, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not specify who directed the movie \"Journey into Gaza\". [Doc 111360]", "latency_ms": 5256.601810455322, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not specify the director and does not invent any information. This matches the gold answer's inability to provide the director, so it is both correct and faithful to the provided information.", "main_total_tokens": 3389, "main_prompt_tokens": 3362, "main_completion_tokens": 27, "judge_total_tokens": 489, "judge_prompt_tokens": 414, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about who directed the movie \"The Last Night at Tremore Beach\". [Doc 135098]", "latency_ms": 5109.842777252197, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer in content: it states that the documents do not contain information about the director, equivalent to saying the database lacks that information. It does not hallucinate a director and remains grounded in the provided documents, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2983, "main_prompt_tokens": 2951, "main_completion_tokens": 32, "judge_total_tokens": 512, "judge_prompt_tokens": 424, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The scenario where a group of archaeology students awaken an ancient mummy occurs in the movie \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 5500.573635101318, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy' and remains fully consistent with the provided information.", "main_total_tokens": 3041, "main_prompt_tokens": 3005, "main_completion_tokens": 36, "judge_total_tokens": 481, "judge_prompt_tokens": 420, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The scenario where Sarah fakes her own death to escape an abusive spouse occurs in the movie \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 5613.428115844727, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Stranger in My Bed,' which matches the gold answer. It does not introduce any conflicting or hallucinated information beyond a generic document reference, so it is fully faithful to the provided context.", "main_total_tokens": 2996, "main_prompt_tokens": 2961, "main_completion_tokens": 35, "judge_total_tokens": 505, "judge_prompt_tokens": 427, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The scenario where a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years is described in the plot of \"Game of Silence\" [Doc 123938].", "latency_ms": 5750.516891479492, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the title as 'Game of Silence,' which matches the gold answer. It does not introduce contradictions or unsupported claims beyond restating the plot description, so it is faithful to the given information.", "main_total_tokens": 2721, "main_prompt_tokens": 2678, "main_completion_tokens": 43, "judge_total_tokens": 508, "judge_prompt_tokens": 434, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie in which a true story about a killer from the site Craigslist is depicted is \"The Craigslist Killer\" [Doc 53039].", "latency_ms": 5191.532373428345, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'The Craigslist Killer.' The generated answer also identifies the movie as 'The Craigslist Killer' and merely rephrases the question with the same title, so it is correct. It does not introduce contradictions or unsupported details; mentioning a document ID does not conflict with the gold answer. Therefore, it is both fully correct and faithful.", "main_total_tokens": 3074, "main_prompt_tokens": 3042, "main_completion_tokens": 32, "judge_total_tokens": 521, "judge_prompt_tokens": 420, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The described scenario happens in the movie \"Wood Job!\" [Doc 66094].", "latency_ms": 4801.48720741272, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as \"Wood Job!\" and does not introduce extraneous or conflicting information. It is both correct and faithful.", "main_total_tokens": 3531, "main_prompt_tokens": 3510, "main_completion_tokens": 21, "judge_total_tokens": 484, "judge_prompt_tokens": 417, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The event described happens in the movie \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 5284.933567047119, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Sesame Street: The Magical Wand Chase,\" which matches the gold answer exactly. It remains faithful to the given information and does not introduce contradictions, only adding a harmless citation-style note.", "main_total_tokens": 3501, "main_prompt_tokens": 3473, "main_completion_tokens": 28, "judge_total_tokens": 510, "judge_prompt_tokens": 433, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The described scenario occurs in the movie \"Beyond\" from 2016 [Doc 74597].", "latency_ms": 4727.348327636719, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Beyond,\" matching the gold answer exactly. It does not introduce contradictions or unsupported details and remains faithful to the provided information.", "main_total_tokens": 3070, "main_prompt_tokens": 3044, "main_completion_tokens": 26, "judge_total_tokens": 492, "judge_prompt_tokens": 428, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The scenario where a woman, fired from a financial corporation during the Asia crisis, returns home with no money occurs in the movie \"6ixtynin9\" [Doc 22478].", "latency_ms": 6263.037204742432, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"6ixtynin9,\" which matches the gold answer. It stays faithful to the query and does not introduce unsupported details, aside from the document citation notation which does not conflict with the given information.", "main_total_tokens": 3220, "main_prompt_tokens": 3177, "main_completion_tokens": 43, "judge_total_tokens": 522, "judge_prompt_tokens": 441, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The described scenario happens in the movie \"Club Dread\" [Doc 32410].", "latency_ms": 4524.240255355835, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer. It stays faithful to the user's question and does not introduce contradictions or extraneous claims, aside from a harmless reference tag, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3183, "main_prompt_tokens": 3162, "main_completion_tokens": 21, "judge_total_tokens": 526, "judge_prompt_tokens": 442, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The described scenario takes place in the movie \"Toilet\" [Doc 48873].", "latency_ms": 5189.760446548462, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" which matches the gold answer exactly. It does not introduce any unsupported information or contradictions, so it is both correct and faithful to the given content.", "main_total_tokens": 3841, "main_prompt_tokens": 3819, "main_completion_tokens": 22, "judge_total_tokens": 504, "judge_prompt_tokens": 432, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The description provided matches the plot of the movie \"Electric Angel\" [Doc 1024].", "latency_ms": 4734.701156616211, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Electric Angel,\" which matches the gold answer, and does not introduce unsupported information beyond that identification.", "main_total_tokens": 3389, "main_prompt_tokens": 3367, "main_completion_tokens": 22, "judge_total_tokens": 494, "judge_prompt_tokens": 435, "judge_completion_tokens": 59}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 based on the provided documents is \"Doomsday Stories\" with a rating of 2.167 [Doc 108366].", "latency_ms": 3949.6538639068604, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'Mega Ape' as the lowest rated movie from 2023, while the generated answer incorrectly states 'Doomsday Stories'. This contradicts the gold label, so the answer is factually incorrect in this evaluation context. It also refers to 'provided documents' and a specific document ID that are not part of the given prompt, so the reasoning is not faithful to the provided information.", "main_total_tokens": 3082, "main_prompt_tokens": 3038, "main_completion_tokens": 44, "judge_total_tokens": 525, "judge_prompt_tokens": 416, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "The highest rated TV show from 2010 among the provided documents is \"Downton Abbey\" with a rating of 8.1 [Doc 119146].", "latency_ms": 3988.7773990631104, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the highest rated TV show from 2010, and the gold answer is 'Tea House'. The generated answer instead claims the show is 'Downton Abbey' with a rating of 8.1, directly contradicting the gold answer. This makes the response factually incorrect relative to the task. It also introduces a specific rating and citation that are not grounded in the provided gold answer, so it is not faithful to the given information.", "main_total_tokens": 3693, "main_prompt_tokens": 3652, "main_completion_tokens": 41, "judge_total_tokens": 538, "judge_prompt_tokens": 415, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest rated Japanese language movie from 1999 based on the provided documents is \"My Neighbors the Yamadas\" with a rating of 7.021 [Doc 22220].", "latency_ms": 6234.339237213135, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is factually incorrect with respect to the gold answer: it claims \"My Neighbors the Yamadas\" is the highest rated Japanese language movie from 1999, whereas the gold answer indicates it is \"The Uncut Story of Mewtwo's Origin.\" It is therefore not correct. It is also unfaithful to the gold/reference since it contradicts it directly, so faithfulness is low.", "main_total_tokens": 3081, "main_prompt_tokens": 3035, "main_completion_tokens": 46, "judge_total_tokens": 544, "judge_prompt_tokens": 429, "judge_completion_tokens": 115}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 in the provided context is \"Bloodeaters\" with a rating of 4.0, and its language is English [Doc 820].", "latency_ms": 5990.3199672698975, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question only asks for the language of the lowest-rated 1980 movie. The generated answer states that the language is English, matching the gold answer exactly. The extra detail about the movie title and rating does not contradict the gold and is consistent with the described document reference, so the response is both correct and faithful to the provided context.", "main_total_tokens": 3128, "main_prompt_tokens": 3086, "main_completion_tokens": 42, "judge_total_tokens": 519, "judge_prompt_tokens": 420, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was \"The Sopranos\" with a popularity score of 59.4284 [Doc 115146].", "latency_ms": 6522.623777389526, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the correct show is 'Law & Order: Special Victims Unit', but the generated answer claims it is 'The Sopranos', so the answer is factually incorrect relative to the gold. It also invents a specific popularity score and cites a document (Doc 115146) that we cannot verify from the prompt, so we cannot be sure this is grounded in the provided evidence; it appears hallucinated. Therefore, both correctness and faithfulness are 0.", "main_total_tokens": 3472, "main_prompt_tokens": 3427, "main_completion_tokens": 45, "judge_total_tokens": 554, "judge_prompt_tokens": 424, "judge_completion_tokens": 130}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 4634.023666381836, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 for \"Bluefin\" and attributes it to a document, indicating it is grounded in the provided context. Both correctness and faithfulness are satisfied.", "main_total_tokens": 3274, "main_prompt_tokens": 3252, "main_completion_tokens": 22, "judge_total_tokens": 473, "judge_prompt_tokens": 400, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in multiple years according to the documents:\n\n1. Year: 1999 [Doc 22481]\n2. Year: 2025 [Doc 112077]\n3. Year: 2019 [Doc 88811]", "latency_ms": 8476.03726387024, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer specifies a single correct year (2019). The generated answer lists multiple years (1999, 2025, 2019) without identifying which is correct, so it is not correct as a response to the question. It also contradicts the given gold label, so with respect to the task definition it is not faithful to the required answer.", "main_total_tokens": 3239, "main_prompt_tokens": 3165, "main_completion_tokens": 74, "judge_total_tokens": 540, "judge_prompt_tokens": 441, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The provided documents do not contain specific information about the number of votes \"My Dog Skip\" received. [Doc 24114]", "latency_ms": 5268.691062927246, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 279 votes, but the generated answer claims the documents do not contain this information instead of providing the number. This is incorrect relative to the gold answer. However, the answer is faithful to the cited document snippet, as it explicitly states the documents do not contain the information, so there is no hallucination beyond the evidence presented.", "main_total_tokens": 3360, "main_prompt_tokens": 3330, "main_completion_tokens": 30, "judge_total_tokens": 503, "judge_prompt_tokens": 407, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The provided documents do not specify the number of votes received by any movie titled \"Oh My God!\". The information given includes ratings and popularity scores but does not mention votes. [Doc 85434]", "latency_ms": 6008.573770523071, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that 'Oh My God!' received 12 votes, but the generated answer claims that the documents do not specify the number of votes. This directly contradicts the gold, so the answer is factually incorrect relative to the gold. However, the response is internally consistent with the cited document and does not invent a specific number; it faithfully reflects the model’s stated interpretation of the provided document, so faithfulness is high.", "main_total_tokens": 3067, "main_prompt_tokens": 3023, "main_completion_tokens": 44, "judge_total_tokens": 535, "judge_prompt_tokens": 421, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 5126.374244689941, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that \"The Last Days of Lehman Brothers\" was released in 2009, matching the gold answer. It is fully consistent with the provided information.", "main_total_tokens": 3133, "main_prompt_tokens": 3104, "main_completion_tokens": 29, "judge_total_tokens": 477, "judge_prompt_tokens": 410, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_openai-large_qwen2.5:32b", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 5005.513906478882, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the original language is English (\"en\"). The generated answer states that the original language is English, matching the gold. There are no contradictions or fabrications beyond the citation, so it is both correct and faithful.", "main_total_tokens": 3617, "main_prompt_tokens": 3593, "main_completion_tokens": 24, "judge_total_tokens": 481, "judge_prompt_tokens": 402, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "Wall Street was released in the year 1987 [Doc 6993].", "latency_ms": 3760.50066947937, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer. It does not introduce contradictions or unsupported details, so it is both correct and faithful.", "main_total_tokens": 2432, "main_prompt_tokens": 2411, "main_completion_tokens": 21, "judge_total_tokens": 462, "judge_prompt_tokens": 394, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie in which classmates have to play a survival game using their cell phones is \"Brain Fluid Explosion Girl\" [Doc 69929].", "latency_ms": 4487.603664398193, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Brain Fluid Explosion Girl,' which matches the gold answer exactly, and it stays faithful to the given information without adding unsupported details.", "main_total_tokens": 2366, "main_prompt_tokens": 2334, "main_completion_tokens": 32, "judge_total_tokens": 481, "judge_prompt_tokens": 416, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show in which six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" from 1983 [Doc 112849].", "latency_ms": 5293.3197021484375, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.4306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.719741384391281, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Dungeons & Dragons\" and does not introduce any unsupported information beyond a harmless reference to a document ID. It is both correct and faithful to the given gold answer.", "main_total_tokens": 2434, "main_prompt_tokens": 2393, "main_completion_tokens": 41, "judge_total_tokens": 498, "judge_prompt_tokens": 424, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie \"Nacht der Wölfe\" from 1982 is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones [Doc 2590].", "latency_ms": 5620.2733516693115, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe', matching the gold answer exactly. It stays faithful to the provided information and does not introduce contradictions or unsupported claims.", "main_total_tokens": 2324, "main_prompt_tokens": 2277, "main_completion_tokens": 47, "judge_total_tokens": 509, "judge_prompt_tokens": 440, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of Picnic at Hanging Rock take place in the year 1900 [Doc 126191].", "latency_ms": 4212.054491043091, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of Picnic at Hanging Rock take place in 1900, which matches the gold answer exactly. It is both accurate and fully supported by the provided information.", "main_total_tokens": 2182, "main_prompt_tokens": 2153, "main_completion_tokens": 29, "judge_total_tokens": 476, "judge_prompt_tokens": 406, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie that matches the description is \"June.\" It tells the story of June, a young violinist who works in a coffee shop in Venice, dreams about her destined love, and faces conflicts when she meets Jack, an engaged man living in her apartment building. [Doc 32483]", "latency_ms": 7360.663175582886, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.7, "justification": "The generated answer correctly identifies the movie as 'June,' matching the gold answer. However, the explanation adds specific plot details (e.g., character Jack, being engaged, living in her apartment building) and cites a document ID that are not present in the question, so some of the reasoning appears at least partially hallucinated relative to the provided prompt.", "main_total_tokens": 2638, "main_prompt_tokens": 2570, "main_completion_tokens": 68, "judge_total_tokens": 597, "judge_prompt_tokens": 494, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1462.5160694122314, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model response is factually incorrect because it fails to provide the correct movie title, instead answering with ignorance. However, it is faithful to its own stated uncertainty and does not fabricate an incorrect movie title, so faithfulness is high while correctness is low.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 399, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 550.9147644042969, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 7.6, but the generated answer says 'I don't know,' which does not provide the correct value, so correctness is 0. However, responding with lack of knowledge instead of fabricating an answer is faithful to the model's uncertainty, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 389, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1387.627363204956, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 0.9828920819566879, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3364045224324153, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the correct gold answer (4), so it is factually incorrect. However, the model is at least addressing the same question (number of movies released in 1987 with rating over 7 in Chinese), so it is faithful to the user query even though the numeric result is wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 394, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 369.72832679748535, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered 0 while the correct gold answer is 2, so the response is factually incorrect. However, the question is straightforward and there is no indication that the model contradicted any information it had stated earlier, so it remains internally faithful to its (incorrect) reasoning.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 395, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1626.0721683502197, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer explicitly states a lack of knowledge rather than providing an incorrect one. Given the gold answer is a specific show, saying 'I don't know' is factually incorrect as a response to the question (so correctness is 0), but it is fully faithful to the model's own stated uncertainty and does not fabricate information (so faithfulness is 1).", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 493, "judge_prompt_tokens": 389, "judge_completion_tokens": 104}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie that is about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" from 1990 [Doc 11205].", "latency_ms": 5088.345050811768, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise', which matches the gold answer exactly. It does not introduce any extraneous or contradictory information and remains faithful to the given data.", "main_total_tokens": 2350, "main_prompt_tokens": 2312, "main_completion_tokens": 38, "judge_total_tokens": 483, "judge_prompt_tokens": 415, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 5548.513412475586, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as \"Won't You Be My Neighbor?\" and stays fully grounded in the provided information.", "main_total_tokens": 2522, "main_prompt_tokens": 2479, "main_completion_tokens": 43, "judge_total_tokens": 504, "judge_prompt_tokens": 440, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The question \"What year was it released?\" does not provide enough context to determine which specific movie or show is being referred to. Multiple entries in the provided documents have different release years. For example:\n\n- \"Year of the Horse\" was released in 1997 [Doc 19452].\n- \"YEAR\" was released in 2023 [Doc 107925].\n- \"What It Was\" was released in 2014 [Doc 65131].\n\nWithout additional context about which specific title is being asked about, it's not possible to provide a single year of release.", "latency_ms": 12388.088941574097, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "Gold answer expects the specific release year 2018, so the correct response should have been '2018'. The generated answer instead claims there is insufficient context and lists various unrelated examples, thus it is incorrect relative to the gold. However, given only the question text as context, the assistant faithfully describes its own reasoning and does not hallucinate a specific year, so it is fully faithful to the (ambiguous) query.", "main_total_tokens": 2732, "main_prompt_tokens": 2598, "main_completion_tokens": 134, "judge_total_tokens": 609, "judge_prompt_tokens": 493, "judge_completion_tokens": 116}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series \"Astro Boy\" from 1980 is an animation about a young boy robot in 2030 AD [Doc 112591].", "latency_ms": 5312.960624694824, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy', which matches the gold answer exactly. The extra detail about it being from 1980 is not contradicted by the gold and is consistent with common knowledge, so the response is both correct and faithful to the task.", "main_total_tokens": 2571, "main_prompt_tokens": 2531, "main_completion_tokens": 40, "judge_total_tokens": 506, "judge_prompt_tokens": 418, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 4570.157051086426, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer exactly. It is fully consistent with the provided information and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 2086, "main_prompt_tokens": 2050, "main_completion_tokens": 36, "judge_total_tokens": 499, "judge_prompt_tokens": 424, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26m before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 5388.818979263306, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the provided gold value of £26m and restates it accurately with consistent context about the Brink's-Mat security depot before the 26 November 1983 break-in. No unsupported information is added.", "main_total_tokens": 2382, "main_prompt_tokens": 2338, "main_completion_tokens": 44, "judge_total_tokens": 504, "judge_prompt_tokens": 428, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6499.612092971802, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific numerical popularity score (80.5772). The generated answer says 'I don't know,' which does not match the correct value, so correctness is 0. However, the model did not hallucinate a value and honestly stated uncertainty, so it is fully faithful to its own stated knowledge, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 500, "judge_prompt_tokens": 396, "judge_completion_tokens": 104}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 285.22467613220215, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that 1 Russian language TV show was released in 1997, while the generated answer is 0. This is factually incorrect relative to the gold, so correctness is 0. The generated answer is a single numeric value and does not contradict its own reasoning or add unsupported explanation, so it is fully faithful to itself, giving faithfulness 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 385, "judge_completion_tokens": 107}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 5437.62993812561, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Angel Eyes' and does not introduce unsupported information.", "main_total_tokens": 2608, "main_prompt_tokens": 2568, "main_completion_tokens": 40, "judge_total_tokens": 486, "judge_prompt_tokens": 429, "judge_completion_tokens": 57}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French [Doc 110681].", "latency_ms": 3883.219003677368, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is factually incorrect because it states the language is French, while the gold label indicates the language is Korean. It is also unfaithful to the provided gold answer, directly contradicting it.", "main_total_tokens": 2407, "main_prompt_tokens": 2385, "main_completion_tokens": 22, "judge_total_tokens": 461, "judge_prompt_tokens": 393, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 671.8447208404541, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.5, "precision_at_5": 0.2, "ndcg_at_5": 0.6131471927654584, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.6131471927654584, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2 but the generated answer is 1, so the response is factually incorrect w.r.t. the labeled ground truth. The evaluation itself is faithful to the user-provided gold and generated answers.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 470, "judge_prompt_tokens": 394, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The provided documents do not contain any movie released in 1979. Therefore, based strictly on the given context, there is no best-rated movie from 1979 that can be identified.", "latency_ms": 5421.0045337677, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that, within the provided context/database, there is no movie from 1979, so no best-rated movie can be identified. The generated answer is faithful to the constraint of using only the given documents and does not hallucinate any movie or rating.", "main_total_tokens": 2442, "main_prompt_tokens": 2399, "main_completion_tokens": 43, "judge_total_tokens": 522, "judge_prompt_tokens": 433, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The provided context does not contain any information about a movie titled \"Back to the Future Part IV\" or its release year. [Doc X]", "latency_ms": 4825.070381164551, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer correctly states that Back to the Future Part IV does not exist in the database. The generated answer states that the provided context has no information about such a movie or its release year, which is consistent with the gold and does not hallucinate a year. Both are effectively saying that the information is unavailable, so the generated response is correct and faithful to the gold.", "main_total_tokens": 2767, "main_prompt_tokens": 2737, "main_completion_tokens": 30, "judge_total_tokens": 535, "judge_prompt_tokens": 429, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 15.330314636230469, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there are no movies from 1980 about the Chernobyl disaster, which is accurate because the Chernobyl disaster occurred in 1986. The generated answer is faithful to the question, explaining the mismatch in time period without adding incorrect details.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 524, "judge_prompt_tokens": 434, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 387.0213031768799, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is numerically incorrect compared to the gold answer (0 vs 4), so correctness is 0. The model is faithfully reporting the provided generated answer without distortion, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 462, "judge_prompt_tokens": 388, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 401.14378929138184, "retrieval_recall": 0.0, "retrieval_mrr": 0.125, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.19342640361727081, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2 and the generated answer is 1, so the generated answer is factually incorrect relative to the labeled gold. However, it is directly responsive to the user question and does not introduce unrelated or fabricated content, so it is faithful to the query itself.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 473, "judge_prompt_tokens": 386, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 216.5052890777588, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show about Super Mario Bros released in the 1990s, while the generated answer says 0. These are in direct conflict, so the generated answer is incorrect relative to the gold. The evaluation itself accurately reflects this mismatch, so it is faithful to the provided gold and prediction.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 486, "judge_prompt_tokens": 389, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 353.717565536499, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0 and the generated answer is also 0. Given typical knowledge of TV shows about Super Mario Bros and no known series released between 2000 and 2010, the generated answer matches the gold and is correct. The explanation is consistent with the provided information, so faithfulness is also maximal.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 391, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 389.83988761901855, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto', but the generated answer says 'I don't know,' which is incorrect because it fails to provide the known correct response. However, the answer is internally faithful to its own uncertainty and does not hallucinate or contradict the prompt, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 386, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The provided documents do not contain information about a Jurassic Park/Power Ranger crossover movie release. [Doc X]", "latency_ms": 4070.6253051757812, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that the movie was not released; the model instead said the documents do not contain information, which is not the same as stating it was never released. However, given the prompt that the model only had the provided documents, its response is faithful to that context.", "main_total_tokens": 2424, "main_prompt_tokens": 2401, "main_completion_tokens": 23, "judge_total_tokens": 494, "judge_prompt_tokens": 407, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "None of the provided documents mention a movie where Barbie fights the Predator. [Doc 68094] [Doc 44920] [Doc 78562] [Doc 73104] [Doc 25632] [Doc 72290] [Doc 36330] [Doc 52809] [Doc 65585] [Doc 94538] [Doc 48119] [Doc 73700]", "latency_ms": 11842.493534088135, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.0, "faithfulness_score": 0.0, "justification": "The gold answer indicates that the correct response is that such a movie was not released, implying there is no existing movie where Barbie fights the Predator. The generated answer instead claims that none of the provided documents mention such a movie, which addresses only the retrieval context but not the actual question. This does not correctly answer the question and is not faithful to the gold answer’s core information that the movie does not exist or was not released.", "main_total_tokens": 2878, "main_prompt_tokens": 2755, "main_completion_tokens": 123, "judge_total_tokens": 589, "judge_prompt_tokens": 468, "judge_completion_tokens": 121}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "Based on the provided context, there is only one Star Wars-related movie mentioned, which is \"Star Wreck IV: The Kilpailu\" from 1996 [Doc 19112]. However, this is not an official Star Wars movie but a Finnish parody. Therefore, the context does not provide information about the 4th oldest official Star Wars movie.", "latency_ms": 8659.016370773315, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The user asks for the 4th oldest Star Wars movie, and the gold answer is 'Star Wars: Droids - The Great Heep'. The generated answer instead claims the context only includes 'Star Wreck IV: The Kilpailu' and concludes the context does not provide the information needed. This does not answer the actual question nor give the correct title, so it is incorrect. However, the response accurately reflects what it says about the (limited) provided context and does not hallucinate an incorrect 4th oldest movie; it faithfully states that the requested information is not present. Thus correctness is 0 while faithfulness to the described context is 1.", "main_total_tokens": 2835, "main_prompt_tokens": 2755, "main_completion_tokens": 80, "judge_total_tokens": 628, "judge_prompt_tokens": 462, "judge_completion_tokens": 166}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The question \"What year was it released?\" does not provide enough context to determine which specific movie or show is being referred to. Multiple entries in the provided documents have different release years. For example:\n\n- \"Year of the Horse\" was released in 1997 [Doc 19452].\n- \"YEAR\" was released in 2023 [Doc 107925].\n- \"What It Was\" was released in 2014 [Doc 65131].\n\nWithout additional context about which specific title is being asked about, it's not possible to provide a single year of release.", "latency_ms": 12353.158235549927, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates a specific year, 1986, so the correct response should have been that year. The generated answer instead claims there is insufficient context and provides several example years from different titles, failing to give the correct one, so it is incorrect. However, the answer is faithful to the premise that the model is limited to the provided documents and honestly explains the ambiguity, without hallucinating a specific year, so faithfulness is high.", "main_total_tokens": 2732, "main_prompt_tokens": 2598, "main_completion_tokens": 134, "judge_total_tokens": 613, "judge_prompt_tokens": 493, "judge_completion_tokens": 120}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 330.3523063659668, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered that it did not know, which does not provide the correct rating (8.466). However, it did not fabricate an incorrect rating and was honest about its lack of knowledge, so it is faithful but incorrect.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 398, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 4740.425109863281, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med', which matches the gold answer exactly. It is faithful to the provided information and does not introduce contradictory or extraneous claims.", "main_total_tokens": 2204, "main_prompt_tokens": 2167, "main_completion_tokens": 37, "judge_total_tokens": 492, "judge_prompt_tokens": 423, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "The seven prisoners escaped from a prison in Argentina [Doc 53582].", "latency_ms": 4217.235088348389, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, correctly identifying Argentina as the country. It is fully supported by the provided information without adding unsupported details.", "main_total_tokens": 2821, "main_prompt_tokens": 2802, "main_completion_tokens": 19, "judge_total_tokens": 466, "judge_prompt_tokens": 406, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In Sweet Trap (2024), the apprentice falls into Jiang Jie's trap [Doc 135231].", "latency_ms": 4393.960475921631, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, stating that the apprentice falls into Jiang Jie's trap. It is both correct and faithful to the provided information.", "main_total_tokens": 2378, "main_prompt_tokens": 2349, "main_completion_tokens": 29, "judge_total_tokens": 467, "judge_prompt_tokens": 405, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 685.8563423156738, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.7095272044910244, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.7095272044910244, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks: 'How many korean-language movies mention assassins after 2023?' with a gold answer of 4. The generated answer is 0, which is factually incorrect given the provided gold answer, so correctness is 0. However, the evaluation here is based solely on the given question, gold, and generated outputs; the assessment of correctness vs. the gold is faithfully reported, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 504, "judge_prompt_tokens": 385, "judge_completion_tokens": 119}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "47", "latency_ms": 169.03018951416016, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The user asks how many movies feature both Steven Seagal and Bruce Lee. The gold answer correctly states that there are no movies with both actors. The generated answer says '47', which is factually wrong. It also contradicts the gold answer and is not grounded in the provided information, so it is unfaithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 395, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 4221.654415130615, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (6.3) and does not introduce any contradictions or extraneous claims. It is both correct and faithful to the provided gold answer.", "main_total_tokens": 2353, "main_prompt_tokens": 2327, "main_completion_tokens": 26, "judge_total_tokens": 470, "judge_prompt_tokens": 403, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in \"The Last Race\" is stock car racing [Doc 86410].", "latency_ms": 4030.4315090179443, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer specifies the style of racing as 'Drift', while the generated answer claims it is stock car racing in 'The Last Race'. This directly contradicts the gold answer, so correctness is 0. The reference to a specific document suggests some grounding, but since we only evaluate against the gold label here and the answer diverges from it, faithfulness is partial at best.", "main_total_tokens": 2556, "main_prompt_tokens": 2532, "main_completion_tokens": 24, "judge_total_tokens": 509, "judge_prompt_tokens": 398, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 5934.966802597046, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Confronting a Serial Killer' and does not introduce contradictions or unsupported claims.", "main_total_tokens": 2372, "main_prompt_tokens": 2321, "main_completion_tokens": 51, "judge_total_tokens": 515, "judge_prompt_tokens": 452, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 4542.896032333374, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold value 7.838 and clearly attributes it as the popularity score of Armageddon (1998), staying faithful to the provided information.", "main_total_tokens": 2540, "main_prompt_tokens": 2510, "main_completion_tokens": 30, "judge_total_tokens": 473, "judge_prompt_tokens": 407, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 4867.469310760498, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('The Sixth Sense') and correctly addresses the question. It is fully supported by the given information, so it is both correct and faithful.", "main_total_tokens": 2283, "main_prompt_tokens": 2248, "main_completion_tokens": 35, "judge_total_tokens": 487, "judge_prompt_tokens": 420, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 4042.1791076660156, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2002) exactly and does not introduce any contradictions or unsupported claims beyond a generic document reference.", "main_total_tokens": 2575, "main_prompt_tokens": 2552, "main_completion_tokens": 23, "judge_total_tokens": 455, "judge_prompt_tokens": 397, "judge_completion_tokens": 58}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 4936.537027359009, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2015 reboot title as 'The Transporter Refueled' and does not introduce unsupported information beyond an irrelevant document reference.", "main_total_tokens": 2616, "main_prompt_tokens": 2581, "main_completion_tokens": 35, "judge_total_tokens": 489, "judge_prompt_tokens": 420, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The movie 'The Transporter Refueled' was released in the year 2015 [Doc 67500].", "latency_ms": 4464.962959289551, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It does not introduce contradictions or extraneous, unsupported claims, so it is both correct and faithful.", "main_total_tokens": 2472, "main_prompt_tokens": 2442, "main_completion_tokens": 30, "judge_total_tokens": 485, "judge_prompt_tokens": 408, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "None of the romantic comedies listed in the provided context have Abraham Lincoln coming back from the dead. [Doc 94414] mentions former U.S. presidents coming back as zombies but does not specify Abraham Lincoln and is categorized under 'Horror' and 'Comedy', not a romantic comedy. Therefore, based on the given documents, there is no romantic comedy where Abraham Lincoln comes back from the dead.", "latency_ms": 8509.959697723389, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a romantic comedy where Abraham Lincoln comes back from the dead. The gold answer says no such movie description can be found. The generated answer correctly concludes that there is no romantic comedy in the provided context featuring Abraham Lincoln coming back from the dead, and explains that the only similar reference is a horror-comedy about former U.S. presidents as zombies, not a romantic comedy and not specifically Abraham Lincoln. This matches the gold answer in substance and is grounded in the described document context, so it is both correct and faithful.", "main_total_tokens": 2330, "main_prompt_tokens": 2244, "main_completion_tokens": 86, "judge_total_tokens": 609, "judge_prompt_tokens": 471, "judge_completion_tokens": 138}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by Ernest Hemingway is \"Genius\" [Doc 73141]. This movie revolves around a literary editor, Max Perkins, who discovers a sprawling, chaotic 1,000-page manuscript by an unknown writer and believes he has found a literary genius. Although the manuscript itself isn't explicitly stated to be by Hemingway in the provided context, the plot mentions that Perkins was the first to sign Ernest Hemingway, among other literary greats, suggesting a connection to Hemingway's work.", "latency_ms": 11069.990873336792, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct answer per the gold is 'Havana Darkness', but the generated answer gives 'Genius', so it is incorrect. It also invents a connection between the movie 'Genius' and a Hemingway manuscript that is not supported in the provided rationale, lowering faithfulness; it correctly describes 'Genius' somewhat but misattributes its relevance to the question.", "main_total_tokens": 2419, "main_prompt_tokens": 2299, "main_completion_tokens": 120, "judge_total_tokens": 599, "judge_prompt_tokens": 493, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "The TV show in which Doll heads to Hollywood to be with her childhood best friend Em is titled \"Doll & Em\" [Doc 122259].", "latency_ms": 4810.307741165161, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em', which matches the gold answer exactly and does not introduce any unsupported information.", "main_total_tokens": 2265, "main_prompt_tokens": 2229, "main_completion_tokens": 36, "judge_total_tokens": 480, "judge_prompt_tokens": 420, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The movie you are referring to is \"Hallam Foe,\" which was released in 2007. In this film, Hallam uses his talent for spying on people as he attempts to uncover the true cause of his mother's death. However, his journey leads him to search for love across the rooftops of the city instead [Doc 39389].", "latency_ms": 8264.30630683899, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the 2007 movie where Hallam, who has a talent for spying on people, seeks the cause of his mother’s death. The generated answer correctly identifies the film as \"Hallam Foe\" and provides an accurate brief description consistent with the given gold answer. There are no contradictions or fabrications beyond the information and it aligns with the source doc reference. Thus, it is both correct and faithful.", "main_total_tokens": 2555, "main_prompt_tokens": 2477, "main_completion_tokens": 78, "judge_total_tokens": 587, "judge_prompt_tokens": 470, "judge_completion_tokens": 117}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The provided context does not contain any information about an individual's last name that might initially sound like they are a friend or a foe. Therefore, based strictly on the given documents, there is no answer to the question.", "latency_ms": 5701.618432998657, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is an explicit contentful label ('A foe'). The generated answer claims there is no answer and does not provide 'A foe', so it is incorrect relative to the gold. However, if we assume the model only had the referenced documents (not shown) and that they indeed lacked this information, then the response is faithful to the provided context because it does not invent an answer and explicitly states insufficiency of information.", "main_total_tokens": 2448, "main_prompt_tokens": 2403, "main_completion_tokens": 45, "judge_total_tokens": 546, "judge_prompt_tokens": 429, "judge_completion_tokens": 117}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "In the 2017 TV show Atypical, Sam is 18 years old [Doc 124583].", "latency_ms": 4289.020776748657, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, which matches the gold answer. It does not introduce any contradictory or extraneous information, and the reference tag does not affect factual content.", "main_total_tokens": 2234, "main_prompt_tokens": 2204, "main_completion_tokens": 30, "judge_total_tokens": 491, "judge_prompt_tokens": 410, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam is on the autism spectrum [Doc 124583].", "latency_ms": 3113.323211669922, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the 2017 TV show Atypical, Sam is on the autism spectrum, which exactly matches the gold answer. It is both factually correct and faithful to the given gold reference.", "main_total_tokens": 2067, "main_prompt_tokens": 2050, "main_completion_tokens": 17, "judge_total_tokens": 475, "judge_prompt_tokens": 399, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "The place that Sandra was born in the movie \"À corde tendue\" is Chamonix [Doc 59659].", "latency_ms": 4727.077484130859, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, correctly identifying Chamonix as Sandra's birthplace. It also explicitly cites the supporting document, so it is both correct and faithful to the provided source.", "main_total_tokens": 2739, "main_prompt_tokens": 2709, "main_completion_tokens": 30, "judge_total_tokens": 492, "judge_prompt_tokens": 422, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In this Japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 4326.921463012695, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and correctly completes the sentence. It is fully consistent with the provided information, with no hallucinations beyond a harmless document reference.", "main_total_tokens": 2599, "main_prompt_tokens": 2573, "main_completion_tokens": 26, "judge_total_tokens": 480, "judge_prompt_tokens": 412, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 4964.598894119263, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which matches the gold answer exactly. It does not introduce any contradictions or unsupported details and appears to be directly grounded in the referenced document.", "main_total_tokens": 2538, "main_prompt_tokens": 2503, "main_completion_tokens": 35, "judge_total_tokens": 499, "judge_prompt_tokens": 424, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is titled \"One Afternoon\" [Doc 96838].", "latency_ms": 5826.47967338562, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title \"One Afternoon\" and does not introduce any contradictions or unsupported claims. It is correct and fully faithful to the provided information.", "main_total_tokens": 2154, "main_prompt_tokens": 2103, "main_completion_tokens": 51, "judge_total_tokens": 513, "judge_prompt_tokens": 449, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie where a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is titled \"The Life Ahead\" [Doc 93260].", "latency_ms": 3590.7065868377686, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Life Ahead,\" which matches the gold answer. It does not introduce unsupported claims beyond citing a document reference, so it is both correct and faithful.", "main_total_tokens": 2347, "main_prompt_tokens": 2305, "main_completion_tokens": 42, "judge_total_tokens": 505, "judge_prompt_tokens": 435, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary “McLaren” is about Bruce McLaren, a New Zealander who founded the McLaren Motor Racing team. [Doc 73859]", "latency_ms": 5357.44571685791, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies that the 2016 documentary 'McLaren' is about Bruce McLaren and gives accurate additional context about him founding the McLaren motor racing team. It is consistent with the gold answer and does not introduce contradictions or hallucinations relative to the prompt.", "main_total_tokens": 2497, "main_prompt_tokens": 2457, "main_completion_tokens": 40, "judge_total_tokens": 505, "judge_prompt_tokens": 419, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The provided context does not specify the name of Sonny Hayes' racing team in the movie \"F1\" [Doc 111503]. Therefore, based solely on the given information, the name of his racing team is not found.", "latency_ms": 6390.91157913208, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the team is the McLaren Motor Racing team. The generated answer claims the name is not found in the provided context. Relative to the gold, this is factually incorrect, so correctness is 0. However, the generated answer is consistent with its stated reliance on the given context and does not hallucinate a name; it accurately reflects that the context did not specify the team name, so faithfulness is 1.", "main_total_tokens": 2673, "main_prompt_tokens": 2621, "main_completion_tokens": 52, "judge_total_tokens": 549, "judge_prompt_tokens": 429, "judge_completion_tokens": 120}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The provided context does not specify a particular individual and hence does not provide information about what country \"he\" is from. The question cannot be answered with the given documents.", "latency_ms": 4948.185682296753, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'New Zealand', so the correct country was knowable. The generated answer incorrectly states that the question cannot be answered, so it is factually wrong (correctness 0). However, it is faithful to the (assumed) lack of explicit mention in the provided context, so it does not hallucinate and remains faithful (faithfulness 1).", "main_total_tokens": 2580, "main_prompt_tokens": 2545, "main_completion_tokens": 35, "judge_total_tokens": 516, "judge_prompt_tokens": 410, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based on the provided context, there is no information that suggests John McClane blows up the Great Wall of China in any 2013 movie. The plot descriptions for \"A Good Day to Die Hard,\" which is a 2013 movie featuring John McClane, do not mention anything about blowing up the Great Wall of China [Doc 58791].", "latency_ms": 8371.315717697144, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer is effectively correct: there is no 2013 movie where John McClane blows up the Great Wall of China, and it correctly points to 'A Good Day to Die Hard' as the 2013 McClane film whose plot does not include that event. However, the gold answer is a more direct admission of absence of data, while the generated answer adds a specific citation [Doc 58791] and references provided context that we do not actually see here, making the grounding slightly uncertain. Thus correctness is 1, but faithfulness is slightly less than perfect.", "main_total_tokens": 2693, "main_prompt_tokens": 2614, "main_completion_tokens": 79, "judge_total_tokens": 623, "judge_prompt_tokens": 471, "judge_completion_tokens": 152}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie that matches the given description is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 4799.214124679565, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the film as 'Tomie: Forbidden Fruit' and does not introduce any unsupported information beyond reiterating the title.", "main_total_tokens": 2560, "main_prompt_tokens": 2527, "main_completion_tokens": 33, "judge_total_tokens": 509, "judge_prompt_tokens": 444, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie \"Lei mi parla ancora,\" Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 5651.762247085571, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, matching the gold answer. It also accurately reflects the information without adding unsupported claims, so it is both correct and faithful.", "main_total_tokens": 2936, "main_prompt_tokens": 2897, "main_completion_tokens": 39, "judge_total_tokens": 498, "judge_prompt_tokens": 424, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for the legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 5738.70849609375, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in 'Scorpion King: Book of Souls' (2018), The Scorpion King and Tala are searching for the Book of Souls, matching the gold answer. The added description 'legendary relic' does not contradict the gold answer and is a reasonable elaboration, so the response is both correct and faithful.", "main_total_tokens": 2794, "main_prompt_tokens": 2752, "main_completion_tokens": 42, "judge_total_tokens": 529, "judge_prompt_tokens": 429, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"Zombies on the Moon\". Therefore, the release year of \"Zombies on the Moon\" is not found in the given context.", "latency_ms": 5173.789501190186, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the release year is not available; the generated answer stays faithful to the provided context and does not fabricate a year.", "main_total_tokens": 2306, "main_prompt_tokens": 2266, "main_completion_tokens": 40, "judge_total_tokens": 493, "judge_prompt_tokens": 430, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided context does not contain any information about a release year for \"The Galactic baker\". [Doc X]", "latency_ms": 4085.615396499634, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer correctly states that the movie is not in the database. The generated answer instead claims that the provided context does not contain the release year and cites a non-existent document, which is both incorrect relative to the task (should admit not in database) and unfaithful to the given information.", "main_total_tokens": 2584, "main_prompt_tokens": 2561, "main_completion_tokens": 23, "judge_total_tokens": 502, "judge_prompt_tokens": 411, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about the release year of \"Return of the Null Pointer\". [Doc X]", "latency_ms": 4226.589918136597, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.0, "faithfulness_score": 0.0, "justification": "The gold answer correctly states that the movie is not in the database. The generated answer incorrectly references 'provided documents' and a '[Doc X]' citation that do not exist in the prompt, which is unfaithful to the given context. It also fabricates a source instead of honestly acknowledging lack of information, so both correctness and faithfulness are 0.", "main_total_tokens": 2542, "main_prompt_tokens": 2518, "main_completion_tokens": 24, "judge_total_tokens": 521, "judge_prompt_tokens": 414, "judge_completion_tokens": 107}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about the release year of \"Data Science: The Movie\". [Doc 76666] to [Doc 99556] list various movies related to data, science, and documentaries but do not mention \"Data Science: The Movie\" specifically. Therefore, based on the given context, the release year cannot be determined.", "latency_ms": 8277.003288269043, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly indicate that the release year of 'Data Science: The Movie' cannot be provided based on the available information. The generated answer is faithful to the described context, explicitly stating that the relevant documents do not mention the movie and therefore the release year cannot be determined. The reasoning does not introduce unsupported facts and stays within the bounds of the given context.", "main_total_tokens": 2556, "main_prompt_tokens": 2478, "main_completion_tokens": 78, "judge_total_tokens": 569, "judge_prompt_tokens": 462, "judge_completion_tokens": 107}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about the release year of \"Python's Revenge 2\". [Doc X]", "latency_ms": 4056.492567062378, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the release year is not available / not in the documents, and the generated answer faithfully reflects that limitation without inventing a date.", "main_total_tokens": 2383, "main_prompt_tokens": 2359, "main_completion_tokens": 24, "judge_total_tokens": 480, "judge_prompt_tokens": 414, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"The Last Pixel.\" Therefore, the release year for \"The Last Pixel\" cannot be determined from the given context.", "latency_ms": 4817.022085189819, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the information about 'The Last Pixel' is not available / not in the provided documents, and thus the release year cannot be determined. This is correct given the supposed context and is fully faithful to the idea that the model should not hallucinate an answer when the movie is not in the database or documents.", "main_total_tokens": 2190, "main_prompt_tokens": 2154, "main_completion_tokens": 36, "judge_total_tokens": 524, "judge_prompt_tokens": 424, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about the release year of \"The man who ate the sun\". [Doc X]", "latency_ms": 4407.00101852417, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the release year is not available / not in the provided information. The generated response remains faithful to the constraint of using only the provided documents and explicitly notes that they lack the needed information.", "main_total_tokens": 2756, "main_prompt_tokens": 2731, "main_completion_tokens": 25, "judge_total_tokens": 493, "judge_prompt_tokens": 416, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not specify who directed the movie \"The Last Days of Lehman Brothers\". [Doc 46069]", "latency_ms": 4890.20037651062, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the provided context does not specify who directed the movie, which aligns with the gold response indicating that the database does not provide the director information. It does not introduce any unsupported claims and remains faithful to the given context.", "main_total_tokens": 2826, "main_prompt_tokens": 2797, "main_completion_tokens": 29, "judge_total_tokens": 502, "judge_prompt_tokens": 422, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about who directed the movie \"A Love to Die For\". [Doc Not Found]", "latency_ms": 4005.8350563049316, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided documents/database. The generated response correctly reflects the limitation of the source and does not hallucinate an answer, so it is both correct and faithful.", "main_total_tokens": 2378, "main_prompt_tokens": 2353, "main_completion_tokens": 25, "judge_total_tokens": 496, "judge_prompt_tokens": 419, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about who directed the movie \"Journey into Gaza\". [Doc 111360]", "latency_ms": 4490.211486816406, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided data. The generated answer correctly notes that the documents do not contain the director information, matching the gold answer in substance. It is correct and faithful to the described source limitations.", "main_total_tokens": 2526, "main_prompt_tokens": 2497, "main_completion_tokens": 29, "judge_total_tokens": 499, "judge_prompt_tokens": 416, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about who directed the movie \"The Last Night at Tremore Beach\". The document only provides details such as its type (TV), year (2024), rating (7.0), popularity (5.1693), language (es), and a brief plot description, but does not mention the director [Doc 135098].", "latency_ms": 7988.698482513428, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not include the director information and accurately summarizes only the details present in the cited document. It does not introduce any unsupported claims, so it is both correct and faithful to the provided gold/database context.", "main_total_tokens": 2170, "main_prompt_tokens": 2088, "main_completion_tokens": 82, "judge_total_tokens": 549, "judge_prompt_tokens": 470, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 4619.346380233765, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy' and adds only a benign citation-style note. It is both correct and faithful to the provided gold response.", "main_total_tokens": 2317, "main_prompt_tokens": 2283, "main_completion_tokens": 34, "judge_total_tokens": 486, "judge_prompt_tokens": 418, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The scenario where Sarah fakes her own death to escape an abusive spouse occurs in the movie \"Stranger in My Bed\" [Doc 35237]. However, it's important to note that a similar plot of a woman faking her death to escape abuse also appears in other movies such as \"Sleeping with the Enemy\" [Doc 11770], \"Hidden Away\" [Doc 61193], and \"Kill Me Twice\" [Doc 89760]. But based on the exact name \"Sarah,\" the movie is \"Stranger in My Bed.\"", "latency_ms": 11617.02013015747, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.6, "justification": "The generated answer correctly identifies the movie as 'Stranger in My Bed', matching the gold answer, so correctness is 1. However, it adds references to multiple other movies with similar plots and cites fabricated document IDs and sources (e.g., [Doc 35237], [Doc 11770]), which are not grounded in the provided context. This extra, unverified information lowers faithfulness.", "main_total_tokens": 2456, "main_prompt_tokens": 2330, "main_completion_tokens": 126, "judge_total_tokens": 620, "judge_prompt_tokens": 507, "judge_completion_tokens": 113}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The scenario described occurs in the movie \"Game of Silence\" [Doc 123938]. However, it is important to note that according to the provided context, \"Game of Silence\" is listed as a TV type and not a movie. Despite this discrepancy, it is the entry that matches the given plot description.", "latency_ms": 6877.511501312256, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer identifies 'Game of Silence' as the correct work, but it is a TV series, not a movie. The generated answer repeats this mismatch and still calls it a movie, so it is factually incorrect regarding the question as asked. It is partially faithful to the provided context by noting that 'Game of Silence' is listed as TV type, but it nonetheless asserts it as a movie, introducing an unfaithful element.", "main_total_tokens": 2083, "main_prompt_tokens": 2015, "main_completion_tokens": 68, "judge_total_tokens": 581, "judge_prompt_tokens": 460, "judge_completion_tokens": 121}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie where the following happens: Based on a true story about a killer from the site Craigslist is \"The Craigslist Killer\" [Doc 53039].", "latency_ms": 4621.424436569214, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer', which matches the gold answer exactly. It stays faithful to the question and does not introduce incorrect details; the added reference tag does not alter the content.", "main_total_tokens": 2208, "main_prompt_tokens": 2173, "main_completion_tokens": 35, "judge_total_tokens": 497, "judge_prompt_tokens": 423, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which Yuki Hirano sets his eyes on the role of a forestry trainee after failing his university entrance exam is \"Wood Job!\" [Doc 66094].", "latency_ms": 5508.911848068237, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Wood Job!' and directly matches the gold answer. There is no hallucinated or contradictory information; the citation marker does not change the substance of the response, so it is fully faithful to the prompt and gold.", "main_total_tokens": 2676, "main_prompt_tokens": 2636, "main_completion_tokens": 40, "judge_total_tokens": 517, "judge_prompt_tokens": 435, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure is titled \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 6667.187213897705, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase.' It does not introduce any contradictions or extraneous claims beyond slightly rephrasing the question in statement form, so it is both correct and faithful.", "main_total_tokens": 2691, "main_prompt_tokens": 2636, "main_completion_tokens": 55, "judge_total_tokens": 544, "judge_prompt_tokens": 460, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a 7-year-old boy must find the hero within him to save his parents who are trapped inside a burning car due to a rock fall is titled \"Beyond\" [Doc 74597].", "latency_ms": 5649.9810218811035, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title given the described plot, and the gold answer is \"Beyond.\" The generated answer correctly identifies the movie as \"Beyond\" and clearly restates the plot, staying consistent with the given information without adding unsupported claims. Therefore, it is both correct and faithful.", "main_total_tokens": 2346, "main_prompt_tokens": 2299, "main_completion_tokens": 47, "judge_total_tokens": 540, "judge_prompt_tokens": 451, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie where a woman, fired from a financial corporation during the Asia crisis, returns home with no money is \"6ixtynin9\" [Doc 22478].", "latency_ms": 5088.025093078613, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, identifying the movie as '6ixtynin9'. It is fully correct and faithful to the given information.", "main_total_tokens": 2247, "main_prompt_tokens": 2207, "main_completion_tokens": 40, "judge_total_tokens": 501, "judge_prompt_tokens": 438, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie in which the described scenario happens is \"Club Dread\" [Doc 32410].", "latency_ms": 4067.5244331359863, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer exactly. It does not introduce any contradictions or unsupported claims beyond a harmless citation placeholder, so it is both correct and faithful to the source description.", "main_total_tokens": 2452, "main_prompt_tokens": 2429, "main_completion_tokens": 23, "judge_total_tokens": 522, "judge_prompt_tokens": 444, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is titled \"Toilet\" from the year 2010 [Doc 48873].", "latency_ms": 7694.065570831299, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet\" and matches the gold answer exactly. It does not introduce contradictions or unsupported claims beyond a minor citation-style note, so it is both correct and faithful to the provided information.", "main_total_tokens": 2993, "main_prompt_tokens": 2928, "main_completion_tokens": 65, "judge_total_tokens": 549, "judge_prompt_tokens": 472, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century is \"Electric Angel\" [Doc 1024].", "latency_ms": 6054.774284362793, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Electric Angel,' which matches the gold answer. It stays faithful to the provided information, only rephrasing the question while giving the correct title and citing the same document reference.", "main_total_tokens": 2600, "main_prompt_tokens": 2550, "main_completion_tokens": 50, "judge_total_tokens": 536, "judge_prompt_tokens": 461, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 based on the provided context is \"Doomsday Stories\" with a rating of 2.167 [Doc 108366].", "latency_ms": 3331.965446472168, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the lowest rated movie from 2023 is 'Mega Ape', while the generated answer claims it is 'Doomsday Stories' with a rating of 2.167. This directly contradicts the gold answer, making the generated response factually incorrect in this evaluation context. Additionally, referencing 'Doomsday Stories' indicates the model is relying on or hallucinating from a different document than the gold label, so it is not faithful to the provided gold information.", "main_total_tokens": 2372, "main_prompt_tokens": 2328, "main_completion_tokens": 44, "judge_total_tokens": 547, "judge_prompt_tokens": 416, "judge_completion_tokens": 131}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 1418.0903434753418, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Tea House', but the model responded with 'I don't know', which is incorrect. However, it did not hallucinate an alternative answer and accurately expressed uncertainty, so it is fully faithful to its own knowledge limits.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 386, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 708.7082862854004, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which is incorrect because the gold answer specifies 'The Uncut Story of Mewtwo's Origin' as the correct response. However, the model does not hallucinate any incorrect information; it simply expresses a lack of knowledge, so it is fully faithful to its own (stated) uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 494, "judge_prompt_tokens": 395, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 according to the provided context is \"Bloodeaters\" with a rating of 4.0, and its language is English [Doc 820].", "latency_ms": 5441.219091415405, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980. The generated answer states that the lowest-rated movie is \"Bloodeaters\" with a rating of 4.0 and that its language is English. The gold answer is simply \"English\". Thus, the generated answer correctly identifies the language and is consistent with the given context, making it both correct and faithful.", "main_total_tokens": 2325, "main_prompt_tokens": 2282, "main_completion_tokens": 43, "judge_total_tokens": 531, "judge_prompt_tokens": 421, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity is \"The Sopranos\" with a popularity score of 59.4284 [Doc 115146].", "latency_ms": 5889.658212661743, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect: according to the gold label, the correct TV show is 'Law & Order: Special Victims Unit', but the generated answer states 'The Sopranos'. Additionally, the answer cites a specific popularity score and document reference that do not appear in the provided gold information, meaning it is not faithfully grounded in the given data.", "main_total_tokens": 2718, "main_prompt_tokens": 2673, "main_completion_tokens": 45, "judge_total_tokens": 526, "judge_prompt_tokens": 424, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 4179.1815757751465, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and explicitly attributes it to a document source. It is both correct and faithful to the provided information.", "main_total_tokens": 2570, "main_prompt_tokens": 2548, "main_completion_tokens": 22, "judge_total_tokens": 464, "judge_prompt_tokens": 400, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in the year 1999 [Doc 22481]. However, there is another entry for a movie with the same title released in 2019 [Doc 88811], and yet another one listed for the year 2025 [Doc 112077].", "latency_ms": 8132.211923599243, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer indicates 2019 as the correct year. The generated answer states 1999 as the release year and then mentions multiple different years (2019 and 2025) as other entries, so it does not correctly answer the question. For faithfulness, the answer appears to be grounded in the (hypothetical) cited documents it references, but since we cannot verify those docs and it introduces multiple conflicting years instead of resolving them, it is only partially faithful to the required single correct answer.", "main_total_tokens": 2448, "main_prompt_tokens": 2368, "main_completion_tokens": 80, "judge_total_tokens": 582, "judge_prompt_tokens": 447, "judge_completion_tokens": 135}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 200.3483772277832, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer (3 votes) does not match the gold answer (279 votes), so it is incorrect. There is no indication it was derived from the provided gold data; thus, it is also unfaithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 458, "judge_prompt_tokens": 383, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "12", "latency_ms": 215.9886360168457, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (12 votes), so it is both correct and faithful to the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 437, "judge_prompt_tokens": 383, "judge_completion_tokens": 54}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in the year 2009 [Doc 46069].", "latency_ms": 4709.229469299316, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 2009 as the release year of the movie and directly reflects the information given in the gold answer, showing both correctness and faithfulness.", "main_total_tokens": 2492, "main_prompt_tokens": 2461, "main_completion_tokens": 31, "judge_total_tokens": 476, "judge_prompt_tokens": 412, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 4502.404689788818, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer specifies 'en' (English), and the generated answer states that the original language of 'The King's Speech' is English, which is correct and aligned with the gold. The mention of a document reference does not contradict anything and is consistent with the given information.", "main_total_tokens": 2790, "main_prompt_tokens": 2766, "main_completion_tokens": 24, "judge_total_tokens": 488, "judge_prompt_tokens": 402, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "1987 [Doc 6993]", "latency_ms": 4074.9640464782715, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer, and does not introduce any contradictions or unsupported information.", "main_total_tokens": 3019, "main_prompt_tokens": 3004, "main_completion_tokens": 15, "judge_total_tokens": 448, "judge_prompt_tokens": 387, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Brain Fluid Explosion Girl", "latency_ms": 9000.313758850098, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie, so it is both correct and faithful.", "main_total_tokens": 3059, "main_prompt_tokens": 2974, "main_completion_tokens": 85, "judge_total_tokens": 445, "judge_prompt_tokens": 392, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 4720.606327056885, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.4306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.719741384391281, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which correctly identifies the TV show as 'Dungeons & Dragons'. It is both correct and fully faithful to the reference.", "main_total_tokens": 3076, "main_prompt_tokens": 3054, "main_completion_tokens": 22, "judge_total_tokens": 458, "judge_prompt_tokens": 394, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 4635.855913162231, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'Nacht der Wölfe.'", "main_total_tokens": 2951, "main_prompt_tokens": 2927, "main_completion_tokens": 24, "judge_total_tokens": 456, "judge_prompt_tokens": 403, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900 [Doc 126191]", "latency_ms": 3925.194263458252, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of Picnic at Hanging Rock take place in 1900, which matches the gold answer. It is fully correct and faithful to the reference.", "main_total_tokens": 2798, "main_prompt_tokens": 2781, "main_completion_tokens": 17, "judge_total_tokens": 461, "judge_prompt_tokens": 394, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June", "latency_ms": 610.3725433349609, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question clearly describes a 2004 movie whose main character is a young violinist named after a calendar month, and the expected (gold) answer is 'June'. The generated answer is also 'June', which exactly matches the gold answer and is consistent with the question. Thus it is both fully correct and fully faithful to the provided information.", "main_total_tokens": 132, "main_prompt_tokens": 130, "main_completion_tokens": 2, "judge_total_tokens": 533, "judge_prompt_tokens": 433, "judge_completion_tokens": 100}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 501.5740394592285, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a specific correct answer provided by the gold, but the generated answer says 'I don't know,' which is incorrect. It is, however, faithful to the model's uncertainty and does not hallucinate, so faithfulness is high while correctness is zero.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 399, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 269.7324752807617, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric rating (7.6). The generated answer says, 'I don't know,' which does not provide the correct value, so correctness is 0. However, the answer does not hallucinate or fabricate a value; it honestly admits lack of knowledge, so it is fully faithful to the model's knowledge limits, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 498, "judge_prompt_tokens": 389, "judge_completion_tokens": 109}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "10", "latency_ms": 4201.139450073242, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 0.9828920819566879, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.3364045224324153, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (10) does not match the gold standard (4), so it is factually incorrect for this evaluation. However, it directly answers the question without adding unsupported reasoning or extraneous claims, so it is fully faithful to the user query.", "main_total_tokens": 1587, "main_prompt_tokens": 1561, "main_completion_tokens": 26, "judge_total_tokens": 476, "judge_prompt_tokens": 394, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 385.7719898223877, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there are 2 such TV shows, but the generated answer claims there are none, so it is factually incorrect. However, the model does not fabricate specific incorrect details and instead admits inability to find matches, which is faithful to its stated limitation.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 489, "judge_prompt_tokens": 404, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 332.25083351135254, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct specific TV show 'Fantômas.' However, it is faithful in that it does not hallucinate an incorrect answer and accurately reflects uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 463, "judge_prompt_tokens": 389, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise [Doc 11205]", "latency_ms": 3797.8947162628174, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise', which matches the gold answer exactly. The addition of the document ID does not change the substance of the answer and remains faithful to the query and gold reference.", "main_total_tokens": 2919, "main_prompt_tokens": 2905, "main_completion_tokens": 14, "judge_total_tokens": 467, "judge_prompt_tokens": 393, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor? [Doc 84838]", "latency_ms": 4285.883665084839, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, including correct movie title. The added doc reference does not change the factual content, so it is both correct and faithful.", "main_total_tokens": 3143, "main_prompt_tokens": 3124, "main_completion_tokens": 19, "judge_total_tokens": 478, "judge_prompt_tokens": 414, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018", "latency_ms": 4810.836315155029, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (2018), so it is both correct and faithful.", "main_total_tokens": 3156, "main_prompt_tokens": 3135, "main_completion_tokens": 21, "judge_total_tokens": 431, "judge_prompt_tokens": 380, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy", "latency_ms": 3407.022714614868, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV series about a young boy robot in 2030 AD as Astro Boy.", "main_total_tokens": 3177, "main_prompt_tokens": 3171, "main_completion_tokens": 6, "judge_total_tokens": 448, "judge_prompt_tokens": 389, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 [Doc 67513]", "latency_ms": 3610.755443572998, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly states that they were held captive for 7 years, matching the gold answer. However, it includes an extraneous reference tag ('[Doc 67513]') that is not supported by the given information or needed to answer the question, so it is not fully faithful to the prompt requirements.", "main_total_tokens": 2674, "main_prompt_tokens": 2661, "main_completion_tokens": 13, "judge_total_tokens": 496, "judge_prompt_tokens": 402, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m [Doc 133469]", "latency_ms": 3317.9521560668945, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the provided gold answer exactly ('£26m') and does not introduce any unsupported information. It is both correct and faithful to the given reference.", "main_total_tokens": 2366, "main_prompt_tokens": 2351, "main_completion_tokens": 15, "judge_total_tokens": 468, "judge_prompt_tokens": 404, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 666.3477420806885, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score (80.5772). The generated answer says only 'I don't know,' which is incorrect because the information is assumed to be answerable and is provided in the gold. However, the model does not hallucinate or fabricate a value; it honestly reports uncertainty, so it is fully faithful to its own stated knowledge limits.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 501, "judge_prompt_tokens": 396, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 323.1320381164551, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 Russian language TV show released in 1997. The generated answer says it cannot find any, which is factually incorrect relative to the gold. However, the model did not invent any specific incorrect facts; it simply reported inability to find results, so it remains faithful to its own (stated) lack of knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 497, "judge_prompt_tokens": 394, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes", "latency_ms": 3585.4365825653076, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Angel Eyes' exactly matches the gold answer and correctly identifies the TV show where Soo-wan regains her sight after an eye transplant.", "main_total_tokens": 3215, "main_prompt_tokens": 3210, "main_completion_tokens": 5, "judge_total_tokens": 459, "judge_prompt_tokens": 397, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "en [Doc 25900]", "latency_ms": 4695.4026222229, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says the language is English, while the gold label is Korean, so the answer is incorrect. It directly addresses the question and is consistent with itself, so it is faithful to the prompt, but factually wrong relative to the gold.", "main_total_tokens": 3140, "main_prompt_tokens": 3118, "main_completion_tokens": 22, "judge_total_tokens": 464, "judge_prompt_tokens": 383, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 748.7554550170898, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.5, "precision_at_5": 0.2, "ndcg_at_5": 0.6131471927654584, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.6131471927654584, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the count of movies, with the gold answer being 2, while the generated answer is 1, which is numerically incorrect. The generated answer is consistent as a direct response to the question, so it is faithful in format and intent but factually wrong relative to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 486, "judge_prompt_tokens": 394, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "None of the provided documents contain information about movies released in 1979. The earliest year mentioned is 1980. Therefore, based strictly on the given context, there is no best-rated movie from 1979 to report. [Doc 42769-585]", "latency_ms": 11835.434436798096, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that there is no information about movies released in 1979 in the available data. The generated answer correctly notes that the documents do not contain any 1979 movies and thus no best-rated movie can be reported, matching the gold answer in substance. It does not add unsupported details beyond referencing the document set and remains faithful to the supposed context.", "main_total_tokens": 3161, "main_prompt_tokens": 3038, "main_completion_tokens": 123, "judge_total_tokens": 556, "judge_prompt_tokens": 449, "judge_completion_tokens": 107}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "I don't know.", "latency_ms": 6990.561962127686, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'I don't know.' is not fully correct because the appropriate response is to state that 'Back to the Future Part IV' does not exist, matching the gold. It is also not fully faithful to the gold answer, which explicitly says the movie does not exist in the database, rather than expressing ignorance.", "main_total_tokens": 705, "main_prompt_tokens": 620, "main_completion_tokens": 85, "judge_total_tokens": 499, "judge_prompt_tokens": 404, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there are no movies from 1980 about the Chernobyl disaster, since the disaster occurred in 1986. The generated answer is faithful to the question and does not invent a movie or score; it simply notes the mismatch in time period, which is accurate and aligned with the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 533, "judge_prompt_tokens": 434, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 370.0582981109619, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because there are multiple TV shows and movies with 'Astro Boy' in the title, and the gold label indicates there are 4. However, the model does not introduce any fabricated details; it simply claims inability to find any, which is consistent with its stated limitation, so it remains faithful to its own reasoning process.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 500, "judge_prompt_tokens": 397, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "2", "latency_ms": 4190.757274627686, "retrieval_recall": 0.0, "retrieval_mrr": 0.125, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.19342640361727081, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, so it is both correct and faithful.", "main_total_tokens": 3720, "main_prompt_tokens": 3715, "main_completion_tokens": 5, "judge_total_tokens": 434, "judge_prompt_tokens": 386, "judge_completion_tokens": 48}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 772.1989154815674, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The gold answer indicates there was 1 TV show in the 1990s about Super Mario Bros, while the generated answer incorrectly claims there were none; thus correctness is 0. The model at least stays within the question domain and does not hallucinate a number, instead expressing uncertainty, so its faithfulness to its own stated limitations is relatively high, though not perfect for evaluation purposes.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 509, "judge_prompt_tokens": 398, "judge_completion_tokens": 111}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 301.0232448577881, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0 shows. The generated answer states that it cannot find any movies or shows matching the criteria, which is consistent with there being 0 such TV shows. It does not introduce contradictory information, so it is both correct with respect to the gold and faithful to the question.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 490, "judge_prompt_tokens": 400, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 247.84111976623535, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Naruto', which is a very popular ninja-themed series. The generated answer says 'I don't know,' which is incorrect. However, it is faithful in that it does not hallucinate an incorrect specific show; it honestly expresses a lack of knowledge, so faithfulness is high even though correctness is zero.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 386, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 1289.63303565979, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the movie was not released, while the generated answer says only 'I don't know.' This is faithful to the lack of information but does not convey the correct fact that there was no such release, so it is faithful but incorrect.", "main_total_tokens": 602, "main_prompt_tokens": 600, "main_completion_tokens": 2, "judge_total_tokens": 471, "judge_prompt_tokens": 389, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 4692.155122756958, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that the movie was not released; saying 'I don't know' is incorrect. However, the generated answer is faithful to the model's uncertainty and does not hallucinate a wrong movie.", "main_total_tokens": 639, "main_prompt_tokens": 591, "main_completion_tokens": 48, "judge_total_tokens": 458, "judge_prompt_tokens": 386, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "I don't know.", "latency_ms": 3838.7725353240967, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The assistant says it does not know, which is not a correct answer because the fourth oldest Star Wars movie does have a specific answer per the gold label. However, the response is faithful in that it does not invent or hallucinate an answer beyond its stated knowledge.", "main_total_tokens": 704, "main_prompt_tokens": 662, "main_completion_tokens": 42, "judge_total_tokens": 477, "judge_prompt_tokens": 393, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "2004", "latency_ms": 4615.388631820679, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer ('2004') does not match the correct year of release ('1986'), so it is factually incorrect. It also contradicts the provided gold answer, so it is unfaithful to the reference.", "main_total_tokens": 3062, "main_prompt_tokens": 3041, "main_completion_tokens": 21, "judge_total_tokens": 456, "judge_prompt_tokens": 380, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 228.09410095214844, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a numerical rating (8.466), but the generated answer says it does not know, which is incorrect. However, it does not fabricate any information and honestly reports uncertainty, so it is fully faithful to the model's knowledge state.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 398, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med [Doc 121125]", "latency_ms": 3895.738363265991, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Mighty Med') and correctly identifies the TV show described. It is fully correct and faithful.", "main_total_tokens": 2795, "main_prompt_tokens": 2778, "main_completion_tokens": 17, "judge_total_tokens": 459, "judge_prompt_tokens": 400, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina [Doc 53582]", "latency_ms": 4186.484813690186, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Argentina as the country, fully matching the gold answer without adding unsupported information.", "main_total_tokens": 3483, "main_prompt_tokens": 3470, "main_completion_tokens": 13, "judge_total_tokens": 449, "judge_prompt_tokens": 398, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "Jiang Jie [Doc 135231]", "latency_ms": 4242.063283920288, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Jiang Jie as the person whose trap the apprentice falls into, matching the gold answer exactly, and does not introduce any extraneous or conflicting information.", "main_total_tokens": 2959, "main_prompt_tokens": 2942, "main_completion_tokens": 17, "judge_total_tokens": 457, "judge_prompt_tokens": 392, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "5", "latency_ms": 4599.950313568115, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.7095272044910244, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.7095272044910244, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The assistant's numeric answer (5) does not match the gold answer (4), so it is incorrect. The response appears to be directly addressing the question asked, so it is faithful to the user query, but factually wrong relative to the provided gold.", "main_total_tokens": 3817, "main_prompt_tokens": 3804, "main_completion_tokens": 13, "judge_total_tokens": 468, "judge_prompt_tokens": 385, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 2244.4238662719727, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that no movies can be found featuring both Bruce Lee and Steven Seagal together. The generated answer is slightly more generic but faithfully conveys the same information without adding incorrect details.", "main_total_tokens": 2419, "main_prompt_tokens": 2417, "main_completion_tokens": 2, "judge_total_tokens": 479, "judge_prompt_tokens": 406, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3 [Doc 42750]", "latency_ms": 3212.049722671509, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the rating of Tokyo Mater (2008). The gold answer is 6.3, and the generated answer is also 6.3, matching exactly. The citation tag does not contradict anything. Therefore, the response is both correct and faithful to the gold.", "main_total_tokens": 2353, "main_prompt_tokens": 2340, "main_completion_tokens": 13, "judge_total_tokens": 476, "judge_prompt_tokens": 393, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "drift racing [Doc 36394]", "latency_ms": 5535.557746887207, "retrieval_recall": 1.0, "retrieval_mrr": 0.08333333333333333, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the style of racing in the movie; the gold answer is 'Drift'. The generated answer 'drift racing' is semantically equivalent and correctly reflects the same concept without adding unsupported details, so it is both correct and faithful.", "main_total_tokens": 3292, "main_prompt_tokens": 3261, "main_completion_tokens": 31, "judge_total_tokens": 468, "judge_prompt_tokens": 386, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer [Doc 131298]", "latency_ms": 4268.908739089966, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, 'Confronting a Serial Killer'. There is no hallucinated or conflicting information; the extra bracketed reference does not change the correctness of the title itself.", "main_total_tokens": 2971, "main_prompt_tokens": 2952, "main_completion_tokens": 19, "judge_total_tokens": 490, "judge_prompt_tokens": 418, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838 [Doc 20603]", "latency_ms": 3460.8311653137207, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and does not introduce any contradictions or unsupported claims. It is both correct and faithful.", "main_total_tokens": 2538, "main_prompt_tokens": 2523, "main_completion_tokens": 15, "judge_total_tokens": 457, "judge_prompt_tokens": 395, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense", "latency_ms": 3115.5388355255127, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the 1999 psychological thriller that alludes to an extra sense as 'The Sixth Sense.'", "main_total_tokens": 2880, "main_prompt_tokens": 2874, "main_completion_tokens": 6, "judge_total_tokens": 455, "judge_prompt_tokens": 394, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002 [Doc 27318]", "latency_ms": 4151.765584945679, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states the release year as 2002, which matches the gold answer. It does not introduce any contradictory or extraneous information, so it is both correct and faithful.", "main_total_tokens": 3176, "main_prompt_tokens": 3160, "main_completion_tokens": 16, "judge_total_tokens": 457, "judge_prompt_tokens": 389, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled", "latency_ms": 3849.712610244751, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, providing the correct title of the 2015 reboot. It is fully correct and faithful to the given reference.", "main_total_tokens": 3220, "main_prompt_tokens": 3211, "main_completion_tokens": 9, "judge_total_tokens": 459, "judge_prompt_tokens": 397, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015 [Doc 67500]", "latency_ms": 4199.944019317627, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9871369406794797, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold year 2015 and does not contradict the given information, so it is both correct and faithful.", "main_total_tokens": 3077, "main_prompt_tokens": 3061, "main_completion_tokens": 16, "judge_total_tokens": 450, "judge_prompt_tokens": 393, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "I don't know.", "latency_ms": 5004.505634307861, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a specific romantic comedy, but the gold answer notes that no such movie description can be found, so the correct response is to not assert a nonexistent title. The generated answer, 'I don't know,' correctly refrains from inventing a movie and is therefore correct. It is also fully faithful to the gold answer, which likewise indicates that no matching movie is known.", "main_total_tokens": 668, "main_prompt_tokens": 611, "main_completion_tokens": 57, "judge_total_tokens": 502, "judge_prompt_tokens": 394, "judge_completion_tokens": 108}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Garden of Eden [Doc 43581]", "latency_ms": 4249.900817871094, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'Garden of Eden' is not the correct movie title; the correct answer is 'Havana Darkness'. It is also unfaithful to the gold answer provided, so both correctness and faithfulness are 0.", "main_total_tokens": 2933, "main_prompt_tokens": 2917, "main_completion_tokens": 16, "judge_total_tokens": 474, "judge_prompt_tokens": 397, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 3303.571939468384, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Doll & Em'. It is both correct and faithful.", "main_total_tokens": 2860, "main_prompt_tokens": 2853, "main_completion_tokens": 7, "judge_total_tokens": 451, "judge_prompt_tokens": 392, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam Foe [Doc 39389]", "latency_ms": 4193.8323974609375, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'Hallam Foe', matching the gold answer, so correctness is 1. However, the inclusion of the bracketed citation-like text '[Doc 39389]' is likely hallucinated and not grounded in the user-provided content, so faithfulness is rated 0.", "main_total_tokens": 3132, "main_prompt_tokens": 3116, "main_completion_tokens": 16, "judge_total_tokens": 504, "judge_prompt_tokens": 409, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Foe [Doc 39389]", "latency_ms": 5891.921758651733, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks whether the last name sounds like he is a friend or a foe, and the gold answer is 'A foe.' The generated answer 'Foe' matches this label, differing only in capitalization and the missing article, which do not change the meaning. The bracketed document tag does not contradict the answer. Therefore, the answer is correct and faithful to the gold.", "main_total_tokens": 3293, "main_prompt_tokens": 3258, "main_completion_tokens": 35, "judge_total_tokens": 501, "judge_prompt_tokens": 394, "judge_completion_tokens": 107}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18 [Doc 124583]", "latency_ms": 3841.766119003296, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (18) and does not introduce any extra or conflicting information, so it is both correct and faithful.", "main_total_tokens": 2835, "main_prompt_tokens": 2820, "main_completion_tokens": 15, "judge_total_tokens": 455, "judge_prompt_tokens": 395, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "autism [Doc 124583]", "latency_ms": 3757.4310302734375, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which spectrum Sam is on; the gold answer is 'Autism spectrum'. The generated answer 'autism' conveys the same concept and is effectively correct and faithful to the gold answer, differing only in phrasing.", "main_total_tokens": 2648, "main_prompt_tokens": 2633, "main_completion_tokens": 15, "judge_total_tokens": 471, "judge_prompt_tokens": 394, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix [Doc 59659]", "latency_ms": 4276.360034942627, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Chamonix') and does not introduce any extraneous or unsupported information, so it is both correct and faithful.", "main_total_tokens": 3365, "main_prompt_tokens": 3350, "main_completion_tokens": 15, "judge_total_tokens": 469, "judge_prompt_tokens": 406, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481]", "latency_ms": 4270.647048950195, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, including wording and meaning. It is therefore fully correct and faithful to the provided gold reference.", "main_total_tokens": 3215, "main_prompt_tokens": 3200, "main_completion_tokens": 15, "judge_total_tokens": 456, "judge_prompt_tokens": 398, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland [Doc 114810]", "latency_ms": 4066.0934448242188, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Greenland' exactly matches the gold answer and stays fully consistent with the provided information.", "main_total_tokens": 3118, "main_prompt_tokens": 3103, "main_completion_tokens": 15, "judge_total_tokens": 453, "judge_prompt_tokens": 401, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon", "latency_ms": 3190.211057662964, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('One Afternoon'), so it is correct and fully faithful to the provided information.", "main_total_tokens": 2713, "main_prompt_tokens": 2707, "main_completion_tokens": 6, "judge_total_tokens": 461, "judge_prompt_tokens": 406, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The Life Ahead [Doc 93260]", "latency_ms": 2453.18865776062, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title of a low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her. The gold answer is 'The Life Ahead'. The generated answer is exactly 'The Life Ahead', which is correct and matches the gold answer without adding extraneous or conflicting information. Therefore, it is fully correct and faithful.", "main_total_tokens": 2948, "main_prompt_tokens": 2933, "main_completion_tokens": 15, "judge_total_tokens": 515, "judge_prompt_tokens": 407, "judge_completion_tokens": 108}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "Bruce McLaren [Doc 73859]", "latency_ms": 4000.0219345092773, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary, matching the gold answer exactly and staying faithful to the information given.", "main_total_tokens": 3068, "main_prompt_tokens": 3054, "main_completion_tokens": 14, "judge_total_tokens": 455, "judge_prompt_tokens": 393, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren Motor Racing team [Doc 73859]", "latency_ms": 5551.424026489258, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0616063116448506, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0616063116448506, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer in substance, giving the same team name with only a minor article difference and an extra citation tag. It is correct and faithful to the reference.", "main_total_tokens": 3042, "main_prompt_tokens": 3012, "main_completion_tokens": 30, "judge_total_tokens": 461, "judge_prompt_tokens": 394, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "New Zealand [Doc 73859]", "latency_ms": 5190.050363540649, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly states 'New Zealand,' matching the gold answer, so correctness is 1. However, the addition of '[Doc 73859]' introduces an unsupported citation-like artifact not present in the gold answer, making the response not fully faithful to the reference; hence faithfulness is 0.", "main_total_tokens": 3053, "main_prompt_tokens": 3026, "main_completion_tokens": 27, "judge_total_tokens": 476, "judge_prompt_tokens": 384, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "A Good Day to Die Hard", "latency_ms": 3816.467761993408, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is wrong; in 'A Good Day to Die Hard' John McClane does not blow up the Great Wall of China, and the gold answer correctly states that such a movie is not in the database. The generated response contradicts the gold, so it is neither correct nor faithful.", "main_total_tokens": 3263, "main_prompt_tokens": 3254, "main_completion_tokens": 9, "judge_total_tokens": 496, "judge_prompt_tokens": 404, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit [Doc 28589]", "latency_ms": 4288.646936416626, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2002 Japanese horror film as 'Tomie: Forbidden Fruit'. It is fully correct and faithful to the gold.", "main_total_tokens": 3186, "main_prompt_tokens": 3169, "main_completion_tokens": 17, "judge_total_tokens": 495, "judge_prompt_tokens": 428, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 [Doc 98566]", "latency_ms": 4283.669471740723, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that Nino and Caterina have been married for 65 years, which matches the gold answer of 65. It directly reflects the information given without adding or contradicting any details, so it is both correct and faithful.", "main_total_tokens": 3568, "main_prompt_tokens": 3554, "main_completion_tokens": 14, "judge_total_tokens": 482, "judge_prompt_tokens": 402, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Book of Souls [Doc 82582]", "latency_ms": 4470.919132232666, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks what the Scorpion King and Tala are searching for in 'Scorpion King: Book of Souls (2018)'. The correct answer is 'The Book of Souls'. The generated answer matches the gold answer exactly in content, so it is both correct and faithful to the provided information (the doc tag is extraneous but does not change the answer).", "main_total_tokens": 3429, "main_prompt_tokens": 3413, "main_completion_tokens": 16, "judge_total_tokens": 508, "judge_prompt_tokens": 404, "judge_completion_tokens": 104}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1125.943899154663, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about a nonexistent or unknown movie. The gold answer explicitly says it is not in the database; the generated answer says 'I don't know,' which is effectively the same level of knowledge and does not introduce incorrect facts. Thus it is fully correct and faithful to the gold response.", "main_total_tokens": 562, "main_prompt_tokens": 560, "main_completion_tokens": 2, "judge_total_tokens": 484, "judge_prompt_tokens": 395, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1032.5026512145996, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly indicates a lack of knowledge, which is appropriate since the gold answer states the movie is not in the database. It is faithful to the source by not inventing a release year or additional details.", "main_total_tokens": 622, "main_prompt_tokens": 620, "main_completion_tokens": 2, "judge_total_tokens": 467, "judge_prompt_tokens": 393, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 900.9373188018799, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer appropriately indicates lack of knowledge, which is effectively equivalent in correctness and faithfulness to the gold response stating the movie is not in the database. There is no invented information or contradiction.", "main_total_tokens": 581, "main_prompt_tokens": 579, "main_completion_tokens": 2, "judge_total_tokens": 465, "judge_prompt_tokens": 395, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 866.3504123687744, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks about the release year of a movie that is apparently not in the database. The gold answer says the movie is not in the database, and the generated answer says 'I don't know.' Both convey lack of knowledge, which is an appropriate and correct response given the database limitation. The generated answer does not introduce any hallucinated information and is faithful to the implied knowledge state.", "main_total_tokens": 563, "main_prompt_tokens": 561, "main_completion_tokens": 2, "judge_total_tokens": 503, "judge_prompt_tokens": 395, "judge_completion_tokens": 108}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 975.0363826751709, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer appropriately express a lack of knowledge about the movie. The generated answer does not hallucinate a release year and matches the gold response in essence, so it is correct and faithful.", "main_total_tokens": 620, "main_prompt_tokens": 618, "main_completion_tokens": 2, "judge_total_tokens": 469, "judge_prompt_tokens": 395, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 4940.226078033447, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is not in the database, so not knowing the release year is correct. The generated response honestly states lack of knowledge and does not invent information, aligning fully with the gold answer.", "main_total_tokens": 617, "main_prompt_tokens": 558, "main_completion_tokens": 59, "judge_total_tokens": 466, "judge_prompt_tokens": 393, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1012.6752853393555, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database, which is equivalent in content to the generated answer saying 'I don't know.' Both convey lack of knowledge without adding incorrect information, so the generated answer is correct and faithful.", "main_total_tokens": 620, "main_prompt_tokens": 618, "main_completion_tokens": 2, "judge_total_tokens": 473, "judge_prompt_tokens": 396, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "J.C. Chandor [Doc 46069]", "latency_ms": 4439.840078353882, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer incorrectly names J.C. Chandor as the director, while the gold standard states the database does not provide this information, so the model should have admitted it could not answer. This is both factually wrong with respect to the given gold response and unfaithful to the database constraints.", "main_total_tokens": 3429, "main_prompt_tokens": 3412, "main_completion_tokens": 17, "judge_total_tokens": 499, "judge_prompt_tokens": 408, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 1002.8510093688965, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold indicates the database does not contain the director information; saying 'I don't know' correctly reflects this lack of information and does not hallucinate an answer.", "main_total_tokens": 582, "main_prompt_tokens": 580, "main_completion_tokens": 2, "judge_total_tokens": 461, "judge_prompt_tokens": 399, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not specify who directed the movie \"Journey into Gaza\". [Doc 111360]", "latency_ms": 4965.089321136475, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not specify who directed the movie, which aligns with the gold answer that the director information is not provided. It does not introduce any unsupported claims and remains faithful to the given evidence.", "main_total_tokens": 3132, "main_prompt_tokens": 3103, "main_completion_tokens": 29, "judge_total_tokens": 490, "judge_prompt_tokens": 414, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about the director of \"The Last Night at Tremore Beach\". The document only provides details such as its type (TV), year (2024), rating (7.0), popularity (5.1693), language (es), and a brief plot description, but no information regarding the director is given [Doc 135098].", "latency_ms": 8966.74394607544, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not provide information about the director and lists only the available metadata (type, year, rating, popularity, language, plot). This matches the gold answer, which also says the director information is not in the database. No unsupported details are introduced, so it is both correct and faithful to the provided evidence.", "main_total_tokens": 2789, "main_prompt_tokens": 2704, "main_completion_tokens": 85, "judge_total_tokens": 572, "judge_prompt_tokens": 471, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 3624.814033508301, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Rise of the Mummy') and correctly identifies the movie where archaeology students awaken an ancient mummy. It is both correct and faithful.", "main_total_tokens": 2911, "main_prompt_tokens": 2902, "main_completion_tokens": 9, "judge_total_tokens": 462, "judge_prompt_tokens": 396, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 4602.336168289185, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Stranger in My Bed'). It is correct and fully faithful to the provided gold label.", "main_total_tokens": 3029, "main_prompt_tokens": 3006, "main_completion_tokens": 23, "judge_total_tokens": 458, "judge_prompt_tokens": 400, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 4133.143424987793, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question actually describes the TV series 'Game of Silence', not a movie. Both Gold and Generated answers match each other and correctly identify the intended work, so the response is correct relative to the task and faithful to the gold.", "main_total_tokens": 2677, "main_prompt_tokens": 2657, "main_completion_tokens": 20, "judge_total_tokens": 477, "judge_prompt_tokens": 400, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 3186.469554901123, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct movie title. It is both correct and fully faithful to the gold.", "main_total_tokens": 2806, "main_prompt_tokens": 2800, "main_completion_tokens": 6, "judge_total_tokens": 453, "judge_prompt_tokens": 395, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 3510.4246139526367, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Wood Job!'. The response is fully correct and faithful to the reference.", "main_total_tokens": 3275, "main_prompt_tokens": 3269, "main_completion_tokens": 6, "judge_total_tokens": 464, "judge_prompt_tokens": 403, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 5615.705728530884, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'Sesame Street: The Magical Wand Chase.'", "main_total_tokens": 3376, "main_prompt_tokens": 3345, "main_completion_tokens": 31, "judge_total_tokens": 472, "judge_prompt_tokens": 417, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond [Doc 74597]", "latency_ms": 3921.2546348571777, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot; the gold answer is 'Beyond' and the generated answer is also 'Beyond [Doc 74597]'. The core movie title is correct and matches the gold, so the answer is correct. The added document reference does not contradict or alter the title, so it remains faithful to the query and gold.", "main_total_tokens": 2935, "main_prompt_tokens": 2922, "main_completion_tokens": 13, "judge_total_tokens": 517, "judge_prompt_tokens": 415, "judge_completion_tokens": 102}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9 [Doc 22478]", "latency_ms": 4123.119831085205, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as '6ixtynin9'. It directly responds to the question without adding unsupported details, so it is both correct and faithful.", "main_total_tokens": 2846, "main_prompt_tokens": 2828, "main_completion_tokens": 18, "judge_total_tokens": 486, "judge_prompt_tokens": 415, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread [Doc 32410]", "latency_ms": 4004.084825515747, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Club Dread'), correctly identifying the movie from the plot description and staying fully faithful to the provided information.", "main_total_tokens": 3101, "main_prompt_tokens": 3087, "main_completion_tokens": 14, "judge_total_tokens": 494, "judge_prompt_tokens": 433, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet", "latency_ms": 3812.3512268066406, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, providing the correct movie title 'Toilet'. It is both correct and fully faithful to the gold.", "main_total_tokens": 3618, "main_prompt_tokens": 3613, "main_completion_tokens": 5, "judge_total_tokens": 475, "judge_prompt_tokens": 415, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel [Doc 1024]", "latency_ms": 3870.319128036499, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Electric Angel') and does not introduce any extraneous or conflicting information, so it is both correct and faithful.", "main_total_tokens": 3155, "main_prompt_tokens": 3142, "main_completion_tokens": 13, "judge_total_tokens": 486, "judge_prompt_tokens": 424, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "Doomsday Stories [Doc 108366]", "latency_ms": 2531.6500663757324, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer does not match the gold answer; the lowest rated movie from 2023 is given as 'Mega Ape', but the model responded with 'Doomsday Stories [Doc 108366]'. This is factually incorrect with respect to the labeled gold answer, and there is no evidence in the prompt that justifies substituting a different title or appending a document reference. Therefore, it is both incorrect and unfaithful to the gold response.", "main_total_tokens": 2942, "main_prompt_tokens": 2924, "main_completion_tokens": 18, "judge_total_tokens": 516, "judge_prompt_tokens": 391, "judge_completion_tokens": 125}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 722.3057746887207, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific factual answer (the highest rated TV show from 2010, whose gold answer is 'Tea House'). The generated response says 'I don't know,' which does not provide the correct answer, so correctness is 0. However, the model is honest about its lack of knowledge and does not hallucinate or contradict given facts, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 495, "judge_prompt_tokens": 386, "judge_completion_tokens": 109}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 318.76611709594727, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific movie title, while the generated response states lack of knowledge and does not attempt an incorrect answer. Thus it is incorrect with respect to the gold, but fully faithful to the model's stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 471, "judge_prompt_tokens": 395, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "en [Doc 820]", "latency_ms": 3629.467248916626, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct answer is 'English', but the generated answer 'en [Doc 820]' includes an extraneous document reference and uses a language code rather than the explicit language name. This makes the response partially faithful but not fully correct in the required form.", "main_total_tokens": 2886, "main_prompt_tokens": 2875, "main_completion_tokens": 11, "judge_total_tokens": 473, "judge_prompt_tokens": 389, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The Sopranos", "latency_ms": 3743.9422607421875, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because 'The Sopranos' is not the gold answer; the correct show per the gold reference is 'Law & Order: Special Victims Unit'. However, the model did not contradict or ignore the given question; it simply chose the wrong show, so it remains faithful to the question but not correct relative to the gold.", "main_total_tokens": 3313, "main_prompt_tokens": 3305, "main_completion_tokens": 8, "judge_total_tokens": 496, "judge_prompt_tokens": 394, "judge_completion_tokens": 102}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2 [Doc 79355]", "latency_ms": 3302.1533489227295, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 and does not introduce contradictions or extraneous information. It is both correct and faithful.", "main_total_tokens": 2574, "main_prompt_tokens": 2561, "main_completion_tokens": 13, "judge_total_tokens": 448, "judge_prompt_tokens": 392, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "1999 [Doc 22481]", "latency_ms": 4072.809934616089, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks about the movie \"The Story of Us\" and the gold label says 2019, but the generated answer is 1999. This is factually incorrect with respect to the provided gold answer, so correctness is 0. The model clearly and directly answered the question without hallucinating extra information, so faithfulness to the user query and its own reasoning is 1.", "main_total_tokens": 2975, "main_prompt_tokens": 2959, "main_completion_tokens": 16, "judge_total_tokens": 502, "judge_prompt_tokens": 393, "judge_completion_tokens": 109}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 2741.788387298584, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer (3) does not match the gold answer (279), so it is factually incorrect. There is no grounding or reasoning shown that could justify the incorrect number, so it is also not faithful.", "main_total_tokens": 2434, "main_prompt_tokens": 2428, "main_completion_tokens": 6, "judge_total_tokens": 457, "judge_prompt_tokens": 383, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 2264.8558616638184, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a specific numeric answer (12 votes), but the generated response says it cannot find any movies or shows matching the criteria and does not provide the correct number. This is factually incorrect relative to the gold answer and also not faithful to the required information.", "main_total_tokens": 2449, "main_prompt_tokens": 2447, "main_completion_tokens": 2, "judge_total_tokens": 477, "judge_prompt_tokens": 392, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "2009 [Doc 46069]", "latency_ms": 4005.404472351074, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer gives the correct release year, 2009, which matches the gold answer. It is fully consistent with the provided information and does not introduce contradictions or unsupported claims.", "main_total_tokens": 3086, "main_prompt_tokens": 3070, "main_completion_tokens": 16, "judge_total_tokens": 463, "judge_prompt_tokens": 396, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-large_qwen2.5:32b", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en [Doc 48035]", "latency_ms": 4187.026262283325, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The original language of 'The King’s Speech' is English, which matches both the gold answer and the generated answer. The bracketed document reference does not change the factual content.", "main_total_tokens": 3408, "main_prompt_tokens": 3395, "main_completion_tokens": 13, "judge_total_tokens": 460, "judge_prompt_tokens": 390, "judge_completion_tokens": 70}
