{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "Wall Street was released in 1987 [Doc 6993].", "latency_ms": 4430.700302124023, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, which matches the gold answer. It is concise, accurate, and does not introduce unsupported information aside from a generic citation marker, so it is both correct and faithful.", "main_total_tokens": 2842, "main_prompt_tokens": 2828, "main_completion_tokens": 14, "judge_total_tokens": 471, "judge_prompt_tokens": 392, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie where classmates have to play a survival game using their cell phones is \"47 Hours to Live\" [Doc 90746].", "latency_ms": 777.3044109344482, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct answer is 'Brain Fluid Explosion Girl'. The generated answer '47 Hours to Live' is incorrect, so correctness is 0. The model at least attempts to answer the question directly rather than changing the task, but it invents a specific movie title without evidence or alignment with the provided gold answer, so faithfulness is partial at best.", "main_total_tokens": 2678, "main_prompt_tokens": 2651, "main_completion_tokens": 27, "judge_total_tokens": 518, "judge_prompt_tokens": 415, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show where six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" from 1983 [Doc 112849].", "latency_ms": 949.6171474456787, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' which matches the gold answer. The additional detail about it being from 1983 is accurate and consistent with known information, so the response is both correct and faithful to the source.", "main_total_tokens": 2745, "main_prompt_tokens": 2712, "main_completion_tokens": 33, "judge_total_tokens": 505, "judge_prompt_tokens": 423, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie \"Nacht der Wölfe\" is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones [Doc 2590].", "latency_ms": 1594.5117473602295, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'Nacht der Wölfe' as the movie about two rival gangs in Munich, and it stays faithful to the provided information without adding unsupported claims.", "main_total_tokens": 3086, "main_prompt_tokens": 3048, "main_completion_tokens": 38, "judge_total_tokens": 509, "judge_prompt_tokens": 436, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of \"Picnic at Hanging Rock\" take place on Valentine's Day, 1900 [Doc 126191].", "latency_ms": 701.3869285583496, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events occur in 1900, matching the gold answer. It also accurately reflects the information from the cited document, so it is both correct and faithful.", "main_total_tokens": 2815, "main_prompt_tokens": 2790, "main_completion_tokens": 25, "judge_total_tokens": 479, "judge_prompt_tokens": 410, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie you are referring to is \"June.\" It is about a young violinist named June who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man destined for her [Doc 32483].", "latency_ms": 1314.0721321105957, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as \"June,\" matching the gold answer, so correctness is 1. However, it fabricates a citation/reference ([Doc 32483]) that is not present in the user-provided context, so the answer is not fully faithful to the given information, resulting in a faithfulness score of 0.", "main_total_tokens": 3100, "main_prompt_tokens": 3038, "main_completion_tokens": 62, "judge_total_tokens": 594, "judge_prompt_tokens": 494, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "The highest rated Russian-language movie released in 1980 is \"Moscow Does Not Believe in Tears\" with a rating of 7.6 [Doc 61].", "latency_ms": 726.2723445892334, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer incorrectly identifies 'Moscow Does Not Believe in Tears' as the highest rated Russian-language movie of 1980, while the gold answer specifies 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger.' This makes the answer factually incorrect. It also conflicts directly with the provided gold reference, so it is not faithful to the specified ground truth.", "main_total_tokens": 3070, "main_prompt_tokens": 3036, "main_completion_tokens": 34, "judge_total_tokens": 535, "judge_prompt_tokens": 429, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest Russian-language movie rating in 1999 is 4.571 for the movie \"Features of the Russian Bath\" [Doc 22144].", "latency_ms": 822.9284286499023, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the highest Russian-language movie rating in 1999 is 7.6, while the generated answer claims it is 4.571 and names a specific movie. This is numerically incorrect relative to the gold and also introduces a specific title and value not supported by the provided gold information, making it both incorrect and unfaithful.", "main_total_tokens": 2514, "main_prompt_tokens": 2483, "main_completion_tokens": 31, "judge_total_tokens": 518, "judge_prompt_tokens": 416, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "There are two movies released in 1987 that have a rating over 7 and are in Chinese:\n\n1. \"A Chinese Ghost Story\" with a rating of 7.3 [Doc 7064].\n2. \"Last Eunuch In China\" with a rating of 7.8 [Doc 7754].", "latency_ms": 850.0096797943115, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 4 such movies, but the generated answer claims there are only 2. The reasoning appears consistent with the cited docs, so it is faithful to the provided evidence, but the final count is incorrect relative to the gold answer.", "main_total_tokens": 2987, "main_prompt_tokens": 2921, "main_completion_tokens": 66, "judge_total_tokens": 541, "judge_prompt_tokens": 458, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "There are no TV shows released in 1987 that have a rating over 7 and are in Chinese.", "latency_ms": 725.060224533081, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 2 such TV shows, but the generated answer says there are none. This directly contradicts the gold, so the content is incorrect. However, the answer is internally consistent and does not contradict its own reasoning or invent extra details, so it is faithful to its (albeit incorrect) conclusion.", "main_total_tokens": 2664, "main_prompt_tokens": 2642, "main_completion_tokens": 22, "judge_total_tokens": 512, "judge_prompt_tokens": 415, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "There is no French language TV show released in 1980 mentioned in the provided documents.", "latency_ms": 641.9804096221924, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the only French language TV show released in 1980, whose gold answer is 'Fantômas'. The generated answer incorrectly claims that there is no such show mentioned, so it is factually incorrect with respect to the question and gold answer, giving a correctness score of 0. However, given the constraint implied in the generated answer ('in the provided documents'), it appears to faithfully report that the documents (as seen by the model) did not contain such a show, so it is not hallucinating additional content; thus the faithfulness is scored as 1.", "main_total_tokens": 3037, "main_prompt_tokens": 3019, "main_completion_tokens": 18, "judge_total_tokens": 547, "judge_prompt_tokens": 403, "judge_completion_tokens": 144}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" [Doc 11205].", "latency_ms": 1596.9367027282715, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' which matches the gold answer exactly. It stays fully grounded in the provided information without adding unsupported details.", "main_total_tokens": 2911, "main_prompt_tokens": 2887, "main_completion_tokens": 24, "judge_total_tokens": 472, "judge_prompt_tokens": 409, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 1101.5911102294922, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title \"Won't You Be My Neighbor?\" and stays faithful to the question with no added incorrect information.", "main_total_tokens": 2992, "main_prompt_tokens": 2953, "main_completion_tokens": 39, "judge_total_tokens": 500, "judge_prompt_tokens": 440, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The movie \"YEAR\" was released in 2023 [Doc 107925].", "latency_ms": 1120.6352710723877, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 2018, but the generated response claims 2023, so it is factually incorrect. It also invents a movie title placeholder (\"YEAR\") and a document reference without basis in the provided gold answer, so it is not faithful.", "main_total_tokens": 3045, "main_prompt_tokens": 3028, "main_completion_tokens": 17, "judge_total_tokens": 479, "judge_prompt_tokens": 394, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series \"Astro Boy\" is an animation about a young boy robot in 2030 AD. In the series, after being abandoned by his creator, the young boy robot is rescued by an Android Advocate, Dr. Elefun, who adopts him and guides him in using his special powers to aid and protect mankind and robot-kind [Doc 112591].", "latency_ms": 1113.2893562316895, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as \"Astro Boy,\" matching the gold answer. The additional description about the plot is plausible and is explicitly attributed to the provided document, so it remains faithful to the given information and does not introduce contradictions.", "main_total_tokens": 3239, "main_prompt_tokens": 3165, "main_completion_tokens": 74, "judge_total_tokens": 542, "judge_prompt_tokens": 461, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her young son were held captive for 7 years [Doc 67513].", "latency_ms": 708.6188793182373, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9463946303571863, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: both state that the woman and her son were held captive for 7 years. There is no contradiction or hallucination, and the detail is correct and faithful to the reference.", "main_total_tokens": 2526, "main_prompt_tokens": 2495, "main_completion_tokens": 31, "judge_total_tokens": 500, "judge_prompt_tokens": 425, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 875.0903606414795, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that the depot was holding gold bullion worth £26 million before the break-in, which matches the provided gold answer of £26m. It does not add unsupported information and remains consistent with the source.", "main_total_tokens": 2764, "main_prompt_tokens": 2731, "main_completion_tokens": 33, "judge_total_tokens": 503, "judge_prompt_tokens": 428, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "The most popular English-speaking TV show released in 1997 is \"South Park,\" with a popularity score of 61.7375 [Doc 114707].", "latency_ms": 1056.9148063659668, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks only for the popularity score, which is 80.5772 per the gold answer. The generated answer provides 61.7375 instead, so the numeric result is incorrect. However, the answer is faithful to the cited document, explicitly attributing the 61.7375 score to Doc 114707, so it does not fabricate beyond its source.", "main_total_tokens": 3579, "main_prompt_tokens": 3546, "main_completion_tokens": 33, "judge_total_tokens": 531, "judge_prompt_tokens": 425, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "There were no Russian language TV shows released in 1997 mentioned in the provided documents.", "latency_ms": 703.7813663482666, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that there was 1 Russian language TV show released in 1997, while the generated answer claims there were none, which is factually incorrect with respect to the gold. However, the generated answer explicitly refers to the limitation of the provided documents and does not introduce unsupported information; it remains faithful to the (assumed) absence of evidence in those documents. Thus, correctness is 0, while faithfulness to the given context is 1.", "main_total_tokens": 2325, "main_prompt_tokens": 2307, "main_completion_tokens": 18, "judge_total_tokens": 526, "judge_prompt_tokens": 401, "judge_completion_tokens": 125}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 784.0197086334229, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Angel Eyes,\" which matches the gold answer. It does not introduce contradictions or unsupported claims beyond a harmless document reference, so it is both correct and faithful.", "main_total_tokens": 2821, "main_prompt_tokens": 2793, "main_completion_tokens": 28, "judge_total_tokens": 494, "judge_prompt_tokens": 422, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French [Doc 110681].", "latency_ms": 663.562536239624, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the gold label says the movie language is Korean, but the model answered French. However, the answer is faithful to the provided document reference [Doc 110681], so it did not hallucinate beyond its cited source.", "main_total_tokens": 2762, "main_prompt_tokens": 2745, "main_completion_tokens": 17, "judge_total_tokens": 474, "judge_prompt_tokens": 393, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "There are no English-speaking Horror movies released in 2009 with a popularity score greater than 7 in the provided documents.", "latency_ms": 1328.6163806915283, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer claims there are none, which is factually incorrect, so correctness is 0. The answer is, however, internally consistent with what it asserts about the provided documents, so it is faithful to its own (albeit incomplete) reasoning, giving a faithfulness score of 1.", "main_total_tokens": 2598, "main_prompt_tokens": 2573, "main_completion_tokens": 25, "judge_total_tokens": 516, "judge_prompt_tokens": 417, "judge_completion_tokens": 99}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The documents provided do not contain any information about movies released in 1979.", "latency_ms": 985.5051040649414, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not contain information about movies released in 1979, which aligns with the gold answer that the database has no such movies. It does not hallucinate a movie title or rating and remains faithful to the limitation described in the gold response.", "main_total_tokens": 2827, "main_prompt_tokens": 2811, "main_completion_tokens": 16, "judge_total_tokens": 498, "judge_prompt_tokens": 411, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The context provided does not mention a movie titled \"Back to the Future Part IV.\" Therefore, there is no information available about its release year.", "latency_ms": 837.8193378448486, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there is no information about a movie titled 'Back to the Future Part IV' or its release year, which is accurate since such a movie does not exist. The generated answer is faithful to the gold response in meaning, only phrased slightly differently while conveying the same lack of data.", "main_total_tokens": 3106, "main_prompt_tokens": 3077, "main_completion_tokens": 29, "judge_total_tokens": 526, "judge_prompt_tokens": 429, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "There is no information available about a movie from 1980 related to the nuclear Chernobyl disaster in the provided documents.", "latency_ms": 1370.8138465881348, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers convey that there is no information / no movie from 1980 about the Chernobyl disaster in the given context, which correctly addresses the question given the documents. The generated answer stays faithful to the limitation of the provided documents and does not introduce unsupported claims.", "main_total_tokens": 3093, "main_prompt_tokens": 3068, "main_completion_tokens": 25, "judge_total_tokens": 520, "judge_prompt_tokens": 431, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are four TV shows or movies with 'Astro Boy' in the title:\n\n1. Astro Boy (TV, 1980) [Doc 112591]\n2. Astro Boy (TV, 2003) [Doc 116265]\n3. Astro Boy (Movie, 2009) [Doc 44947]\n4. Astro Boy vs The Junkyard Pirates (Movie, 2010) [Doc 49121]", "latency_ms": 1152.726173400879, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 1.1510196182278052, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.5330476224182168, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there are four titles with 'Astro Boy' in the title, matching the gold answer of 4, and the listed examples are consistent with that count. The response is both correct and faithful to the provided information.", "main_total_tokens": 3007, "main_prompt_tokens": 2917, "main_completion_tokens": 90, "judge_total_tokens": 557, "judge_prompt_tokens": 476, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "None of the movies from 2025 in the provided documents allude to the Holocaust. The movies listed for 2025 are \"Everyone Is Going to Die\" [Doc 111564], \"The Future Awaits\" [Doc 112009], \"What Happens After the Massacre?\" [Doc 112547], \"We're All Gonna Die\" [Doc 111988], \"Shadow of God\" [Doc 111520], \"Mazel Tov\" [Doc 112087], and \"Nobody\" [Doc 111668], and none of their descriptions mention the Holocaust.", "latency_ms": 1322.8802680969238, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 2 such movies, while the generated answer claims there are none, so it is factually incorrect relative to the labeled gold. However, the generated answer is internally consistent and grounded in the cited documents, so it is faithful to the provided context.", "main_total_tokens": 3321, "main_prompt_tokens": 3201, "main_completion_tokens": 120, "judge_total_tokens": 591, "judge_prompt_tokens": 504, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "There were two TV shows released in the 1990s about Super Mario Bros:\n\n1. \"The Adventures of Super Mario Bros. 3\" (1990) [Doc 113661].\n2. \"Super Mario World\" (1991) [Doc 113814].", "latency_ms": 1036.9455814361572, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.8010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says there was 1 TV show, but the generated answer claims there were 2. This contradicts the gold and thus is incorrect in context. It is also unfaithful to the provided gold standard, which it should match.", "main_total_tokens": 3124, "main_prompt_tokens": 3067, "main_completion_tokens": 57, "judge_total_tokens": 525, "judge_prompt_tokens": 444, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "There are no TV shows about Super Mario Bros released between 2000 and 2010 mentioned in the provided documents.", "latency_ms": 1059.1957569122314, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0, and the generated answer states that there are no TV shows about Super Mario Bros released between 2000 and 2010 mentioned in the documents, which aligns with the gold. It also correctly restricts itself to the provided documents, so both correctness and faithfulness are satisfied.", "main_total_tokens": 2776, "main_prompt_tokens": 2752, "main_completion_tokens": 24, "judge_total_tokens": 506, "judge_prompt_tokens": 413, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular TV show or movie about ninjas is \"House of Ninjas,\" a TV show from 2024, with a popularity rating of 4.4317 [Doc 135161].", "latency_ms": 1101.2351512908936, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'Naruto' as the most popular ninja-related TV show, whereas the generated answer claims 'House of Ninjas' is the most popular, which contradicts the gold. This makes the answer factually incorrect relative to the task. Additionally, the reference to a specific document and rating appears to be a hallucinated or misused citation rather than grounded support, so the response is not faithful.", "main_total_tokens": 3201, "main_prompt_tokens": 3160, "main_completion_tokens": 41, "judge_total_tokens": 537, "judge_prompt_tokens": 423, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "There is no information in the provided documents about a Jurassic Park/Power Ranger crossover movie.", "latency_ms": 852.4620532989502, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the movie was not released. The generated answer instead claims there is no information in the provided documents, which does not correctly address the question and contradicts the gold. However, it is still faithful to the (hypothetical) provided documents it references, so faithfulness is high but correctness relative to the gold is low.", "main_total_tokens": 2988, "main_prompt_tokens": 2970, "main_completion_tokens": 18, "judge_total_tokens": 504, "judge_prompt_tokens": 403, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "There is no movie in the provided documents where Barbie fights the Predator. Barbie movies and Predator movies are separate franchises and do not intersect in the given context.", "latency_ms": 1020.5743312835693, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The gold answer clarifies that such a movie does not exist because it was not released, while the generated answer incorrectly ties its reasoning to the scope of 'provided documents' and 'given context'. This adds an unjustified limitation that is not in the question or gold. Although the conclusion that no such movie exists is factually correct, the explanation is not faithful to the gold rationale, which is why faithfulness is 0.", "main_total_tokens": 2957, "main_prompt_tokens": 2926, "main_completion_tokens": 31, "judge_total_tokens": 530, "judge_prompt_tokens": 413, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The 4th oldest Star Wars movie is \"Return of the Jedi,\" which was released in 1983 [Doc 2858].", "latency_ms": 763.7312412261963, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The generated answer is incorrect: the 4th oldest Star Wars movie per the gold standard is 'Star Wars: Droids - The Great Heep,' not 'Return of the Jedi.' In terms of faithfulness, the model cites a document [Doc 2858] as the source for its claim, but we are evaluating relative to the gold answer rather than the document corpus. Since the statement directly contradicts the gold fact, it is not faithful to the ground truth, though it is internally consistent with its cited source. Thus correctness is 0 and faithfulness is partial.", "main_total_tokens": 3032, "main_prompt_tokens": 3004, "main_completion_tokens": 28, "judge_total_tokens": 567, "judge_prompt_tokens": 417, "judge_completion_tokens": 150}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The movie \"YEAR\" was released in 2023 [Doc 107925].", "latency_ms": 616.286039352417, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states the movie was released in 2023, while the correct (gold) release year is 1986, so it is factually incorrect. There is no indication it is being faithful to any provided source; instead, it introduces an incorrect year, so faithfulness is also scored as 0.", "main_total_tokens": 3045, "main_prompt_tokens": 3028, "main_completion_tokens": 17, "judge_total_tokens": 489, "judge_prompt_tokens": 394, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The rating of the movie \"Forrest Gump\" is 8.466 [Doc 15236].", "latency_ms": 1270.0059413909912, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating value 8.466 and correctly refers to the movie \"Forrest Gump\" (not the documentary). It is fully supported by the cited document and contains no extra, fabricated information.", "main_total_tokens": 2667, "main_prompt_tokens": 2645, "main_completion_tokens": 22, "judge_total_tokens": 493, "judge_prompt_tokens": 416, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show is \"Mighty Med\" [Doc 121125].", "latency_ms": 684.316873550415, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med', which matches the gold answer exactly. It faithfully reflects the information without adding incorrect details.", "main_total_tokens": 2828, "main_prompt_tokens": 2810, "main_completion_tokens": 18, "judge_total_tokens": 472, "judge_prompt_tokens": 410, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In \"Túnel de los huesos\" (2011), seven prisoners escaped from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 792.2458648681641, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the country; the generated answer specifies the prison and city and correctly includes the country as Argentina, matching the gold answer. It stays within the information and is faithful to the query and gold.", "main_total_tokens": 3138, "main_prompt_tokens": 3104, "main_completion_tokens": 34, "judge_total_tokens": 498, "judge_prompt_tokens": 425, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice Li Nai falls into a trap designed by Jiang Jie [Doc 135231].", "latency_ms": 1470.499038696289, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The gold answer states simply that the apprentice falls into Jiang Jie's trap. The generated answer also attributes the trap to Jiang Jie, which matches the gold, so the core answer is correct. It adds the detail that the apprentice is Li Nai and that the trap was designed by Jiang Jie, which is consistent with the referenced document and does not contradict the gold. However, it relies on a specific document citation and adds extra specifics not present in the gold, so faithfulness is slightly less than perfect but still high.", "main_total_tokens": 2833, "main_prompt_tokens": 2806, "main_completion_tokens": 27, "judge_total_tokens": 547, "judge_prompt_tokens": 411, "judge_completion_tokens": 136}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "There are no Korean-language movies that mention assassins after 2023 in the provided documents.", "latency_ms": 716.829776763916, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says 4, but the generated answer claims there are none, which is incorrect relative to the gold, so correctness is 0. However, the model explicitly grounds its answer in the limitation of the provided documents, so it remains faithful to its stated evidence and constraints, yielding a faithfulness score of 1.", "main_total_tokens": 2736, "main_prompt_tokens": 2717, "main_completion_tokens": 19, "judge_total_tokens": 499, "judge_prompt_tokens": 402, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "The provided documents do not contain any information about movies featuring both Steven Seagal and Bruce Lee.", "latency_ms": 887.3388767242432, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information in the provided documents about movies featuring both Steven Seagal and Bruce Lee, which matches the gold answer in meaning. It is factually correct relative to the given context and faithful to it, without adding unsupported claims.", "main_total_tokens": 3278, "main_prompt_tokens": 3259, "main_completion_tokens": 19, "judge_total_tokens": 496, "judge_prompt_tokens": 412, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 620.1455593109131, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating value of 6.3 and does not add unsupported information, so it is both correct and faithful.", "main_total_tokens": 3031, "main_prompt_tokens": 3011, "main_completion_tokens": 20, "judge_total_tokens": 463, "judge_prompt_tokens": 403, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The movie \"Fast Track: No Limits\" centers around a group of street racers [Doc 41929].", "latency_ms": 911.064863204956, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.7, "justification": "The question asks specifically for the style of racing in the movie, with the gold answer being 'Drift.' The generated answer talks about street racing and does not mention drifting at all, so it is incorrect. In terms of faithfulness, the answer does not hallucinate details far outside the likely content of the document (a movie about racing could involve street racers), but it is not grounded to the specific required detail. Thus, correctness is 0 and faithfulness is partial.", "main_total_tokens": 3075, "main_prompt_tokens": 3053, "main_completion_tokens": 22, "judge_total_tokens": 529, "judge_prompt_tokens": 400, "judge_completion_tokens": 129}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 877.9730796813965, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Confronting a Serial Killer') and does not introduce any contradictions or unsupported claims. It is correct and faithful.", "main_total_tokens": 3008, "main_prompt_tokens": 2988, "main_completion_tokens": 20, "judge_total_tokens": 489, "judge_prompt_tokens": 426, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 1288.809061050415, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and does not introduce any new unsupported information. It is both correct and faithful to the provided gold data.", "main_total_tokens": 3057, "main_prompt_tokens": 3035, "main_completion_tokens": 22, "judge_total_tokens": 468, "judge_prompt_tokens": 407, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 958.7547779083252, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6895405204413555, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'The Sixth Sense'. It is fully faithful to the provided information without adding incorrect details.", "main_total_tokens": 2672, "main_prompt_tokens": 2643, "main_completion_tokens": 29, "judge_total_tokens": 484, "judge_prompt_tokens": 420, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 840.5053615570068, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold year (2002) and is fully supported by the provided information, with no contradictions or fabrication.", "main_total_tokens": 2978, "main_prompt_tokens": 2961, "main_completion_tokens": 17, "judge_total_tokens": 454, "judge_prompt_tokens": 397, "judge_completion_tokens": 57}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 1791.201114654541, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly giving the title as 'The Transporter Refueled' with no contradictions or unsupported claims.", "main_total_tokens": 2909, "main_prompt_tokens": 2880, "main_completion_tokens": 29, "judge_total_tokens": 480, "judge_prompt_tokens": 420, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in the year 2015 [Doc 67500].", "latency_ms": 845.5986976623535, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It is fully consistent and supported, so both correctness and faithfulness are maximal.", "main_total_tokens": 2972, "main_prompt_tokens": 2950, "main_completion_tokens": 22, "judge_total_tokens": 479, "judge_prompt_tokens": 406, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "None of the romantic comedies listed in the documents involve Abraham Lincoln coming back from the dead. The movies titled \"A Romantic Comedy\" [Doc 47970], \"Romantic Comedy\" [Doc 3294], and \"Romantic Comedy\" [Doc 94894] do not mention Abraham Lincoln or any plot involving him returning from the dead.", "latency_ms": 1261.9547843933105, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a specific romantic comedy where Abraham Lincoln comes back from the dead. Both the gold and generated answers state that no such movie description can be found in the provided documents. The generated answer correctly references the documents and notes that none mention Abraham Lincoln or his resurrection, matching the gold answer’s conclusion. It is correct with respect to the question and faithful to the supposed source documents.", "main_total_tokens": 2737, "main_prompt_tokens": 2666, "main_completion_tokens": 71, "judge_total_tokens": 570, "judge_prompt_tokens": 461, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of the movie that alludes to a manuscript written by Ernest Hemingway is \"Havana Darkness\" [Doc 90923].", "latency_ms": 904.799222946167, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'Havana Darkness' as the movie title. It is faithful to the provided information without adding unsupported details.", "main_total_tokens": 2769, "main_prompt_tokens": 2742, "main_completion_tokens": 27, "judge_total_tokens": 479, "judge_prompt_tokens": 415, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "In the TV show \"Doll & Em,\" Doll heads to Hollywood to be with her childhood best friend Em, who is now a successful actress [Doc 122259].", "latency_ms": 1130.9776306152344, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Doll & Em\" and accurately states that Doll goes to Hollywood to be with her childhood best friend Em, matching the gold answer and not adding unsupported claims beyond a minor elaboration consistent with the premise.", "main_total_tokens": 2868, "main_prompt_tokens": 2833, "main_completion_tokens": 35, "judge_total_tokens": 506, "judge_prompt_tokens": 424, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in the 2007 movie \"Hallam Foe\" [Doc 39389].", "latency_ms": 1023.9014625549316, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2007 movie as \"Hallam Foe,\" matching the gold answer exactly. It does not add any contradictory or unsupported information and stays faithful to the given question and gold answer.", "main_total_tokens": 2877, "main_prompt_tokens": 2839, "main_completion_tokens": 38, "judge_total_tokens": 511, "judge_prompt_tokens": 437, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The context provided does not specify any character's last name or whether it sounds like \"friend\" or \"foe.\" Therefore, the answer is not found.", "latency_ms": 1072.8650093078613, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'A foe'. The generated answer claims the information is not found, which is incorrect given the gold label, so correctness is 0. However, based on the (unstated) context the model had, it did not hallucinate an answer and accurately stated that the context did not specify this, so its reasoning was faithful to the given context, yielding faithfulness 1.", "main_total_tokens": 2732, "main_prompt_tokens": 2700, "main_completion_tokens": 32, "judge_total_tokens": 529, "judge_prompt_tokens": 418, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "In the 2017 TV show \"Atypical,\" Sam is 18 years old [Doc 124583].", "latency_ms": 1052.9670715332031, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show 'Atypical,' which matches the gold answer exactly and does not introduce unsupported information beyond a generic citation placeholder.", "main_total_tokens": 2686, "main_prompt_tokens": 2662, "main_completion_tokens": 24, "judge_total_tokens": 484, "judge_prompt_tokens": 411, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show \"Atypical,\" Sam is on the autism spectrum [Doc 124583].", "latency_ms": 1224.266529083252, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the 2017 TV show 'Atypical,' Sam is on the autism spectrum, which exactly matches the gold answer. The citation does not contradict the claim, so the response is both correct and faithful.", "main_total_tokens": 2857, "main_prompt_tokens": 2833, "main_completion_tokens": 24, "judge_total_tokens": 491, "judge_prompt_tokens": 411, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "The information about the movie \"À corde tendue\" and the birthplace of Sandra is not available in the provided documents.", "latency_ms": 2890.7575607299805, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Chamonix', but the generated answer claims the information is not available instead of providing this, so it is factually incorrect. However, the response is faithful to the (lack of) information it says it has and does not invent a birthplace, so it is faithful to its stated context.", "main_total_tokens": 3111, "main_prompt_tokens": 3087, "main_completion_tokens": 24, "judge_total_tokens": 516, "judge_prompt_tokens": 421, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "The answer is not found in the provided documents.", "latency_ms": 1127.05397605896, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'rains'. The generated answer incorrectly claims the answer is not found, so it is factually wrong. However, given the prompt context, it does not contradict any provided document content, so it remains faithful to what it states (it just fails to answer).", "main_total_tokens": 2852, "main_prompt_tokens": 2842, "main_completion_tokens": 10, "judge_total_tokens": 489, "judge_prompt_tokens": 401, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 896.1067199707031, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which matches the gold answer exactly and remains faithful to the provided information.", "main_total_tokens": 3043, "main_prompt_tokens": 3021, "main_completion_tokens": 22, "judge_total_tokens": 478, "judge_prompt_tokens": 416, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is \"One Afternoon\" [Doc 96838].", "latency_ms": 774.9114036560059, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'One Afternoon,' which matches the gold answer. It introduces a citation-style note '[Doc 96838]' but does not contradict the source or add unsupported claims, so it remains faithful to the given information.", "main_total_tokens": 2771, "main_prompt_tokens": 2727, "main_completion_tokens": 44, "judge_total_tokens": 529, "judge_prompt_tokens": 448, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" from 2020, with a rating of 6.746 [Doc 93260].", "latency_ms": 7445.448637008667, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the name of the low-rated Italian-language movie where a Holocaust survivor cares for a 12-year-old street kid who robbed her. The gold answer is \"The Life Ahead\". The generated answer correctly identifies the movie as \"The Life Ahead\" (2020) and stays consistent with the described plot. The added rating detail does not contradict the gold and is plausibly sourced from the cited document, so the answer is both correct and faithful to the given information.", "main_total_tokens": 3435, "main_prompt_tokens": 3385, "main_completion_tokens": 50, "judge_total_tokens": 576, "judge_prompt_tokens": 448, "judge_completion_tokens": 128}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 1105.349063873291, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and aligns with the gold answer. It does not introduce contradictions or unsupported claims beyond a minor, accurate elaboration (noting he founded the McLaren racing team).", "main_total_tokens": 3099, "main_prompt_tokens": 3066, "main_completion_tokens": 33, "judge_total_tokens": 498, "judge_prompt_tokens": 417, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The name of the racing team is not mentioned in the provided documents.", "latency_ms": 1651.3235569000244, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states the team is the McLaren Motor Racing team. The generated answer claims the team name is not mentioned, which is factually incorrect relative to the gold. However, it correctly reflects that, in the assistant's view, the documents did not specify the name, so it is faithful to the (mis)interpretation of the provided context rather than hallucinating a name.", "main_total_tokens": 3132, "main_prompt_tokens": 3118, "main_completion_tokens": 14, "judge_total_tokens": 506, "judge_prompt_tokens": 397, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context does not provide information about the country of origin for any specific individual.", "latency_ms": 755.6912899017334, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the correct country is New Zealand, but the generated answer claims that the context does not provide information, effectively giving no country. This is factually incorrect with respect to the gold label. However, the model response is faithful to the context as described (it does not hallucinate a country and accurately reflects that the context, as it saw it, did not specify one).", "main_total_tokens": 2920, "main_prompt_tokens": 2904, "main_completion_tokens": 16, "judge_total_tokens": 502, "judge_prompt_tokens": 392, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "There is no 2013 movie in the provided documents where John McClane blows up the Great Wall of China. The movie \"A Good Day to Die Hard\" from 2013 features John McClane, but it is set in Moscow and involves a nuclear weapons heist, not the Great Wall of China [Doc 58791].", "latency_ms": 1475.409984588623, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no 2013 movie where John McClane blows up the Great Wall of China and correctly identifies that 'A Good Day to Die Hard' (2013) is set in Moscow with a nuclear plot, not involving the Great Wall. This is consistent with the gold answer and with the referenced document, so it is both factually correct and faithful to the provided information.", "main_total_tokens": 2944, "main_prompt_tokens": 2874, "main_completion_tokens": 70, "judge_total_tokens": 582, "judge_prompt_tokens": 468, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with that description is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 856.4679622650146, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the film as 'Tomie: Forbidden Fruit'. It is fully correct and consistent with the provided information.", "main_total_tokens": 2945, "main_prompt_tokens": 2920, "main_completion_tokens": 25, "judge_total_tokens": 505, "judge_prompt_tokens": 442, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie \"Lei mi parla ancora,\" Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 849.8344421386719, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (65 years) and does not introduce extraneous or conflicting information, so it is both correct and faithful.", "main_total_tokens": 3170, "main_prompt_tokens": 3140, "main_completion_tokens": 30, "judge_total_tokens": 485, "judge_prompt_tokens": 424, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In \"Scorpion King: Book of Souls\" (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls, which will allow them to put an end to an evil warlord [Doc 82582].", "latency_ms": 1078.3936977386475, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in 'Scorpion King: Book of Souls' (2018), the Scorpion King and Tala are searching for the Book of Souls, matching the gold answer. The added detail about it being a legendary relic that can end an evil warlord is consistent with the film’s plot and does not contradict the given information, so it is faithful to the prompt and source.", "main_total_tokens": 3558, "main_prompt_tokens": 3507, "main_completion_tokens": 51, "judge_total_tokens": 556, "judge_prompt_tokens": 445, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Zombies on the Moon\" is not found in the provided documents.", "latency_ms": 907.20534324646, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie that is not in the known data. The gold answer correctly states that the movie is not in the database. The generated answer similarly states that the release year is not found in the provided documents. This does not invent a year and remains faithful to the limitation of the available information, so it is both correct and faithful relative to the gold.", "main_total_tokens": 2623, "main_prompt_tokens": 2604, "main_completion_tokens": 19, "judge_total_tokens": 519, "judge_prompt_tokens": 410, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Galactic baker\" is not found in the provided documents.", "latency_ms": 1452.3212909698486, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both gold and generated answers state that the release year is not available/found; no hallucinated year is given, so the response is correct and faithful to the (lack of) information.", "main_total_tokens": 3225, "main_prompt_tokens": 3208, "main_completion_tokens": 17, "judge_total_tokens": 475, "judge_prompt_tokens": 406, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Return of the Null Pointer\" is not found in the provided documents.", "latency_ms": 1356.706142425537, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers convey that the release year is not available; the generated answer is consistent with the information given and does not invent a date.", "main_total_tokens": 2929, "main_prompt_tokens": 2910, "main_completion_tokens": 19, "judge_total_tokens": 472, "judge_prompt_tokens": 410, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Data Science: The Movie\" is not found in the provided documents.", "latency_ms": 1471.2190628051758, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available/found. This is correct and the generated answer is fully faithful to the gold response in meaning.", "main_total_tokens": 2837, "main_prompt_tokens": 2818, "main_completion_tokens": 19, "judge_total_tokens": 475, "judge_prompt_tokens": 410, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Python's Revenge 2\" is not found in the provided documents.", "latency_ms": 773.1995582580566, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database. The generated answer states that the release year is not found in the provided documents, which is effectively the same claim and thus correct. It does not hallucinate a year and remains faithful to the lack of information in the context.", "main_total_tokens": 3231, "main_prompt_tokens": 3212, "main_completion_tokens": 19, "judge_total_tokens": 498, "judge_prompt_tokens": 410, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document set does not contain any information about a movie titled \"The Last Pixel.\"", "latency_ms": 1393.176794052124, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that there is no information about a movie titled 'The Last Pixel' in the available sources. This appropriately avoids hallucinating a release year. The generated response is consistent with the provided document set and the gold answer, so it is both correct and faithful.", "main_total_tokens": 2552, "main_prompt_tokens": 2535, "main_completion_tokens": 17, "judge_total_tokens": 495, "judge_prompt_tokens": 406, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The man who ate the sun\" is not found in the provided documents.", "latency_ms": 876.3329982757568, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year / information is not available in the given resources. The generated answer is correct with respect to the provided documents and faithfully reflects the lack of information without fabricating a year.", "main_total_tokens": 3438, "main_prompt_tokens": 3418, "main_completion_tokens": 20, "judge_total_tokens": 488, "judge_prompt_tokens": 412, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Days of Lehman Brothers\" is not mentioned in the provided documents.", "latency_ms": 762.0487213134766, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director is not provided/mentioned in the available data. This matches the constraint of answering only from the database, so the response is correct and faithful.", "main_total_tokens": 3196, "main_prompt_tokens": 3174, "main_completion_tokens": 22, "judge_total_tokens": 489, "judge_prompt_tokens": 419, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"A Love to Die For\" is not mentioned in the provided documents.", "latency_ms": 1312.11519241333, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned, aligning with the gold answer that the database does not provide this information. It does not introduce unsupported details, so it is both correct and faithful to the given context.", "main_total_tokens": 3020, "main_prompt_tokens": 3000, "main_completion_tokens": 20, "judge_total_tokens": 491, "judge_prompt_tokens": 415, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not mentioned in the provided documents.", "latency_ms": 759.0072154998779, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director of 'Journey into Gaza' is not mentioned in the provided documents, which aligns with the gold answer that the information is not provided in the database. It does not introduce unsupported details and remains faithful to the given information.", "main_total_tokens": 3122, "main_prompt_tokens": 3104, "main_completion_tokens": 18, "judge_total_tokens": 494, "judge_prompt_tokens": 411, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Night at Tremore Beach\" is not mentioned in the provided documents. [Doc 135098]", "latency_ms": 780.3432941436768, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided sources. This matches the database constraint and does not hallucinate a director, so it is both correct and faithful to the documents.", "main_total_tokens": 2673, "main_prompt_tokens": 2645, "main_completion_tokens": 28, "judge_total_tokens": 499, "judge_prompt_tokens": 425, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 724.2717742919922, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold: it identifies the movie as 'Rise of the Mummy'. There is no contradiction or fabrication beyond the given information; referencing [Doc 98375] does not alter the correctness or faithfulness to the prompt and gold.", "main_total_tokens": 2794, "main_prompt_tokens": 2767, "main_completion_tokens": 27, "judge_total_tokens": 496, "judge_prompt_tokens": 418, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse is \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 991.1081790924072, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, identifying the movie as \"Stranger in My Bed.\" It is correct and remains consistent with the provided information without adding unsupported details.", "main_total_tokens": 2818, "main_prompt_tokens": 2789, "main_completion_tokens": 29, "judge_total_tokens": 491, "judge_prompt_tokens": 425, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie you are referring to is not a movie, but a TV show titled \"Game of Silence\" [Doc 123938].", "latency_ms": 900.0027179718018, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question explicitly asks for a movie, while the gold answer and the generated response identify 'Game of Silence', which is a TV show, not a movie. Therefore, relative to the gold, the generated answer is faithful but factually incorrect in the question’s terms because it contradicts the requested format (movie vs. TV show) and thus does not provide a correct movie title.", "main_total_tokens": 2441, "main_prompt_tokens": 2414, "main_completion_tokens": 27, "judge_total_tokens": 532, "judge_prompt_tokens": 424, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie in which a true story about a killer from the site Craigslist is depicted is \"The Craigslist Killer\" [Doc 53039].", "latency_ms": 1015.5117511749268, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'The Craigslist Killer,' and the generated answer correctly identifies the same movie and stays aligned with the given information. It is both correct and faithful to the prompt and gold.", "main_total_tokens": 2665, "main_prompt_tokens": 2637, "main_completion_tokens": 28, "judge_total_tokens": 489, "judge_prompt_tokens": 420, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which this happens is \"Wood Job!\" [Doc 66094].", "latency_ms": 744.2965507507324, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Wood Job!', exactly matching the gold answer. It remains faithful to the provided information and does not introduce contradictions or extraneous claims.", "main_total_tokens": 4075, "main_prompt_tokens": 4058, "main_completion_tokens": 17, "judge_total_tokens": 483, "judge_prompt_tokens": 417, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure is \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 1079.8711776733398, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Sesame Street: The Magical Wand Chase' and aligns with the described plot event. It is fully correct and faithful to the provided gold answer.", "main_total_tokens": 3175, "main_prompt_tokens": 3126, "main_completion_tokens": 49, "judge_total_tokens": 528, "judge_prompt_tokens": 459, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a rock fall traps a boy's parents inside a burning car, and the 7-year-old boy must find the hero within him to save their lives, is \"Beyond\" [Doc 74597].", "latency_ms": 1282.7222347259521, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is simply the title 'Beyond', but the generated answer adds an unsupported citation-like reference '[Doc 74597]', which is not grounded in the user-provided information. While the movie title itself matches the gold, introducing a fabricated document reference makes the response partially unfaithful to the source. Under strict evaluation, any hallucinated source or reference renders the response unfaithful, and thus incorrect as a whole.", "main_total_tokens": 2652, "main_prompt_tokens": 2607, "main_completion_tokens": 45, "judge_total_tokens": 570, "judge_prompt_tokens": 453, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman, fired from a financial corporation during the Asia crisis, returns home with no money is \"6ixtynin9\" [Doc 22478].", "latency_ms": 952.5306224822998, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as '6ixtynin9'. It is correct and consistent with the provided information, with no hallucinations or contradictions.", "main_total_tokens": 2756, "main_prompt_tokens": 2719, "main_completion_tokens": 37, "judge_total_tokens": 508, "judge_prompt_tokens": 439, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie in which a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort is \"Club Dread\" [Doc 32410].", "latency_ms": 969.3253040313721, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread', which matches the gold answer exactly. It stays faithful to the given information and does not add any incorrect or extraneous claims.", "main_total_tokens": 3059, "main_prompt_tokens": 3027, "main_completion_tokens": 32, "judge_total_tokens": 525, "judge_prompt_tokens": 456, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is \"Toilet\" [Doc 48873].", "latency_ms": 944.6959495544434, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" matching the gold answer exactly, and does not introduce any extraneous or contradictory information. It is fully faithful to the provided information.", "main_total_tokens": 3420, "main_prompt_tokens": 3369, "main_completion_tokens": 51, "judge_total_tokens": 534, "judge_prompt_tokens": 465, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century is \"Electric Angel\" [Doc 1024].", "latency_ms": 978.7986278533936, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Electric Angel,' which matches the gold answer exactly. It stays faithful to the information given in the prompt and does not introduce contradictions or hallucinations beyond restating the description and citing a document.", "main_total_tokens": 3251, "main_prompt_tokens": 3206, "main_completion_tokens": 45, "judge_total_tokens": 539, "judge_prompt_tokens": 461, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is \"Bottom of the Water\" with a rating of 4.2 [Doc 106899].", "latency_ms": 1249.8559951782227, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the lowest rated movie from 2023 is 'Mega Ape', while the generated answer claims it is 'Bottom of the Water' with a rating of 4.2. This directly contradicts the gold answer, so the response is factually incorrect in this evaluation context. Additionally, the generated answer introduces a specific rating and citation without basis in the provided gold information, so it is not faithful to the source.", "main_total_tokens": 2613, "main_prompt_tokens": 2584, "main_completion_tokens": 29, "judge_total_tokens": 529, "judge_prompt_tokens": 410, "judge_completion_tokens": 119}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "The highest rated TV show from 2010 is \"Sherlock,\" with a rating of 8.511 [Doc 119150].", "latency_ms": 925.1692295074463, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the highest rated TV show from 2010, but the gold answer is simply 'Tea House', which the generated answer does not match. The generated answer claims the show is 'Sherlock' with a specific rating and citation, which directly contradicts the gold answer. Since we must treat the gold as ground truth, the generated answer is incorrect. It is also unfaithful because it invents a specific show and rating that do not align with the provided gold answer.", "main_total_tokens": 2946, "main_prompt_tokens": 2918, "main_completion_tokens": 28, "judge_total_tokens": 535, "judge_prompt_tokens": 410, "judge_completion_tokens": 125}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest rated Japanese language movie from 1999 is \"Kikujiro,\" with a rating of 7.5 [Doc 22213].", "latency_ms": 1127.5949478149414, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold answer identifies 'The Uncut Story of Mewtwo's Origin' as the highest rated Japanese language movie from 1999, while the model claims it is 'Kikujiro.' This directly contradicts the gold label. It is also unfaithful to the provided gold answer and thus scores 0 on faithfulness.", "main_total_tokens": 3112, "main_prompt_tokens": 3081, "main_completion_tokens": 31, "judge_total_tokens": 527, "judge_prompt_tokens": 422, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is \"Read Lips,\" which has a rating of 4.5. The language of this movie is Chinese (cn) [Doc 354].", "latency_ms": 886.4896297454834, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980. The gold answer is English, but the generated answer incorrectly states the language as Chinese. It does, however, remain grounded in the provided document reference, so it is faithful to its cited source but factually incorrect relative to the gold.", "main_total_tokens": 2662, "main_prompt_tokens": 2624, "main_completion_tokens": 38, "judge_total_tokens": 515, "judge_prompt_tokens": 421, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show from 1999 with the highest popularity is \"Popular,\" with a popularity score of 4.5113 [Doc 115196].", "latency_ms": 887.2482776641846, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold label indicates that the correct show is 'Law & Order: Special Victims Unit,' not 'Popular.' It is also unfaithful, as it invents a specific popularity score and cites a document (Doc 115196) that does not align with the provided gold answer, indicating reliance on or fabrication of evidence inconsistent with the ground truth.", "main_total_tokens": 3856, "main_prompt_tokens": 3825, "main_completion_tokens": 31, "judge_total_tokens": 529, "judge_prompt_tokens": 421, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 649.040937423706, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and correctly attributes it to the document, so it is both correct and faithful to the provided source.", "main_total_tokens": 2870, "main_prompt_tokens": 2852, "main_completion_tokens": 18, "judge_total_tokens": 464, "judge_prompt_tokens": 400, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "There are multiple movies titled \"The Story of Us\" released in different years. The years of release are 1999 [Doc 22481], 2019 [Doc 88811], and 2025 [Doc 112077].", "latency_ms": 1019.1206932067871, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the release year of the movie \"The Story of Us\" with a gold answer of 2019. The generated answer lists multiple years (1999, 2019, 2025) without selecting 2019 as the specific answer, so it is incorrect relative to the gold. It is also unfaithful to the provided supervision, which expects a single year (2019), and the mention of a 2025 release appears speculative and unsupported.", "main_total_tokens": 2826, "main_prompt_tokens": 2777, "main_completion_tokens": 49, "judge_total_tokens": 560, "judge_prompt_tokens": 433, "judge_completion_tokens": 127}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The number of votes \"My Dog Skip\" received is not provided in the document.", "latency_ms": 787.3859405517578, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that \"My Dog Skip\" received 279 votes. The generated answer claims that the number of votes is not provided in the document, which contradicts the gold answer and is therefore factually incorrect. However, the response is internally consistent with what it asserts about the (purported) document and does not hallucinate a specific number, so it is faithful to its own stated access to information.", "main_total_tokens": 3049, "main_prompt_tokens": 3032, "main_completion_tokens": 17, "judge_total_tokens": 507, "judge_prompt_tokens": 398, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The number of votes for \"Oh My God!\" is not provided in the given documents.", "latency_ms": 664.8788452148438, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that 'Oh My God!' received 12 votes. The generated answer claims the number of votes is not provided, which contradicts the gold. This is incorrect relative to the gold. However, the answer is internally consistent and does not fabricate a number; it faithfully states that the information is unavailable, so it is faithful to its (incorrect) reading of the context.", "main_total_tokens": 2896, "main_prompt_tokens": 2878, "main_completion_tokens": 18, "judge_total_tokens": 503, "judge_prompt_tokens": 399, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in the year 2009 [Doc 46069].", "latency_ms": 917.9258346557617, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'The Last Days of Lehman Brothers' was released in 2009, which matches the gold answer. It does not introduce any unsupported information beyond citing a document, so it is both correct and faithful to the given gold answer.", "main_total_tokens": 3192, "main_prompt_tokens": 3167, "main_completion_tokens": 25, "judge_total_tokens": 498, "judge_prompt_tokens": 412, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_openai-small_gpt-4o", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 849.1876125335693, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the original language of 'The King\\'s Speech' is English, matching the gold answer 'en'. It is fully consistent with the provided information and does not add any conflicting or unsupported details.", "main_total_tokens": 3165, "main_prompt_tokens": 3146, "main_completion_tokens": 19, "judge_total_tokens": 479, "judge_prompt_tokens": 402, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The movie \"Wall Street\" was released in the year 1987 [Doc 6993].", "latency_ms": 1250.81205368042, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'Wall Street' was released in 1987, which matches the gold answer. It is also faithful because it directly answers the question without adding unsupported information.", "main_total_tokens": 2272, "main_prompt_tokens": 2252, "main_completion_tokens": 20, "judge_total_tokens": 467, "judge_prompt_tokens": 398, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie where classmates have to play a survival game using their cell phones is \"47 Hours to Live\" [Doc 90746].", "latency_ms": 840.972900390625, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.91, "justification": "The correct answer is 'Brain Fluid Explosion Girl.' The generated answer incorrectly states '47 Hours to Live,' so it is factually wrong, giving correctness 0. The model, however, is faithful to the (incorrect) document it cites, so faithfulness is high but not perfect since it presents the citation as authoritative without qualification.", "main_total_tokens": 2014, "main_prompt_tokens": 1987, "main_completion_tokens": 27, "judge_total_tokens": 515, "judge_prompt_tokens": 415, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show in which six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" from 1983 [Doc 112849].", "latency_ms": 840.6562805175781, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons' and adds accurate contextual detail (the 1983 series), without contradicting the gold answer or introducing errors. It is both correct and faithful to the gold response.", "main_total_tokens": 2069, "main_prompt_tokens": 2035, "main_completion_tokens": 34, "judge_total_tokens": 503, "judge_prompt_tokens": 424, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is \"Nacht der Wölfe\" [Doc 2590].", "latency_ms": 965.7130241394043, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe', which matches the gold answer exactly. It is fully faithful to the provided information and does not introduce unsupported details beyond restating the description.", "main_total_tokens": 2259, "main_prompt_tokens": 2220, "main_completion_tokens": 39, "judge_total_tokens": 511, "judge_prompt_tokens": 437, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of \"Picnic at Hanging Rock\" take place in the year 1900 [Doc 126191].", "latency_ms": 717.8213596343994, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of 'Picnic at Hanging Rock' take place in the year 1900, which matches the gold answer. It is also faithful to the provided information.", "main_total_tokens": 2068, "main_prompt_tokens": 2044, "main_completion_tokens": 24, "judge_total_tokens": 480, "judge_prompt_tokens": 409, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie about a young violinist named June, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man destined for her, is titled \"June\" [Doc 32483].", "latency_ms": 1055.798053741455, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title, whose answer is simply 'June'. The generated answer states that the movie is titled \"June\" and correctly restates the description from the prompt. It matches the gold answer exactly and does not introduce contradictory or extraneous information, so it is both correct and faithful to the given data.", "main_total_tokens": 2395, "main_prompt_tokens": 2336, "main_completion_tokens": 59, "judge_total_tokens": 587, "judge_prompt_tokens": 491, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 200.53696632385254, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific movie title, and the gold answer provides it. The generated answer says 'I don't know,' which does not state the correct movie, so correctness is 0. However, the generated answer does not invent any false facts and honestly expresses uncertainty, so it is fully faithful to the model's uncertainty, giving a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 505, "judge_prompt_tokens": 399, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 285.186767578125, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies 7.6 as the correct rating, while the generated answer says 'I don't know,' which is incorrect. However, the generated response does not fabricate any information and clearly admits lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 389, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 606.4441204071045, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.5143371794949736, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.013881288441812, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is factually incorrect. However, there is no indication that the model ignored or contradicted the user query; it attempted to answer the exact question asked, so it is faithful to the prompt.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 394, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 686.2459182739258, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is factually incorrect. However, the model directly provided a numeric response without hallucinated reasoning or unsupported claims beyond that number, so it remains internally faithful to its (albeit wrong) output.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 482, "judge_prompt_tokens": 395, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 372.3142147064209, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model response is incorrect because it does not provide the correct answer 'Fantômas'. However, it is faithful to the model's uncertainty, as it honestly states 'I don't know' without fabricating an answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 464, "judge_prompt_tokens": 389, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" [Doc 11205].", "latency_ms": 755.2785873413086, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise', which matches the gold answer. It does not introduce contradictions or unsupported claims, and the bracketed document reference does not affect faithfulness in this context.", "main_total_tokens": 2275, "main_prompt_tokens": 2251, "main_completion_tokens": 24, "judge_total_tokens": 482, "judge_prompt_tokens": 409, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 885.9870433807373, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Won't You Be My Neighbor?' and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 2311, "main_prompt_tokens": 2272, "main_completion_tokens": 39, "judge_total_tokens": 504, "judge_prompt_tokens": 440, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The movie \"YEAR\" was released in 2023 [Doc 107925].", "latency_ms": 1149.6775150299072, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer gives the year 2023, while the gold answer specifies 2018, so it is factually incorrect. It also invents a placeholder movie title \"YEAR\" and cites a document without any provided context, so the response is not faithful to the given information.", "main_total_tokens": 2354, "main_prompt_tokens": 2337, "main_completion_tokens": 17, "judge_total_tokens": 482, "judge_prompt_tokens": 394, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is \"Astro Boy\" [Doc 112591].", "latency_ms": 1235.2890968322754, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy,' which matches the gold answer exactly. It stays faithful to the prompt and adds only a citation-like note without contradicting information.", "main_total_tokens": 2340, "main_prompt_tokens": 2311, "main_completion_tokens": 29, "judge_total_tokens": 486, "judge_prompt_tokens": 416, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 702.1262645721436, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9463946303571863, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer exactly. It is fully supported by the given information and does not introduce any unsupported details, so both correctness and faithfulness are complete.", "main_total_tokens": 1895, "main_prompt_tokens": 1865, "main_completion_tokens": 30, "judge_total_tokens": 506, "judge_prompt_tokens": 424, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 786.0584259033203, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before the break-in, which matches the provided gold answer. It stays faithful to the given information and does not introduce contradictions or unsupported details.", "main_total_tokens": 2131, "main_prompt_tokens": 2098, "main_completion_tokens": 33, "judge_total_tokens": 508, "judge_prompt_tokens": 428, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 576.1489868164062, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific numeric popularity score (80.5772), while the generated answer says 'I don't know,' which does not match the correct value. This makes the generated answer incorrect. However, the answer does not invent any incorrect facts; it simply expresses a lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 497, "judge_prompt_tokens": 396, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 280.5044651031494, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (1), so it is incorrect. The evaluation that it is incorrect is faithful to the provided gold and generated outputs.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 452, "judge_prompt_tokens": 385, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 1372.392177581787, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the TV show as 'Angel Eyes' and does not introduce any contradictions or unsupported claims. Citation style is extraneous but not incorrect.", "main_total_tokens": 2096, "main_prompt_tokens": 2061, "main_completion_tokens": 35, "judge_total_tokens": 497, "judge_prompt_tokens": 429, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French [Doc 110681].", "latency_ms": 717.5674438476562, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states the movie language is French, but the gold label says it is Korean, so the answer is factually incorrect. There is no support in the provided gold information for the claim that it is French, so the response is also unfaithful to the given data.", "main_total_tokens": 1982, "main_prompt_tokens": 1965, "main_completion_tokens": 17, "judge_total_tokens": 481, "judge_prompt_tokens": 393, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 626.2063980102539, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is factually incorrect. However, there is no indication that the model deviated from the user’s request format or added unsupported assumptions beyond providing the wrong numeric value, so it is fully faithful to the (incorrect) reasoning it followed.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 489, "judge_prompt_tokens": 394, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The context provided does not include information about movies released in 1979, so I cannot determine the best-rated movie from that year based on the given documents.", "latency_ms": 1178.5991191864014, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that, given the available/contextual data (the database or provided documents), it is not possible to identify the best-rated movie from 1979. The generated answer is fully faithful to the implied constraint (using only given context) and gives no unsupported information.", "main_total_tokens": 2189, "main_prompt_tokens": 2157, "main_completion_tokens": 32, "judge_total_tokens": 518, "judge_prompt_tokens": 427, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The context provided does not mention a movie titled \"Back to the Future Part IV\" or its release year. The documents only reference the original trilogy and related documentaries or series [Doc 10621], [Doc 9405], [Doc 4834], [Doc 113788], [Doc 68724], [Doc 47044]. Therefore, the release year of \"Back to the Future Part IV\" is not found in the provided context.", "latency_ms": 1091.6686058044434, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.0, "faithfulness_score": 0.5, "justification": "The correct answer is that 'Back to the Future Part IV' does not exist (no release year). The generated answer instead claims the release year is simply not found in the context, implying it might exist but is undocumented, which is factually incorrect. However, it is somewhat faithful to the given (imagined) context because it correctly notes that the documents only refer to the original trilogy and related materials, but it fails to incorporate the crucial fact (from the gold answer) that the movie does not exist at all.", "main_total_tokens": 2521, "main_prompt_tokens": 2429, "main_completion_tokens": 92, "judge_total_tokens": 632, "judge_prompt_tokens": 492, "judge_completion_tokens": 140}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there are no movies in 1980 about the Chernobyl disaster, which is accurate because the Chernobyl disaster happened in 1986. The generated answer remains faithful to the question and does not introduce fabricated data, only suggesting checking the event timing.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 526, "judge_prompt_tokens": 434, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 183.55369567871094, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer ('0') is factually incorrect given the gold answer of 4, so correctness is 0. The model did directly attempt to answer the question without going off-topic or adding unsupported claims, so faithfulness to the user query is high, thus 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 388, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "1", "latency_ms": 247.42603302001953, "retrieval_recall": 0.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.17723928678404774, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1) does not match the gold answer (2), so it is incorrect. The model did not hallucinate extra details beyond giving a numeric answer, so it is faithful to its own (albeit wrong) prediction.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 386, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 231.79078102111816, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that there was 1 TV show released in the 1990s about Super Mario Bros, but the generated answer says 0. This directly contradicts the gold, so the generated answer is incorrect. The faithfulness score is 1 because the evaluation accurately reflects the discrepancy between the gold answer and the generated answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 488, "judge_prompt_tokens": 389, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 184.90290641784668, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks how many TV shows were released between 2000 and 2010 about Super Mario Bros. The gold answer is 0, and the generated answer is also 0. This matches exactly and is consistent with known Super Mario media releases, so the response is both correct and faithful to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 391, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 231.7337989807129, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Naruto', but the generated answer says 'I don't know', which is incorrect. However, the model does not fabricate any incorrect information; it simply admits not knowing, so it is fully faithful to its own stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 467, "judge_prompt_tokens": 386, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The documents provided do not mention a Jurassic Park/Power Ranger crossover movie, nor do they provide a release date for such a film.", "latency_ms": 733.1032752990723, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer clarifies that the movie does not exist and thus was never released. The generated answer only states that the documents do not mention such a movie or a release date, which does not address the factual status of the movie and is therefore incorrect relative to the gold. It is also unfaithful to the gold answer because it fails to convey that the movie was not released at all.", "main_total_tokens": 2219, "main_prompt_tokens": 2192, "main_completion_tokens": 27, "judge_total_tokens": 522, "judge_prompt_tokens": 412, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "There is no movie in the provided documents where Barbie fights the Predator. The documents list various Barbie movies and a movie titled \"The Predator,\" but none of them describe a crossover or interaction between Barbie and the Predator [Doc 68094, Doc 82000, Doc 36330, Doc 48119, Doc 72290, Doc 92886, Doc 52809, Doc 104474, Doc 84103, Doc 78562, Doc 48011, Doc 55216, Doc 25632, Doc 33670, Doc 73700].", "latency_ms": 1435.2962970733643, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer is effectively correct: there is no such movie, which matches the gold answer that the movie was not released. It clearly states that no movie exists where Barbie fights the Predator. However, it adds references to specific documents and claims those documents list certain movies. Without seeing those documents, we cannot fully verify that those references are accurate, so faithfulness is slightly less than perfect.", "main_total_tokens": 2324, "main_prompt_tokens": 2206, "main_completion_tokens": 118, "judge_total_tokens": 611, "judge_prompt_tokens": 500, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The 4th oldest Star Wars movie, based on the provided documents, is \"Return of the Jedi,\" which was released in 1983 [Doc 2858]. The documents do not provide a complete list of all Star Wars movies, but from the available information, \"Return of the Jedi\" is the third movie in the original trilogy, following \"Star Wars\" (1977) and \"The Empire Strikes Back\" (1980) [Doc 3].", "latency_ms": 1472.91898727417, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer identifies 'Star Wars: Droids - The Great Heep' as the 4th oldest Star Wars movie, while the generated answer claims it is 'Return of the Jedi.' This is incorrect relative to the gold, so correctness is 0. However, the generated answer is consistent with and supported by the cited documents (it explicitly notes limitations of the documents and bases its conclusion on them), so it is faithful to the provided context; faithfulness is 1.", "main_total_tokens": 2475, "main_prompt_tokens": 2379, "main_completion_tokens": 96, "judge_total_tokens": 613, "judge_prompt_tokens": 485, "judge_completion_tokens": 128}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The movie \"YEAR\" was released in 2023 [Doc 107925].", "latency_ms": 733.9701652526855, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct release year is 1986, but the generated answer states 2023, so it is factually incorrect. It also fabricates a placeholder movie title (\"YEAR\") and a document reference without basis in the provided gold answer, so it is unfaithful to the source.", "main_total_tokens": 2354, "main_prompt_tokens": 2337, "main_completion_tokens": 17, "judge_total_tokens": 484, "judge_prompt_tokens": 394, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 262.98999786376953, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the movie rating (a specific numeric value). The gold answer is 8.466, but the model responded with 'I don't know', which does not provide the requested value and is therefore incorrect. However, it does not introduce any incorrect facts and honestly admits uncertainty, so it is fully faithful to its own (lack of) knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 501, "judge_prompt_tokens": 398, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 844.2652225494385, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med,' which matches the gold answer exactly and remains faithful to the provided information.", "main_total_tokens": 2036, "main_prompt_tokens": 2005, "main_completion_tokens": 31, "judge_total_tokens": 482, "judge_prompt_tokens": 423, "judge_completion_tokens": 59}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In \"Túnel de los huesos\" (2011), seven prisoners escaped from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 986.6735935211182, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Argentina as the country, matching the gold answer. The mention of Villa Devoto prison in Buenos Aires is plausible and does not contradict the gold answer, though it is an extra detail not present in the gold. Overall, the response is both correct and faithful to the question.", "main_total_tokens": 2455, "main_prompt_tokens": 2421, "main_completion_tokens": 34, "judge_total_tokens": 516, "judge_prompt_tokens": 425, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice Li Nai falls into a trap designed by Jiang Jie [Doc 135231].", "latency_ms": 1202.3613452911377, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The gold answer is simply 'Jiang Jie', and the generated answer states that the apprentice falls into a trap designed by Jiang Jie, which matches the gold. The added detail that the apprentice is Li Nai is not contradicted by the gold but is extra information. Since we cannot verify that detail from the given gold, there is a small uncertainty in faithfulness, though the core QA pair is correct.", "main_total_tokens": 2154, "main_prompt_tokens": 2127, "main_completion_tokens": 27, "judge_total_tokens": 525, "judge_prompt_tokens": 411, "judge_completion_tokens": 114}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 400.23255348205566, "retrieval_recall": 0.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.19519002499605084, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.2, "ndcg_at_10": 0.3080351663449861, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (4), so it is incorrect. However, the assistant gives a single numerical answer without adding unsupported explanations or hallucinated details, so it is faithful to its own (incorrect) reasoning.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 466, "judge_prompt_tokens": 385, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "46", "latency_ms": 566.6780471801758, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '46' is factually incorrect because there are no movies featuring both Bruce Lee and Steven Seagal together, matching the gold response. It is also unfaithful to the input question and gold answer, which explicitly indicate that no such movies can be found.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 395, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of \"Tokyo Mater\" (2008) is 6.3 [Doc 42750].", "latency_ms": 1203.0489444732666, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3 for Tokyo Mater (2008) and does not introduce any unsupported information beyond citing a document, so it is both correct and faithful.", "main_total_tokens": 2299, "main_prompt_tokens": 2277, "main_completion_tokens": 22, "judge_total_tokens": 476, "judge_prompt_tokens": 405, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in the movie \"Fast Track: No Limits\" is street racing [Doc 41929].", "latency_ms": 654.7646522521973, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the style of racing, and the gold answer is 'Drift'. The generated answer says 'street racing', which is incorrect. It also introduces a specific movie title and citation not present in the prompt, so it is not faithful to the provided information.", "main_total_tokens": 2308, "main_prompt_tokens": 2285, "main_completion_tokens": 23, "judge_total_tokens": 487, "judge_prompt_tokens": 401, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 755.2294731140137, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Confronting a Serial Killer' and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 2299, "main_prompt_tokens": 2253, "main_completion_tokens": 46, "judge_total_tokens": 516, "judge_prompt_tokens": 452, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of \"Armageddon\" (1998) is 7.838 [Doc 20603].", "latency_ms": 670.2742576599121, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and does not introduce contradictions or unsupported claims. It is both correct and faithful.", "main_total_tokens": 2317, "main_prompt_tokens": 2293, "main_completion_tokens": 24, "judge_total_tokens": 465, "judge_prompt_tokens": 409, "judge_completion_tokens": 56}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"The Sixth Sense\" [Doc 22098].", "latency_ms": 748.4793663024902, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6895405204413555, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'The Sixth Sense' and does not introduce unsupported information aside from a harmless citation placeholder.", "main_total_tokens": 1960, "main_prompt_tokens": 1931, "main_completion_tokens": 29, "judge_total_tokens": 484, "judge_prompt_tokens": 420, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 584.2134952545166, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer and not introducing any additional unsupported information beyond a generic document reference.", "main_total_tokens": 2259, "main_prompt_tokens": 2242, "main_completion_tokens": 17, "judge_total_tokens": 461, "judge_prompt_tokens": 397, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 763.9269828796387, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly stating that the 2015 reboot is titled 'The Transporter Refueled' and does not introduce any unsupported information.", "main_total_tokens": 2281, "main_prompt_tokens": 2252, "main_completion_tokens": 29, "judge_total_tokens": 486, "judge_prompt_tokens": 420, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "\"The Transporter Refueled\" was released in the year 2015 [Doc 67500].", "latency_ms": 1133.6135864257812, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, which matches the gold answer. It does not add unsupported information, so it is both correct and faithful.", "main_total_tokens": 2302, "main_prompt_tokens": 2281, "main_completion_tokens": 21, "judge_total_tokens": 479, "judge_prompt_tokens": 406, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "The context provided does not mention any romantic comedy in which Abraham Lincoln comes back from the dead. The documents list various movies and TV shows related to Abraham Lincoln, but none are categorized as romantic comedies or involve him coming back from the dead in such a genre. If you are looking for a specific title, it is not found in the provided documents.", "latency_ms": 1295.9809303283691, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a romantic comedy where Abraham Lincoln comes back from the dead. Both the gold and generated answers state that no such movie is found in the available descriptions/context. This directly and accurately answers the question given the information, and the generated answer clearly grounds its response in the provided documents, matching the gold answer’s conclusion. Therefore, the answer is both correct and faithful to the context.", "main_total_tokens": 2106, "main_prompt_tokens": 2035, "main_completion_tokens": 71, "judge_total_tokens": 571, "judge_prompt_tokens": 461, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of the movie that alludes to a manuscript written by Ernest Hemingway is \"Havana Darkness\" [Doc 90923].", "latency_ms": 1101.8683910369873, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'Havana Darkness' as the movie that alludes to a manuscript written by Ernest Hemingway, and does not introduce unsupported information.", "main_total_tokens": 2052, "main_prompt_tokens": 2025, "main_completion_tokens": 27, "judge_total_tokens": 483, "judge_prompt_tokens": 415, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show \"Doll & Em\" [Doc 122259].", "latency_ms": 654.9556255340576, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em', which matches the gold answer exactly. It stays faithful to the provided information and does not introduce contradictions.", "main_total_tokens": 2184, "main_prompt_tokens": 2155, "main_completion_tokens": 29, "judge_total_tokens": 483, "judge_prompt_tokens": 417, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The 2007 movie in which Hallam has a talent for spying on people and attempts to find the cause of his mother’s death is \"Hallam Foe\" [Doc 39389].", "latency_ms": 817.7223205566406, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Hallam Foe,' which matches the gold answer and does not introduce unsupported information.", "main_total_tokens": 2108, "main_prompt_tokens": 2068, "main_completion_tokens": 40, "judge_total_tokens": 496, "judge_prompt_tokens": 439, "judge_completion_tokens": 57}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The last name \"Friend\" in the context of \"20th Century Boys 3: Redemption\" initially sounds like he could be a friend, as the name itself suggests friendliness or companionship. However, the plot description indicates a mystery surrounding the character, asking \"who is Friend?\" and suggesting a need to stop him, which implies that despite the name, he may actually be a foe [Doc 46423].", "latency_ms": 1882.8086853027344, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The question asks whether the last name initially sounds like he is a friend or a foe, with the gold answer being 'A foe.' The generated answer argues that the name sounds like he could be a friend at first because it suggests friendliness, which is the opposite of the gold. So it is incorrect. It is partially faithful to the provided context (discussing the character called 'Friend' and his role as a foe), but it contradicts the required label, so faithfulness is not perfect.", "main_total_tokens": 2066, "main_prompt_tokens": 1983, "main_completion_tokens": 83, "judge_total_tokens": 603, "judge_prompt_tokens": 469, "judge_completion_tokens": 134}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "In the 2017 TV show \"Atypical,\" Sam is 18 years old [Doc 124583].", "latency_ms": 548.1948852539062, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show 'Atypical,' which matches the gold answer. It does not introduce contradictions or unsupported claims beyond a generic document reference, so it is both correct and faithful.", "main_total_tokens": 1980, "main_prompt_tokens": 1956, "main_completion_tokens": 24, "judge_total_tokens": 494, "judge_prompt_tokens": 411, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show \"Atypical,\" Sam is on the autism spectrum [Doc 124583].", "latency_ms": 1374.4072914123535, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the TV show 'Atypical,' Sam is on the autism spectrum, which exactly matches the gold answer. It is both factually correct and faithful to the provided information.", "main_total_tokens": 2201, "main_prompt_tokens": 2177, "main_completion_tokens": 24, "judge_total_tokens": 484, "judge_prompt_tokens": 411, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "The document does not provide information about the place where Sandra was born in the movie \"À corde tendue\" released in 2013.", "latency_ms": 1241.769790649414, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that Sandra was born in Chamonix, but the generated answer claims the document does not provide this information. This is factually incorrect with respect to the gold, so correctness is 0. However, the generated answer is internally consistent and does not contradict its own stated source, so it is fully faithful to its (albeit incomplete) reasoning, yielding a faithfulness score of 1.", "main_total_tokens": 2305, "main_prompt_tokens": 2277, "main_completion_tokens": 28, "judge_total_tokens": 534, "judge_prompt_tokens": 425, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "The context provided does not contain information about a Japanese language movie where a character named Hazuki clips her nails every time it [Doc X].", "latency_ms": 1235.149621963501, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct completion is 'rains', but the generated answer does not provide this and instead states that the context does not contain the information, so it is incorrect. However, it accurately reflects that it cannot find the answer from the given context and does not hallucinate, so it is faithful to the provided information.", "main_total_tokens": 2112, "main_prompt_tokens": 2084, "main_completion_tokens": 28, "judge_total_tokens": 513, "judge_prompt_tokens": 419, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the animated series \"Tabaluga,\" Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 1203.3374309539795, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the place of which Tabaluga is crown-prince. The gold answer is \"Greenland.\" The generated answer states he is the crown-prince of Greenland, matching the gold exactly. It does not add any conflicting or incorrect information. Therefore it is both correct and faithful to the gold answer.", "main_total_tokens": 2415, "main_prompt_tokens": 2384, "main_completion_tokens": 31, "judge_total_tokens": 520, "judge_prompt_tokens": 425, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers\" is \"One Afternoon\" [Doc 96838].", "latency_ms": 785.3438854217529, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'One Afternoon', matching the gold answer. However, it adds a bracketed citation-like reference '[Doc 96838]' that is not supported or mentioned in the prompt, which slightly reduces faithfulness.", "main_total_tokens": 2211, "main_prompt_tokens": 2167, "main_completion_tokens": 44, "judge_total_tokens": 530, "judge_prompt_tokens": 448, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" [Doc 93260].", "latency_ms": 911.0257625579834, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Life Ahead,\" which matches the gold answer. It stays faithful to the question and does not introduce contradictions, only adding a neutral citation placeholder that does not affect correctness or faithfulness.", "main_total_tokens": 2607, "main_prompt_tokens": 2570, "main_completion_tokens": 37, "judge_total_tokens": 512, "judge_prompt_tokens": 435, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 670.1977252960205, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and aligns with the gold answer. It does not contradict the provided information and simply adds accurate contextual detail about him founding the McLaren motor racing team, which is consistent with known facts.", "main_total_tokens": 2108, "main_prompt_tokens": 2075, "main_completion_tokens": 33, "judge_total_tokens": 502, "judge_prompt_tokens": 417, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The name of the racing team is not provided in the given documents.", "latency_ms": 733.7689399719238, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the team as 'The McLaren Motor Racing team', so the generated answer is factually incorrect. However, if the provided documents to the model truly did not include this information, then stating that the name is not provided would be faithful to those documents. Thus correctness is 0 and faithfulness is 1.", "main_total_tokens": 2330, "main_prompt_tokens": 2316, "main_completion_tokens": 14, "judge_total_tokens": 495, "judge_prompt_tokens": 397, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context does not provide information about the specific country the person in question is from.", "latency_ms": 1304.1603565216064, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the correct country is New Zealand. The generated answer claims the context does not provide the information, which contradicts the gold label and is therefore factually incorrect relative to the gold. However, given only the user question and no visible supporting context, the model response is internally consistent with the (unstated) premise that the context lacks this information, so it remains faithful to its own stated limitations.", "main_total_tokens": 2249, "main_prompt_tokens": 2232, "main_completion_tokens": 17, "judge_total_tokens": 508, "judge_prompt_tokens": 393, "judge_completion_tokens": 115}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "The 2013 movie in which John McClane appears is \"A Good Day to Die Hard\" [Doc 58791]. However, there is no mention in the provided documents of John McClane blowing up the Great Wall of China in this or any other movie.", "latency_ms": 2691.680431365967, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.8, "faithfulness_score": 1, "justification": "The question is effectively a trick: there is no 2013 movie where John McClane blows up the Great Wall of China. The gold answer states that such a movie does not exist in the database. The generated answer correctly notes that the 2013 John McClane movie is \"A Good Day to Die Hard\" and that there is no mention of him blowing up the Great Wall of China in the documents, which aligns with the gold answer in substance. It is slightly less direct than the gold (it does not explicitly say the database lacks such a movie), so I rate correctness a bit below perfect, but it is fully faithful to the provided documents.", "main_total_tokens": 2211, "main_prompt_tokens": 2155, "main_completion_tokens": 56, "judge_total_tokens": 621, "judge_prompt_tokens": 454, "judge_completion_tokens": 167}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with the description \"When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental\" is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 848.6683368682861, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the film as 'Tomie: Forbidden Fruit'. It correctly restates the description and attributes it to the right 2002 Japanese horror movie, with no added incorrect details, so it is both correct and faithful.", "main_total_tokens": 2351, "main_prompt_tokens": 2293, "main_completion_tokens": 58, "judge_total_tokens": 559, "judge_prompt_tokens": 475, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie \"Lei mi parla ancora,\" Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 993.7593936920166, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, which matches the gold answer. It does not introduce contradictions or unsupported information beyond a neutral document reference, so it is both correct and faithful.", "main_total_tokens": 2338, "main_prompt_tokens": 2308, "main_completion_tokens": 30, "judge_total_tokens": 502, "judge_prompt_tokens": 424, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In \"Scorpion King: Book of Souls\" (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls, which will allow them to put an end to an evil warlord [Doc 82582].", "latency_ms": 1128.6556720733643, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.98, "justification": "The generated answer correctly identifies that The Scorpion King and Tala are searching for the Book of Souls, matching the gold answer. The added detail that it is a legendary relic that can end an evil warlord is consistent with the film's plot and does not contradict the given information, but it goes slightly beyond the provided gold snippet, so faithfulness is scored just under perfect.", "main_total_tokens": 2615, "main_prompt_tokens": 2564, "main_completion_tokens": 51, "judge_total_tokens": 553, "judge_prompt_tokens": 445, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Zombies on the Moon\" is not found in the provided documents.", "latency_ms": 1118.8673973083496, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie that is not in the database. The gold answer acknowledges this. The generated answer similarly states that the release year is not found in the provided documents, which is effectively equivalent and does not hallucinate a year. Thus it is both correct relative to the task and faithful to the given information.", "main_total_tokens": 1964, "main_prompt_tokens": 1945, "main_completion_tokens": 19, "judge_total_tokens": 509, "judge_prompt_tokens": 410, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Galactic Baker\" is not found in the provided documents.", "latency_ms": 865.2009963989258, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database; the generated answer similarly states that the release year is not found in the provided documents. Both convey that the information is unavailable, which is correct given the context and faithful to the (implicit) source constraints.", "main_total_tokens": 2396, "main_prompt_tokens": 2379, "main_completion_tokens": 17, "judge_total_tokens": 491, "judge_prompt_tokens": 406, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year of release for \"Return of the Null Pointer\" is not found in the provided documents.", "latency_ms": 830.0721645355225, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers convey that the release year is unknown/not available. The generated answer correctly states that the year is not found in the provided documents, which is faithful to the implied context and matches the gold answer in substance.", "main_total_tokens": 2085, "main_prompt_tokens": 2065, "main_completion_tokens": 20, "judge_total_tokens": 489, "judge_prompt_tokens": 411, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year of release for \"Data Science: The Movie\" is not found in the provided documents.", "latency_ms": 801.8319606781006, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available/known. The generated answer is correct and faithfully reflects uncertainty and lack of information from the (implied) source documents.", "main_total_tokens": 2166, "main_prompt_tokens": 2146, "main_completion_tokens": 20, "judge_total_tokens": 481, "judge_prompt_tokens": 411, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Python's Revenge 2\" is not found in the provided documents.", "latency_ms": 827.8687000274658, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year is not found, which aligns with the gold answer that the movie is not in the database. It does not introduce any unsupported facts and is fully faithful to the information given.", "main_total_tokens": 2421, "main_prompt_tokens": 2402, "main_completion_tokens": 19, "judge_total_tokens": 485, "judge_prompt_tokens": 410, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The Last Pixel\" is not found in the provided documents.", "latency_ms": 693.659782409668, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available/known, which is correct given the context. The generated answer stays faithful to the information source (the provided documents) and does not hallucinate a year.", "main_total_tokens": 1824, "main_prompt_tokens": 1807, "main_completion_tokens": 17, "judge_total_tokens": 483, "judge_prompt_tokens": 406, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"The man who ate the sun\" is not found in the provided documents.", "latency_ms": 815.1528835296631, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year / information is not available; neither invents a year or contradicts the given data. The generated answer is fully consistent with the (unspecified) documents and with the gold response indicating lack of information.", "main_total_tokens": 2578, "main_prompt_tokens": 2558, "main_completion_tokens": 20, "judge_total_tokens": 495, "judge_prompt_tokens": 412, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Days of Lehman Brothers\" is not mentioned in the provided documents.", "latency_ms": 1184.4062805175781, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not in the database. The generated answer states that the director is not mentioned in the provided documents. Both convey the same limitation and do not fabricate a director, so the answer is correct and faithful to the given information.", "main_total_tokens": 2580, "main_prompt_tokens": 2558, "main_completion_tokens": 22, "judge_total_tokens": 503, "judge_prompt_tokens": 419, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not contain information about a movie titled \"A Love to Die For,\" nor does it mention the director of such a movie. Therefore, I cannot provide an answer based on the given documents.", "latency_ms": 1370.2747821807861, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the director of 'A Love to Die For.' The gold answer explicitly states that this information is not in the database. The generated answer similarly states that the context does not contain information about the movie or its director and therefore it cannot answer based on the given documents. This matches the gold answer in substance, is correct with respect to the provided context, and does not introduce any unsupported facts, so it is fully faithful.", "main_total_tokens": 2426, "main_prompt_tokens": 2384, "main_completion_tokens": 42, "judge_total_tokens": 556, "judge_prompt_tokens": 437, "judge_completion_tokens": 119}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not mentioned in the provided documents.", "latency_ms": 1133.033037185669, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned and aligns with the gold answer that the information is not provided in the database. It does not hallucinate a director and stays faithful to the given context.", "main_total_tokens": 2432, "main_prompt_tokens": 2414, "main_completion_tokens": 18, "judge_total_tokens": 484, "judge_prompt_tokens": 411, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Night at Tremore Beach\" is not mentioned in the provided documents. [Doc 135098]", "latency_ms": 1807.6367378234863, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about the director of the movie, but both the gold answer and the generated answer correctly state that this information is not provided in the available sources. The generated answer does not fabricate a director and explicitly ties its statement to the provided documents, so it is both correct with respect to the task and faithful to the given context.", "main_total_tokens": 2078, "main_prompt_tokens": 2050, "main_completion_tokens": 28, "judge_total_tokens": 523, "judge_prompt_tokens": 425, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 579.359769821167, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying \"Rise of the Mummy\" as the correct movie. It does not contradict or add unsupported information aside from a benign document reference, so it is both correct and faithful.", "main_total_tokens": 2156, "main_prompt_tokens": 2129, "main_completion_tokens": 27, "judge_total_tokens": 494, "judge_prompt_tokens": 418, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse is \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 1288.6707782745361, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as \"Stranger in My Bed\" and does not introduce any contradictions or extra unsupported details.", "main_total_tokens": 2154, "main_prompt_tokens": 2125, "main_completion_tokens": 29, "judge_total_tokens": 488, "judge_prompt_tokens": 425, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The scenario where a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years occurs in the TV show \"Game of Silence\" [Doc 123938].", "latency_ms": 1072.6442337036133, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question explicitly asks for a movie, and the gold answer is the TV show 'Game of Silence', but the generated response also calls it a TV show rather than a movie. This contradicts the question, so it is incorrect. Additionally, the reference to a specific document ID suggests reliance on an external source not present in the prompt, so the answer is not fully grounded in the provided information.", "main_total_tokens": 1872, "main_prompt_tokens": 1836, "main_completion_tokens": 36, "judge_total_tokens": 544, "judge_prompt_tokens": 433, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie in which the plot involves a killer from the site Craigslist is \"The Craigslist Killer\" [Doc 53039].", "latency_ms": 696.8991756439209, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'The Craigslist Killer' and remains fully faithful to the provided information.", "main_total_tokens": 2094, "main_prompt_tokens": 2069, "main_completion_tokens": 25, "judge_total_tokens": 476, "judge_prompt_tokens": 417, "judge_completion_tokens": 59}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which Yuki Hirano sets his eyes on the role of a forestry trainee after failing his university entrance exam is \"Wood Job!\" [Doc 66094].", "latency_ms": 788.1302833557129, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", matching the gold answer exactly. It stays faithful to the provided information and does not introduce contradictions or unsupported claims.", "main_total_tokens": 2910, "main_prompt_tokens": 2875, "main_completion_tokens": 35, "judge_total_tokens": 501, "judge_prompt_tokens": 435, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure is \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 770.0409889221191, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase' and does not introduce any contradictions or hallucinations beyond a benign reference tag.", "main_total_tokens": 2392, "main_prompt_tokens": 2343, "main_completion_tokens": 49, "judge_total_tokens": 529, "judge_prompt_tokens": 459, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a 7-year-old boy must find the hero within him to save his parents trapped inside a burning car due to a rock fall is \"Beyond\" [Doc 74597].", "latency_ms": 1145.0326442718506, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Beyond,' which matches the gold answer. It does not introduce contradictions or unsupported details, so it is both correct and faithful to the provided information.", "main_total_tokens": 1994, "main_prompt_tokens": 1954, "main_completion_tokens": 40, "judge_total_tokens": 517, "judge_prompt_tokens": 448, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman, fired from a financial corporation during the Asia crisis, returns home with no money is \"6ixtynin9\" [Doc 22478].", "latency_ms": 725.5198955535889, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', which matches the gold answer exactly. It does not introduce any contradictions or unsupported claims, and is fully grounded in the provided information.", "main_total_tokens": 2113, "main_prompt_tokens": 2076, "main_completion_tokens": 37, "judge_total_tokens": 512, "judge_prompt_tokens": 439, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie in which a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort is \"Club Dread\" [Doc 32410].", "latency_ms": 776.3357162475586, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'Club Dread'. The generated answer correctly identifies the movie as 'Club Dread' and does not introduce contradictions or unsupported information beyond a harmless citation, so it is both correct and faithful.", "main_total_tokens": 2421, "main_prompt_tokens": 2389, "main_completion_tokens": 32, "judge_total_tokens": 530, "judge_prompt_tokens": 456, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is \"Toilet\" [Doc 48873].", "latency_ms": 1118.5095310211182, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" which matches the gold answer. It directly restates the question in sentence form and names the same movie title, with no added contradictions or hallucinated details, so it is both correct and faithful.", "main_total_tokens": 2639, "main_prompt_tokens": 2588, "main_completion_tokens": 51, "judge_total_tokens": 547, "judge_prompt_tokens": 465, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century is \"Electric Angel\" [Doc 1024].", "latency_ms": 812.4983310699463, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot. The gold answer is 'Electric Angel'. The generated answer correctly identifies the movie as 'Electric Angel' and does not introduce contradictory or extraneous information beyond a citation marker, so it is both correct and faithful to the gold.", "main_total_tokens": 2432, "main_prompt_tokens": 2387, "main_completion_tokens": 45, "judge_total_tokens": 547, "judge_prompt_tokens": 461, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023, based on the provided documents, is \"Bottom of the Water\" with a rating of 4.2 [Doc 106899].", "latency_ms": 720.616340637207, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'Mega Ape' as the lowest rated movie from 2023, while the generated answer incorrectly states 'Bottom of the Water' with a rating of 4.2. This contradicts the gold label, so the response is factually incorrect for the task. It is also unfaithful to the provided gold answer, inventing a different movie and rating without support from the reference.", "main_total_tokens": 2016, "main_prompt_tokens": 1980, "main_completion_tokens": 36, "judge_total_tokens": 525, "judge_prompt_tokens": 417, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 1360.544204711914, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific highest-rated TV show from 2010, with the gold answer being 'Tea House'. The generated answer says 'I don't know,' which does not provide the correct show, so correctness is 0. However, the model is honest about its lack of knowledge and does not hallucinate an incorrect answer, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 386, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 266.05892181396484, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is provided in the gold as 'The Uncut Story of Mewtwo's Origin', but the generated answer says 'I don't know', which is incorrect. However, the model does not hallucinate an incorrect movie title; it simply expresses lack of knowledge, so it is faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 490, "judge_prompt_tokens": 395, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The language of the lowest-rated movie from 1980 is Chinese (cn) for the movie \"Read Lips,\" which has a rating of 4.5 [Doc 354].", "latency_ms": 1070.5890655517578, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect relative to the gold label: it claims the language is Chinese, while the gold answer states English. However, the answer appears to be grounded in the cited document [Doc 354] and does not hallucinate beyond that source, so it is faithful to its referenced document.", "main_total_tokens": 2072, "main_prompt_tokens": 2035, "main_completion_tokens": 37, "judge_total_tokens": 513, "judge_prompt_tokens": 420, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show from 1999 with the highest popularity, based on the provided documents, is \"Popular\" with a popularity score of 4.5113 [Doc 115196].", "latency_ms": 930.201530456543, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Law & Order: Special Victims Unit,' but the generated answer claims 'Popular' is the most popular show. This is factually incorrect relative to the gold, so correctness is 0. However, the generated answer explicitly bases its claim on the provided documents and cites a specific document and score, indicating it is faithful to those documents, so faithfulness is 1.", "main_total_tokens": 2602, "main_prompt_tokens": 2564, "main_completion_tokens": 38, "judge_total_tokens": 539, "judge_prompt_tokens": 428, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 918.8222885131836, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the rating of 'Bluefin' is 7.2, which matches the gold answer. It also appears to be grounded in a cited document ([Doc 79355]), so it is faithful to the provided source.", "main_total_tokens": 2149, "main_prompt_tokens": 2131, "main_completion_tokens": 18, "judge_total_tokens": 481, "judge_prompt_tokens": 400, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in 1999 [Doc 22481].", "latency_ms": 600.9106636047363, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states the movie was released in 1999, but the gold label says 2019, so the answer is factually incorrect relative to the provided gold. However, the answer is internally consistent with its own cited document, so it is faithful to its stated source.", "main_total_tokens": 2147, "main_prompt_tokens": 2127, "main_completion_tokens": 20, "judge_total_tokens": 492, "judge_prompt_tokens": 404, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 169.64125633239746, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 279 votes, but the generated answer is 3, which is numerically incorrect. However, the answer format directly addresses the question, so it is faithful to the user query, just factually wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 460, "judge_prompt_tokens": 383, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "12", "latency_ms": 130.00822067260742, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (12), so it is both correct and faithful to the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 436, "judge_prompt_tokens": 383, "judge_completion_tokens": 53}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in the year 2009 [Doc 46069].", "latency_ms": 633.636474609375, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie was released in 2009, matching the gold answer. It is fully supported by the cited document, so it is both correct and faithful.", "main_total_tokens": 2246, "main_prompt_tokens": 2221, "main_completion_tokens": 25, "judge_total_tokens": 480, "judge_prompt_tokens": 412, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_openai-small_gpt-4o", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 761.4076137542725, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the original language is English, and the generated answer correctly states that the original language of \"The King’s Speech\" is English, matching the gold. The citation tag does not contradict anything, so the answer is both correct and faithful.", "main_total_tokens": 2448, "main_prompt_tokens": 2429, "main_completion_tokens": 19, "judge_total_tokens": 484, "judge_prompt_tokens": 402, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "1987 [Doc 6993].", "latency_ms": 1754.7717094421387, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987 and does not add any contradictory or speculative information. It is fully correct and faithful to the gold answer.", "main_total_tokens": 2812, "main_prompt_tokens": 2803, "main_completion_tokens": 9, "judge_total_tokens": 453, "judge_prompt_tokens": 387, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Signal 100", "latency_ms": 987.9586696624756, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the movie where classmates play a survival game using their cell phones. The gold answer is 'Brain Fluid Explosion Girl', but the generated answer is 'Signal 100', which is a different film. Therefore, the generated answer is incorrect with respect to the gold, though the model is consistently referring to that same answer, so faithfulness is high.", "main_total_tokens": 2532, "main_prompt_tokens": 2528, "main_completion_tokens": 4, "judge_total_tokens": 490, "judge_prompt_tokens": 391, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 832.8127861022949, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3010299956639813, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Dungeons & Dragons.'", "main_total_tokens": 2625, "main_prompt_tokens": 2620, "main_completion_tokens": 5, "judge_total_tokens": 446, "judge_prompt_tokens": 394, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 1131.9537162780762, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies 'Nacht der Wölfe' as the movie about two rival gangs in Munich, one German and one Turkish.", "main_total_tokens": 2756, "main_prompt_tokens": 2749, "main_completion_tokens": 7, "judge_total_tokens": 469, "judge_prompt_tokens": 403, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900", "latency_ms": 1300.4093170166016, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (1900), so it is both correct and fully faithful to the reference.", "main_total_tokens": 2586, "main_prompt_tokens": 2583, "main_completion_tokens": 3, "judge_total_tokens": 443, "judge_prompt_tokens": 388, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June", "latency_ms": 560.6939792633057, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title, which is correctly identified as 'June' both in the gold answer and the generated answer. The generated answer exactly matches the gold and aligns with the question, so it is fully correct and faithful.", "main_total_tokens": 138, "main_prompt_tokens": 137, "main_completion_tokens": 1, "judge_total_tokens": 511, "judge_prompt_tokens": 433, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 241.35327339172363, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which is incorrect because there is a specific correct movie title provided in the gold answer. However, the response does not fabricate any information and accurately reflects a lack of knowledge, so it is fully faithful to the model's stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 399, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 308.20345878601074, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a numeric rating (7.6), while the generated answer says \"I don't know,\" which does not provide the correct rating. However, the generated answer does not invent any incorrect facts; it simply admits lack of knowledge, so it is faithful to the model's uncertainty but incorrect relative to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 389, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "8", "latency_ms": 1097.6741313934326, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.5143371794949736, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.013881288441812, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (8) does not match the gold answer (4), so it is incorrect. However, it appears to directly address the question asked, so it is faithful to the user query.", "main_total_tokens": 1457, "main_prompt_tokens": 1442, "main_completion_tokens": 15, "judge_total_tokens": 465, "judge_prompt_tokens": 394, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 225.08645057678223, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer says there are no movies or shows matching the criteria, which is factually incorrect. However, the model does not fabricate specific data and instead admits inability to find matches, so it remains faithful to its own (limited) information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 404, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 311.9206428527832, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not correctly identify the TV show 'Fantômas' and is therefore incorrect. However, it does not introduce any incorrect facts; it simply expresses a lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 476, "judge_prompt_tokens": 389, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise", "latency_ms": 717.8745269775391, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Kill Cruise') for the described movie, so it is both correct and faithful.", "main_total_tokens": 2809, "main_prompt_tokens": 2806, "main_completion_tokens": 3, "judge_total_tokens": 443, "judge_prompt_tokens": 387, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 1060.2669715881348, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct title of the movie about Fred Rogers and his TV series. It is both correct and fully faithful to the gold.", "main_total_tokens": 2881, "main_prompt_tokens": 2873, "main_completion_tokens": 8, "judge_total_tokens": 475, "judge_prompt_tokens": 408, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018", "latency_ms": 1835.402488708496, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (2018), so it is both correct and faithful.", "main_total_tokens": 2782, "main_prompt_tokens": 2766, "main_completion_tokens": 16, "judge_total_tokens": 431, "judge_prompt_tokens": 380, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy", "latency_ms": 1249.9165534973145, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the animated TV series about a young boy robot in 2030 AD as 'Astro Boy'. The response is both correct and faithful.", "main_total_tokens": 2899, "main_prompt_tokens": 2895, "main_completion_tokens": 4, "judge_total_tokens": 459, "judge_prompt_tokens": 389, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 [Doc 67513]", "latency_ms": 1487.5352382659912, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9463946303571863, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer '7' is essentially correct in meaning, as the woman and her son were held captive for 7 years, matching the gold answer. However, the generated answer includes an extraneous citation marker '[Doc 67513]', which is not supported by the provided gold data and constitutes unfaithful fabrication beyond the grounded content.", "main_total_tokens": 2472, "main_prompt_tokens": 2464, "main_completion_tokens": 8, "judge_total_tokens": 501, "judge_prompt_tokens": 402, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26 million [Doc 133469].", "latency_ms": 771.294116973877, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer (£26 million) matches the gold amount stated in the question (£26m) and aligns with the referenced document, so it is both correct and faithful to the provided information.", "main_total_tokens": 2120, "main_prompt_tokens": 2111, "main_completion_tokens": 9, "judge_total_tokens": 473, "judge_prompt_tokens": 404, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 531.5890312194824, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is a specific numeric value (80.5772). The generated answer says, 'I don't know,' which does not provide the required value, so correctness is 0. However, the model is honest about its lack of knowledge and does not hallucinate a number, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 396, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 295.02224922180176, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer states that there was 1 Russian language TV show released in 1997, while the generated answer claims that no such movies or shows could be found, which is factually incorrect relative to the gold. Thus correctness is 0. However, the model does not hallucinate a specific incorrect number; it instead expresses inability to find results, which is partially aligned with an uncertainty stance, so faithfulness is somewhat preserved, though still not aligned with the required answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 523, "judge_prompt_tokens": 394, "judge_completion_tokens": 129}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes", "latency_ms": 802.4272918701172, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Angel Eyes' and correctly identifies the TV show in which Soo-wan regains her sight after an eye transplant.", "main_total_tokens": 2671, "main_prompt_tokens": 2668, "main_completion_tokens": 3, "judge_total_tokens": 460, "judge_prompt_tokens": 397, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "en", "latency_ms": 1782.719612121582, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer says the language is English (\"en\"), but the gold label is Korean. This is factually incorrect and not faithful to the gold answer.", "main_total_tokens": 2947, "main_prompt_tokens": 2935, "main_completion_tokens": 12, "judge_total_tokens": 441, "judge_prompt_tokens": 377, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 583.1880569458008, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The gold answer states there are 2 such movies, while the generated answer claims there are none, so it is factually incorrect. It is somewhat faithful in that it does not hallucinate specific movie data or numbers, but instead admits inability to find matches; however, that still conflicts with the known correct answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 499, "judge_prompt_tokens": 403, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 802.4940490722656, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "Gold answer states specifically that there are no 1979 movies in its database, implying the correct response is to say that limitation. The generated answer simply says 'I don't know,' which does not correctly convey the database limitation and is therefore incorrect relative to the gold. However, it does not invent any facts or contradict the provided information beyond being incomplete, so it is fully faithful to the (lack of) knowledge.", "main_total_tokens": 564, "main_prompt_tokens": 563, "main_completion_tokens": 1, "judge_total_tokens": 513, "judge_prompt_tokens": 399, "judge_completion_tokens": 114}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "I don't know.", "latency_ms": 604.3918132781982, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer is correct, since there is no movie titled 'Back to the Future Part IV' and thus no release year can be given. However, it is not faithful to the gold answer, which explicitly states that the movie does not exist in its database, while the generated answer just says 'I don't know' without that clarification.", "main_total_tokens": 593, "main_prompt_tokens": 592, "main_completion_tokens": 1, "judge_total_tokens": 503, "judge_prompt_tokens": 404, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot find any movies or shows matching the criteria, which aligns with the gold answer that there are no movies from 1980 related to the Chernobyl disaster. It is faithful to the question and does not introduce unsupported details, simply suggesting the user verify the event timing.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 527, "judge_prompt_tokens": 434, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 407.67741203308105, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 titles with 'Astro Boy' in the title, but the generated response incorrectly says it cannot find any, so the correctness is 0. The response is faithful to the model's stated uncertainty and does not hallucinate, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 397, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "3", "latency_ms": 699.6073722839355, "retrieval_recall": 0.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.5, "precision_at_10": 0.1, "ndcg_at_10": 0.17723928678404774, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2 and the generated answer is 3, so the generated answer is numerically incorrect. However, the assistant directly answered the question with a number and did not contradict or invent any other details, so it is fully faithful to the (incorrect) reasoning it implied.", "main_total_tokens": 3367, "main_prompt_tokens": 3362, "main_completion_tokens": 5, "judge_total_tokens": 475, "judge_prompt_tokens": 386, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 168.05624961853027, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there was 1 TV show, while the generated answer claims there were none. This directly contradicts the gold answer, so the correctness is 0. The generated answer does not invent extra facts beyond saying it cannot find any, so it is internally faithful, giving it a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 495, "judge_prompt_tokens": 398, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 325.6690502166748, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The gold answer is 0, and the generated answer states there are none, which is effectively the same result, so correctness is 1. However, the prompt evaluation cares whether the model followed the gold exactly; the model added uncertainty and mentioned inability to find any rather than directly answering 0, so faithfulness to the gold answer is reduced.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 501, "judge_prompt_tokens": 400, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 197.70479202270508, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto', which is indeed a very popular ninja-themed show. The generated answer says 'I don't know,' which is incorrect. However, it is faithful in that it does not hallucinate or invent an answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 465, "judge_prompt_tokens": 386, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 905.8990478515625, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer explains that the movie does not exist, so the correct response should state that there was no such release. The generated answer says 'I don't know,' which is incorrect relative to the gold and therefore not faithful to it.", "main_total_tokens": 529, "main_prompt_tokens": 528, "main_completion_tokens": 1, "judge_total_tokens": 467, "judge_prompt_tokens": 389, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 453.7656307220459, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that the movie was not released; saying 'I don't know' is incorrect with respect to the gold answer. However, the model did not hallucinate or contradict the known information—it simply expressed ignorance—so it remains faithful to its own stated uncertainty.", "main_total_tokens": 561, "main_prompt_tokens": 560, "main_completion_tokens": 1, "judge_total_tokens": 471, "judge_prompt_tokens": 386, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "Return of the Jedi [Doc 2858].", "latency_ms": 1253.6425590515137, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The generated answer is incorrect. The 4th oldest Star Wars movie per the provided gold label is 'Star Wars: Droids - The Great Heep', but the system answered 'Return of the Jedi'. The answer appears internally consistent and does not contradict itself, so it is mostly faithful to its own reasoning, but it does not match the gold answer, hence correctness is 0. Faithfulness is high (no hallucinated reasoning steps given), estimated at 0.8.", "main_total_tokens": 2965, "main_prompt_tokens": 2954, "main_completion_tokens": 11, "judge_total_tokens": 529, "judge_prompt_tokens": 399, "judge_completion_tokens": 130}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "1983", "latency_ms": 1674.0896701812744, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 1983, but the correct year is 1986, so it is factually incorrect. However, it is fully faithful to the (incorrect) reasoning implied by the single numeric answer, so faithfulness is high.", "main_total_tokens": 3030, "main_prompt_tokens": 3018, "main_completion_tokens": 12, "judge_total_tokens": 460, "judge_prompt_tokens": 380, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 254.91714477539062, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer directly provides the rating (8.466). The generated answer says 'I don't know,' which is incorrect relative to the gold label. However, the assistant is honestly expressing uncertainty rather than fabricating information, so it is fully faithful to its own stated knowledge. Thus correctness is 0, faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 496, "judge_prompt_tokens": 398, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med", "latency_ms": 1342.4463272094727, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct title of the TV show described.", "main_total_tokens": 2575, "main_prompt_tokens": 2571, "main_completion_tokens": 4, "judge_total_tokens": 445, "judge_prompt_tokens": 394, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina [Doc 53582].", "latency_ms": 1050.5201816558838, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer, Argentina, matches the gold answer and is therefore correct. However, the reference to a specific document ID ([Doc 53582]) is fabricated and not grounded in the provided context, reducing faithfulness.", "main_total_tokens": 3047, "main_prompt_tokens": 3039, "main_completion_tokens": 8, "judge_total_tokens": 473, "judge_prompt_tokens": 398, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "I don't know.", "latency_ms": 586.4989757537842, "retrieval_recall": 1.0, "retrieval_mrr": 0.1, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Jiang Jie'. The generated answer says 'I don't know', which does not match the gold answer, so it is incorrect. However, it does not hallucinate or fabricate an incorrect specific answer and accurately reflects not knowing, so it is fully faithful.", "main_total_tokens": 536, "main_prompt_tokens": 535, "main_completion_tokens": 1, "judge_total_tokens": 475, "judge_prompt_tokens": 388, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "2", "latency_ms": 911.2849235534668, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 0.25, "precision_at_5": 0.2, "ndcg_at_5": 0.19519002499605084, "hit_rate_at_5": 1.0, "recall_at_10": 0.5, "precision_at_10": 0.2, "ndcg_at_10": 0.3080351663449861, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model output (2) does not match the gold answer (4), so it is incorrect. However, the evaluation here is directly based on the provided gold and generated answers without adding unsupported assumptions, so it is faithful to the given information.", "main_total_tokens": 3483, "main_prompt_tokens": 3480, "main_completion_tokens": 3, "judge_total_tokens": 465, "judge_prompt_tokens": 385, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 536.1735820770264, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.9, "faithfulness_score": 1, "justification": "The gold answer states that no movies with both Bruce Lee and Steven Seagal can be found. The generated answer similarly says it cannot find any movies or shows matching all of those criteria, which aligns in meaning. It is essentially correct and fully faithful to the gold, with only a slight wording difference (adding 'or shows') that does not change the core answer.", "main_total_tokens": 2209, "main_prompt_tokens": 2208, "main_completion_tokens": 1, "judge_total_tokens": 512, "judge_prompt_tokens": 406, "judge_completion_tokens": 106}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3 [Doc 42750].", "latency_ms": 671.116828918457, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer gives the correct rating (6.3), matching the gold value, so correctness is 1. However, it cites a specific document ID ([Doc 42750]) as its source, which is not present in the provided context, making that reference ungrounded and thus not fully faithful, so faithfulness is 0.", "main_total_tokens": 2299, "main_prompt_tokens": 2290, "main_completion_tokens": 9, "judge_total_tokens": 493, "judge_prompt_tokens": 393, "judge_completion_tokens": 100}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "Drift-style racing [Doc 42750].", "latency_ms": 2170.1200008392334, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer 'Drift-style racing' is essentially correct, as it identifies the racing style as drifting, which matches the gold answer 'Drift'. However, it adds an unnecessary reference marker '[Doc 42750]' that is not supported or explained by the given context, slightly reducing faithfulness to the provided information.", "main_total_tokens": 3004, "main_prompt_tokens": 2981, "main_completion_tokens": 23, "judge_total_tokens": 485, "judge_prompt_tokens": 387, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer", "latency_ms": 918.6129570007324, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Confronting a Serial Killer'. It is both correct and faithful.", "main_total_tokens": 2855, "main_prompt_tokens": 2848, "main_completion_tokens": 7, "judge_total_tokens": 473, "judge_prompt_tokens": 412, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838 [Doc 20603]", "latency_ms": 856.6615581512451, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.3868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and does not introduce any unsupported information beyond citing a document reference, so it is both correct and faithful.", "main_total_tokens": 2315, "main_prompt_tokens": 2306, "main_completion_tokens": 9, "judge_total_tokens": 461, "judge_prompt_tokens": 395, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The Sixth Sense", "latency_ms": 1172.6257801055908, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6895405204413555, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the 1999 psychological thriller that alludes to an extra sense as 'The Sixth Sense.'", "main_total_tokens": 2499, "main_prompt_tokens": 2495, "main_completion_tokens": 4, "judge_total_tokens": 455, "judge_prompt_tokens": 394, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002 [Doc 27318].", "latency_ms": 1952.5814056396484, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer exactly. The citation format does not contradict the answer, so the response is both correct and faithful.", "main_total_tokens": 2830, "main_prompt_tokens": 2821, "main_completion_tokens": 9, "judge_total_tokens": 460, "judge_prompt_tokens": 389, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled", "latency_ms": 840.0323390960693, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the title of the 2015 reboot of 'The Transporter' as 'The Transporter Refueled'.", "main_total_tokens": 2851, "main_prompt_tokens": 2844, "main_completion_tokens": 7, "judge_total_tokens": 463, "judge_prompt_tokens": 397, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015", "latency_ms": 1071.7494487762451, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2015), which is the correct release year of 'The Transporter Refueled', and it does not introduce any additional unsupported information.", "main_total_tokens": 2867, "main_prompt_tokens": 2864, "main_completion_tokens": 3, "judge_total_tokens": 454, "judge_prompt_tokens": 387, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "There is no romantic comedy listed in the provided documents where Abraham Lincoln comes back from the dead.", "latency_ms": 1020.1425552368164, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no such movie description can be found, and the generated answer correctly and faithfully restates that there is no romantic comedy in the provided documents where Abraham Lincoln comes back from the dead. It introduces no new claims and aligns with the given information.", "main_total_tokens": 2634, "main_prompt_tokens": 2614, "main_completion_tokens": 20, "judge_total_tokens": 493, "judge_prompt_tokens": 409, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Havana Darkness", "latency_ms": 1017.3113346099854, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which correctly identifies 'Havana Darkness' as the movie title that alludes to a manuscript written by Ernest Hemingway. It is both correct and faithful to the provided reference.", "main_total_tokens": 2581, "main_prompt_tokens": 2577, "main_completion_tokens": 4, "judge_total_tokens": 464, "judge_prompt_tokens": 390, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 1962.8117084503174, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Doll & Em'. It is both correct and faithful to the reference.", "main_total_tokens": 2830, "main_prompt_tokens": 2796, "main_completion_tokens": 34, "judge_total_tokens": 454, "judge_prompt_tokens": 392, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam Foe", "latency_ms": 1462.6502990722656, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the 2007 movie as 'Hallam Foe'. It is both correct and faithful to the provided information.", "main_total_tokens": 2676, "main_prompt_tokens": 2671, "main_completion_tokens": 5, "judge_total_tokens": 468, "judge_prompt_tokens": 403, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Foe", "latency_ms": 1341.4664268493652, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'A foe' and the generated answer is 'Foe,' which is semantically equivalent. The response is correct and faithful to the gold.", "main_total_tokens": 2863, "main_prompt_tokens": 2840, "main_completion_tokens": 23, "judge_total_tokens": 452, "judge_prompt_tokens": 388, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18", "latency_ms": 781.8388938903809, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 in the 2017 TV show Atypical, matching the gold answer without adding unsupported information.", "main_total_tokens": 2545, "main_prompt_tokens": 2543, "main_completion_tokens": 2, "judge_total_tokens": 450, "judge_prompt_tokens": 389, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Autism [Doc 124583].", "latency_ms": 1596.994400024414, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.7, "justification": "The gold answer is 'Autism spectrum'. The generated answer is 'Autism [Doc 124583]'. Substantively, 'Autism' correctly identifies the relevant spectrum (the autism spectrum), so the core factual content is correct. However, it omits the word 'spectrum' and adds an unexplained citation-like tag '[Doc 124583]', which is not present in the gold answer or question. This extraneous bracketed text is unfaithful to the provided data/instruction context, so faithfulness is partially reduced.", "main_total_tokens": 2774, "main_prompt_tokens": 2765, "main_completion_tokens": 9, "judge_total_tokens": 537, "judge_prompt_tokens": 394, "judge_completion_tokens": 143}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "I don't know.", "latency_ms": 486.59777641296387, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that Sandra was born in Chamonix. The generated answer says 'I don't know,' which does not match the correct answer, so correctness is 0. However, the generated answer does not fabricate information and truthfully indicates uncertainty, so faithfulness is 1.", "main_total_tokens": 569, "main_prompt_tokens": 568, "main_completion_tokens": 1, "judge_total_tokens": 490, "judge_prompt_tokens": 401, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "I don't know.", "latency_ms": 496.1833953857422, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'rains'. The generated answer 'I don't know.' is incorrect with respect to the gold, so correctness is 0. However, the model did not hallucinate an answer and honestly stated its lack of knowledge, so faithfulness to its own uncertainty is 1.", "main_total_tokens": 554, "main_prompt_tokens": 553, "main_completion_tokens": 1, "judge_total_tokens": 480, "judge_prompt_tokens": 395, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland [Doc 114810].", "latency_ms": 1151.8683433532715, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Greenland') and does not introduce any contradictions or unsupported information. It is both correct and faithful.", "main_total_tokens": 3002, "main_prompt_tokens": 2993, "main_completion_tokens": 9, "judge_total_tokens": 461, "judge_prompt_tokens": 401, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon", "latency_ms": 936.4235401153564, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the movie as 'One Afternoon.'", "main_total_tokens": 2749, "main_prompt_tokens": 2746, "main_completion_tokens": 3, "judge_total_tokens": 455, "judge_prompt_tokens": 406, "judge_completion_tokens": 49}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" [Doc 93260].", "latency_ms": 1829.3662071228027, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Life Ahead,\" which matches the gold answer exactly. It stays faithful to the information implied by the question and does not introduce contradictions; the extra bracketed reference does not alter the core answer.", "main_total_tokens": 3195, "main_prompt_tokens": 3157, "main_completion_tokens": 38, "judge_total_tokens": 514, "judge_prompt_tokens": 435, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "Bruce McLaren [Doc 73859].", "latency_ms": 1319.965124130249, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and fully matches the gold answer, aside from an irrelevant citation marker. It is both correct and faithful.", "main_total_tokens": 2668, "main_prompt_tokens": 2658, "main_completion_tokens": 10, "judge_total_tokens": 462, "judge_prompt_tokens": 393, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren Motor Racing team [Doc 73859].", "latency_ms": 2376.3229846954346, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly matches the gold answer in content, giving the same team name. The added document reference tag does not change the factual content. Thus it is both correct and faithful to the gold.", "main_total_tokens": 2923, "main_prompt_tokens": 2899, "main_completion_tokens": 24, "judge_total_tokens": 465, "judge_prompt_tokens": 394, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "New Zealand [Doc 73859].", "latency_ms": 2153.8984775543213, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the country as New Zealand, matching the gold answer. The bracketed document reference does not alter the factual content, so the response is both correct and faithful to the gold.", "main_total_tokens": 2936, "main_prompt_tokens": 2916, "main_completion_tokens": 20, "judge_total_tokens": 455, "judge_prompt_tokens": 384, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "I don't know.", "latency_ms": 454.4405937194824, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that there is no such movie in the database, which effectively conveys lack of knowledge about a 2013 film where John McClane blows up the Great Wall of China. The generated answer, 'I don't know,' expresses the same lack of knowledge, so it is correct in context and faithful to the gold answer.", "main_total_tokens": 547, "main_prompt_tokens": 546, "main_completion_tokens": 1, "judge_total_tokens": 501, "judge_prompt_tokens": 402, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit [Doc 28589].", "latency_ms": 1435.4252815246582, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the film as 'Tomie: Forbidden Fruit'. The added bracketed document reference does not change the substance of the answer, so it is both correct and faithful.", "main_total_tokens": 2935, "main_prompt_tokens": 2923, "main_completion_tokens": 12, "judge_total_tokens": 502, "judge_prompt_tokens": 428, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 [Doc 98566].", "latency_ms": 1182.6257705688477, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer matches the gold answer (65 years), so correctness is 1. However, it references an unspecified document '[Doc 98566]' that is not provided in the context, so the provenance is not verifiable, reducing faithfulness to 0.", "main_total_tokens": 2911, "main_prompt_tokens": 2903, "main_completion_tokens": 8, "judge_total_tokens": 486, "judge_prompt_tokens": 402, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King and Tala search for a legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 1387.665033340454, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that they search for The Book of Souls, matching the gold answer, and adding that it is a legendary relic is consistent and not contradictory.", "main_total_tokens": 3208, "main_prompt_tokens": 3184, "main_completion_tokens": 24, "judge_total_tokens": 481, "judge_prompt_tokens": 417, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 899.5883464813232, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, effectively conveying lack of knowledge. The generated answer explicitly says 'I don't know,' which is consistent with having no information and does not introduce any incorrect facts. Therefore it is fully correct and faithful to the gold response.", "main_total_tokens": 558, "main_prompt_tokens": 557, "main_completion_tokens": 1, "judge_total_tokens": 482, "judge_prompt_tokens": 395, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 948.7345218658447, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, which implies the system does not know the release year. The generated answer 'I don't know.' correctly expresses this lack of knowledge and does not introduce any unsupported information, so it is both correct and faithful to the gold.", "main_total_tokens": 571, "main_prompt_tokens": 570, "main_completion_tokens": 1, "judge_total_tokens": 481, "judge_prompt_tokens": 393, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 514.2414569854736, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database, which effectively means the answer is unknown. The generated response 'I don't know.' correctly reflects this lack of knowledge, so it is both correct and faithful to the gold answer.", "main_total_tokens": 535, "main_prompt_tokens": 534, "main_completion_tokens": 1, "judge_total_tokens": 473, "judge_prompt_tokens": 395, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 442.9600238800049, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database, which is equivalent in content to saying 'I don't know.' The generated answer does not introduce any incorrect facts or contradictions and matches the uncertainty expressed in the gold answer, so it is both correct and faithful.", "main_total_tokens": 552, "main_prompt_tokens": 551, "main_completion_tokens": 1, "judge_total_tokens": 479, "judge_prompt_tokens": 395, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year of release for \"Python's Revenge 2\" is not provided in the given documents.", "latency_ms": 2023.1826305389404, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year is not provided and does not hallucinate a date. This matches the gold answer, which says the movie is not in the database, so it is both correct and faithful to the given information.", "main_total_tokens": 3004, "main_prompt_tokens": 2983, "main_completion_tokens": 21, "judge_total_tokens": 490, "judge_prompt_tokens": 411, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 781.2094688415527, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is not in the database, equivalent in content to saying 'I don't know.' The generated answer matches this uncertainty, so it is both correct and faithful to the gold.", "main_total_tokens": 504, "main_prompt_tokens": 503, "main_completion_tokens": 1, "judge_total_tokens": 464, "judge_prompt_tokens": 393, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 708.7728977203369, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database, indicating the system lacks the information. The generated answer says 'I don't know,' which is equivalent in meaning. It is therefore correct relative to the gold and faithfully reflects the absence of knowledge.", "main_total_tokens": 576, "main_prompt_tokens": 575, "main_completion_tokens": 1, "judge_total_tokens": 477, "judge_prompt_tokens": 396, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Days of Lehman Brothers\" is not mentioned in the provided documents.", "latency_ms": 1859.919786453247, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not provided in the database. The generated answer states that the director is not mentioned in the provided documents. Both convey the same limitation and do not introduce incorrect facts, so the response is both correct and faithful to the gold.", "main_total_tokens": 3163, "main_prompt_tokens": 3140, "main_completion_tokens": 23, "judge_total_tokens": 504, "judge_prompt_tokens": 419, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 537.0306968688965, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not provided in the database, so the correct response is to not provide a director and instead state lack of knowledge. The generated answer, 'I don't know,' aligns with this and does not introduce unsupported information, so it is both correct and faithful to the database limitation.", "main_total_tokens": 574, "main_prompt_tokens": 573, "main_completion_tokens": 1, "judge_total_tokens": 493, "judge_prompt_tokens": 399, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 601.7966270446777, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the director information is not available; the generated answer says 'I don't know,' which is equivalent in meaning. It is both correct and faithful to the gold.", "main_total_tokens": 545, "main_prompt_tokens": 544, "main_completion_tokens": 1, "judge_total_tokens": 464, "judge_prompt_tokens": 397, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 615.8945560455322, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the system cannot answer because the director information is not in the database. The generated answer says \"I don't know,\" which is equivalent in meaning and therefore correct. It is also fully faithful to the gold intent of acknowledging lack of information.", "main_total_tokens": 566, "main_prompt_tokens": 565, "main_completion_tokens": 1, "judge_total_tokens": 484, "judge_prompt_tokens": 401, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 1433.4948062896729, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy' without adding or omitting information.", "main_total_tokens": 2725, "main_prompt_tokens": 2719, "main_completion_tokens": 6, "judge_total_tokens": 455, "judge_prompt_tokens": 396, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 1416.3703918457031, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Stranger in My Bed' and is fully faithful to the provided information.", "main_total_tokens": 2780, "main_prompt_tokens": 2763, "main_completion_tokens": 17, "judge_total_tokens": 461, "judge_prompt_tokens": 400, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 920.8581447601318, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question actually refers to a TV series, not a movie. The gold answer and the generated answer both give the same title, 'Game of Silence', which correctly identifies the work in question. The generated answer is therefore both correct relative to the gold and faithful to the prompt.", "main_total_tokens": 2414, "main_prompt_tokens": 2410, "main_completion_tokens": 4, "judge_total_tokens": 487, "judge_prompt_tokens": 400, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 2282.869577407837, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct movie title for the described plot. It is both correct and faithful.", "main_total_tokens": 2706, "main_prompt_tokens": 2693, "main_completion_tokens": 13, "judge_total_tokens": 453, "judge_prompt_tokens": 395, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 1409.803867340088, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Wood Job!') and correctly identifies the movie based on the description, showing both correctness and faithfulness.", "main_total_tokens": 3494, "main_prompt_tokens": 3490, "main_completion_tokens": 4, "judge_total_tokens": 464, "judge_prompt_tokens": 403, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 1920.461893081665, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase' with no added or incorrect information.", "main_total_tokens": 3039, "main_prompt_tokens": 3016, "main_completion_tokens": 23, "judge_total_tokens": 479, "judge_prompt_tokens": 417, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond", "latency_ms": 1202.9657363891602, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Beyond') and correctly identifies the movie based on the description, so it is both correct and faithful.", "main_total_tokens": 2542, "main_prompt_tokens": 2540, "main_completion_tokens": 2, "judge_total_tokens": 469, "judge_prompt_tokens": 409, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9", "latency_ms": 802.8104305267334, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold reference movie title, so it is both correct and faithful to the provided information.", "main_total_tokens": 2670, "main_prompt_tokens": 2663, "main_completion_tokens": 7, "judge_total_tokens": 463, "judge_prompt_tokens": 409, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread [Doc 32410].", "latency_ms": 1486.2585067749023, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as 'Club Dread'. It is both correct and faithful to the provided information.", "main_total_tokens": 3036, "main_prompt_tokens": 3026, "main_completion_tokens": 10, "judge_total_tokens": 493, "judge_prompt_tokens": 433, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet", "latency_ms": 1204.0505409240723, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: the movie is 'Toilet'. It is correct and fully consistent with the provided information.", "main_total_tokens": 3206, "main_prompt_tokens": 3203, "main_completion_tokens": 3, "judge_total_tokens": 473, "judge_prompt_tokens": 415, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel", "latency_ms": 802.8771877288818, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Electric Angel'). The question is asking for a movie title and both gold and generated responses provide the same, so it is fully correct and faithful to the provided reference.", "main_total_tokens": 2970, "main_prompt_tokens": 2967, "main_completion_tokens": 3, "judge_total_tokens": 491, "judge_prompt_tokens": 418, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "Bottom of the Water [Doc 106899].", "latency_ms": 1584.4295024871826, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer does not match the gold answer ('Mega Ape') and is therefore incorrect. There is no indication that 'Bottom of the Water [Doc 106899]' comes from the provided gold information, so the response is also unfaithful to the reference.", "main_total_tokens": 2508, "main_prompt_tokens": 2497, "main_completion_tokens": 11, "judge_total_tokens": 475, "judge_prompt_tokens": 391, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 224.6253490447998, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Tea House', but the generated answer says 'I don't know', which is incorrect. However, it does not hallucinate an alternative answer and honestly admits lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 467, "judge_prompt_tokens": 386, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 412.08910942077637, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct movie title 'The Uncut Story of Mewtwo's Origin.' Therefore, it is incorrect. However, it does not invent any false information or contradict the prompt; it simply expresses lack of knowledge, so it is faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 395, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "cn", "latency_ms": 902.5425910949707, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'cn' is incorrect; the gold answer is 'English'. There is no support in the provided information for 'cn', so it is both factually wrong and not grounded in the context.", "main_total_tokens": 2563, "main_prompt_tokens": 2561, "main_completion_tokens": 2, "judge_total_tokens": 452, "judge_prompt_tokens": 384, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "Popular", "latency_ms": 786.7450714111328, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'Popular' is incorrect; the correct show is 'Law & Order: Special Victims Unit'. It also does not reflect or engage with the gold answer, so it is unfaithful.", "main_total_tokens": 3137, "main_prompt_tokens": 3135, "main_completion_tokens": 2, "judge_total_tokens": 464, "judge_prompt_tokens": 391, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2 [Doc 79355].", "latency_ms": 1024.7125625610352, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 and does not introduce unsupported information; the bracketed doc reference does not alter the factual content.", "main_total_tokens": 2153, "main_prompt_tokens": 2144, "main_completion_tokens": 9, "judge_total_tokens": 455, "judge_prompt_tokens": 392, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "2025 [Doc 112077].", "latency_ms": 1235.6269359588623, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0177825608059992, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0177825608059992, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the correct release year is 2019, but the generated answer states 2025, which is factually incorrect. Additionally, the generated answer appears to introduce an unsupported citation ([Doc 112077]) without basis, making it unfaithful to the source information.", "main_total_tokens": 2693, "main_prompt_tokens": 2684, "main_completion_tokens": 9, "judge_total_tokens": 482, "judge_prompt_tokens": 393, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 632.9286098480225, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the number of votes the film 'My Dog Skip' received, with the gold answer being 279. The generated response states it cannot find any matching movies or shows, which is incorrect and does not provide the requested numeric value. It also contradicts the known gold answer, so it is neither correct nor faithful.", "main_total_tokens": 2179, "main_prompt_tokens": 2178, "main_completion_tokens": 1, "judge_total_tokens": 490, "judge_prompt_tokens": 392, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 516.5390968322754, "retrieval_recall": 1.0, "retrieval_mrr": 0.16666666666666666, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3562071871080222, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a specific numeric answer (12 votes). The generated answer instead claims it cannot find any matching movies or shows, which is incorrect and does not align with the gold answer. Therefore, it is neither correct nor faithful.", "main_total_tokens": 2217, "main_prompt_tokens": 2216, "main_completion_tokens": 1, "judge_total_tokens": 470, "judge_prompt_tokens": 392, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "2009 [Doc 46069].", "latency_ms": 1175.33278465271, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2009) exactly and does not introduce unsupported information beyond citing a document reference.", "main_total_tokens": 2810, "main_prompt_tokens": 2801, "main_completion_tokens": 9, "judge_total_tokens": 451, "judge_prompt_tokens": 396, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_openai-small_gpt-4o", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en", "latency_ms": 857.0187091827393, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.8868528072345416, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.8868528072345416, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The original language of 'The King's Speech' is English, which matches both the gold answer and the generated answer.", "main_total_tokens": 3003, "main_prompt_tokens": 3001, "main_completion_tokens": 2, "judge_total_tokens": 438, "judge_prompt_tokens": 384, "judge_completion_tokens": 54}
