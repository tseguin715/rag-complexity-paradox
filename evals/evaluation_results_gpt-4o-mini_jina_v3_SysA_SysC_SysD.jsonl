{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "Wall Street was released in 1987 [Doc 6993].", "latency_ms": 5704.613924026489, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer. It does not introduce any contradictions or unsupported details and remains faithful to the provided information.", "main_total_tokens": 2761, "main_prompt_tokens": 2747, "main_completion_tokens": 14, "judge_total_tokens": 460, "judge_prompt_tokens": 392, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "In the movie \"Brain Fluid Explosion Girl,\" classmates are forced to take part in a survival game using their cellphones [Doc 69929].", "latency_ms": 1180.6774139404297, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Brain Fluid Explosion Girl' and matches the gold answer exactly. It does not introduce contradictions or unsupported claims beyond a citation marker, so it is both correct and faithful.", "main_total_tokens": 2484, "main_prompt_tokens": 2455, "main_completion_tokens": 29, "judge_total_tokens": 490, "judge_prompt_tokens": 417, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "In the TV show \"Dungeons & Dragons,\" six friends are transported into the Dungeons & Dragons realm and must try to find a way home with the help of their guide 'Dungeon Master' [Doc 112849].", "latency_ms": 1346.3654518127441, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' which matches the gold answer. It also accurately states that six friends are transported into a Dungeons & Dragons realm and must find a way home with the help of the Dungeon Master, which is consistent with the typical description of the series. The slight deduction in faithfulness is due to the specific citation format ['[Doc 112849]'], which we cannot verify against the provided context, but the substantive content remains faithful to the question and gold answer.", "main_total_tokens": 2541, "main_prompt_tokens": 2496, "main_completion_tokens": 45, "judge_total_tokens": 573, "judge_prompt_tokens": 435, "judge_completion_tokens": 138}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is titled \"Nacht der Wölfe,\" released in 1982 [Doc 2590].", "latency_ms": 1698.171854019165, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe,' matching the gold answer exactly. It only adds release year and a document reference, which does not contradict the given information, so the response is both correct and faithful to the source.", "main_total_tokens": 2549, "main_prompt_tokens": 2504, "main_completion_tokens": 45, "judge_total_tokens": 526, "judge_prompt_tokens": 443, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of \"Picnic at Hanging Rock\" take place in the year 1900 [Doc 126191].", "latency_ms": 987.1158599853516, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of 'Picnic at Hanging Rock' take place in the year 1900, which matches the gold answer. It is fully correct and faithful to the provided information.", "main_total_tokens": 2773, "main_prompt_tokens": 2749, "main_completion_tokens": 24, "judge_total_tokens": 482, "judge_prompt_tokens": 409, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The movie you are referring to is \"June,\" which was released in 2004. It features a young violinist named June who works in a coffee shop and dreams about love while navigating her life and relationships [Doc 32483].", "latency_ms": 1477.2913455963135, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'June,' matching the gold answer and the described plot, so correctness is high. However, it adds an extra citation-like reference '[Doc 32483]' that is not grounded in the provided context, slightly reducing faithfulness.", "main_total_tokens": 2867, "main_prompt_tokens": 2819, "main_completion_tokens": 48, "judge_total_tokens": 567, "judge_prompt_tokens": 480, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "The highest rated Russian-language movie released in 1980 is \"Moscow Does Not Believe in Tears,\" which has a rating of 7.6 [Doc 61].", "latency_ms": 1271.069049835205, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect: according to the gold label, the highest rated Russian-language movie released in 1980 is \"The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger,\" not \"Moscow Does Not Believe in Tears.\" However, the model’s answer is faithful to the cited document (Doc 61), so faithfulness is high even though the final factual answer is wrong.", "main_total_tokens": 3002, "main_prompt_tokens": 2967, "main_completion_tokens": 35, "judge_total_tokens": 541, "judge_prompt_tokens": 430, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest Russian-language movie rating in 1999 is 8.1 for the TV series \"Kamenskaya\" [Doc 115256].", "latency_ms": 1024.5361328125, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the highest Russian-language movie rating in 1999 is 7.6, whereas the generated answer claims it is 8.1 for the TV series 'Kamenskaya'. This contradicts the gold answer, so the correctness is 0. The generated answer also introduces a specific title and rating not supported by the gold data, so its faithfulness to the given information is also 0.", "main_total_tokens": 2893, "main_prompt_tokens": 2863, "main_completion_tokens": 30, "judge_total_tokens": 531, "judge_prompt_tokens": 415, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "There are two movies released in 1987 that have a rating over 7 and are in Chinese:\n\n1. **A Chinese Ghost Story** - Rating: 7.3 [Doc 7064]\n2. **Last Eunuch In China** - Rating: 7.8 [Doc 7754]", "latency_ms": 2104.87961769104, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 4 such movies, but the generated answer states there are 2, so the numerical result is incorrect. However, the assistant faithfully describes the two movies it lists without contradicting its own evidence, so it is internally faithful to the provided information.", "main_total_tokens": 2771, "main_prompt_tokens": 2707, "main_completion_tokens": 64, "judge_total_tokens": 543, "judge_prompt_tokens": 456, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "There are no TV shows released in 1987 that have a rating over 7 and are in Chinese based on the provided context.", "latency_ms": 879.9996376037598, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The gold answer is 2, but the generated answer says there are none, so it is factually incorrect (correctness 0). The model does reference the provided context as the basis for its claim, so it is mostly faithful to the (albeit incorrect) interpretation of that context, without inventing extra details, but it still contradicts the gold label, so partial faithfulness (0.8).", "main_total_tokens": 2971, "main_prompt_tokens": 2944, "main_completion_tokens": 27, "judge_total_tokens": 531, "judge_prompt_tokens": 420, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "The only French language TV show released in 1980 is \"Therese Raquin\" [Doc 112644].", "latency_ms": 1343.2257175445557, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'Fantômas', but the generated answer states 'Therese Raquin' as the only French language TV show released in 1980, which is incorrect. Furthermore, the model cites a document reference [Doc 112644] that contradicts the known correct answer, so the response is not faithful to the provided gold standard.", "main_total_tokens": 2883, "main_prompt_tokens": 2859, "main_completion_tokens": 24, "judge_total_tokens": 511, "judge_prompt_tokens": 409, "judge_completion_tokens": 102}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is \"Kill Cruise,\" released in 1990 [Doc 11205].", "latency_ms": 1274.1339206695557, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Kill Cruise,\" matching the gold answer exactly. It adds the release year and a document reference, which are plausible supporting details and do not contradict the gold answer. Therefore, both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2717, "main_prompt_tokens": 2688, "main_completion_tokens": 29, "judge_total_tokens": 497, "judge_prompt_tokens": 414, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 1568.8679218292236, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as \"Won't You Be My Neighbor?\" and does not introduce any contradictions or unsupported claims, only adding a reference marker.", "main_total_tokens": 2924, "main_prompt_tokens": 2885, "main_completion_tokens": 39, "judge_total_tokens": 511, "judge_prompt_tokens": 440, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The movie \"1969\" was released in the year 1988 [Doc 8389].", "latency_ms": 956.108808517456, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question refers to an unspecified item that, according to the gold answer, was released in 2018. The generated answer instead talks about a movie titled '1969' and claims it was released in 1988, which does not match the gold year and introduces irrelevant, unsupported details. Therefore, it is both incorrect and unfaithful to the gold answer.", "main_total_tokens": 2938, "main_prompt_tokens": 2918, "main_completion_tokens": 20, "judge_total_tokens": 502, "judge_prompt_tokens": 397, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is \"Astro Boy,\" which was released in 1980. The plot involves a young boy robot who is rescued by an Android Advocate, Dr. Elefun, after being abandoned by his creator [Doc 112591].", "latency_ms": 1872.7540969848633, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy', matching the gold answer, so correctness is 1. However, it adds specific plot details and mentions an 'Android Advocate, Dr. Elefun' and being 'rescued' after abandonment that are not supported by the provided gold text, making some of the explanation potentially fabricated, so faithfulness is slightly reduced.", "main_total_tokens": 2740, "main_prompt_tokens": 2677, "main_completion_tokens": 63, "judge_total_tokens": 560, "judge_prompt_tokens": 450, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her young son were held captive for 7 years [Doc 67513].", "latency_ms": 1767.348051071167, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, stating they were held captive for 7 years. It is correct and faithful to the provided gold information.", "main_total_tokens": 2511, "main_prompt_tokens": 2480, "main_completion_tokens": 31, "judge_total_tokens": 486, "judge_prompt_tokens": 425, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 1995.0296878814697, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before the 26 November 1983 break-in, matching the provided gold value. It is faithful to the given information, including the cited document reference.", "main_total_tokens": 2826, "main_prompt_tokens": 2793, "main_completion_tokens": 33, "judge_total_tokens": 511, "judge_prompt_tokens": 428, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "The most popular English-speaking TV show released in 1997 is \"Recess,\" which has a popularity score of 10.3614 [Doc 114720].", "latency_ms": 1182.4333667755127, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question asks for the popularity score of the most popular English-speaking TV show released in 1997; the gold answer is 80.5772. The generated answer instead gives 10.3614 for 'Recess', so the numeric answer is incorrect. In terms of faithfulness, the model cites a specific document and does not contradict its own cited source, but it clearly fails to identify the truly most popular show, so its reasoning relative to the task is flawed. This warrants a low but not zero faithfulness score.", "main_total_tokens": 2977, "main_prompt_tokens": 2943, "main_completion_tokens": 34, "judge_total_tokens": 566, "judge_prompt_tokens": 426, "judge_completion_tokens": 140}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "There is one Russian language TV show released in 1997, which is \"A House for Rent with All the Inconveniences\" [Doc 72036].", "latency_ms": 1277.3785591125488, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says there is 1 Russian language TV show released in 1997. The generated answer also states there is one such show and names it. This matches the gold factually (correctness) and stays within the information implied by the question and gold answer (faithfulness).", "main_total_tokens": 3047, "main_prompt_tokens": 3013, "main_completion_tokens": 34, "judge_total_tokens": 506, "judge_prompt_tokens": 417, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 1474.2062091827393, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Angel Eyes') and does not introduce any contradictions or unsupported claims. It is both correct and faithful to the provided information.", "main_total_tokens": 3231, "main_prompt_tokens": 3196, "main_completion_tokens": 35, "judge_total_tokens": 493, "judge_prompt_tokens": 429, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French (fr) [Doc 110681].", "latency_ms": 903.4602642059326, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer claims the language is French, while the gold label says Korean, so it is factually incorrect. It also introduces an unsupported movie title (\"Foreign Language\") and document reference not grounded in the provided QA pair, so it is unfaithful to the given information.", "main_total_tokens": 2942, "main_prompt_tokens": 2922, "main_completion_tokens": 20, "judge_total_tokens": 484, "judge_prompt_tokens": 396, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "There are no English-speaking horror movies released in 2009 with a popularity score greater than 7. The highest popularity score among the listed movies is 5.5547 for \"9\" [Doc 44837].", "latency_ms": 1784.0712070465088, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer contradicts the gold answer: it claims there are no such movies, while the correct count is 2. However, the answer is internally consistent with the (incorrect) evidence it cites, so it is faithful to its own stated evidence but factually wrong relative to the gold.", "main_total_tokens": 3014, "main_prompt_tokens": 2969, "main_completion_tokens": 45, "judge_total_tokens": 527, "judge_prompt_tokens": 437, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The context provided does not include any movies released in 1979, so I cannot provide an answer.", "latency_ms": 908.4110260009766, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly reflects that no 1979 movies are available in the provided context, matching the gold response in substance. It does not introduce external information and stays faithful to the given context and limitations.", "main_total_tokens": 2883, "main_prompt_tokens": 2862, "main_completion_tokens": 21, "judge_total_tokens": 488, "judge_prompt_tokens": 416, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The movie \"Back to the Future Part IV\" is not mentioned in the provided context, so I cannot provide an answer.", "latency_ms": 1299.577236175537, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because it frames the limitation as absence from the provided context rather than acknowledging that Back to the Future Part IV does not exist, which is the key fact in the gold answer. It is also unfaithful to the gold explanation, which states explicitly that the movie does not exist in the database, not merely that it is missing from the context.", "main_total_tokens": 3703, "main_prompt_tokens": 3678, "main_completion_tokens": 25, "judge_total_tokens": 530, "judge_prompt_tokens": 425, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "There is no movie from 1980 about the nuclear Chernobyl disaster in the provided context. The earliest mentioned movie related to Chernobyl is \"Collapse,\" which was released in 1990 and has a popularity score of 5.4109 [Doc 10620].", "latency_ms": 1701.5233039855957, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question asks for the popularity score of the least popular movie in 1980 about the Chernobyl disaster. The gold answer correctly notes that there are no such movies from 1980. The generated answer introduces an irrelevant movie ('Collapse' from 1990) with a popularity score, which does not answer the question and goes beyond the gold’s scope. Thus, it is incorrect. It is partially faithful because it correctly states that there is no movie from 1980 about Chernobyl in the provided context, but it then adds extra, unsupported detail about another movie and a specific popularity score that are not in the gold answer, so faithfulness is only partial.", "main_total_tokens": 2959, "main_prompt_tokens": 2902, "main_completion_tokens": 57, "judge_total_tokens": 632, "judge_prompt_tokens": 463, "judge_completion_tokens": 169}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are three entries with 'Astro Boy' in the title:\n\n1. **Astro Boy** (TV, 1980) - [Doc 112591]\n2. **Astro Boy** (Movie, 2009) - [Doc 44947]\n3. **Astro Boy** (TV, 2003) - [Doc 116265]", "latency_ms": 2241.211175918579, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.8318724637288826, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.8318724637288826, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 4, while the generated response lists only 3 titles, so it is numerically incorrect. However, the assistant faithfully describes the three entries it does list and does not invent extra items, so it is faithful to its own content, just incomplete relative to the gold.", "main_total_tokens": 2875, "main_prompt_tokens": 2798, "main_completion_tokens": 77, "judge_total_tokens": 553, "judge_prompt_tokens": 463, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "There are no movies listed in the provided context for the year 2025 that allude to the Holocaust.", "latency_ms": 1128.5908222198486, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there are 2 such movies, while the generated answer claims there are none. This is factually incorrect relative to the gold, so correctness is 0. However, the model appears to be faithfully describing what it believes from the given (stated) context, saying none are listed, so faithfulness to its stated reasoning/context is 1.", "main_total_tokens": 2824, "main_prompt_tokens": 2802, "main_completion_tokens": 22, "judge_total_tokens": 511, "judge_prompt_tokens": 406, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "There were three TV shows released in the 1990s about Super Mario Bros:\n\n1. The Super Mario Bros. Super Show! (1989) - Although it started in 1989, it is often associated with the 1990s due to its airing schedule [Doc 113517].\n2. The Adventures of Super Mario Bros. 3 (1990) [Doc 113661].\n3. Super Mario World (1991) [Doc 113814].", "latency_ms": 2167.4911975860596, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 1 show, but the generated answer lists 3 shows and miscounts those from the 1990s, so correctness is 0. It is also unfaithful to the provided gold answer, which clearly states 1, so faithfulness is 0.", "main_total_tokens": 2881, "main_prompt_tokens": 2784, "main_completion_tokens": 97, "judge_total_tokens": 573, "judge_prompt_tokens": 484, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "Based on the provided context, there is one TV show about Super Mario Bros released between 2000 and 2010:\n\n1. **The Adventures of Super Mario Bros. 3** - Released in 1990 (not within the specified range).\n2. **Super Mario World** - Released in 1991 (not within the specified range).\n3. **The Super Mario Bros. Super Show!** - Released in 1989 (not within the specified range).\n4. **Mario** - Released in 2013 (not within the specified range).\n\nNo TV shows specifically about Super Mario Bros were released between 2000 and 2010 according to the provided documents.", "latency_ms": 3058.502435684204, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The question asks for the number of TV shows released between 2000 and 2010 about Super Mario Bros., and the gold answer is 0. The generated answer first incorrectly states that there is one TV show in that period, then lists shows all outside 2000–2010 and finally concludes that there were no such shows in that period. Because it explicitly claims both that there is one TV show and that there are none, it does not provide a clear, correct numerical answer, so correctness is 0. In terms of faithfulness, the model does base its reasoning on the provided context (listing the shows and their years) and correctly notes that they fall outside the time range, but it contradicts itself with the initial statement. Since it largely uses the given data but introduces a mistaken summary, faithfulness is partial.", "main_total_tokens": 2912, "main_prompt_tokens": 2774, "main_completion_tokens": 138, "judge_total_tokens": 728, "judge_prompt_tokens": 527, "judge_completion_tokens": 201}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular TV show about ninjas is \"UNDER NINJA,\" which has a popularity rating of 3.1982 [Doc 133621]. Among movies, \"Ninja\" from 2009 has the highest popularity rating of 2.2476 [Doc 45297].", "latency_ms": 2045.3300476074219, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the most popular TV show or movie about ninjas, and the gold answer is 'Naruto,' a globally well-known and clearly more popular ninja-themed series. The generated answer instead cites 'UNDER NINJA' and the 2009 film 'Ninja' based on specific popularity ratings from some document set, which contradicts both general real-world popularity and the gold label. Thus the content is incorrect relative to the task. It is also unfaithful to the intended answer, as it ignores 'Naruto' entirely and relies solely on document-specific metrics rather than the ground-truth label provided.", "main_total_tokens": 3156, "main_prompt_tokens": 3096, "main_completion_tokens": 60, "judge_total_tokens": 598, "judge_prompt_tokens": 442, "judge_completion_tokens": 156}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The context provided does not mention a Jurassic Park/Power Ranger crossover movie, so I cannot provide that information.", "latency_ms": 1413.34867477417, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the movie was not released. The generated answer instead says it cannot provide the information because the context does not mention it, which implies uncertainty rather than stating that it does not exist or was not released. This is incorrect with respect to the gold label, so correctness is 0. However, the generated answer is faithful to the given context (it accurately reflects that the context does not mention such a movie), so faithfulness is 1.", "main_total_tokens": 2934, "main_prompt_tokens": 2912, "main_completion_tokens": 22, "judge_total_tokens": 531, "judge_prompt_tokens": 407, "judge_completion_tokens": 124}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "The context provided does not mention any movie where Barbie fights the Predator. Therefore, I cannot provide an answer based on the given information.", "latency_ms": 1668.778419494629, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer states that the movie was not released, implying such a movie was planned or rumored but never actually came out. The generated answer instead claims that the context does not mention any such movie and therefore gives no substantive answer about its existence or release, which conflicts with the gold. This is factually incorrect with respect to the labeled answer. It is partially faithful in that it does not hallucinate a specific movie title, but it fails to address the core fact (that it was not released), so both correctness and faithfulness are low.", "main_total_tokens": 3182, "main_prompt_tokens": 3155, "main_completion_tokens": 27, "judge_total_tokens": 551, "judge_prompt_tokens": 409, "judge_completion_tokens": 142}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The 4th oldest Star Wars movie is \"Star Wars: The Clone Wars,\" which was released in 2008 [Doc 42090].", "latency_ms": 1542.2024726867676, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect: the 4th oldest Star Wars movie per the gold reference is 'Star Wars: Droids - The Great Heep,' not 'Star Wars: The Clone Wars.' However, the answer is faithful to its cited document, so faithfulness is scored as 1.", "main_total_tokens": 3228, "main_prompt_tokens": 3198, "main_completion_tokens": 30, "judge_total_tokens": 510, "judge_prompt_tokens": 419, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The movie \"1969\" was released in the year 1988 [Doc 8389].", "latency_ms": 827.5833129882812, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer gives the year 1988, but the correct year is 1986, so the answer is factually incorrect. However, it is faithful to the given generated text, accurately reflecting what was produced, so faithfulness is high.", "main_total_tokens": 2938, "main_prompt_tokens": 2918, "main_completion_tokens": 20, "judge_total_tokens": 478, "judge_prompt_tokens": 397, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The rating of the movie \"Forrest Gump\" is 8.466 [Doc 15236].", "latency_ms": 1287.285566329956, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating value 8.466 and clearly refers to the correct movie, so it is both correct and faithful to the provided information.", "main_total_tokens": 2811, "main_prompt_tokens": 2789, "main_completion_tokens": 22, "judge_total_tokens": 480, "judge_prompt_tokens": 416, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 1168.1580543518066, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the show as 'Mighty Med'. It is fully supported by the given information and does not introduce unsupported content.", "main_total_tokens": 2359, "main_prompt_tokens": 2328, "main_completion_tokens": 31, "judge_total_tokens": 488, "judge_prompt_tokens": 423, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in Argentina [Doc 53582].", "latency_ms": 925.1790046691895, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Argentina as the country where the prison break occurred, matching the gold answer exactly and staying faithful to the provided information.", "main_total_tokens": 2734, "main_prompt_tokens": 2708, "main_completion_tokens": 26, "judge_total_tokens": 476, "judge_prompt_tokens": 417, "judge_completion_tokens": 59}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice Li Nai falls into a trap designed by the \"Chef Assassin\" Jiang Jie [Doc 135231].", "latency_ms": 1327.791452407837, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The question asks whose trap the apprentice falls into; the gold answer is 'Jiang Jie'. The generated answer clearly states that the trap is designed by Jiang Jie, so it is correct. It adds extra detail (\"Li Nai\", \"Chef Assassin\", and a document reference) that is not in the gold but does not contradict it, so it is mostly faithful with minor speculative embellishment.", "main_total_tokens": 2896, "main_prompt_tokens": 2864, "main_completion_tokens": 32, "judge_total_tokens": 528, "judge_prompt_tokens": 416, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "There are two Korean-language movies that mention assassins and were released after 2023:\n\n1. **The Assassin** (2023) - Rating: 5.581, Genres: ['Action'] [Doc 106219].\n2. **Assassin** (2023) - Rating: 5.229, Genres: ['Action', 'Thriller', 'Science Fiction', 'Crime'] [Doc 105970]. \n\nNo other Korean-language movies mentioning assassins are listed after 2023.", "latency_ms": 2417.625665664673, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there are 4 such movies, while the generated answer claims there are 2, so the numerical result is incorrect. The generated answer also cites 2023 releases when the question asks for movies after 2023, so its reasoning does not faithfully follow the query constraints.", "main_total_tokens": 2846, "main_prompt_tokens": 2743, "main_completion_tokens": 103, "judge_total_tokens": 576, "judge_prompt_tokens": 486, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "The context provided does not contain any information about the number of movies featuring Steven Seagal or Bruce Lee. Therefore, I cannot provide an answer to that question.", "latency_ms": 1515.852451324463, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.5, "faithfulness_score": 0.2, "justification": "The gold answer clarifies that there are no movies featuring both Bruce Lee and Steven Seagal together. The generated answer instead claims the context does not contain information about the number of movies featuring Steven Seagal or Bruce Lee at all, which misinterprets the question and ignores the specific information that there are zero joint movies. It is partially correct in saying it cannot provide a number, but it is not aligned with the gold response and is not faithful to the given answer about no such movies existing.", "main_total_tokens": 3144, "main_prompt_tokens": 3112, "main_completion_tokens": 32, "judge_total_tokens": 559, "judge_prompt_tokens": 425, "judge_completion_tokens": 134}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 1124.6981620788574, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (6.3) and correctly attributes it, so it is both correct and faithful to the provided information.", "main_total_tokens": 2959, "main_prompt_tokens": 2939, "main_completion_tokens": 20, "judge_total_tokens": 464, "judge_prompt_tokens": 403, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The movie \"Fast Track: No Limits\" centers around street racing, as it is described as an action movie focused on a group of street racers [Doc 41929].", "latency_ms": 1220.3710079193115, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.88, "justification": "The gold answer specifies the style of racing as 'Drift', whereas the generated answer says the movie centers around 'street racing'. While drifting can occur in street racing, the question asks specifically for the style, and the correct label is drift. Thus the generated answer is incorrect. It is, however, faithful to its cited document, which describes street racing, so faithfulness (to the provided evidence) is high but not perfect in case the doc is incomplete.", "main_total_tokens": 3138, "main_prompt_tokens": 3103, "main_completion_tokens": 35, "judge_total_tokens": 539, "judge_prompt_tokens": 413, "judge_completion_tokens": 126}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 1285.1579189300537, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly giving the movie title as 'Confronting a Serial Killer' and appears grounded in the cited document, so it is both correct and faithful.", "main_total_tokens": 2855, "main_prompt_tokens": 2835, "main_completion_tokens": 20, "judge_total_tokens": 496, "judge_prompt_tokens": 426, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 866.4219379425049, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold value (7.838) and does not introduce contradictions or extraneous, ungrounded information. It is correct and faithful to the provided gold answer.", "main_total_tokens": 2965, "main_prompt_tokens": 2943, "main_completion_tokens": 22, "judge_total_tokens": 476, "judge_prompt_tokens": 407, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"Victim\" [Doc 22488].", "latency_ms": 1668.6592102050781, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct title is 'The Sixth Sense'. The generated answer incorrectly states the title as 'Victim', so it is factually wrong, yielding a correctness score of 0. In terms of faithfulness, the answer appears to be grounded in some cited document ('Doc 22488') rather than being hallucinated without reference, so it is partially faithful to its stated source, but not to the gold answer, warranting a mid-level faithfulness score.", "main_total_tokens": 2746, "main_prompt_tokens": 2718, "main_completion_tokens": 28, "judge_total_tokens": 544, "judge_prompt_tokens": 419, "judge_completion_tokens": 125}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 906.2480926513672, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer, and it stays faithful to the provided information without introducing unsupported claims.", "main_total_tokens": 3066, "main_prompt_tokens": 3049, "main_completion_tokens": 17, "judge_total_tokens": 462, "judge_prompt_tokens": 397, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is 'The Transporter Refueled' [Doc 67500].", "latency_ms": 1419.1434383392334, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly states the title of the 2015 reboot as 'The Transporter Refueled'. It is fully correct and faithful to the reference.", "main_total_tokens": 2886, "main_prompt_tokens": 2857, "main_completion_tokens": 29, "judge_total_tokens": 489, "judge_prompt_tokens": 420, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in 2015 [Doc 67500].", "latency_ms": 956.2413692474365, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It is faithful as it does not introduce any contradictions or unsupported information beyond a generic document reference.", "main_total_tokens": 3170, "main_prompt_tokens": 3150, "main_completion_tokens": 20, "judge_total_tokens": 480, "judge_prompt_tokens": 404, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "There is no romantic comedy in the provided context that features Abraham Lincoln coming back from the dead. The closest related title is \"Abraham Lincoln: Vampire Hunter,\" but it is not a romantic comedy; it is an action, fantasy, and horror film [Doc 54943].", "latency_ms": 1424.3133068084717, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a romantic comedy where Abraham Lincoln comes back from the dead. The gold answer says no such movie description can be found. The generated answer correctly states that there is no romantic comedy in the provided context featuring Abraham Lincoln coming back from the dead and notes that the closest match is 'Abraham Lincoln: Vampire Hunter,' which is not a romantic comedy. This aligns with the gold answer and stays within the given context, so it is both correct and faithful.", "main_total_tokens": 2613, "main_prompt_tokens": 2557, "main_completion_tokens": 56, "judge_total_tokens": 571, "judge_prompt_tokens": 446, "judge_completion_tokens": 125}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of the movie that alludes to a manuscript written by Ernest Hemingway is \"Havana Darkness\" [Doc 90923].", "latency_ms": 887.078046798706, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Havana Darkness' and correctly addresses the question. It is fully supported by the given information and does not introduce any unsupported claims.", "main_total_tokens": 2590, "main_prompt_tokens": 2563, "main_completion_tokens": 27, "judge_total_tokens": 481, "judge_prompt_tokens": 415, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show \"Doll & Em\" [Doc 122259].", "latency_ms": 1690.6678676605225, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em', matching the gold answer exactly. It does not introduce any contradictions or unsupported claims and appears consistent with the cited document, so it is both correct and faithful.", "main_total_tokens": 2984, "main_prompt_tokens": 2955, "main_completion_tokens": 29, "judge_total_tokens": 494, "judge_prompt_tokens": 417, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The movie is \"Hallam Foe,\" released in 2007. Hallam's talent for spying on people reveals his darkest fears and peculiar desires as he tries to expose the true cause of his mother's death [Doc 39389].", "latency_ms": 1561.1612796783447, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Hallam Foe,\" matching the gold answer. The additional descriptive sentence is consistent with the film's plot and does not contradict the question or known information, so it is faithful.", "main_total_tokens": 3077, "main_prompt_tokens": 3029, "main_completion_tokens": 48, "judge_total_tokens": 523, "judge_prompt_tokens": 447, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The context does not provide information about a specific character's last name or whether it sounds like a friend or a foe. Therefore, I cannot answer the question based on the provided context.", "latency_ms": 1058.9871406555176, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is an unambiguous factual label: 'A foe'. The generated answer claims the question cannot be answered from context, which contradicts the gold label; thus it is incorrect. However, we are only to judge faithfulness to the provided context, not to the hidden gold. Since the assistant states that the context lacks the necessary information and refrains from fabricating details, its answer is faithful to the described context. Therefore correctness is 0, faithfulness is 1.", "main_total_tokens": 3048, "main_prompt_tokens": 3011, "main_completion_tokens": 37, "judge_total_tokens": 553, "judge_prompt_tokens": 423, "judge_completion_tokens": 130}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "Sam is 18 years old in the 2017 TV show Atypical [Doc 124583].", "latency_ms": 1711.0779285430908, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, which matches the gold answer exactly. It does not introduce any contradictions or unsupported claims beyond citing a document, so it is both correct and faithful.", "main_total_tokens": 2646, "main_prompt_tokens": 2624, "main_completion_tokens": 22, "judge_total_tokens": 492, "judge_prompt_tokens": 409, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 3002.7999877929688, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the 2017 TV show Atypical, Sam is on the autism spectrum, which matches the gold answer exactly. It is fully accurate and faithful to the reference.", "main_total_tokens": 2580, "main_prompt_tokens": 2557, "main_completion_tokens": 23, "judge_total_tokens": 482, "judge_prompt_tokens": 410, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "In \"À corde tendue,\" Sandra was born in Chamonix [Doc 59659].", "latency_ms": 1147.6256847381592, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that Sandra was born in Chamonix, which exactly matches the gold answer. It does not introduce any contradictions or extraneous claims beyond a harmless document reference. Thus it is both correct and faithful.", "main_total_tokens": 3015, "main_prompt_tokens": 2995, "main_completion_tokens": 20, "judge_total_tokens": 492, "judge_prompt_tokens": 417, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In this Japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 1242.3698902130127, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly fills in the blank with 'rains,' exactly matching the gold answer. It is fully faithful, directly using the provided gold completion without introducing new or conflicting information.", "main_total_tokens": 3336, "main_prompt_tokens": 3315, "main_completion_tokens": 21, "judge_total_tokens": 480, "judge_prompt_tokens": 412, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the low-rated German language animation \"Tabaluga,\" the character is the last of the dragons and the crown-prince of Greenland, a magical place inhabited by talking animals of many different species [Doc 114810].", "latency_ms": 1965.3279781341553, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Greenland as the place where Tabaluga is crown-prince, matching the gold answer exactly. It does not introduce contradictions or unsupported claims; the extra descriptive detail is plausible and does not affect the core factual alignment, so the response is both correct and faithful.", "main_total_tokens": 3196, "main_prompt_tokens": 3151, "main_completion_tokens": 45, "judge_total_tokens": 527, "judge_prompt_tokens": 439, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is titled \"One Afternoon,\" released in 2021 [Doc 96838].", "latency_ms": 1564.255952835083, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'One Afternoon,' which matches the gold answer exactly. The additional detail about the 2021 release and document reference is consistent with the source and does not contradict the gold label, so the response is both correct and faithful to the given information.", "main_total_tokens": 2473, "main_prompt_tokens": 2423, "main_completion_tokens": 50, "judge_total_tokens": 542, "judge_prompt_tokens": 454, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead,\" which has a rating of 6.746 [Doc 93260].", "latency_ms": 3582.1681022644043, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies the movie as 'The Life Ahead,' matching the gold answer, so correctness is 1.0. However, it adds an extra detail about the rating (\"low-rated\" with a specific numeric value 6.746) that appears to be drawn from an external document and is not required or necessarily accurate in the context of the question, reducing faithfulness slightly.", "main_total_tokens": 2911, "main_prompt_tokens": 2865, "main_completion_tokens": 46, "judge_total_tokens": 556, "judge_prompt_tokens": 444, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team, showcasing how a man of humble beginnings could take on the elite of motor racing and win [Doc 73859].", "latency_ms": 1656.5392017364502, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary, matching the gold answer. The extra descriptive details are plausible and do not contradict the core fact, so the response is both correct and faithful to the prompt.", "main_total_tokens": 2808, "main_prompt_tokens": 2757, "main_completion_tokens": 51, "judge_total_tokens": 515, "judge_prompt_tokens": 435, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The context does not provide the name of the racing team associated with any of the movies or characters mentioned. Therefore, I cannot provide that information.", "latency_ms": 1689.4066333770752, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the racing team is 'The McLaren Motor Racing team'. The generated answer claims the context does not provide the name of the racing team and refuses to answer. This directly contradicts the gold answer, so it is factually incorrect. However, given only the provided generated response and no additional context, it appears internally consistent with its stated lack of information, so it is faithful to its own reasoning, though incorrect relative to the gold.", "main_total_tokens": 2911, "main_prompt_tokens": 2882, "main_completion_tokens": 29, "judge_total_tokens": 535, "judge_prompt_tokens": 412, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context does not provide information about the country of origin for any character or individual mentioned. Therefore, I cannot answer your question.", "latency_ms": 1262.9756927490234, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The gold answer is 'New Zealand'. The generated answer claims the context does not provide the information and refuses to answer, which is incorrect given the gold. Thus correctness is 0. In terms of faithfulness, the model is internally consistent with its stated view of the context (it does not hallucinate a country), so it is largely faithful to that premise, though we cannot fully verify without the context; assign high but not perfect faithfulness.", "main_total_tokens": 2517, "main_prompt_tokens": 2491, "main_completion_tokens": 26, "judge_total_tokens": 520, "judge_prompt_tokens": 402, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "There is no mention of John McClane blowing up the Great Wall of China in any 2013 movie. The movie \"A Good Day to Die Hard\" features John McClane, but it is set in Moscow, not China, and does not involve the Great Wall of China [Doc 58791].", "latency_ms": 2185.46986579895, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no 2013 movie in which John McClane blows up the Great Wall of China and clarifies that 'A Good Day to Die Hard' is set in Moscow, which aligns with known facts. This matches the gold answer's intent (no such movie exists in the database) and stays faithful to it while adding accurate, non-contradictory detail. Therefore, both correctness and faithfulness merit full scores.", "main_total_tokens": 3145, "main_prompt_tokens": 3081, "main_completion_tokens": 64, "judge_total_tokens": 586, "judge_prompt_tokens": 462, "judge_completion_tokens": 124}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The movie you are referring to is \"Tomie: Forbidden Fruit,\" which was released in 2002. The description matches exactly: \"When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental\" [Doc 28589].", "latency_ms": 1722.533941268921, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2002 Japanese horror film as \"Tomie: Forbidden Fruit,\" which matches the gold answer exactly. It does not introduce contradictions or extraneous fabricated details and stays faithful to the provided description.", "main_total_tokens": 3090, "main_prompt_tokens": 3024, "main_completion_tokens": 66, "judge_total_tokens": 559, "judge_prompt_tokens": 483, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 1152.9490947723389, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, matching the gold answer. It directly reflects the given information without introducing unsupported details, so it is both correct and faithful.", "main_total_tokens": 3373, "main_prompt_tokens": 3344, "main_completion_tokens": 29, "judge_total_tokens": 497, "judge_prompt_tokens": 423, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In \"Scorpion King: Book of Souls\" (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls, which will allow them to put an end to an evil warlord [Doc 82582].", "latency_ms": 1805.0851821899414, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies that in 'Scorpion King: Book of Souls' (2018), the Scorpion King and Tala are searching for the Book of Souls, which matches the gold answer. The added detail about it being a legendary relic that can help end an evil warlord is broadly accurate to the film’s plot, though the exact phrasing and the bracketed reference [Doc 82582] are not grounded in the provided gold statement; the doc reference appears fabricated. Hence, correctness is full, but faithfulness is slightly reduced due to the extra, ungrounded citation-style content.", "main_total_tokens": 3448, "main_prompt_tokens": 3397, "main_completion_tokens": 51, "judge_total_tokens": 599, "judge_prompt_tokens": 445, "judge_completion_tokens": 154}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Zombies on the Moon\" is not found in the provided context.", "latency_ms": 994.8067665100098, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, and the generated answer correctly says the release year is not found in the provided context. Both effectively convey that the information is unavailable, so the response is both correct and faithful to the gold.", "main_total_tokens": 3140, "main_prompt_tokens": 3121, "main_completion_tokens": 19, "judge_total_tokens": 492, "judge_prompt_tokens": 410, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The title \"The Galactic Baker\" is not found in the provided context.", "latency_ms": 807.8000545501709, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the movie/title is not found or not in the database/context. This matches the available information, so the generated response is both correct and faithful to the given context.", "main_total_tokens": 2896, "main_prompt_tokens": 2881, "main_completion_tokens": 15, "judge_total_tokens": 476, "judge_prompt_tokens": 404, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year of release for \"Return of the Null Pointer\" is not found in the provided context.", "latency_ms": 960.0441455841064, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database. The generated answer similarly states that the year of release is not found in the provided context, which is effectively equivalent in meaning. It does not hallucinate a year and correctly reflects lack of information, so it is both correct and faithful to the gold.", "main_total_tokens": 2592, "main_prompt_tokens": 2572, "main_completion_tokens": 20, "judge_total_tokens": 505, "judge_prompt_tokens": 411, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The context does not provide any information about a movie titled \"Data Science: The Movie.\" Therefore, I cannot answer your question.", "latency_ms": 1537.1010303497314, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database. The generated answer states that the context does not provide information about such a movie and therefore it cannot answer. Both effectively convey inability to answer and do not fabricate a date, so the response is correct and faithful to the gold.", "main_total_tokens": 3131, "main_prompt_tokens": 3105, "main_completion_tokens": 26, "judge_total_tokens": 505, "judge_prompt_tokens": 417, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no mention of a movie titled \"Python's Revenge 2\" in the provided context. The closest titles are \"Python\" (2000) and \"Python 2\" (2002), but neither is referred to as \"Python's Revenge 2.\" Therefore, I cannot provide an answer to your question.", "latency_ms": 1752.2590160369873, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The gold answer says the movie is not in the database. The generated answer similarly states that it cannot provide an answer, which is effectively correct given the information and does not invent a release year, so correctness is 1. However, it adds extra details about other movies (\"Python\" and \"Python 2\") that are not present in the gold and may not be grounded in the same database context, slightly reducing faithfulness, so faithfulness is 0.8.", "main_total_tokens": 2994, "main_prompt_tokens": 2929, "main_completion_tokens": 65, "judge_total_tokens": 585, "judge_prompt_tokens": 456, "judge_completion_tokens": 129}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year of release for \"The Last Pixel\" is not found in the provided context.", "latency_ms": 914.2534732818604, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers convey that the release year is unknown/not available. The generated answer is correct given the context and faithfully reflects the lack of information without hallucinating a year.", "main_total_tokens": 2602, "main_prompt_tokens": 2584, "main_completion_tokens": 18, "judge_total_tokens": 475, "judge_prompt_tokens": 407, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The title \"The man who ate the sun\" is not found in the provided context. Therefore, I cannot provide the year of its release.", "latency_ms": 1393.3453559875488, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database. The generated answer similarly states the title is not found and that it cannot provide the release year. This matches the gold in content and intent, so it is both correct and faithful.", "main_total_tokens": 3395, "main_prompt_tokens": 3366, "main_completion_tokens": 29, "judge_total_tokens": 500, "judge_prompt_tokens": 421, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Days of Lehman Brothers\" is not mentioned in the provided context.", "latency_ms": 848.3686447143555, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not in the database. The generated answer states that the director is not mentioned in the provided context. Both convey the same limitation and do not hallucinate a director, so the response is both correct and faithful to the gold.", "main_total_tokens": 3306, "main_prompt_tokens": 3284, "main_completion_tokens": 22, "judge_total_tokens": 504, "judge_prompt_tokens": 419, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The information regarding the director of the movie \"A Love to Die For\" is not found in the provided context.", "latency_ms": 1657.7565670013428, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the given context/database. The generated response correctly refrains from hallucinating a director and is faithful to the lack of information in the prompt.", "main_total_tokens": 2946, "main_prompt_tokens": 2923, "main_completion_tokens": 23, "judge_total_tokens": 492, "judge_prompt_tokens": 418, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not mentioned in the provided context.", "latency_ms": 1288.1464958190918, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not mentioned in the provided context, which matches the gold answer that the director information is not in the database. It is both correct and faithful to the given information.", "main_total_tokens": 2910, "main_prompt_tokens": 2892, "main_completion_tokens": 18, "judge_total_tokens": 484, "judge_prompt_tokens": 411, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The information regarding the director of \"The Last Night at Tremore Beach\" is not provided in the context.", "latency_ms": 882.6079368591309, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director information is not provided and aligns with the gold answer. It does not introduce any unsupported details and remains faithful to the given context and database limitations.", "main_total_tokens": 2869, "main_prompt_tokens": 2847, "main_completion_tokens": 22, "judge_total_tokens": 487, "judge_prompt_tokens": 419, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy,\" released in 2021 [Doc 98375].", "latency_ms": 1416.2793159484863, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'Rise of the Mummy,' matching the gold answer, so correctness is 1. However, it cites a specific release year and a document reference [Doc 98375] that are not supported by the provided context, making those details ungrounded; thus faithfulness is 0.", "main_total_tokens": 2748, "main_prompt_tokens": 2716, "main_completion_tokens": 32, "judge_total_tokens": 521, "judge_prompt_tokens": 423, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse is \"Stranger in My Bed\" (2005) [Doc 35237].", "latency_ms": 1107.4786186218262, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as \"Stranger in My Bed\". It does not introduce any contradictions or unsupported claims. The citation tag does not affect faithfulness to the provided gold answer.", "main_total_tokens": 2762, "main_prompt_tokens": 2729, "main_completion_tokens": 33, "judge_total_tokens": 506, "judge_prompt_tokens": 429, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The plot you described occurs in the TV show \"Game of Silence,\" where a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years [Doc 123938].", "latency_ms": 1387.5412940979004, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The question explicitly asks for a movie, and the gold answer is the TV series 'Game of Silence.' The generated answer correctly identifies 'Game of Silence' and matches the gold, but incorrectly labels it as a TV show when the question says 'movie.' This makes the core identification faithful to the gold but technically incorrect with respect to the question's requirement, so correctness is 0. The answer is based entirely on the provided gold snippet, so faithfulness to the given information is high, though not perfect due to the mismatch in type (movie vs. TV show).", "main_total_tokens": 2235, "main_prompt_tokens": 2197, "main_completion_tokens": 38, "judge_total_tokens": 582, "judge_prompt_tokens": 435, "judge_completion_tokens": 147}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie you are referring to is \"The Craigslist Killer,\" which is based on the true story of Philip Markoff, a med student who leads a double life as a brutal sexual deviant abusing prostitutes he finds via Craigslist [Doc 53039].", "latency_ms": 2111.75537109375, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Craigslist Killer,\" matching the gold answer. The added detail about Philip Markoff being a med student who attacked women via Craigslist is accurate and consistent with the true story on which the film is based, so it remains faithful to the source description and does not introduce contradictions.", "main_total_tokens": 2645, "main_prompt_tokens": 2595, "main_completion_tokens": 50, "judge_total_tokens": 536, "judge_prompt_tokens": 442, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which Yuki Hirano sets his eyes on the role of a forestry trainee after failing his university entrance exam is \"Wood Job!\" [Doc 66094].", "latency_ms": 1399.2583751678467, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", which matches the gold answer. It directly restates the question context and provides the correct title without adding unsupported information, so it is both correct and faithful.", "main_total_tokens": 3093, "main_prompt_tokens": 3058, "main_completion_tokens": 35, "judge_total_tokens": 510, "judge_prompt_tokens": 435, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure, is titled \"Sesame Street: The Magical Wand Chase\" from the year 2017 [Doc 78373].", "latency_ms": 2239.424467086792, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies the movie as \"Sesame Street: The Magical Wand Chase,\" matching the gold answer, so correctness is 1.0. However, it adds an extra detail (the year 2017 and a specific document reference [Doc 78373]) that is not supported or verifiable from the provided information, slightly reducing faithfulness.", "main_total_tokens": 2794, "main_prompt_tokens": 2737, "main_completion_tokens": 57, "judge_total_tokens": 572, "judge_prompt_tokens": 467, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a 7 year-old boy must find the hero within him to save his parents trapped inside a burning car due to a rock fall is titled \"Beyond,\" released in 2016 [Doc 74597].", "latency_ms": 1505.1233768463135, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Beyond,\" which matches the gold answer. It does not introduce any contradictions with the question or gold; mentioning the 2016 release year adds plausible detail and remains faithful to the described scenario.", "main_total_tokens": 2330, "main_prompt_tokens": 2284, "main_completion_tokens": 46, "judge_total_tokens": 532, "judge_prompt_tokens": 454, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie you are referring to is \"6ixtynin9\" from 1999. In this film, a woman is fired from a financial corporation during the Asia crisis and returns home with no money [Doc 22478].", "latency_ms": 1885.7975006103516, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', so correctness is 1. However, it references an external document ID ('[Doc 22478]') that is not present in the provided context, making that part unfaithful to the given information, so faithfulness is 0.", "main_total_tokens": 2402, "main_prompt_tokens": 2354, "main_completion_tokens": 48, "judge_total_tokens": 546, "judge_prompt_tokens": 450, "judge_completion_tokens": 96}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie you are referring to is \"Club Dread,\" which was released in 2004. [Doc 32410]", "latency_ms": 1292.6268577575684, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' matching the gold answer, and does not introduce any contradictions or hallucinations. It is fully correct and faithful.", "main_total_tokens": 2944, "main_prompt_tokens": 2918, "main_completion_tokens": 26, "judge_total_tokens": 516, "judge_prompt_tokens": 450, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie you are referring to is \"Toilet,\" which was released in 2010. The plot involves a 30-something Gundam fan and engineer who is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral [Doc 48873].", "latency_ms": 2082.9222202301025, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" which matches the gold answer exactly. The explanation about the plot is consistent with the question and does not introduce unsupported or contradictory details. Therefore, it is both correct and faithful to the given information.", "main_total_tokens": 3140, "main_prompt_tokens": 3075, "main_completion_tokens": 65, "judge_total_tokens": 561, "judge_prompt_tokens": 479, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie you are referring to is \"Electric Angel,\" which is a typical experimental film that uses a variety of audiovisual techniques to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century [Doc 1024].", "latency_ms": 1716.071605682373, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Electric Angel,' which matches the gold answer. It stays faithful to the information given in the prompt and does not introduce contradictions, only lightly rephrasing the description.", "main_total_tokens": 2747, "main_prompt_tokens": 2694, "main_completion_tokens": 53, "judge_total_tokens": 542, "judge_prompt_tokens": 469, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is \"Rare Objects,\" which has a rating of 4.3 [Doc 105190].", "latency_ms": 759.5164775848389, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer per the gold standard is 'Mega Ape', but the generated answer claims 'Rare Objects' is the lowest rated movie from 2023, which is incorrect. It is also unfaithful because it contradicts the provided gold answer and introduces a different movie without basis in the given reference.", "main_total_tokens": 2806, "main_prompt_tokens": 2778, "main_completion_tokens": 28, "judge_total_tokens": 496, "judge_prompt_tokens": 409, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "The highest rated TV show from 2010 is \"Giant,\" which has a rating of 8.1 [Doc 119282].", "latency_ms": 792.5608158111572, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states that the highest rated TV show from 2010 is 'Giant' with a rating of 8.1, but the gold answer specifies 'Tea House' as the correct show. Thus the content is incorrect relative to the gold. However, the answer appears internally consistent and grounded in its cited document (Doc 119282), so it is faithful to its own source, even though that conflicts with the gold.", "main_total_tokens": 3173, "main_prompt_tokens": 3144, "main_completion_tokens": 29, "judge_total_tokens": 530, "judge_prompt_tokens": 411, "judge_completion_tokens": 119}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest rated Japanese language movie from 1999 is \"Kikujiro,\" which has a rating of 7.5 [Doc 22213].", "latency_ms": 1691.3821697235107, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the highest rated Japanese language movie from 1999 in the provided context is 'The Uncut Story of Mewtwo's Origin', not 'Kikujiro'. Additionally, the answer fabricates a specific rating and citation without grounding it in the given gold information, so it is not faithful to the source.", "main_total_tokens": 3102, "main_prompt_tokens": 3070, "main_completion_tokens": 32, "judge_total_tokens": 523, "judge_prompt_tokens": 423, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is \"Don't Answer the Phone!\" with a rating of 4.6, and its language is English [Doc 768].", "latency_ms": 1543.3263778686523, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The question asks only for the language of the lowest-rated movie from 1980. The generated answer correctly states that the language is English, matching the gold answer. However, it adds specific details about the movie title, rating, and a document citation that are not present in the prompt and whose provenance cannot be verified here, so these extra claims are not fully supported by the given information, reducing faithfulness.", "main_total_tokens": 2970, "main_prompt_tokens": 2936, "main_completion_tokens": 34, "judge_total_tokens": 530, "judge_prompt_tokens": 417, "judge_completion_tokens": 113}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was \"Freaks and Geeks,\" which had a popularity rating of 5.8043 [Doc 115187].", "latency_ms": 1775.3050327301025, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect relative to the gold label: it names 'Freaks and Geeks' instead of 'Law & Order: Special Victims Unit' as the TV show with the highest popularity in 1999. However, the answer is faithful to the cited document, explicitly referencing Doc 115187 and using the popularity rating given there, so it does not hallucinate beyond its source.", "main_total_tokens": 3372, "main_prompt_tokens": 3335, "main_completion_tokens": 37, "judge_total_tokens": 541, "judge_prompt_tokens": 427, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 1205.1026821136475, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 for 'Bluefin' and correctly attributes it. It is both correct and faithful to the provided information.", "main_total_tokens": 2900, "main_prompt_tokens": 2882, "main_completion_tokens": 18, "judge_total_tokens": 465, "judge_prompt_tokens": 400, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "There are multiple movies titled \"The Story of Us\" released in different years: \n\n1. The 1999 version.\n2. The 2019 version.\n3. The 2025 version.\n\n[Doc 22481], [Doc 88811], [Doc 112077].", "latency_ms": 1515.310287475586, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies 2019 as the correct year, while the generated answer lists multiple years (1999, 2019, 2025) and does not identify 2019 as the specific correct one. This makes the answer incorrect relative to the gold. However, the generated answer is faithful to its own content and the cited documents, as it simply lists multiple movies with that title and does not invent a specific year beyond what it presents, so faithfulness is high.", "main_total_tokens": 2924, "main_prompt_tokens": 2865, "main_completion_tokens": 59, "judge_total_tokens": 571, "judge_prompt_tokens": 443, "judge_completion_tokens": 128}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The context does not provide information about the number of votes \"My Dog Skip\" received. Therefore, I cannot answer that question.", "latency_ms": 1253.0312538146973, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 279 votes, but the generated answer claims it cannot answer, so it is factually incorrect. However, given only the context described for the generated response (which it says lacks the needed information), the model does not fabricate any numbers and correctly states its uncertainty, so it is fully faithful to the provided context.", "main_total_tokens": 2656, "main_prompt_tokens": 2630, "main_completion_tokens": 26, "judge_total_tokens": 505, "judge_prompt_tokens": 407, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The context does not provide information about the number of votes received by \"Oh My God!\" or any of the other movies listed. Therefore, I cannot answer that question.", "latency_ms": 1376.6331672668457, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 12, but the generated answer claims the information is not provided and does not give 12, so it is factually incorrect. However, given only the stated generated answer and not the original context, it does not introduce any contradictory details; it simply asserts lack of information, so it is faithful to its (implied) view of the context.", "main_total_tokens": 2950, "main_prompt_tokens": 2916, "main_completion_tokens": 34, "judge_total_tokens": 521, "judge_prompt_tokens": 415, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 920.2208518981934, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that \"The Last Days of Lehman Brothers\" was released in 2009, matching the gold answer. It does not introduce any contradictions or unsupported claims beyond the cited document reference, so it is both correct and faithful.", "main_total_tokens": 3284, "main_prompt_tokens": 3261, "main_completion_tokens": 23, "judge_total_tokens": 491, "judge_prompt_tokens": 410, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 831.0074806213379, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'en' (English), and the generated answer correctly states that the original language of 'The King's Speech' is English. There is no contradiction or fabrication beyond the given information, so it is both correct and faithful.", "main_total_tokens": 3047, "main_prompt_tokens": 3028, "main_completion_tokens": 19, "judge_total_tokens": 481, "judge_prompt_tokens": 402, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "Wall Street was released in the year 1987 [Doc 6993].", "latency_ms": 785.919189453125, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer. It does not introduce contradictions or hallucinations beyond a benign document reference, so it is both correct and faithful.", "main_total_tokens": 2189, "main_prompt_tokens": 2173, "main_completion_tokens": 16, "judge_total_tokens": 467, "judge_prompt_tokens": 394, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie in which classmates have to play a survival game using their cell phones is \"Brain Fluid Explosion Girl,\" released in 2015. In this film, Hana Ichii and her classmates find themselves locked in a cage and are forced to participate in a survival game that utilizes their cellphones [Doc 69929].", "latency_ms": 1981.0781478881836, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Brain Fluid Explosion Girl,' matching the gold answer. The added details about the plot and year are consistent with the described premise and do not contradict the given information, so the response is both correct and faithful to the prompt.", "main_total_tokens": 1896, "main_prompt_tokens": 1832, "main_completion_tokens": 64, "judge_total_tokens": 536, "judge_prompt_tokens": 452, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show in which six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons,\" which aired in 1983. In this show, the friends are transported into the Dungeons & Dragons realm and must find a way home with the help of their guide, the Dungeon Master [Doc 112849].", "latency_ms": 2056.8289756774902, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the show as 'Dungeons & Dragons' and accurately describes the premise that six friends rely on the Dungeon Master to help them get home. It matches the gold answer and stays faithful to the provided information, adding only a minor, plausible detail about the air date without contradicting the source.", "main_total_tokens": 1903, "main_prompt_tokens": 1835, "main_completion_tokens": 68, "judge_total_tokens": 552, "judge_prompt_tokens": 458, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is titled **Nacht der Wölfe**. It was released in 1982 and is described as the German version of \"West Side Story\" [Doc 2590].", "latency_ms": 2156.1625003814697, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe', matching the gold answer, so the correctness is 1. However, it adds extra details such as the 1982 release year and the description as a German version of 'West Side Story' with a citation-style reference ('[Doc 2590]') that are not present in the provided gold answer or prompt. Since the additional information is not grounded in the given context, faithfulness is scored 0.", "main_total_tokens": 1911, "main_prompt_tokens": 1850, "main_completion_tokens": 61, "judge_total_tokens": 589, "judge_prompt_tokens": 459, "judge_completion_tokens": 130}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of \"Picnic at Hanging Rock\" take place on Valentine's Day, 1900 [Doc 126191].", "latency_ms": 1163.7341976165771, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the year as 1900, matching the gold answer, and simply adds the detail of Valentine’s Day, which is accurate and consistent with the source citation.", "main_total_tokens": 2142, "main_prompt_tokens": 2117, "main_completion_tokens": 25, "judge_total_tokens": 481, "judge_prompt_tokens": 410, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie you are referring to is \"June.\" It features a young violinist named June, who spends her days working in a quirky coffee shop in Venice, rehearses for a professional music career at night, and daydreams about a beautiful man destined for her. The plot also includes her discovering that this man, Jack, is engaged to someone else, leading to romantic conflict [Doc 32483].", "latency_ms": 2784.240484237671, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer correctly identifies the movie as \"June,\" matching the gold answer, so correctness is full. However, it adds extra plot specifics (e.g., a man named Jack, engagement subplot, document reference) that are not grounded in the question or gold answer, so faithfulness is slightly reduced.", "main_total_tokens": 2188, "main_prompt_tokens": 2103, "main_completion_tokens": 85, "judge_total_tokens": 611, "judge_prompt_tokens": 517, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1450.895071029663, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct movie title, so it is incorrect. However, it does not fabricate any information and accurately reflects a lack of knowledge, so it is faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 474, "judge_prompt_tokens": 399, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 319.74053382873535, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 7.6, but the generated answer says 'I don't know,' which is incorrect. However, it does not introduce any fabricated information and honestly indicates uncertainty, so it is fully faithful to the model's knowledge state.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 389, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1107.861042022705, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.366123019050001, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is factually incorrect. However, the answer is directly addressing the user’s question without adding extraneous or contradictory information, so it is faithful to the query itself.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 394, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 269.50740814208984, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is factually incorrect. However, it is faithful to the user question in that it attempts to directly answer how many TV shows meet the specified criteria, without adding extraneous or contradictory information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 395, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1641.404628753662, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct TV show 'Fantômas.' However, it does not introduce any incorrect fabricated information; it simply states a lack of knowledge, so it is fully faithful but incorrect.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 389, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is titled **Kill Cruise**. It was released in 1990 and is described as a thriller where the yacht captain does not know what he's in for when he takes the two British beauties on their trip [Doc 11205].", "latency_ms": 1746.4263439178467, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise', which matches the gold answer. The added detail about the release year and brief plot description is consistent with known information and does not contradict the question, so the response is both correct and faithful to the source description.", "main_total_tokens": 2091, "main_prompt_tokens": 2031, "main_completion_tokens": 60, "judge_total_tokens": 530, "judge_prompt_tokens": 445, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 1555.2630424499512, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as \"Won't You Be My Neighbor?\" and remains faithful to the information given.", "main_total_tokens": 2354, "main_prompt_tokens": 2315, "main_completion_tokens": 39, "judge_total_tokens": 503, "judge_prompt_tokens": 440, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The movie \"1969\" was released in the year 1988 [Doc 8389].", "latency_ms": 869.905948638916, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the correct year is 2018, but the generated answer states 1988 and refers to a different movie (\"1969\"), so it is factually incorrect. It also is not faithful to the provided gold answer, as it contradicts it both in year and subject.", "main_total_tokens": 2265, "main_prompt_tokens": 2245, "main_completion_tokens": 20, "judge_total_tokens": 489, "judge_prompt_tokens": 397, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is **Astro Boy**. In this series, after being abandoned by his creator, a young boy robot is rescued by an Android Advocate, Dr. Elefun, who adopts him and guides him in using his special powers to aid and protect mankind and robot-kind [Doc 112591].", "latency_ms": 2421.701192855835, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy', matching the gold answer, so correctness is 1.0. The added plot description is broadly accurate to Astro Boy but introduces a specific source reference [Doc 112591] and a more modern-sounding synopsis that may or may not precisely match the referenced document; without verifying that doc, there is a small risk of minor hallucinated detail, so faithfulness is slightly reduced to 0.9.", "main_total_tokens": 2071, "main_prompt_tokens": 1995, "main_completion_tokens": 76, "judge_total_tokens": 591, "judge_prompt_tokens": 463, "judge_completion_tokens": 128}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room,\" released in 2015, the woman and her son were held captive for 7 years in an enclosed space [Doc 67513].", "latency_ms": 1388.3726596832275, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: both state they were held captive for 7 years. It does not introduce contradictions or unsupported claims beyond a generic reference, so it is both correct and faithful.", "main_total_tokens": 1780, "main_prompt_tokens": 1746, "main_completion_tokens": 34, "judge_total_tokens": 500, "judge_prompt_tokens": 428, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 1524.3947505950928, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983, matching the provided gold answer exactly and remaining faithful to it.", "main_total_tokens": 2039, "main_prompt_tokens": 2006, "main_completion_tokens": 33, "judge_total_tokens": 505, "judge_prompt_tokens": 428, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6466.043949127197, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric popularity score (80.5772). The generated answer says 'I don't know,' which is incorrect. However, it does not fabricate any information and honestly states lack of knowledge, so it is fully faithful to the model's uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 481, "judge_prompt_tokens": 396, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 392.8985595703125, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there was 1 Russian language TV show released in 1997, but the generated answer was 0, which is incorrect. The evaluation correctly identifies the mismatch without adding unsupported information, so it is faithful to the given data but incorrect in content relative to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 474, "judge_prompt_tokens": 385, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 1505.810260772705, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Angel Eyes') and correctly identifies the TV show. It is fully consistent with the provided information and does not introduce unsupported content.", "main_total_tokens": 2427, "main_prompt_tokens": 2392, "main_completion_tokens": 35, "judge_total_tokens": 494, "judge_prompt_tokens": 429, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French (fr) [Doc 110681].", "latency_ms": 1046.7984676361084, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer identifies the language as French, while the gold label specifies Korean, so the content is factually incorrect. It also introduces a movie title (\"Foreign Language\") and a document reference that are not grounded in the provided QA pair, making the reasoning unfaithful to the given information.", "main_total_tokens": 2111, "main_prompt_tokens": 2091, "main_completion_tokens": 20, "judge_total_tokens": 487, "judge_prompt_tokens": 396, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 582.9982757568359, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is factually incorrect. However, there is no indication that the model contradicted or ignored the given question; it simply produced an incorrect count, so faithfulness to the question is preserved.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 394, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The context provided does not include any information about movies released in 1979, including their ratings. Therefore, I cannot determine the best-rated movie from that year based on the available documents.", "latency_ms": 1711.967945098877, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that there are no movies from 1979 in the database, so it cannot answer the question. The generated answer similarly explains that the provided context has no information about 1979 movies or their ratings and thus it cannot determine the best-rated movie. This matches the limitation implied by the gold and does not hallucinate an answer, so it is both correct with respect to the task and faithful to the given context.", "main_total_tokens": 2017, "main_prompt_tokens": 1979, "main_completion_tokens": 38, "judge_total_tokens": 551, "judge_prompt_tokens": 433, "judge_completion_tokens": 118}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The movie \"Back to the Future Part IV\" was not mentioned in the provided context, and therefore, I cannot provide a release year for it.", "latency_ms": 1268.6669826507568, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the generated answer and the gold answer correctly state that they cannot provide a release year for 'Back to the Future Part IV.' The movie does not exist, and thus there is no release year. The generated response is faithful to the (implied) context and does not hallucinate a date, matching the intent of the gold answer.", "main_total_tokens": 2858, "main_prompt_tokens": 2828, "main_completion_tokens": 30, "judge_total_tokens": 529, "judge_prompt_tokens": 430, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot find any movies or shows matching the criteria, which aligns with the gold answer that there were no movies from 1980 related to the Chernobyl disaster. The assistant also appropriately hints that the event timing may be inconsistent with the question. It does not introduce incorrect facts or unjustified speculation, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 542, "judge_prompt_tokens": 434, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 332.20434188842773, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states there are 0 TV shows or movies with 'Astro Boy' in the title, while the gold answer is 4, so it is factually incorrect. However, it does directly address the question and does not add unrelated or hallucinated information, so it is faithful to the user query.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 388, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 231.0023307800293, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies 2, while the generated answer is 0, so the generated answer is incorrect. However, the model stayed within the information scope (there is no evidence it fabricated a specific number from a cited source), so it is faithful to its own (implicit) reasoning despite being wrong relative to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 482, "judge_prompt_tokens": 386, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 242.0063018798828, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 TV show released in the 1990s about Super Mario Bros, while the generated answer says 0. Since 1 is the correct count, the generated answer is incorrect, so correctness is 0. The model is consistently wrong rather than contradicting itself or hallucinating beyond the question, so its faithfulness to its own (incorrect) reasoning is high, hence faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 508, "judge_prompt_tokens": 389, "judge_completion_tokens": 119}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 353.3170223236084, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the number of TV shows about Super Mario Bros released between 2000 and 2010; the gold answer is 0 and the generated answer is also 0. This matches the gold exactly, so correctness is 1. The answer is a simple numeric fact consistent with the provided gold and does not introduce unsupported information, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 498, "judge_prompt_tokens": 391, "judge_completion_tokens": 107}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 244.20928955078125, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto'. The generated answer says 'I don't know', which is incorrect because it fails to provide the correct response. However, it does not hallucinate or contradict the prompt; it simply expresses a lack of knowledge, so it is fully faithful to what the model output actually contained.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 479, "judge_prompt_tokens": 386, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The context provided does not mention a Jurassic Park/Power Ranger crossover movie or its release date. Therefore, I cannot provide an answer to your question.", "latency_ms": 1169.9557304382324, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer states that the movie was not released. The generated answer instead says it cannot provide an answer due to lack of context, which fails to convey the key fact that the movie does not exist/was not released. Thus it is incorrect relative to the gold. It is partially faithful to the prompt context (it does not hallucinate a release date), but it omits the crucial information that the correct answer is that the movie was never released.", "main_total_tokens": 2293, "main_prompt_tokens": 2263, "main_completion_tokens": 30, "judge_total_tokens": 539, "judge_prompt_tokens": 415, "judge_completion_tokens": 124}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "There is no mention of a movie where Barbie fights the Predator in the provided context.", "latency_ms": 1979.011058807373, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "Gold answer clarifies that such a movie does not exist or was not released. Generated answer says there is no mention of such a movie in the provided context, which is slightly weaker and context-dependent but still implies there is no such known movie. So it's essentially correct but less direct than the gold, hence slightly lower faithfulness.", "main_total_tokens": 2323, "main_prompt_tokens": 2306, "main_completion_tokens": 17, "judge_total_tokens": 498, "judge_prompt_tokens": 399, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "The 4th oldest Star Wars movie is \"Star Wars: The Last Jedi,\" which was released in 2017 [Doc 76818]. The oldest Star Wars movies in chronological order are:\n\n1. \"Star Wars: Episode IV - A New Hope\" (1977)\n2. \"Star Wars: Episode V - The Empire Strikes Back\" (1980)\n3. \"Star Wars: Episode VI - Return of the Jedi\" (1983)\n4. \"Star Wars: The Last Jedi\" (2017)", "latency_ms": 2545.390844345093, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect: the 4th oldest Star Wars movie per the provided gold label is 'Star Wars: Droids - The Great Heep,' not 'Star Wars: The Last Jedi.' Additionally, the explanation invents an inaccurate chronological ordering of the films and provides an unsupported citation, so it is not faithful to the gold answer or to known release history.", "main_total_tokens": 2403, "main_prompt_tokens": 2295, "main_completion_tokens": 108, "judge_total_tokens": 603, "judge_prompt_tokens": 497, "judge_completion_tokens": 106}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The year of release for the movie \"1969\" is 1988 [Doc 8389].", "latency_ms": 1280.627965927124, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer gives the year 1988, while the gold answer is 1986, so it is factually incorrect. It also introduces a different movie title (\"1969\") that is not supported by the provided gold answer, so it is not faithful to the reference.", "main_total_tokens": 2266, "main_prompt_tokens": 2245, "main_completion_tokens": 21, "judge_total_tokens": 486, "judge_prompt_tokens": 398, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 361.8438243865967, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a numeric rating of 8.466, while the generated answer says 'I don't know.' This is incorrect since the model should have provided the rating. However, it is faithful because the model did not fabricate an answer and correctly expressed uncertainty instead of guessing.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 398, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 991.6515350341797, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the show as 'Mighty Med' and directly matches the gold answer without adding incorrect information. It is fully consistent and faithful to the provided gold response.", "main_total_tokens": 1807, "main_prompt_tokens": 1776, "main_completion_tokens": 31, "judge_total_tokens": 490, "judge_prompt_tokens": 423, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In \"Túnel de los huesos\" (2011), seven prisoners escaped from a prison in Argentina, specifically from the Villa Devoto prison in Buenos Aires [Doc 53582].", "latency_ms": 1882.6897144317627, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the country where the prison was located. The gold answer is Argentina. The generated answer states that the prisoners escaped from a prison in Argentina, which matches the gold answer. The additional detail about Villa Devoto prison in Buenos Aires is plausible context and does not contradict the gold answer. Therefore, the response is correct and faithful to the given information.", "main_total_tokens": 2032, "main_prompt_tokens": 1993, "main_completion_tokens": 39, "judge_total_tokens": 534, "judge_prompt_tokens": 430, "judge_completion_tokens": 104}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice Li Nai falls into a trap designed by the \"Chef Assassin\" Jiang Jie [Doc 135231].", "latency_ms": 1794.957160949707, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The core factual question is whose trap the apprentice falls into, and both the gold answer and the generated answer identify Jiang Jie, so it is correct. However, the generated answer introduces extra specific details (the apprentice name Li Nai, the description \"Chef Assassin,\" and a document citation) that are not present in the gold answer. Without evidence these additional details are grounded in the same source, this goes beyond the provided gold information, so the response is not fully faithful to it.", "main_total_tokens": 2218, "main_prompt_tokens": 2186, "main_completion_tokens": 32, "judge_total_tokens": 543, "judge_prompt_tokens": 416, "judge_completion_tokens": 127}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 691.1852359771729, "retrieval_recall": 0.0, "retrieval_mrr": 0.07692307692307693, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (4), so it is factually incorrect for this question, yielding a correctness score of 0. However, it is a direct, unembellished numeric response to the question with no added hallucinations or unrelated content, so faithfulness to the user query and the model’s own output is preserved, resulting in a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 500, "judge_prompt_tokens": 385, "judge_completion_tokens": 115}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "37", "latency_ms": 108.19029808044434, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '37' is factually incorrect because there are no movies featuring both Bruce Lee and Steven Seagal together. It is also unfaithful to the gold answer, which explicitly states that no such movies can be found. The model appears to have guessed a number without evidence.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 484, "judge_prompt_tokens": 395, "judge_completion_tokens": 89}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of \"Tokyo Mater\" (2008) is 6.3 [Doc 42750].", "latency_ms": 1053.753137588501, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3 for Tokyo Mater (2008) and does not introduce unsupported information, so it is both correct and faithful.", "main_total_tokens": 2244, "main_prompt_tokens": 2222, "main_completion_tokens": 22, "judge_total_tokens": 471, "judge_prompt_tokens": 405, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in the movie \"Fast Track: No Limits\" is centered around street racing, as it focuses on a group of street racers [Doc 41929].", "latency_ms": 1333.2664966583252, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The gold answer specifies the racing style as 'Drift', while the generated answer describes it as 'street racing', which is a different style. Therefore, the generated answer is incorrect with respect to the gold. It is, however, mostly faithful to its cited document about the movie, so faithfulness is relatively high but not perfect due to not addressing the more specific style mentioned in the gold.", "main_total_tokens": 2424, "main_prompt_tokens": 2389, "main_completion_tokens": 35, "judge_total_tokens": 525, "judge_prompt_tokens": 413, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 1436.9182586669922, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Confronting a Serial Killer' and it stays faithful to the information given in the prompt.", "main_total_tokens": 2234, "main_prompt_tokens": 2188, "main_completion_tokens": 46, "judge_total_tokens": 518, "judge_prompt_tokens": 452, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of \"Armageddon\" (1998) is 7.838 [Doc 20603].", "latency_ms": 1372.0409870147705, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and states it clearly, so it is both correct and faithful to the provided information.", "main_total_tokens": 2299, "main_prompt_tokens": 2275, "main_completion_tokens": 24, "judge_total_tokens": 471, "judge_prompt_tokens": 409, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The title of the 1999 psychological thriller movie that alludes to an extra sense is \"Victim\" [Doc 22488].", "latency_ms": 1141.0679817199707, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is \"The Sixth Sense.\" The generated answer gives a different title, \"Victim,\" which is factually incorrect. There is no indication in the prompt that any source document actually supports the claim that the film is titled \"Victim,\" so the answer is also unfaithful to the provided gold reference.", "main_total_tokens": 2195, "main_prompt_tokens": 2167, "main_completion_tokens": 28, "judge_total_tokens": 515, "judge_prompt_tokens": 419, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 1261.629581451416, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, which matches the gold answer. It is fully consistent with the provided information and does not introduce unsupported content.", "main_total_tokens": 2359, "main_prompt_tokens": 2342, "main_completion_tokens": 17, "judge_total_tokens": 465, "judge_prompt_tokens": 397, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is **The Transporter Refueled** [Doc 67500].", "latency_ms": 1101.8786430358887, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer gives the correct title, 'The Transporter Refueled', matching the gold answer. It adds an unverified citation-style reference '[Doc 67500]', which is not grounded in the provided gold data, so there is a small faithfulness issue.", "main_total_tokens": 2360, "main_prompt_tokens": 2331, "main_completion_tokens": 29, "judge_total_tokens": 507, "judge_prompt_tokens": 420, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in the year 2015 [Doc 67500].", "latency_ms": 1083.486795425415, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It does not introduce any contradictions or unsupported claims beyond the citation placeholder, so it is both correct and faithful.", "main_total_tokens": 2474, "main_prompt_tokens": 2452, "main_completion_tokens": 22, "judge_total_tokens": 485, "judge_prompt_tokens": 406, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "The romantic comedy in which Abraham Lincoln comes back from the dead is not explicitly mentioned in the provided context. The closest related titles include \"Abraham Lincoln: Vampire Hunter\" and \"Abraham Lincoln vs. Zombies,\" but these are not romantic comedies. Therefore, based on the available information, I cannot provide a specific title that fits your request.", "latency_ms": 2370.1577186584473, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a romantic comedy where Abraham Lincoln comes back from the dead. The gold answer says no such movie description can be found. The generated answer similarly concludes that no specific romantic comedy fitting this description can be identified, noting that related Lincoln resurrection films are not romantic comedies, and thus it cannot provide a title. This aligns with the gold answer and does not introduce unsupported facts. Therefore it is both correct and faithful to the given information.", "main_total_tokens": 1943, "main_prompt_tokens": 1873, "main_completion_tokens": 70, "judge_total_tokens": 581, "judge_prompt_tokens": 460, "judge_completion_tokens": 121}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by Ernest Hemingway is \"Havana Darkness\" (2019). The plot involves three friends investigating the origins of a mysterious manuscript supposedly written by Hemingway, which leads them into a terrifying situation in Havana, Cuba [Doc 90923].", "latency_ms": 1339.0960693359375, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer \"Havana Darkness\" and correctly describes it as a movie alluding to a manuscript written by Ernest Hemingway. There are no contradictions with the given information, so it is both correct and faithful.", "main_total_tokens": 2002, "main_prompt_tokens": 1943, "main_completion_tokens": 59, "judge_total_tokens": 525, "judge_prompt_tokens": 447, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "The TV show in which Doll heads to Hollywood to be with her childhood best friend Em is titled \"Doll & Em\" [Doc 122259].", "latency_ms": 1243.7667846679688, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em,' which matches the gold answer exactly. It is faithful since it restates the information without adding unsupported claims and aligns with the referenced document.", "main_total_tokens": 2213, "main_prompt_tokens": 2182, "main_completion_tokens": 31, "judge_total_tokens": 493, "judge_prompt_tokens": 420, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The 2007 movie in which Hallam has a talent for spying on people and attempts to find the cause of his mother’s death is titled \"Hallam Foe\" [Doc 39389].", "latency_ms": 1728.6086082458496, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Hallam Foe', which matches the gold answer exactly. It is fully supported by the cited document reference and does not introduce any unsupported information.", "main_total_tokens": 2214, "main_prompt_tokens": 2173, "main_completion_tokens": 41, "judge_total_tokens": 509, "judge_prompt_tokens": 440, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The context provided does not specify any individual's last name, so it is not possible to determine whether it sounds like a friend or a foe. Therefore, I cannot answer the question based on the given information.", "latency_ms": 1519.439935684204, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'A foe', indicating the last name sounds like a foe. The generated answer says it cannot be determined from context, which contradicts the gold. Thus, the generated answer is incorrect. However, it remains faithful to the (stated) lack of context and does not invent details, so it is fully faithful to the information it claims to have.", "main_total_tokens": 2336, "main_prompt_tokens": 2295, "main_completion_tokens": 41, "judge_total_tokens": 528, "judge_prompt_tokens": 427, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "In the 2017 TV show \"Atypical,\" Sam is described as an 18-year-old on the autism spectrum [Doc 124583].", "latency_ms": 906.5537452697754, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, matching the gold answer exactly. It does not add any contradictory or unsupported information, so it is both correct and faithful.", "main_total_tokens": 2016, "main_prompt_tokens": 1985, "main_completion_tokens": 31, "judge_total_tokens": 496, "judge_prompt_tokens": 418, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show *Atypical*, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 848.0339050292969, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which matches the gold answer exactly. It is fully faithful to the source information and does not introduce any contradictions.", "main_total_tokens": 1986, "main_prompt_tokens": 1962, "main_completion_tokens": 24, "judge_total_tokens": 477, "judge_prompt_tokens": 411, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "In the movie À corde tendue, Sandra was born in Chamonix [Doc 59659].", "latency_ms": 1136.5745067596436, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that Sandra was born in Chamonix, which matches the gold answer exactly. It does not introduce extraneous or unsupported information beyond a citation placeholder, so it is both correct and faithful.", "main_total_tokens": 2403, "main_prompt_tokens": 2382, "main_completion_tokens": 21, "judge_total_tokens": 491, "judge_prompt_tokens": 418, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In the movie \"When the Rain Falls,\" Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 913.7036800384521, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.7, "justification": "The key missing word is 'rains', which the generated answer correctly supplies, so it is correct. However, the added movie title 'When the Rain Falls' and citation '[Doc 100481]' are hallucinated and not grounded in the provided gold answer, reducing faithfulness.", "main_total_tokens": 2529, "main_prompt_tokens": 2505, "main_completion_tokens": 24, "judge_total_tokens": 505, "judge_prompt_tokens": 415, "judge_completion_tokens": 90}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the low-rated German language animation \"Tabaluga,\" the character Tabaluga is the last of the dragons and the crown-prince of Greenland, a magical place inhabited by talking animals of many different species [Doc 114810].", "latency_ms": 1747.3170757293701, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Greenland as the place where Tabaluga is crown-prince. It exactly matches the gold answer and does not introduce contradictions. The added descriptive detail about it being a magical place with talking animals does not conflict with the source and remains faithful to the likely document reference, so both correctness and faithfulness are high.", "main_total_tokens": 2250, "main_prompt_tokens": 2202, "main_completion_tokens": 48, "judge_total_tokens": 540, "judge_prompt_tokens": 442, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie that matches the description of \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers\" is **One Afternoon**. It was released in 2021 and has a rating of 6.8 [Doc 96838].", "latency_ms": 1734.2112064361572, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'One Afternoon', which matches the gold answer. However, it adds extra details about release year, rating, and a document reference ([Doc 96838]) that are not supported or verifiable from the given prompt, so faithfulness is slightly reduced.", "main_total_tokens": 1841, "main_prompt_tokens": 1778, "main_completion_tokens": 63, "judge_total_tokens": 559, "judge_prompt_tokens": 467, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is titled \"The Life Ahead,\" released in 2020. It has a rating of 6.746 [Doc 93260].", "latency_ms": 1710.0260257720947, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as \"The Life Ahead,\" which matches the gold answer and accurately restates the plot, so correctness is high. However, it fabricates a precise rating \"6.746 [Doc 93260]\" without evidence from the provided context, making the explanation partially hallucinated and thus not fully faithful to the source.", "main_total_tokens": 2123, "main_prompt_tokens": 2070, "main_completion_tokens": 53, "judge_total_tokens": 553, "judge_prompt_tokens": 451, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team. The film illustrates how he, coming from humble beginnings, was able to take on the elite of motor racing and achieve success [Doc 73859].", "latency_ms": 2418.118953704834, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies that the 2016 documentary 'McLaren' is about Bruce McLaren and provides consistent, accurate context about him founding the McLaren racing team. It aligns fully with the gold answer and does not introduce contradictions.", "main_total_tokens": 2139, "main_prompt_tokens": 2081, "main_completion_tokens": 58, "judge_total_tokens": 521, "judge_prompt_tokens": 442, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The context provided does not specify the name of any racing team associated with a character or individual. Therefore, I cannot provide an answer to your question based on the available information.", "latency_ms": 1432.0940971374512, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer explicitly provides the racing team name: 'The McLaren Motor Racing team.' The generated answer claims the context does not specify any racing team name, which is incorrect given the gold. However, it remains faithful to its (mistaken) interpretation of the context and does not hallucinate, so faithfulness is high while correctness is zero relative to the gold.", "main_total_tokens": 2237, "main_prompt_tokens": 2202, "main_completion_tokens": 35, "judge_total_tokens": 523, "judge_prompt_tokens": 418, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context does not provide specific information about the country of the individual in question. Therefore, I cannot answer the question based on the provided documents.", "latency_ms": 1607.2049140930176, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'New Zealand'. The generated answer claims the context does not provide the information and therefore does not give 'New Zealand', so it is factually incorrect relative to the gold. However, given the model's stated assumption about the context (that it lacks the needed information), the answer is internally faithful to that assumption and does not hallucinate a country, so faithfulness is high.", "main_total_tokens": 1940, "main_prompt_tokens": 1911, "main_completion_tokens": 29, "judge_total_tokens": 516, "judge_prompt_tokens": 405, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "In the 2013 movie \"A Good Day to Die Hard,\" John McClane does not blow up the Great Wall of China. The film follows him as he travels to Moscow to help his son, who is involved in a nuclear weapons heist, but there is no mention of the Great Wall of China being blown up in this context [Doc 58791].", "latency_ms": 3008.0997943878174, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.2, "faithfulness_score": 0.8, "justification": "The gold answer states that there is no such movie in the database, i.e., essentially 'no known movie where this happens.' The generated answer instead names a specific 2013 movie, \"A Good Day to Die Hard,\" and asserts that in that film McClane does not blow up the Great Wall of China. This is not equivalent to the gold response: the user asked \"In which 2013 movie does John McClane blow up the Great Wall of China?\" The correct response per the gold is that such a movie is not present (effectively, no such movie), not to single out a specific title and then describe its plot. Thus the generated answer does not match the required answer and is not correct in this evaluation setting. However, the generated answer is internally consistent and grounded in the prompt and its doc citation, so it is largely faithful to its own stated context, though it diverges from the gold intent, so I reduce faithfulness slightly.", "main_total_tokens": 2486, "main_prompt_tokens": 2411, "main_completion_tokens": 75, "judge_total_tokens": 705, "judge_prompt_tokens": 473, "judge_completion_tokens": 232}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie that matches the description is **\"Tomie: Forbidden Fruit.\"** The plot involves a lonely high school girl who befriends a strange, mysterious, and beautiful girl, and it becomes clear that their meeting was not purely accidental [Doc 28589].", "latency_ms": 1816.0769939422607, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as 'Tomie: Forbidden Fruit,' matching the gold answer, so correctness is 1. However, it explicitly references a source '[Doc 28589]' which is not present in the provided context, introducing an ungrounded citation, so faithfulness is 0.", "main_total_tokens": 2251, "main_prompt_tokens": 2192, "main_completion_tokens": 59, "judge_total_tokens": 569, "judge_prompt_tokens": 476, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie \"Lei mi parla ancora,\" Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 1918.698787689209, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Nino and Caterina have been married for 65 years, matching the gold answer. It is also faithful to the provided information without adding unsupported details.", "main_total_tokens": 2613, "main_prompt_tokens": 2583, "main_completion_tokens": 30, "judge_total_tokens": 492, "judge_prompt_tokens": 424, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In \"Scorpion King: Book of Souls\" (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls, which will allow them to put an end to an evil warlord [Doc 82582].", "latency_ms": 1588.7730121612549, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The gold answer is: 'The Book of Souls.' The generated answer states they search for 'a legendary relic known as The Book of Souls,' which correctly identifies the object of the search, so it is correct. However, it adds extra detail about the relic allowing them to end an evil warlord that is not present in the gold answer; this slightly reduces faithfulness to the reference, though it remains largely accurate to the movie plot.", "main_total_tokens": 2525, "main_prompt_tokens": 2474, "main_completion_tokens": 51, "judge_total_tokens": 566, "judge_prompt_tokens": 445, "judge_completion_tokens": 121}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year for \"Zombies on the Moon\" is not found in the provided context.", "latency_ms": 882.4830055236816, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database; the generated answer states the release year is not found in the provided context. Both correctly convey that the release year is unknown and do not introduce fabricated information, so the response is both correct and faithful to the gold.", "main_total_tokens": 2258, "main_prompt_tokens": 2239, "main_completion_tokens": 19, "judge_total_tokens": 496, "judge_prompt_tokens": 410, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The context provided does not mention \"The Galactic Baker\" or its release year. Therefore, I cannot provide an answer to your question.", "latency_ms": 1136.8072032928467, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates that the movie is not in the database, i.e., the system cannot provide the year. The generated answer likewise states that the context does not mention the film or its release year and therefore it cannot provide an answer. This is consistent with the gold response and does not fabricate information, so it is both correct and faithful.", "main_total_tokens": 2136, "main_prompt_tokens": 2109, "main_completion_tokens": 27, "judge_total_tokens": 516, "judge_prompt_tokens": 416, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"Return of the Null Pointer\" was released is not found in the provided context.", "latency_ms": 743.6888217926025, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year is not available/known, which is correct given no such movie is in the database or context. The generated answer is faithful to the absence of information and does not hallucinate a year.", "main_total_tokens": 1893, "main_prompt_tokens": 1874, "main_completion_tokens": 19, "judge_total_tokens": 490, "judge_prompt_tokens": 410, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year of \"Data Science: The Movie\" is not found in the provided context.", "latency_ms": 1350.4843711853027, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database; the generated answer says the release year is not found in the provided context. Both convey the same idea (no information about the release year), so the generated response is correct and faithful to the gold.", "main_total_tokens": 2419, "main_prompt_tokens": 2400, "main_completion_tokens": 19, "judge_total_tokens": 494, "judge_prompt_tokens": 410, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The release year of \"Python's Revenge 2\" is not found in the provided context.", "latency_ms": 1017.0350074768066, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database. The generated answer says the release year is not found in the provided context, which is effectively equivalent in meaning. It does not fabricate a year and remains consistent with the given information, so it is both correct and faithful to the gold.", "main_total_tokens": 2257, "main_prompt_tokens": 2238, "main_completion_tokens": 19, "judge_total_tokens": 501, "judge_prompt_tokens": 410, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"The Last Pixel\" was released is not found in the provided context.", "latency_ms": 1516.2420272827148, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database. The generated answer states that the release year is not found in the provided context, which is effectively equivalent in this setting. It does not hallucinate a year and remains faithful to the absence of information, so both correctness and faithfulness are high.", "main_total_tokens": 1935, "main_prompt_tokens": 1918, "main_completion_tokens": 17, "judge_total_tokens": 498, "judge_prompt_tokens": 406, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The year \"The Man Who Ate the Sun\" was released is not found in the provided context.", "latency_ms": 1019.2959308624268, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database. The generated answer similarly says the release year is not found in the provided context, which is effectively the same limitation. It does not hallucinate a year and remains consistent with the information given, so it is both correct relative to the task and faithful to the context.", "main_total_tokens": 2665, "main_prompt_tokens": 2645, "main_completion_tokens": 20, "judge_total_tokens": 508, "judge_prompt_tokens": 412, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was directed by **Michael Samuels** [Doc 46069].", "latency_ms": 1708.2347869873047, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer explicitly states that the director information is not provided in the database and therefore cannot be answered. The generated answer names Michael Samuels as the director and even cites a document, which contradicts the given database limitation. This makes it factually incorrect with respect to the task setup and unfaithful to the provided evidence and constraints.", "main_total_tokens": 2589, "main_prompt_tokens": 2564, "main_completion_tokens": 25, "judge_total_tokens": 521, "judge_prompt_tokens": 422, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The context provided does not contain any information about a movie titled \"A Love to Die For,\" including its director. Therefore, I cannot provide an answer to your question.", "latency_ms": 1126.3678073883057, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided context/database and therefore the question cannot be answered. The generated answer is correct and fully faithful to the gold intent, with only minor wording differences that do not change the meaning.", "main_total_tokens": 2370, "main_prompt_tokens": 2336, "main_completion_tokens": 34, "judge_total_tokens": 512, "judge_prompt_tokens": 429, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"Journey into Gaza\" is not specified in the provided context. Therefore, I cannot provide that information.", "latency_ms": 877.2351741790771, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not specified and cannot be provided, which matches the gold answer. It does not introduce unsupported information and stays faithful to the given context and database limitation.", "main_total_tokens": 2234, "main_prompt_tokens": 2208, "main_completion_tokens": 26, "judge_total_tokens": 489, "judge_prompt_tokens": 419, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The director of the movie \"The Last Night at Tremore Beach\" is not mentioned in the provided context. Therefore, I cannot provide that information.", "latency_ms": 2150.9780883789062, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the database does not provide the director information and thus it cannot be answered. The generated answer states that the director is not mentioned in the provided context and therefore it cannot provide that information. This is correct with respect to the task and fully aligned with the given context, so both correctness and faithfulness are maximal.", "main_total_tokens": 1976, "main_prompt_tokens": 1946, "main_completion_tokens": 30, "judge_total_tokens": 524, "judge_prompt_tokens": 427, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is titled **\"Rise of the Mummy,\"** released in 2021. In this film, the students become trapped in a time loop and must defeat the mummy to escape, as it collects the souls of those who awakened it to walk the earth permanently [Doc 98375].", "latency_ms": 1582.1130275726318, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.95, "justification": "The generated answer correctly identifies the movie as 'Rise of the Mummy,' which matches the gold answer, so correctness is 1. The extra plot details (time loop, collecting souls, etc.) are not supported by the given gold text and may be hallucinated, slightly reducing faithfulness, hence 0.95.", "main_total_tokens": 2009, "main_prompt_tokens": 1938, "main_completion_tokens": 71, "judge_total_tokens": 560, "judge_prompt_tokens": 462, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The movie in which Sarah fakes her own death to escape her abusive spouse is \"Stranger in My Bed,\" released in 2005 [Doc 35237].", "latency_ms": 1292.7310466766357, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Stranger in My Bed,\" matching the gold answer exactly. It remains faithful to the given information without adding unsupported claims; the added year (2005) is accurate and does not contradict anything in the prompt.", "main_total_tokens": 2122, "main_prompt_tokens": 2088, "main_completion_tokens": 34, "judge_total_tokens": 512, "judge_prompt_tokens": 430, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie you are referring to is \"Game of Silence,\" which is a TV series from 2016. The plot involves a successful Atlanta attorney whose long-lost childhood friends unexpectedly reappear after 25 years, leading to the resurfacing of a dark secret from their past [Doc 123938].", "latency_ms": 2185.9028339385986, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks specifically for a movie title. The correct gold answer is \"Game of Silence\" (treated as the requested title), but the generated answer identifies it as a TV series instead of a movie and adds extra plot detail and a citation not present in the question. Thus it is not strictly correct relative to the question as stated, and it is not faithful because it introduces unsupported details and source notation.", "main_total_tokens": 1725, "main_prompt_tokens": 1664, "main_completion_tokens": 61, "judge_total_tokens": 570, "judge_prompt_tokens": 458, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie you are referring to is \"The Craigslist Killer,\" which was released in 2011. It is based on the true story of Philip Markoff, a med student who leads a double life as a brutal sexual deviant, abusing prostitutes he finds through Craigslist [Doc 53039].", "latency_ms": 1346.7748165130615, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer' and gives accurate contextual information about the plot and basis on a true story, so correctness is high. However, it references a specific document [Doc 53039] that is not part of the user's prompt and may not exist in the given context, which slightly reduces faithfulness.", "main_total_tokens": 2083, "main_prompt_tokens": 2024, "main_completion_tokens": 59, "judge_total_tokens": 553, "judge_prompt_tokens": 451, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which Yuki Hirano sets his eyes on the role of a forestry trainee after failing his university entrance exam is titled **Wood Job!**. It was released in 2014 and falls under the genres of drama and comedy. The plot follows Yuki as he embarks on a training course and eventually works in Kamusari Village, where he connects with nature and the local community [Doc 66094].", "latency_ms": 2601.4654636383057, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'Wood Job!', matching the gold answer, so correctness is 1. The extra details about release year, genre, and plot are generally accurate, but the reference to a specific document [Doc 66094] is unverifiable within the given context, which slightly impacts faithfulness.", "main_total_tokens": 2412, "main_prompt_tokens": 2326, "main_completion_tokens": 86, "judge_total_tokens": 584, "judge_prompt_tokens": 486, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on a colorful adventure, is **\"Sesame Street: The Magical Wand Chase\"** from 2017 [Doc 78373].", "latency_ms": 1445.5344676971436, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies the movie as \"Sesame Street: The Magical Wand Chase,\" matching the gold answer, so correctness is 1. However, it cites a specific source document ID and year (\"from 2017 [Doc 78373]\") that are not present in the user prompt, which counts as hallucinated grounding details, so faithfulness is 0.", "main_total_tokens": 2137, "main_prompt_tokens": 2081, "main_completion_tokens": 56, "judge_total_tokens": 574, "judge_prompt_tokens": 466, "judge_completion_tokens": 108}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which a 7-year-old boy must find the hero within him to save his parents trapped inside a burning car due to a rock fall is titled **Beyond**, released in **2016** [Doc 74597].", "latency_ms": 1528.9435386657715, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is simply the movie title 'Beyond'. The generated answer adds an incorrect release year (2016) and an invented citation '[Doc 74597]', which are not supported by the given gold. Thus, it is not fully correct or faithful to the reference.", "main_total_tokens": 1776, "main_prompt_tokens": 1729, "main_completion_tokens": 47, "judge_total_tokens": 541, "judge_prompt_tokens": 455, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie in which a woman is fired from a financial corporation during the Asia crisis and returns home with no money is titled **6ixtynin9**. In this film, she finds a box with a fortune in front of her door and decides to keep it, but the people who left it there soon want it back [Doc 22478].", "latency_ms": 2217.3445224761963, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', matching the gold reference. The added plot detail about finding a box of money and others wanting it back is consistent with the known plot of the film and does not contradict the question or the cited document. Thus it is both correct and faithful to the provided information.", "main_total_tokens": 1884, "main_prompt_tokens": 1812, "main_completion_tokens": 72, "judge_total_tokens": 573, "judge_prompt_tokens": 474, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie you are referring to is **Club Dread** (2004). In this film, a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort, a hedonistic island paradise for swingers, and it's up to the club's staff to stop the violence or at least hide it [Doc 32410].", "latency_ms": 3306.5426349639893, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread', matching the gold answer exactly. It stays faithful to the given question and gold label, only adding a year and minor elaboration that does not contradict any provided information.", "main_total_tokens": 2208, "main_prompt_tokens": 2139, "main_completion_tokens": 69, "judge_total_tokens": 570, "judge_prompt_tokens": 493, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie you are referring to is titled \"Toilet,\" which was released in 2010. The plot involves a 30-something Gundam fan and engineer who is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral [Doc 48873].", "latency_ms": 1740.7810688018799, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" matching the gold answer. It restates the plot accurately and stays consistent with the provided information, citing the document as its source. There are no hallucinations or contradictions, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2456, "main_prompt_tokens": 2390, "main_completion_tokens": 66, "judge_total_tokens": 567, "judge_prompt_tokens": 480, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie you are referring to is \"Electric Angel,\" released in 1981. It is described as a typical experimental film that employs a variety of audiovisual techniques to evoke a sense of polymorphic eroticism, drawing from European and Mediterranean cinematography of the 20th century [Doc 1024].", "latency_ms": 2063.59601020813, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie as 'Electric Angel,' matching the gold answer. However, it fabricates additional details such as the 1981 release year and a citation to [Doc 1024] that are not supported by the provided information, so it is not fully faithful.", "main_total_tokens": 2127, "main_prompt_tokens": 2066, "main_completion_tokens": 61, "judge_total_tokens": 568, "judge_prompt_tokens": 477, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 is \"Down Low,\" which has a rating of 5.207 [Doc 106995].", "latency_ms": 803.1833171844482, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'Mega Ape' as the lowest rated movie from 2023, whereas the generated answer claims it is 'Down Low' with a specific rating. This directly contradicts the gold label, so the generated answer is factually incorrect relative to the task. Additionally, it appears to fabricate or misinterpret the cited document ID and rating, so it is not faithful to the provided evidence.", "main_total_tokens": 2013, "main_prompt_tokens": 1985, "main_completion_tokens": 28, "judge_total_tokens": 521, "judge_prompt_tokens": 409, "judge_completion_tokens": 112}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 266.03126525878906, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Tea House', but the generated answer says 'I don't know', which is incorrect. However, it does not hallucinate or provide incorrect information beyond admitting uncertainty, so it is fully faithful to the model's stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 466, "judge_prompt_tokens": 386, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 827.5597095489502, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific movie title. The gold answer gives a concrete film, but the generated answer says 'I don't know,' which does not provide the required information and is therefore incorrect. However, it does not introduce any fabricated or misleading information; it simply admits lack of knowledge, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 490, "judge_prompt_tokens": 395, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The language of the lowest-rated movie from 1980 is English, as the movie \"Don't Answer the Phone!\" has a rating of 4.6, which is the lowest among the listed films from that year [Doc 768].", "latency_ms": 1656.3923358917236, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 0.5, "justification": "The generated answer correctly states that the language is English, matching the gold answer. However, it adds an unsupported and likely hallucinated detail about a specific movie, its rating, and a document reference that are not present in the gold answer. Thus, while the final result is correct, the explanation is only partially faithful to the source.", "main_total_tokens": 2406, "main_prompt_tokens": 2358, "main_completion_tokens": 48, "judge_total_tokens": 531, "judge_prompt_tokens": 431, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was \"Freaks and Geeks,\" which had a popularity score of 5.8043 [Doc 115187].", "latency_ms": 1429.3675422668457, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold label states that the TV show in 1999 with the highest popularity was 'Law & Order: Special Victims Unit,' whereas the model answered 'Freaks and Geeks.' This directly contradicts the gold answer. Additionally, the reference to a specific document and popularity score appears fabricated or at least not verifiable from the provided gold information, so the answer is not faithful to the given ground truth.", "main_total_tokens": 2351, "main_prompt_tokens": 2314, "main_completion_tokens": 37, "judge_total_tokens": 548, "judge_prompt_tokens": 427, "judge_completion_tokens": 121}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 1024.0094661712646, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and correctly attributes the information to the cited document. It is both correct and faithful to the provided gold answer.", "main_total_tokens": 2221, "main_prompt_tokens": 2203, "main_completion_tokens": 18, "judge_total_tokens": 466, "judge_prompt_tokens": 400, "judge_completion_tokens": 66}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in the year 1999 [Doc 22481].", "latency_ms": 958.9266777038574, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer gives 1999, while the gold answer is 2019, so it is factually incorrect relative to the gold. However, the model clearly bases its answer on the cited document [Doc 22481], so it is faithful to its stated source, even though that source conflicts with the gold.", "main_total_tokens": 2220, "main_prompt_tokens": 2198, "main_completion_tokens": 22, "judge_total_tokens": 501, "judge_prompt_tokens": 406, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "2", "latency_ms": 190.11664390563965, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct number of votes is 279, but the generated answer was 2. This is incorrect. However, the answer directly addresses the question and makes no extraneous claims, so it is faithful to the prompt.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 458, "judge_prompt_tokens": 383, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "11", "latency_ms": 212.94116973876953, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (11) does not match the gold answer (12), so it is factually incorrect relative to the provided gold. However, there is no evidence of the model contradicting or inventing context beyond the query; it simply gives the wrong number, so faithfulness to the (implicit) context is preserved.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 479, "judge_prompt_tokens": 383, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in the year 2009 [Doc 46069].", "latency_ms": 921.5779304504395, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie \"The Last Days of Lehman Brothers\" was released in 2009, which matches the gold answer. It directly uses the provided document reference, so it is faithful to the source.", "main_total_tokens": 2563, "main_prompt_tokens": 2538, "main_completion_tokens": 25, "judge_total_tokens": 489, "judge_prompt_tokens": 412, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 1433.5448741912842, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the original language is English (\"en\"). The generated answer states the original language is English, which matches the gold. No hallucinated or contradictory details are present, and the cited document reference does not conflict with the answer.", "main_total_tokens": 2241, "main_prompt_tokens": 2222, "main_completion_tokens": 19, "judge_total_tokens": 483, "judge_prompt_tokens": 402, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "1987", "latency_ms": 1017.0798301696777, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (1987), which is the correct release year of the film 'Wall Street.'", "main_total_tokens": 2734, "main_prompt_tokens": 2731, "main_completion_tokens": 3, "judge_total_tokens": 437, "judge_prompt_tokens": 381, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Brain Fluid Explosion Girl", "latency_ms": 1208.5261344909668, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'Brain Fluid Explosion Girl.'", "main_total_tokens": 2391, "main_prompt_tokens": 2386, "main_completion_tokens": 5, "judge_total_tokens": 443, "judge_prompt_tokens": 392, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 1098.3285903930664, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question refers to the animated TV show where six friends are transported to a fantasy realm and must rely on the Dungeon Master to find a way home. Both the gold answer and the generated answer correctly identify this show as 'Dungeons & Dragons.'", "main_total_tokens": 2390, "main_prompt_tokens": 2385, "main_completion_tokens": 5, "judge_total_tokens": 474, "judge_prompt_tokens": 394, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 1368.5719966888428, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer 'Nacht der Wölfe', which is the correct movie about rival German and Turkish teen gangs in Munich. Both correctness and faithfulness are perfect.", "main_total_tokens": 2412, "main_prompt_tokens": 2405, "main_completion_tokens": 7, "judge_total_tokens": 473, "judge_prompt_tokens": 403, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900", "latency_ms": 1644.7241306304932, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (1900), so it is both correct and faithful.", "main_total_tokens": 2695, "main_prompt_tokens": 2692, "main_completion_tokens": 3, "judge_total_tokens": 439, "judge_prompt_tokens": 388, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June", "latency_ms": 566.7855739593506, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the name (a calendar month) of the young violinist in the described 2004 movie. Both the gold answer and the generated answer are \"June\", which correctly satisfies the question. The generated answer matches the gold exactly, so it is both correct and faithful.", "main_total_tokens": 128, "main_prompt_tokens": 127, "main_completion_tokens": 1, "judge_total_tokens": 522, "judge_prompt_tokens": 433, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 389.96171951293945, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states 'I don't know,' which does not provide the correct movie title, so correctness is 0. However, it does not invent any incorrect facts and simply admits lack of knowledge, so it is fully faithful to its own (non-)information, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 399, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 197.28803634643555, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric rating (7.6). The generated answer says 'I don't know,' which is factually incorrect relative to the gold answer and does not provide the requested value. Therefore correctness is 0. However, the answer is internally consistent with the model's stated uncertainty and does not hallucinate a wrong rating, so faithfulness to its own claim is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 499, "judge_prompt_tokens": 389, "judge_completion_tokens": 110}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "10", "latency_ms": 1336.3957405090332, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.366123019050001, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (10) does not match the gold answer (4), so it is incorrect. The assistant clearly responded with a single numeric value, so it is internally consistent and thus faithful to its own output, but factually wrong relative to the gold.", "main_total_tokens": 1461, "main_prompt_tokens": 1442, "main_completion_tokens": 19, "judge_total_tokens": 477, "judge_prompt_tokens": 394, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 375.57482719421387, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a count (2). The generated answer claims none match, which is incorrect, so correctness is 0. However, the model honestly states it cannot find any, without fabricating specific shows or numbers, so it remains faithful to its (limited) retrieval, yielding high faithfulness.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 496, "judge_prompt_tokens": 404, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 229.0956974029541, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific French-language TV show released in 1980, and the gold answer is 'Fantômas'. The generated answer says 'I don't know,' which does not provide the correct show, so correctness is 0. However, it does not invent any incorrect facts and honestly expresses lack of knowledge, so it is fully faithful to the model's uncertainty, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 500, "judge_prompt_tokens": 389, "judge_completion_tokens": 111}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise", "latency_ms": 854.7403812408447, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, and correctly identifies the movie as 'Kill Cruise' based on the description. It is fully correct and faithful.", "main_total_tokens": 2579, "main_prompt_tokens": 2576, "main_completion_tokens": 3, "judge_total_tokens": 449, "judge_prompt_tokens": 387, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 1609.7474098205566, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie title, so it is both correct and faithful.", "main_total_tokens": 2939, "main_prompt_tokens": 2931, "main_completion_tokens": 8, "judge_total_tokens": 462, "judge_prompt_tokens": 408, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018", "latency_ms": 1936.5341663360596, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, both stating the release year as 2018. This is fully correct and faithful to the reference.", "main_total_tokens": 2882, "main_prompt_tokens": 2866, "main_completion_tokens": 16, "judge_total_tokens": 440, "judge_prompt_tokens": 380, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy", "latency_ms": 1117.3551082611084, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer, 'Astro Boy,' matches the gold answer exactly and correctly identifies the animated TV series about a young boy robot in 2030 AD. It is both factually correct and fully faithful to the gold reference.", "main_total_tokens": 2594, "main_prompt_tokens": 2590, "main_completion_tokens": 4, "judge_total_tokens": 466, "judge_prompt_tokens": 389, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 years", "latency_ms": 1146.4769840240479, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer of 7 years, which is also correct for the film 'Room' (2015).", "main_total_tokens": 2332, "main_prompt_tokens": 2329, "main_completion_tokens": 3, "judge_total_tokens": 455, "judge_prompt_tokens": 397, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m", "latency_ms": 623.4035491943359, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer (£26m) matches the provided gold answer (£26m), so it is both correct and faithful.", "main_total_tokens": 2022, "main_prompt_tokens": 2019, "main_completion_tokens": 3, "judge_total_tokens": 453, "judge_prompt_tokens": 398, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 507.0664882659912, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered with 'I don't know' instead of providing the correct numeric popularity score 80.5772, so it is incorrect. However, it did not hallucinate a wrong score or fabricate information and explicitly acknowledged not knowing, so it is fully faithful to its own knowledge state.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 485, "judge_prompt_tokens": 396, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 378.2081604003906, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies there was 1 Russian language TV show released in 1997, but the generated answer claims it cannot find any, which is factually incorrect relative to the gold. However, the generated response is internally consistent with its own stated inability to find data and does not introduce fabricated details, so it is faithful to its own (albeit incomplete) reasoning.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 500, "judge_prompt_tokens": 394, "judge_completion_tokens": 106}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes", "latency_ms": 694.3755149841309, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Angel Eyes' exactly matches the gold answer and correctly identifies the TV show in which Soo-wan regains her sight after an eye transplant surgery.", "main_total_tokens": 3003, "main_prompt_tokens": 3000, "main_completion_tokens": 3, "judge_total_tokens": 461, "judge_prompt_tokens": 397, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "en", "latency_ms": 1332.6330184936523, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer 'en' is incorrect because the gold label specifies the movie language is Korean. The evaluation correctly compares the generated output to the gold label, so the faithfulness of this judgment to the provided information is high, but the generated answer itself is wrong.", "main_total_tokens": 2903, "main_prompt_tokens": 2893, "main_completion_tokens": 10, "judge_total_tokens": 461, "judge_prompt_tokens": 377, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 627.5765895843506, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that there are 2 such movies, while the generated answer claims there are none, making it factually incorrect. However, the model does not fabricate specific numbers or movies; it instead admits inability to find matches, so its response is faithful to its (apparent) uncertainty and limitations.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 496, "judge_prompt_tokens": 403, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 601.0713577270508, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states explicitly that there are no movies from 1979 in the database. The generated answer says only \"I don't know,\" which is not equivalent to asserting lack of data for that year. It does not provide the same specific information as the gold. Therefore it is not fully correct relative to the gold response and is not fully faithful to it.", "main_total_tokens": 568, "main_prompt_tokens": 567, "main_completion_tokens": 1, "judge_total_tokens": 502, "judge_prompt_tokens": 399, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The movie Back to the Future Part IV was not released in any year, as there is no record of such a film existing in the provided context. The last installment of the original trilogy is Back to the Future Part III, released in 1990 [Doc 10621].", "latency_ms": 1646.2266445159912, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Back to the Future Part IV was not released and notes that the trilogy ends with Part III in 1990. This aligns with the gold answer and does not introduce unsupported claims, so it is both correct and faithful to the provided context.", "main_total_tokens": 3486, "main_prompt_tokens": 3429, "main_completion_tokens": 57, "judge_total_tokens": 541, "judge_prompt_tokens": 456, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot find any movies or shows matching the criteria, which aligns with the gold answer that there are no movies from 1980 related to the Chernobyl disaster. The reasoning that the event timing may not match is also factually faithful to the prompt. Therefore, both correctness and faithfulness are high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 533, "judge_prompt_tokens": 434, "judge_completion_tokens": 99}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 343.9164161682129, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states there are 4 titles with 'Astro Boy' in the title, while the generated answer incorrectly claims there are none, so it is factually wrong. It also contradicts the provided gold answer, so it is unfaithful to the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 397, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 581.8121433258057, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The correct answer is 2 but the generated answer says none, so it is factually wrong. It is somewhat faithful because, given the model’s knowledge cutoff and lack of future data, saying it cannot find any may reflect its limitations rather than hallucinating a number, but it still contradicts the gold answer.", "main_total_tokens": 3285, "main_prompt_tokens": 3284, "main_completion_tokens": 1, "judge_total_tokens": 491, "judge_prompt_tokens": 395, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 352.44226455688477, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 1, meaning there was one TV show about Super Mario Bros released in the 1990s. The generated answer claims there were none, which is factually incorrect, so correctness is 0. However, the model does not contradict the prompt or invent extra details; it merely states inability to find any, so with respect to the (very short) reasoning it is internally faithful, thus faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 518, "judge_prompt_tokens": 398, "judge_completion_tokens": 120}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 153.69224548339844, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says there were 0 TV shows, and the generated answer states it cannot find any movies or shows matching those criteria, which effectively implies zero. This matches the gold label, so it is correct. The response is faithful to the question and does not introduce unsupported claims or hallucinated titles.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 400, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 230.99994659423828, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto'. The generated answer says 'I don't know', which is incorrect relative to the gold. However, it does not invent false information; it simply expresses ignorance, so it is fully faithful to what the model purportedly knows.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 386, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 632.9727172851562, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the movie was not released, so there is a definite correct answer. The generated answer says only 'I don't know,' which is incorrect given the gold, so correctness is 0. However, the generated answer does not fabricate any information and accurately reflects uncertainty, so it is fully faithful to the model's own (claimed) knowledge, giving faithfulness 1.", "main_total_tokens": 531, "main_prompt_tokens": 530, "main_completion_tokens": 1, "judge_total_tokens": 498, "judge_prompt_tokens": 389, "judge_completion_tokens": 109}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 582.4301242828369, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that the movie was not released, but the generated answer says 'I don't know,' which is incorrect. However, the model does not fabricate information; it simply states uncertainty, so it remains fully faithful to its own knowledge and does not hallucinate.", "main_total_tokens": 554, "main_prompt_tokens": 553, "main_completion_tokens": 1, "judge_total_tokens": 471, "judge_prompt_tokens": 386, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "Star Wars: The Last Jedi", "latency_ms": 1223.747730255127, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because 'Star Wars: The Last Jedi' is not the 4th oldest Star Wars movie; according to the provided gold answer, it should be 'Star Wars: Droids - The Great Heep'. However, the model directly answered the question without adding unrelated or fabricated information, so it is fully faithful to its own (albeit incorrect) reasoning.", "main_total_tokens": 2844, "main_prompt_tokens": 2837, "main_completion_tokens": 7, "judge_total_tokens": 505, "judge_prompt_tokens": 395, "judge_completion_tokens": 110}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "2017", "latency_ms": 1959.775447845459, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 1986, but the generated answer says 2017, so it is factually incorrect. The question is simple, and the model directly answered with a year, so it is faithful to the query but wrong in content.", "main_total_tokens": 2795, "main_prompt_tokens": 2781, "main_completion_tokens": 14, "judge_total_tokens": 461, "judge_prompt_tokens": 380, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 322.0069408416748, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the rating as 8.466, but the model responded with 'I don't know,' which is incorrect. However, it did not fabricate an answer and honestly acknowledged not knowing, so it is fully faithful to its own (lack of) knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 398, "judge_completion_tokens": 85}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med", "latency_ms": 1249.0818500518799, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct name of the TV show. It is both correct and fully faithful to the reference.", "main_total_tokens": 2360, "main_prompt_tokens": 2356, "main_completion_tokens": 4, "judge_total_tokens": 455, "judge_prompt_tokens": 394, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina", "latency_ms": 1286.9482040405273, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly; the country is correctly identified as Argentina and faithfully reflects the provided reference.", "main_total_tokens": 2606, "main_prompt_tokens": 2604, "main_completion_tokens": 2, "judge_total_tokens": 446, "judge_prompt_tokens": 392, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "The apprentice falls into a trap designed by Jiang Jie [Doc 135231].", "latency_ms": 1524.867057800293, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer is correct: it states that the apprentice falls into a trap designed by Jiang Jie, which matches the gold answer \"Jiang Jie.\" However, the reference to \"[Doc 135231]\" is fabricated and not grounded in the provided information, so the response is not fully faithful to the source context.", "main_total_tokens": 2771, "main_prompt_tokens": 2754, "main_completion_tokens": 17, "judge_total_tokens": 496, "judge_prompt_tokens": 400, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "5", "latency_ms": 825.2451419830322, "retrieval_recall": 0.0, "retrieval_mrr": 0.07692307692307693, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (5) does not match the gold answer (4), so it is incorrect. The model seems internally consistent with its own output, hence faithfulness is high, but the final numeric result is wrong with respect to the gold.", "main_total_tokens": 3432, "main_prompt_tokens": 3423, "main_completion_tokens": 9, "judge_total_tokens": 465, "judge_prompt_tokens": 385, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 716.8436050415039, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that there are no movies with both Bruce Lee and Steven Seagal, and the generated answer effectively conveys the same idea by saying it cannot find any movies or shows matching all of those criteria. Both are factually correct and the generated answer is faithful to the gold in meaning, though phrased slightly more generally.", "main_total_tokens": 2222, "main_prompt_tokens": 2221, "main_completion_tokens": 1, "judge_total_tokens": 503, "judge_prompt_tokens": 406, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3", "latency_ms": 626.0225772857666, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated rating 6.3 exactly matches the gold rating 6.3, so it is both correct and faithful.", "main_total_tokens": 2238, "main_prompt_tokens": 2235, "main_completion_tokens": 3, "judge_total_tokens": 443, "judge_prompt_tokens": 387, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "Drift racing [Doc 33742].", "latency_ms": 2113.1131649017334, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The question asks for the style of racing, and the gold answer is 'Drift.' The generated answer 'Drift racing' is effectively the same concept and thus correct. However, the added reference '[Doc 33742]' and the slight elaboration are not present in the gold answer, so it is not perfectly faithful to the gold wording or format.", "main_total_tokens": 3127, "main_prompt_tokens": 3105, "main_completion_tokens": 22, "judge_total_tokens": 489, "judge_prompt_tokens": 386, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer", "latency_ms": 1756.1521530151367, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct title of the movie. It is fully correct and faithful to the provided reference.", "main_total_tokens": 2786, "main_prompt_tokens": 2779, "main_completion_tokens": 7, "judge_total_tokens": 472, "judge_prompt_tokens": 412, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838", "latency_ms": 831.7070007324219, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838), so it is both correct and faithful.", "main_total_tokens": 2291, "main_prompt_tokens": 2288, "main_completion_tokens": 3, "judge_total_tokens": 442, "judge_prompt_tokens": 389, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "Victim", "latency_ms": 917.2658920288086, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer 'Victim' is incorrect; the correct title is 'The Sixth Sense'. However, the evaluation correctly reflects the mismatch between the generated answer and the gold answer, so the faithfulness of this assessment is high.", "main_total_tokens": 2733, "main_prompt_tokens": 2730, "main_completion_tokens": 3, "judge_total_tokens": 470, "judge_prompt_tokens": 393, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002", "latency_ms": 1658.5898399353027, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (2002), so it is both correct and faithful.", "main_total_tokens": 2925, "main_prompt_tokens": 2922, "main_completion_tokens": 3, "judge_total_tokens": 434, "judge_prompt_tokens": 383, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled", "latency_ms": 1208.0752849578857, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct title of the 2015 reboot of 'The Transporter'.", "main_total_tokens": 2925, "main_prompt_tokens": 2918, "main_completion_tokens": 7, "judge_total_tokens": 455, "judge_prompt_tokens": 397, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015", "latency_ms": 1051.4676570892334, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2015), which is the correct release year for 'The Transporter Refueled'. The response is both correct and faithful to the expected output.", "main_total_tokens": 3043, "main_prompt_tokens": 3040, "main_completion_tokens": 3, "judge_total_tokens": 456, "judge_prompt_tokens": 387, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "I don't know.", "latency_ms": 353.76620292663574, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no such movie description can be found, effectively conveying lack of knowledge. The generated answer says 'I don't know,' which is consistent with that and does not introduce incorrect information. Therefore it is correct and faithful to the gold response.", "main_total_tokens": 550, "main_prompt_tokens": 549, "main_completion_tokens": 1, "judge_total_tokens": 471, "judge_prompt_tokens": 394, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The Old Man and the Sea", "latency_ms": 1314.7029876708984, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for a movie title that alludes to a manuscript written by Ernest Hemingway. The gold answer is 'Havana Darkness', which is a horror film explicitly involving an unpublished Hemingway manuscript. The generated answer 'The Old Man and the Sea' is itself a Hemingway work and film adaptation, not a movie whose title alludes to a Hemingway manuscript. Therefore, it is incorrect and not faithful to the reference answer.", "main_total_tokens": 2499, "main_prompt_tokens": 2492, "main_completion_tokens": 7, "judge_total_tokens": 509, "judge_prompt_tokens": 394, "judge_completion_tokens": 115}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 2386.9240283966064, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct TV show title: 'Doll & Em'. It is both correct and faithful.", "main_total_tokens": 2854, "main_prompt_tokens": 2820, "main_completion_tokens": 34, "judge_total_tokens": 453, "judge_prompt_tokens": 392, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in the movie \"Hallam Foe\" [Doc 39389].", "latency_ms": 2477.0963191986084, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Hallam Foe,\" which matches the gold answer. It is fully faithful to the question and does not introduce unsupported information beyond the movie title, so both correctness and faithfulness are maximal.", "main_total_tokens": 2815, "main_prompt_tokens": 2779, "main_completion_tokens": 36, "judge_total_tokens": 512, "judge_prompt_tokens": 434, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "His last name sounds at first like he’s a friend. [Doc 39389]", "latency_ms": 2282.6709747314453, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'A foe' while the generated answer says 'a friend,' which is the opposite. This is incorrect relative to the gold. The evaluation is faithful because it accurately reflects the discrepancy between the gold and generated answers.", "main_total_tokens": 2797, "main_prompt_tokens": 2760, "main_completion_tokens": 37, "judge_total_tokens": 482, "judge_prompt_tokens": 404, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18", "latency_ms": 1384.8087787628174, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: Sam is 18 in the 2017 TV show Atypical. It is correct and fully supported by the provided gold answer.", "main_total_tokens": 2524, "main_prompt_tokens": 2522, "main_completion_tokens": 2, "judge_total_tokens": 456, "judge_prompt_tokens": 389, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam is on the autism spectrum [Doc 124583].", "latency_ms": 1080.4588794708252, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which matches the gold answer exactly and does not introduce any contradictions or extraneous claims.", "main_total_tokens": 2548, "main_prompt_tokens": 2535, "main_completion_tokens": 13, "judge_total_tokens": 460, "judge_prompt_tokens": 399, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix", "latency_ms": 1082.5200080871582, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'Chamonix' exactly matches the gold answer, so it is both correct and faithful to the provided reference.", "main_total_tokens": 2976, "main_prompt_tokens": 2972, "main_completion_tokens": 4, "judge_total_tokens": 457, "judge_prompt_tokens": 400, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "Hazuki clips her nails every time it rains. [Doc 100481]", "latency_ms": 1792.2444343566895, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold completion ('rains') and appropriately fills in the blank. It is fully correct and faithful.", "main_total_tokens": 3138, "main_prompt_tokens": 3121, "main_completion_tokens": 17, "judge_total_tokens": 464, "judge_prompt_tokens": 407, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland [Doc 114810]", "latency_ms": 1253.8187503814697, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Greenland') and does not introduce any additional unsupported information. Therefore it is both correct and faithful.", "main_total_tokens": 2794, "main_prompt_tokens": 2785, "main_completion_tokens": 9, "judge_total_tokens": 461, "judge_prompt_tokens": 401, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon", "latency_ms": 932.551383972168, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('One Afternoon'), so it is correct and fully faithful to the provided information.", "main_total_tokens": 2364, "main_prompt_tokens": 2361, "main_completion_tokens": 3, "judge_total_tokens": 461, "judge_prompt_tokens": 406, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" [Doc 93260].", "latency_ms": 1264.5347118377686, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title of a low-rated Italian language movie in which a Holocaust survivor cares for a 12-year-old street kid who robbed her. Both the gold answer and the generated answer identify the movie as \"The Life Ahead,\" which is correct. The generated answer is directly based on and consistent with the gold answer, with no added contradictions or hallucinations, so it is fully faithful.", "main_total_tokens": 2684, "main_prompt_tokens": 2646, "main_completion_tokens": 38, "judge_total_tokens": 546, "judge_prompt_tokens": 435, "judge_completion_tokens": 111}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary “McLaren” is about Bruce McLaren, who founded the McLaren Motor Racing team [Doc 73859].", "latency_ms": 1622.4699020385742, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies that the 2016 documentary 'McLaren' is about Bruce McLaren. The added detail that he founded the McLaren motor racing team is also accurate and consistent with the gold answer, so the response is both correct and faithful.", "main_total_tokens": 2666, "main_prompt_tokens": 2636, "main_completion_tokens": 30, "judge_total_tokens": 496, "judge_prompt_tokens": 413, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren", "latency_ms": 1987.4906539916992, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer \"McLaren\" is essentially correct and matches the gold answer \"The McLaren Motor Racing team\" at the level of the core entity (the racing team is McLaren). It is a shortened but faithful form of the same team name, so it is both correct and faithful to the gold reference.", "main_total_tokens": 2988, "main_prompt_tokens": 2973, "main_completion_tokens": 15, "judge_total_tokens": 479, "judge_prompt_tokens": 385, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "I don't know.", "latency_ms": 1331.4826488494873, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because the gold label specifies that he is from New Zealand, while the model responded with 'I don't know.' However, the answer is faithful to the model's claimed uncertainty and does not fabricate information, so faithfulness is high.", "main_total_tokens": 635, "main_prompt_tokens": 626, "main_completion_tokens": 9, "judge_total_tokens": 464, "judge_prompt_tokens": 380, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "A Good Day to Die Hard", "latency_ms": 1687.3502731323242, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect: in 'A Good Day to Die Hard' John McClane does not blow up the Great Wall of China, and there is no known 2013 movie where he does so. The gold answer correctly states that no such movie exists in the database. The generated answer also contradicts the gold answer, so it is not faithful.", "main_total_tokens": 3021, "main_prompt_tokens": 3014, "main_completion_tokens": 7, "judge_total_tokens": 508, "judge_prompt_tokens": 404, "judge_completion_tokens": 104}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit", "latency_ms": 1079.9224376678467, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the 2002 Japanese horror film as 'Tomie: Forbidden Fruit.' The reasoning is consistent with the provided information.", "main_total_tokens": 2801, "main_prompt_tokens": 2795, "main_completion_tokens": 6, "judge_total_tokens": 487, "judge_prompt_tokens": 422, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65", "latency_ms": 1037.024736404419, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years), so it is both correct and fully faithful to the reference.", "main_total_tokens": 3180, "main_prompt_tokens": 3178, "main_completion_tokens": 2, "judge_total_tokens": 451, "judge_prompt_tokens": 396, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Scorpion King and Tala search for a legendary relic known as The Book of Souls, which will allow them to put an end to an evil warlord [Doc 82582].", "latency_ms": 1978.6627292633057, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.96, "justification": "The generated answer correctly states that they are searching for The Book of Souls, matching the gold answer. It adds extra detail (legendary relic, end an evil warlord), which is consistent with the film’s plot and does not contradict the source, but the bracketed citation [Doc 82582] is fabricated in this context, slightly reducing faithfulness to the provided material.", "main_total_tokens": 3136, "main_prompt_tokens": 3098, "main_completion_tokens": 38, "judge_total_tokens": 540, "judge_prompt_tokens": 431, "judge_completion_tokens": 109}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 598.4768867492676, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database, which is equivalent in content to saying 'I don't know.' The generated response does not hallucinate a year or contradict the gold; it simply expresses uncertainty. Therefore it is both correct relative to the gold and faithful to it.", "main_total_tokens": 557, "main_prompt_tokens": 556, "main_completion_tokens": 1, "judge_total_tokens": 483, "judge_prompt_tokens": 395, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 841.2694931030273, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is not in the database, equivalent in content to saying 'I don't know.' The generated response correctly reflects lack of knowledge without fabricating information and stays faithful to the gold intent.", "main_total_tokens": 594, "main_prompt_tokens": 593, "main_completion_tokens": 1, "judge_total_tokens": 466, "judge_prompt_tokens": 393, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 476.1788845062256, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, which is equivalent in content to the generated answer saying it does not know. Both correctly avoid hallucinating a release year and are semantically aligned, so the response is both correct and faithful.", "main_total_tokens": 536, "main_prompt_tokens": 535, "main_completion_tokens": 1, "judge_total_tokens": 477, "judge_prompt_tokens": 395, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 652.5561809539795, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly indicate lack of knowledge about the movie rather than hallucinating a release year, so the response is correct and faithful to the gold.", "main_total_tokens": 566, "main_prompt_tokens": 565, "main_completion_tokens": 1, "judge_total_tokens": 459, "judge_prompt_tokens": 395, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "2002", "latency_ms": 1169.044017791748, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer gives a specific year (2002), contradicting the gold response that the movie is not in the database. There is no evidence that 2002 is correct, so correctness is 0. It is also unfaithful to the gold answer, which explicitly states lack of information, so faithfulness is 0.", "main_total_tokens": 2821, "main_prompt_tokens": 2818, "main_completion_tokens": 3, "judge_total_tokens": 492, "judge_prompt_tokens": 394, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 744.2526817321777, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database, which is equivalent to not knowing the release year. The generated answer, 'I don't know,' correctly aligns with that and does not add any unsupported information, so it is both correct and faithful.", "main_total_tokens": 533, "main_prompt_tokens": 532, "main_completion_tokens": 1, "judge_total_tokens": 475, "judge_prompt_tokens": 393, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 507.91096687316895, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold answer and the generated answer correctly indicate a lack of knowledge about the movie, and the generated answer is faithful to the gold response in content and intent.", "main_total_tokens": 560, "main_prompt_tokens": 559, "main_completion_tokens": 1, "judge_total_tokens": 460, "judge_prompt_tokens": 396, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was directed by Adrian Sitaru [Doc 46069].", "latency_ms": 1299.8173236846924, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer says the database does not provide the director information, so the model should not name a director. The generated answer asserts a specific director and even cites a document, which contradicts the gold answer and fabricates information, making it both incorrect and unfaithful to the source/database limitations.", "main_total_tokens": 3153, "main_prompt_tokens": 3128, "main_completion_tokens": 25, "judge_total_tokens": 512, "judge_prompt_tokens": 421, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The movie \"A Love to Die For\" is not listed in the provided context, so I cannot provide the name of its director.", "latency_ms": 1607.734203338623, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director cannot be identified from the provided information. The generated answer correctly reflects uncertainty and does not hallucinate a director, matching the database/context limitation. Therefore it is both correct and faithful to the given information.", "main_total_tokens": 2949, "main_prompt_tokens": 2921, "main_completion_tokens": 28, "judge_total_tokens": 503, "judge_prompt_tokens": 422, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The movie \"Journey into Gaza\" was directed by an unspecified director in the provided context.", "latency_ms": 1172.7254390716553, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer implies that the movie had a director but labels it as unspecified, rather than clearly stating that the director information is not provided in the database and thus cannot be answered. This is incorrect relative to the gold answer, which explicitly acknowledges the lack of information. However, the model does not introduce a specific incorrect director name or external facts beyond the given context; it stays within the idea that the director is not identified in the provided information, so it is faithful to the context but incorrect in phrasing compared to the gold.", "main_total_tokens": 2797, "main_prompt_tokens": 2778, "main_completion_tokens": 19, "judge_total_tokens": 548, "judge_prompt_tokens": 411, "judge_completion_tokens": 137}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The movie \"The Last Night at Tremore Beach\" is directed by an unspecified individual in the provided context.", "latency_ms": 1547.6083755493164, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The gold answer states that the director information is not provided in the database, so the correct response is to say it cannot be answered. The generated answer instead asserts that the movie is directed by an unspecified individual, which introduces an invented fact rather than explicitly acknowledging the absence of information. This is factually incorrect with respect to the task, and only loosely faithful to the context because it hints that the director is not specified but still frames it as a definitive statement about the director.", "main_total_tokens": 2556, "main_prompt_tokens": 2533, "main_completion_tokens": 23, "judge_total_tokens": 547, "judge_prompt_tokens": 419, "judge_completion_tokens": 128}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 816.6568279266357, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Rise of the Mummy'), correctly identifying the movie and remaining fully faithful to the provided information.", "main_total_tokens": 2552, "main_prompt_tokens": 2546, "main_completion_tokens": 6, "judge_total_tokens": 455, "judge_prompt_tokens": 396, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 1165.1873588562012, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Stranger in My Bed'), correctly identifying the movie and fully aligning with the provided reference.", "main_total_tokens": 2687, "main_prompt_tokens": 2681, "main_completion_tokens": 6, "judge_total_tokens": 458, "judge_prompt_tokens": 400, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 998.741626739502, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold exactly and correctly identifies the title as 'Game of Silence,' so it is both correct and faithful.", "main_total_tokens": 2236, "main_prompt_tokens": 2232, "main_completion_tokens": 4, "judge_total_tokens": 457, "judge_prompt_tokens": 400, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 1539.283037185669, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question describes a movie based on a true story about a killer from Craigslist. The gold answer is 'The Craigslist Killer' and the generated answer is identical. This is correct with no contradictions or hallucinations.", "main_total_tokens": 2644, "main_prompt_tokens": 2625, "main_completion_tokens": 19, "judge_total_tokens": 468, "judge_prompt_tokens": 395, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 1052.983045578003, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold reference exactly and correctly identifies the movie as 'Wood Job!'. It is both correct and faithful.", "main_total_tokens": 2942, "main_prompt_tokens": 2938, "main_completion_tokens": 4, "judge_total_tokens": 460, "judge_prompt_tokens": 403, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 2157.770872116089, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase' and is fully faithful to the provided information.", "main_total_tokens": 2751, "main_prompt_tokens": 2723, "main_completion_tokens": 28, "judge_total_tokens": 481, "judge_prompt_tokens": 417, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond", "latency_ms": 773.5044956207275, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and correctly identifies the movie as 'Beyond'. The response is fully consistent with the provided reference.", "main_total_tokens": 2301, "main_prompt_tokens": 2299, "main_completion_tokens": 2, "judge_total_tokens": 467, "judge_prompt_tokens": 409, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9", "latency_ms": 1242.1834468841553, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which correctly identifies the movie as '6ixtynin9'. It is both correct and fully faithful to the reference.", "main_total_tokens": 2382, "main_prompt_tokens": 2375, "main_completion_tokens": 7, "judge_total_tokens": 474, "judge_prompt_tokens": 409, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread", "latency_ms": 1541.6195392608643, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Club Dread') and correctly identifies the movie described. It is both correct and faithful to the reference.", "main_total_tokens": 2740, "main_prompt_tokens": 2736, "main_completion_tokens": 4, "judge_total_tokens": 488, "judge_prompt_tokens": 427, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet", "latency_ms": 1127.8154850006104, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, and it correctly identifies the movie as 'Toilet.' The response is therefore both correct and faithful to the provided reference.", "main_total_tokens": 2994, "main_prompt_tokens": 2991, "main_completion_tokens": 3, "judge_total_tokens": 479, "judge_prompt_tokens": 415, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel", "latency_ms": 1103.6882400512695, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Electric Angel'). It is both correct and fully faithful to the provided reference.", "main_total_tokens": 2634, "main_prompt_tokens": 2631, "main_completion_tokens": 3, "judge_total_tokens": 473, "judge_prompt_tokens": 418, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "5.207", "latency_ms": 1278.395652770996, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '5.207' is not a movie title and does not correspond to the correct answer 'Mega Ape'. It is both incorrect and not faithful to the gold answer.", "main_total_tokens": 2535, "main_prompt_tokens": 2531, "main_completion_tokens": 4, "judge_total_tokens": 453, "judge_prompt_tokens": 385, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 184.4768524169922, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer according to the gold is 'Tea House', but the model said 'I don't know', which is incorrect. However, the response is fully faithful to the model’s own uncertainty and does not fabricate an answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 462, "judge_prompt_tokens": 386, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 312.0112419128418, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect because the gold answer provides a specific movie title while the model responded with not knowing. However, the model did not fabricate any information; it honestly stated a lack of knowledge, so it is fully faithful to its own response process.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 479, "judge_prompt_tokens": 395, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "fr", "latency_ms": 6306.979656219482, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'fr' is incorrect because the gold answer is 'English'. It also is not faithful to the gold answer, directly contradicting it.", "main_total_tokens": 2928, "main_prompt_tokens": 2926, "main_completion_tokens": 2, "judge_total_tokens": 446, "judge_prompt_tokens": 384, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "Freaks and Geeks", "latency_ms": 1197.831392288208, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'Freaks and Geeks' does not match the gold answer 'Law & Order: Special Victims Unit' and is factually incorrect as the TV show with the highest popularity in 1999 per the gold label. It is therefore neither correct nor faithful to the provided gold answer.", "main_total_tokens": 2868, "main_prompt_tokens": 2861, "main_completion_tokens": 7, "judge_total_tokens": 488, "judge_prompt_tokens": 395, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2", "latency_ms": 1087.5983238220215, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2, so it is both correct and faithful.", "main_total_tokens": 2219, "main_prompt_tokens": 2216, "main_completion_tokens": 3, "judge_total_tokens": 439, "judge_prompt_tokens": 386, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "1999", "latency_ms": 1035.3479385375977, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states the movie was released in 1999, while the gold answer is 2019. This is factually incorrect, so correctness is 0. However, the assistant directly answers the question without adding unsupported details, so faithfulness to the user query is high, yielding a faithfulness score of 1.", "main_total_tokens": 2777, "main_prompt_tokens": 2774, "main_completion_tokens": 3, "judge_total_tokens": 484, "judge_prompt_tokens": 387, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "1", "latency_ms": 631.119966506958, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1) does not match the correct number of votes (279), so it is incorrect. However, there is no evidence of hallucinated reasoning or contradiction of the given context; the answer is simply numerically wrong, so faithfulness is high but correctness is zero.", "main_total_tokens": 2154, "main_prompt_tokens": 2153, "main_completion_tokens": 1, "judge_total_tokens": 471, "judge_prompt_tokens": 383, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "10", "latency_ms": 789.8638248443604, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (10) does not match the gold answer (12), so it is factually incorrect. However, it does directly answer the question asked, so it is faithful to the prompt.", "main_total_tokens": 2252, "main_prompt_tokens": 2233, "main_completion_tokens": 19, "judge_total_tokens": 454, "judge_prompt_tokens": 383, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "2009", "latency_ms": 2005.1908493041992, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, and the movie was indeed released in 2009. The response is both correct and faithful to the reference.", "main_total_tokens": 3121, "main_prompt_tokens": 3118, "main_completion_tokens": 3, "judge_total_tokens": 452, "judge_prompt_tokens": 390, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_gpt-4o-mini", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en", "latency_ms": 1144.4473266601562, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer and the generated answer both correctly state that the original language of 'The King's Speech' is English, so the response is fully correct and faithful.", "main_total_tokens": 2798, "main_prompt_tokens": 2796, "main_completion_tokens": 2, "judge_total_tokens": 447, "judge_prompt_tokens": 384, "judge_completion_tokens": 63}
