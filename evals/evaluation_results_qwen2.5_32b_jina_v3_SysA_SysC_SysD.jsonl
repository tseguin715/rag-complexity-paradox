{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "Wall Street was released in 1987 [Doc 6993].", "latency_ms": 8814.19563293457, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer. It is fully consistent and faithful to the given information.", "main_total_tokens": 3061, "main_prompt_tokens": 3042, "main_completion_tokens": 19, "judge_total_tokens": 454, "judge_prompt_tokens": 392, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "In the movie \"Brain Fluid Explosion Girl\" from 2015, Hana Ichii and her classmates find themselves locked in a cage and forced to take part in a survival game using their cellphones [Doc 69929].", "latency_ms": 6726.940155029297, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie involves classmates playing a survival game using their cell phones. The generated answer correctly identifies the film as \"Brain Fluid Explosion Girl\" and the added description about classmates being forced into a cellphone-based survival game aligns with the gold answer and remains consistent with the cited document. There are no unsupported or contradictory claims relative to the prompt, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2775, "main_prompt_tokens": 2723, "main_completion_tokens": 52, "judge_total_tokens": 543, "judge_prompt_tokens": 433, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "In the TV show \"Dungeons & Dragons\" from 1983, six friends are transported into the Dungeons & Dragons realm and must try to find a way home with the help of their guide 'Dungeon Master'. [Doc 112849]", "latency_ms": 7293.821096420288, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons' and accurately describes the premise involving six friends guided by the Dungeon Master to find a way home. It is fully consistent with the gold answer and faithful to the provided supporting document snippet.", "main_total_tokens": 2814, "main_prompt_tokens": 2757, "main_completion_tokens": 57, "judge_total_tokens": 522, "judge_prompt_tokens": 440, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie that tells the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is \"Nacht der Wölfe\" [Doc 2590].", "latency_ms": 5955.601215362549, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as 'Nacht der Wölfe'. It correctly restates the question and attributes the information to the document. There are no errors or hallucinations, so it is both correct and faithful to the provided information.", "main_total_tokens": 2823, "main_prompt_tokens": 2780, "main_completion_tokens": 43, "judge_total_tokens": 524, "judge_prompt_tokens": 438, "judge_completion_tokens": 86}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of Picnic at Hanging Rock take place in the year 1900 [Doc 126191].", "latency_ms": 4900.456190109253, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of Picnic at Hanging Rock take place in 1900, which matches the gold answer and is fully supported by the cited document.", "main_total_tokens": 3072, "main_prompt_tokens": 3043, "main_completion_tokens": 29, "judge_total_tokens": 471, "judge_prompt_tokens": 406, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie that matches this description is \"June\" [Doc 32483].", "latency_ms": 4521.288871765137, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a 2004 movie about a young violinist working in a Venice coffee shop, rehearsing for a music career, and daydreaming about her destined love, with her name being a calendar month. The gold answer is \"June\" and the generated answer is also \"June,\" directly matching the gold. This is both factually correct and fully aligned with the given gold answer, so correctness and faithfulness are both 1.", "main_total_tokens": 3139, "main_prompt_tokens": 3114, "main_completion_tokens": 25, "judge_total_tokens": 573, "judge_prompt_tokens": 451, "judge_completion_tokens": 122}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "The highest rated Russian-language (ru) movie released in 1980 based on the provided documents is \"Moscow Does Not Believe in Tears\" with a rating of 7.6 [Doc 61].", "latency_ms": 6594.367265701294, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates that the highest rated Russian-language movie released in 1980 is \"The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger\", but the generated answer instead names \"Moscow Does Not Believe in Tears\". This is factually incorrect relative to the gold. Additionally, the generated answer explicitly cites a document and claims it is based on the provided documents, but we know from the gold label that this conclusion is wrong, so its use of the source is not faithful to the correct grounding. Therefore both correctness and faithfulness are scored 0.", "main_total_tokens": 3272, "main_prompt_tokens": 3226, "main_completion_tokens": 46, "judge_total_tokens": 583, "judge_prompt_tokens": 437, "judge_completion_tokens": 146}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest Russian-language movie rating in 1999, based on the provided documents, is 8.1 for \"Kamenskaya\" [Doc 115256].", "latency_ms": 6517.754316329956, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the highest Russian-language movie rating in 1999 is 7.6, while the generated answer claims it is 8.1 for 'Kamenskaya', which contradicts the gold. Additionally, the generated answer references a specific document not present in the provided context, making it unfaithful to the given information.", "main_total_tokens": 3188, "main_prompt_tokens": 3145, "main_completion_tokens": 43, "judge_total_tokens": 519, "judge_prompt_tokens": 419, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "Based on the provided documents, there are two movies from 1987 with a rating over 7 and in Chinese (cn):\n\n1. \"A Chinese Ghost Story\" [Doc 7064] - Rating: 7.3\n2. \"Last Eunuch In China\" [Doc 7754] - Rating: 7.8\n\nTherefore, the answer is 2 movies.", "latency_ms": 10301.766872406006, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 4 such movies, but the generated answer concludes there are 2. This is factually incorrect relative to the gold. However, the assistant’s reasoning is consistent with the (limited) documents it cites and does not introduce unsupported claims; it faithfully explains how it arrived at 2 using those documents.", "main_total_tokens": 3063, "main_prompt_tokens": 2975, "main_completion_tokens": 88, "judge_total_tokens": 570, "judge_prompt_tokens": 472, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "None of the TV shows listed that were released in 1987 and have a rating over 7 are in Chinese. The only entry in Chinese is \"Tian Mimi\" [Doc 117988], but it was released in 2007, not 1987.", "latency_ms": 8644.108295440674, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies there are 2 such TV shows, but the generated answer claims there are none, directly contradicting the correct count. However, the reasoning in the generated answer is internally consistent with the provided document snippet about 'Tian Mimi' being from 2007 and not 1987, so it is faithful to the (apparently incomplete) evidence it cites, though ultimately incorrect relative to the gold.", "main_total_tokens": 3332, "main_prompt_tokens": 3265, "main_completion_tokens": 67, "judge_total_tokens": 563, "judge_prompt_tokens": 448, "judge_completion_tokens": 115}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "Based on the provided documents, the only French language TV show released in 1980 is \"Therese Raquin\" [Doc 112644]. However, it's important to note that another TV show from 1980, \"Luzie, der Schrecken der Straße\" [Doc 112657], was also released in the same year, but it is in German, not French.", "latency_ms": 11371.150255203247, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct answer from the gold label is 'Fantômas', but the generated answer states 'Therese Raquin', so the correctness is 0. The answer is partially grounded in the (implied) documents it references (Doc 112644 and Doc 112657) and accurately distinguishes language and year based on those, so it is somewhat faithful to its cited sources, though it fails relative to the gold. Hence faithfulness is set to 0.5.", "main_total_tokens": 3283, "main_prompt_tokens": 3187, "main_completion_tokens": 96, "judge_total_tokens": 587, "judge_prompt_tokens": 464, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" [Doc 11205].", "latency_ms": 4936.025857925415, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise', which matches the gold answer exactly. It stays faithful to the given information without adding unsupported details, aside from a generic citation placeholder that does not contradict the content.", "main_total_tokens": 2996, "main_prompt_tokens": 2966, "main_completion_tokens": 30, "judge_total_tokens": 484, "judge_prompt_tokens": 409, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 6310.765504837036, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as \"Won't You Be My Neighbor?\" and does not introduce any unsupported information beyond a neutral citation marker.", "main_total_tokens": 3207, "main_prompt_tokens": 3164, "main_completion_tokens": 43, "judge_total_tokens": 508, "judge_prompt_tokens": 440, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The document does not specify a single release year for \"it\" as multiple movies are listed with different release years. Please clarify which movie you are asking about.", "latency_ms": 5503.41272354126, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly expects the year 2018, implying the referent of 'it' is unambiguous in context. The generated answer claims ambiguity and does not provide the correct year, so it is incorrect relative to the gold. However, given only the generated text and no document context, it faithfully describes its own reasoning about multiple movies and does not hallucinate a year, so it is faithful to the provided context.", "main_total_tokens": 3360, "main_prompt_tokens": 3327, "main_completion_tokens": 33, "judge_total_tokens": 526, "judge_prompt_tokens": 409, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is \"Astro Boy\" [Doc 112591].", "latency_ms": 5425.041437149048, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy,' which matches the gold answer exactly. It remains faithful to the provided information and does not introduce contradictions or extraneous claims.", "main_total_tokens": 3022, "main_prompt_tokens": 2986, "main_completion_tokens": 36, "judge_total_tokens": 485, "judge_prompt_tokens": 416, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 5316.5013790130615, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer. It is fully consistent with the provided information and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 2778, "main_prompt_tokens": 2742, "main_completion_tokens": 36, "judge_total_tokens": 498, "judge_prompt_tokens": 424, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26m before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 6306.137323379517, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before the 26 November 1983 break-in, which matches the provided gold answer exactly and stays faithful to the source without adding unsupported details.", "main_total_tokens": 3160, "main_prompt_tokens": 3116, "main_completion_tokens": 44, "judge_total_tokens": 509, "judge_prompt_tokens": 428, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "The most popular English-speaking TV show released in 1997 based on the provided documents is \"Recess\" with a popularity score of 10.3614 [Doc 114720].", "latency_ms": 6873.161554336548, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the popularity score of the most popular English-speaking TV show released in 1997. The gold answer is 80.5772, but the generated answer gives 10.3614 for the show \"Recess\", which is incorrect. Additionally, the generated answer asserts this is based on the provided documents and cites a specific document, but given the gold label, this claim is not faithful to the ground truth, so both correctness and faithfulness are 0.", "main_total_tokens": 3339, "main_prompt_tokens": 3291, "main_completion_tokens": 48, "judge_total_tokens": 558, "judge_prompt_tokens": 430, "judge_completion_tokens": 128}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "Based on the provided documents, there are no Russian language TV shows that were released in 1997. The only Russian entry is a movie titled \"A House ‎for Rent with All the Inconveniences\" from 2016 [Doc 72036].", "latency_ms": 8088.587045669556, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 Russian language TV show released in 1997, but the generated answer claims there were none, which is factually incorrect relative to the gold. However, the generated answer is faithful to the provided documents it references, explicitly stating its conclusion is based on those documents.", "main_total_tokens": 3437, "main_prompt_tokens": 3376, "main_completion_tokens": 61, "judge_total_tokens": 528, "judge_prompt_tokens": 436, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 6223.793029785156, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Angel Eyes') and correctly identifies the TV show. It is fully supported by the given information and introduces no unsupported claims.", "main_total_tokens": 3523, "main_prompt_tokens": 3483, "main_completion_tokens": 40, "judge_total_tokens": 493, "judge_prompt_tokens": 429, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"I Dream in Another Language\" is es [Doc 78127].", "latency_ms": 4628.506660461426, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold label states the language is Korean, while the model answered Spanish (es). It is also unfaithful to the provided gold answer, directly contradicting it.", "main_total_tokens": 3224, "main_prompt_tokens": 3200, "main_completion_tokens": 24, "judge_total_tokens": 467, "judge_prompt_tokens": 396, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "None of the English-speaking Horror movies listed from 2009 have a popularity score greater than 7. The highest popularity score for an English-speaking Horror movie in this list is 5.5547 for the movie \"9\" [Doc 44837], which is not even in the Horror genre but in Action, Adventure, Animation, Science Fiction, and Thriller. Therefore, based on the provided documents, there are no English-speaking Horror movies from 2009 with a popularity score greater than 7.", "latency_ms": 12956.114768981934, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The gold answer states there are 2 such movies, while the generated answer concludes there are none, so it is factually incorrect relative to the gold (correctness = 0). The reasoning given in the generated answer is consistent with the documents it references (it checks genres, year, language, and popularity and explains why it finds zero), so it is mostly faithful to the (possibly incomplete) provided evidence, though it makes a strong universal claim based only on the given list; this slight overreach warrants a small deduction (faithfulness = 0.8).", "main_total_tokens": 3340, "main_prompt_tokens": 3227, "main_completion_tokens": 113, "judge_total_tokens": 641, "judge_prompt_tokens": 494, "judge_completion_tokens": 147}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The provided documents do not contain information about any movies released in 1979. Therefore, it is not possible to determine the best-rated movie from that year based on the given context.", "latency_ms": 6145.610570907593, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there is no information about movies released in 1979 in the available data, so the best-rated movie cannot be determined. The generated answer is faithful to the limitation of the provided documents and does not introduce external information.", "main_total_tokens": 3186, "main_prompt_tokens": 3146, "main_completion_tokens": 40, "judge_total_tokens": 516, "judge_prompt_tokens": 432, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "Based on the provided documents, there is no information about a movie titled \"Back to the Future Part IV\" being released. The documents only mention three parts of the Back to the Future series, with the last installment, \"Back to the Future Part III,\" released in 1990 [Doc 10621].", "latency_ms": 9392.690658569336, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both answers correctly convey that there is no known release year for a non-existent 'Back to the Future Part IV'. The gold answer states it is not in the database; the generated answer notes that only three parts exist and the last was released in 1990. This is factually correct and remains faithful to the provided context, not hallucinating a release year.", "main_total_tokens": 4113, "main_prompt_tokens": 4044, "main_completion_tokens": 69, "judge_total_tokens": 567, "judge_prompt_tokens": 463, "judge_completion_tokens": 104}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "The provided documents do not contain any movies about the Chernobyl disaster that were released in 1980. The earliest release year for a Chernobyl-related movie in the given context is 1990, for the film \"Collapse\" [Doc 10620]. Therefore, based on the provided information, there is no answer to this question.", "latency_ms": 9665.988683700562, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.5, "faithfulness_score": 1, "justification": "Gold answer: says it could not find any movies from 1980 related to Chernobyl. Generated answer: states that in the provided documents there are no Chernobyl movies from 1980 and thus no answer, adding that the earliest one listed is from 1990. With respect to the question, both effectively convey that there is no such 1980 Chernobyl film in the data, which aligns with the gold answer in intent. However, the generated answer introduces a specific film and year (\"Collapse\" [Doc 10620], 1990) that are not in the gold and may be speculative relative to the reference, so it is not fully correct in content. It is, however, fully faithful to the information it claims (it explicitly relies on the provided documents and does not contradict them in its own context).", "main_total_tokens": 3249, "main_prompt_tokens": 3171, "main_completion_tokens": 78, "judge_total_tokens": 679, "judge_prompt_tokens": 476, "judge_completion_tokens": 203}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are three TV shows or movies with 'Astro Boy' in the title:\n\n1. Astro Boy (TV, 1980) [Doc 112591]\n2. Astro Boy (Movie, 2009) [Doc 44947]\n3. Astro Boy (TV, 2003) [Doc 116265]", "latency_ms": 10255.86462020874, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.8318724637288826, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.8318724637288826, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 titles, but the generated answer says there are three and lists only three. This directly contradicts the correct count, so the answer is incorrect. However, the answer is internally consistent with itself and does not contradict its own statements, so it is faithful to the model’s reasoning and given information.", "main_total_tokens": 3164, "main_prompt_tokens": 3078, "main_completion_tokens": 86, "judge_total_tokens": 547, "judge_prompt_tokens": 454, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "The provided documents do not contain information about any movies from the year 2025 that allude to the Holocaust. Therefore, based on the given context, no movies in 2025 allude to the Holocaust.", "latency_ms": 6665.581464767456, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 2 movies, while the generated answer concludes there are 0 based on lack of evidence, so it is factually incorrect with respect to the gold. However, given the stated limitation of the provided documents, the answer is faithful to the context it had, explicitly reasoning from absence of information rather than inventing data.", "main_total_tokens": 3133, "main_prompt_tokens": 3085, "main_completion_tokens": 48, "judge_total_tokens": 528, "judge_prompt_tokens": 427, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "Based on the provided documents, there are three TV shows about Super Mario Bros that were released in the 1990s:\n\n1. The Adventures of Super Mario Bros. 3 [Doc 113661], which was released in 1990.\n2. Super Mario World [Doc 113814], which was released in 1991.\n\nTherefore, two TV shows about Super Mario Bros were released in the 1990s according to the given context. [Doc 113517] mentions \"The Super Mario Bros. Super Show!\" but it was released in 1989, which is not within the 1990s decade.", "latency_ms": 16614.545106887817, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer says there was 1 TV show, but the generated answer concludes there were 2. This is factually incorrect relative to the gold, so correctness is 0. The reasoning is internally consistent with the documents it cites (it correctly notes 1990 and 1991 releases and excludes 1989), so it is partially faithful to the provided context, but the final count conflicts with the gold label, so faithfulness is set to 0.5.", "main_total_tokens": 3244, "main_prompt_tokens": 3091, "main_completion_tokens": 153, "judge_total_tokens": 644, "judge_prompt_tokens": 515, "judge_completion_tokens": 129}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "Based on the provided documents, there are no TV shows about Super Mario Bros that were released between the years 2000 and 2010. The closest entries are \"The Super Mario Bros. Super Show!\" from 1989 and \"Super Mario World\" from 1991, both of which fall outside the specified time frame [Doc 113517] [Doc 113814].", "latency_ms": 10829.0114402771, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0, and the generated answer explicitly states that there are no TV shows about Super Mario Bros released between 2000 and 2010, matching the gold. The mention of earlier shows as closest entries is consistent with the (implied) documents and does not introduce contradictions. Therefore, the response is both correct and faithful to the provided information.", "main_total_tokens": 3170, "main_prompt_tokens": 3075, "main_completion_tokens": 95, "judge_total_tokens": 572, "judge_prompt_tokens": 467, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular TV show or movie about ninjas, based on the provided popularity scores, is \"UNDER NINJA\" with a popularity of 3.1982 [Doc 133621].", "latency_ms": 6824.638605117798, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question expects 'Naruto' as the most popular ninja-related TV show, but the generated answer incorrectly identifies 'UNDER NINJA' instead. This makes the answer factually incorrect relative to the gold. However, it is faithful to the provided document information (it explicitly cites the doc and its popularity score), so faithfulness is high.", "main_total_tokens": 3392, "main_prompt_tokens": 3345, "main_completion_tokens": 47, "judge_total_tokens": 522, "judge_prompt_tokens": 422, "judge_completion_tokens": 100}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The provided documents do not contain information about a Jurassic Park/Power Rangers crossover movie. Therefore, I cannot provide a release date for such a movie based on the given context.", "latency_ms": 5691.879987716675, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie was not released. The generated answer does not assert a release date and instead says it cannot provide one based on the documents, which is consistent with there being no such released movie. It is therefore correct relative to the question and faithful to the provided context.", "main_total_tokens": 3231, "main_prompt_tokens": 3195, "main_completion_tokens": 36, "judge_total_tokens": 509, "judge_prompt_tokens": 420, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "Based on the provided documents, there is no movie where Barbie fights a Predator.", "latency_ms": 4184.14568901062, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no movie where Barbie fights a Predator, which is equivalent in meaning to the gold answer that such a movie was not released. It is faithful to the information implied by the gold answer and does not introduce unsupported claims.", "main_total_tokens": 3460, "main_prompt_tokens": 3443, "main_completion_tokens": 17, "judge_total_tokens": 475, "judge_prompt_tokens": 398, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "To determine the 4th oldest Star Wars movie, we need to identify and order the movies by their release year:\n\n1. **Return of the Jedi** - 1983 [Doc 2858]\n2. **Star Wars: The Force Awakens** - 2015 [Doc 67357]\n3. **Star Wars: The Last Jedi** - 2017 [Doc 76818]\n4. **The Skywalker Legacy** - 2020 [Doc 93599]\n\nBased on the provided documents, the 4th oldest Star Wars movie is **\"The Skywalker Legacy\"**, which was released in 2020 [Doc 93599].", "latency_ms": 17576.534509658813, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The answer is incorrect: the 4th oldest Star Wars movie per the gold label is 'Star Wars: Droids - The Great Heep', but the generated answer gives 'The Skywalker Legacy'. Additionally, the reasoning about ordering by release year is not faithful to the true chronological order of Star Wars movies and instead is constrained to a limited, incorrect subset of documents; it ignores many earlier films and specials. The chain-of-thought is partly grounded in the (incomplete) provided docs, but the final conclusion is not faithful to the actual task or gold answer.", "main_total_tokens": 3623, "main_prompt_tokens": 3463, "main_completion_tokens": 160, "judge_total_tokens": 674, "judge_prompt_tokens": 526, "judge_completion_tokens": 148}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The document does not specify a single release year for \"it\" without additional context on which movie is being referred to. Please provide more details so I can give you the correct information.", "latency_ms": 6154.3285846710205, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question clearly refers to a specific item with a known gold answer of 1986. The generated response incorrectly claims the document does not specify a single release year and asks for more context, directly contradicting the given gold answer. Therefore it is neither correct nor faithful to the gold.", "main_total_tokens": 3365, "main_prompt_tokens": 3327, "main_completion_tokens": 38, "judge_total_tokens": 502, "judge_prompt_tokens": 414, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The rating of the movie Forrest Gump is 8.466 [Doc 15236].", "latency_ms": 4567.438840866089, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating value 8.466 and does not introduce any contradictions or unsupported claims. It is both correct and faithful.", "main_total_tokens": 3078, "main_prompt_tokens": 3053, "main_completion_tokens": 25, "judge_total_tokens": 474, "judge_prompt_tokens": 413, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 5329.968690872192, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med', which matches the gold answer exactly. It is fully faithful to the provided information and does not introduce any contradictions or unsupported content.", "main_total_tokens": 2648, "main_prompt_tokens": 2611, "main_completion_tokens": 37, "judge_total_tokens": 493, "judge_prompt_tokens": 423, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "The seven prisoners escaped from a prison in Argentina in the movie Túnel de los huesos (2011) [Doc 53582].", "latency_ms": 5403.854131698608, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Argentina as the country where the prison is located, matching the gold answer exactly. It remains faithful to the provided information without adding any contradictory or speculative details.", "main_total_tokens": 3033, "main_prompt_tokens": 2998, "main_completion_tokens": 35, "judge_total_tokens": 487, "judge_prompt_tokens": 420, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice falls into Jiang Jie's trap [Doc 135231].", "latency_ms": 5376.488447189331, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold reference exactly, correctly stating that the apprentice falls into Jiang Jie's trap, and does not introduce any unsupported information.", "main_total_tokens": 3174, "main_prompt_tokens": 3143, "main_completion_tokens": 31, "judge_total_tokens": 467, "judge_prompt_tokens": 407, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "Based on the provided documents, there are no Korean-language movies mentioning assassins that were released after 2023. The most recent Korean-language movie related to assassins is \"Secret Reunion\" from 2010 [Doc 48744], and \"Assassination\" from 2015 [Doc 67739].", "latency_ms": 9517.46129989624, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 4 such movies, while the generated answer claims there are none, so it is factually incorrect with respect to the gold. However, the generated answer accurately reflects the contents and limitations of the provided documents, explicitly stating its conclusion is based on them, so it is faithful to the source context.", "main_total_tokens": 3095, "main_prompt_tokens": 3017, "main_completion_tokens": 78, "judge_total_tokens": 544, "judge_prompt_tokens": 447, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_040", "question_type": "unanswerable", "error": "Connection error.", "error_type": "APIConnectionError"}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 5096.450567245483, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating value of 6.3 and does not introduce any additional unsupported information beyond citing a document reference.", "main_total_tokens": 3245, "main_prompt_tokens": 3219, "main_completion_tokens": 26, "judge_total_tokens": 462, "judge_prompt_tokens": 403, "judge_completion_tokens": 59}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The provided context does not specify the style of racing for the movie \"Fast Track: No Limits.\" It only mentions that it is centered around a group of street racers [Doc 41929].", "latency_ms": 7973.667860031128, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the racing style is drift, whereas the generated answer claims the context does not specify the racing style. This is incorrect relative to the gold. It also indicates that the assistant is ignoring or contradicting the provided gold/context, so it is not faithful to the given information.", "main_total_tokens": 3447, "main_prompt_tokens": 3403, "main_completion_tokens": 44, "judge_total_tokens": 506, "judge_prompt_tokens": 417, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 7027.301549911499, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the title as \"Confronting a Serial Killer\" and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 3168, "main_prompt_tokens": 3117, "main_completion_tokens": 51, "judge_total_tokens": 515, "judge_prompt_tokens": 452, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 5258.606910705566, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold value 7.838 and does not introduce any unsupported information beyond citing a document reference, so it is both correct and faithful.", "main_total_tokens": 3276, "main_prompt_tokens": 3246, "main_completion_tokens": 30, "judge_total_tokens": 471, "judge_prompt_tokens": 407, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The provided documents do not contain a 1999 psychological thriller movie that specifically alludes to an extra sense. The closest match is \"Victim\" from 1999, but its plot does not explicitly mention an extra sense [Doc 22488].", "latency_ms": 7639.869451522827, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'The Sixth Sense', but the generated answer incorrectly states that the documents do not contain such a movie and instead mentions 'Victim'. This is factually wrong, so correctness is 0. However, the model is internally consistent with the (incorrect) premise it relies on and does not contradict its own referenced document, so faithfulness to its cited context is high.", "main_total_tokens": 3031, "main_prompt_tokens": 2972, "main_completion_tokens": 59, "judge_total_tokens": 551, "judge_prompt_tokens": 442, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 4645.694017410278, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, which matches the gold answer, and it does not introduce any contradictions or unsupported claims beyond citing a document.", "main_total_tokens": 3343, "main_prompt_tokens": 3320, "main_completion_tokens": 23, "judge_total_tokens": 465, "judge_prompt_tokens": 397, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 5584.1686725616455, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2015 reboot title as 'The Transporter Refueled' and does not introduce unsupported information apart from a harmless document reference.", "main_total_tokens": 3156, "main_prompt_tokens": 3121, "main_completion_tokens": 35, "judge_total_tokens": 490, "judge_prompt_tokens": 420, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in the year 2015 [Doc 67500].", "latency_ms": 5096.67181968689, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, stating that 'The Transporter Refueled' was released in 2015. It does not introduce any contradictory or extraneous information, and the citation style does not affect correctness or faithfulness.", "main_total_tokens": 3452, "main_prompt_tokens": 3424, "main_completion_tokens": 28, "judge_total_tokens": 486, "judge_prompt_tokens": 406, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "None of the provided documents mention a romantic comedy where Abraham Lincoln comes back from the dead. [Doc 54943] and [Doc 57213] involve supernatural elements with Abraham Lincoln, but they are categorized as 'Action', 'Fantasy', 'Horror', and 'Thriller', not romantic comedies.", "latency_ms": 8547.301292419434, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a romantic comedy where Abraham Lincoln comes back from the dead. The gold answer says such a movie description cannot be found. The generated answer correctly states that none of the documents mention a romantic comedy fitting this description, and clarifies that the ones involving supernatural Abraham Lincoln are in other genres. This matches the gold answer and does not invent unsupported details, so it is both correct and faithful to the described evidence.", "main_total_tokens": 2916, "main_prompt_tokens": 2845, "main_completion_tokens": 71, "judge_total_tokens": 569, "judge_prompt_tokens": 453, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by Ernest Hemingway is \"Havana Darkness\" [Doc 90923].", "latency_ms": 5220.260143280029, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'Havana Darkness' as the movie title. It is fully consistent with the provided information and does not introduce unsupported content.", "main_total_tokens": 2950, "main_prompt_tokens": 2917, "main_completion_tokens": 33, "judge_total_tokens": 482, "judge_prompt_tokens": 415, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the TV show \"Doll & Em\" [Doc 122259].", "latency_ms": 5562.563180923462, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em' and is fully consistent with the gold answer, with no added incorrect information.", "main_total_tokens": 3277, "main_prompt_tokens": 3243, "main_completion_tokens": 34, "judge_total_tokens": 478, "judge_prompt_tokens": 417, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The 2007 movie in which Hallam has a talent for spying on people and attempts to find the cause of his mother's death is titled \"Hallam Foe\" [Doc 39389].", "latency_ms": 6780.602693557739, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2007 movie as \"Hallam Foe,\" which matches the gold answer exactly. It does not introduce any contradictions or extraneous, ungrounded information (the citation reference is neutral), so it is both correct and faithful.", "main_total_tokens": 3359, "main_prompt_tokens": 3312, "main_completion_tokens": 47, "judge_total_tokens": 523, "judge_prompt_tokens": 439, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The provided documents do not contain information about a character whose last name sounds like they could be either a friend or a foe. Therefore, based solely on the given context, I cannot answer this question. [No relevant source]", "latency_ms": 6638.700723648071, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is a concrete label ('A foe'), so an answer is expected, not an abstention. The generated answer claims it cannot answer from the documents, which conflicts with the correct output, so it is incorrect. However, it is faithful to the (unstated) context in that it does not hallucinate an answer and honestly reports lack of evidence, so it is fully faithful to its own reasoning and the cited context.", "main_total_tokens": 3352, "main_prompt_tokens": 3306, "main_completion_tokens": 46, "judge_total_tokens": 549, "judge_prompt_tokens": 431, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "Sam, the character in the 2017 TV show Atypical, is 18 years old. [Doc 124583]", "latency_ms": 5252.4755001068115, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old, which matches the gold answer. There is no contradiction or hallucination relative to the provided information.", "main_total_tokens": 2942, "main_prompt_tokens": 2908, "main_completion_tokens": 34, "judge_total_tokens": 477, "judge_prompt_tokens": 414, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 4719.010353088379, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which matches the gold answer exactly. It is fully faithful to the provided information.", "main_total_tokens": 2862, "main_prompt_tokens": 2833, "main_completion_tokens": 29, "judge_total_tokens": 470, "judge_prompt_tokens": 410, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Sandra was born in Chamonix according to the plot description of the movie \"À corde tendue\" [Doc 59659].", "latency_ms": 5503.409385681152, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, which matches the gold answer exactly. It also remains faithful to the provided information without introducing contradictions or irrelevant details.", "main_total_tokens": 3331, "main_prompt_tokens": 3298, "main_completion_tokens": 33, "judge_total_tokens": 491, "judge_prompt_tokens": 424, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "in this japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 5015.943765640259, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly fills the blank with 'rains' and matches the provided gold answer exactly, restating it faithfully without adding incorrect information.", "main_total_tokens": 3604, "main_prompt_tokens": 3578, "main_completion_tokens": 26, "judge_total_tokens": 472, "judge_prompt_tokens": 412, "judge_completion_tokens": 60}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In the German language animation \"Tabaluga\", Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 5884.748458862305, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which matches the gold answer. It is fully consistent with the provided information and does not introduce contradictions or unsupported details, so both correctness and faithfulness are maximal.", "main_total_tokens": 3468, "main_prompt_tokens": 3431, "main_completion_tokens": 37, "judge_total_tokens": 506, "judge_prompt_tokens": 426, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is \"One Afternoon\" [Doc 96838].", "latency_ms": 6535.545110702515, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"One Afternoon,\" which matches the gold answer exactly. It stays fully grounded in the provided information without adding unsupported details, so it is both correct and faithful.", "main_total_tokens": 2748, "main_prompt_tokens": 2698, "main_completion_tokens": 50, "judge_total_tokens": 519, "judge_prompt_tokens": 448, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie where a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" [Doc 93260].", "latency_ms": 9496.634006500244, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Life Ahead,' which matches the gold answer. It stays faithful to the information implied in the prompt, only restating the correct title with a brief description consistent with the question and gold.", "main_total_tokens": 3208, "main_prompt_tokens": 3167, "main_completion_tokens": 41, "judge_total_tokens": 512, "judge_prompt_tokens": 434, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary \"McLaren\" is about Bruce McLaren, a New Zealander who founded the McLaren Motor Racing team. [Doc 73859]", "latency_ms": 5787.949562072754, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and adds accurate contextual information (his nationality and founding the McLaren Motor Racing team). It matches the gold answer and does not introduce unsupported or incorrect claims, so it is both correct and faithful.", "main_total_tokens": 3042, "main_prompt_tokens": 3002, "main_completion_tokens": 40, "judge_total_tokens": 506, "judge_prompt_tokens": 419, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The provided documents do not specify the name of any particular racing team associated with a main character, so I cannot provide the name of his racing team based solely on the given context.", "latency_ms": 5759.133815765381, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly states the racing team is 'The McLaren Motor Racing team'. The generated answer claims the documents do not specify the team name and refuses to answer, which directly contradicts the gold. Therefore, it is factually incorrect. However, it is faithful to the (incorrect) reasoning it presents about the given context, so faithfulness is high, but correctness relative to the gold is zero.", "main_total_tokens": 3198, "main_prompt_tokens": 3161, "main_completion_tokens": 37, "judge_total_tokens": 527, "judge_prompt_tokens": 419, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The provided context does not specify a particular country for a singular \"he\" that would apply across all documents. Different documents refer to characters or individuals from various countries, such as Poland, Germany, the US, and others, but there isn't a specific \"he\" whose country of origin is mentioned in a way that answers the question directly. [Doc X]", "latency_ms": 8841.414451599121, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly specifies 'New Zealand' as the correct country, but the generated answer claims the context does not specify a particular country and does not provide 'New Zealand'. This makes the generated answer incorrect. However, it appears to faithfully describe what it (incorrectly) believes about the context without hallucinating a specific country, so it is faithful to its own reading of the context, just not correct relative to the gold.", "main_total_tokens": 2833, "main_prompt_tokens": 2759, "main_completion_tokens": 74, "judge_total_tokens": 565, "judge_prompt_tokens": 448, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based on the provided documents, there is no movie from 2013 where John McClane blows up the Great Wall of China. The movie \"A Good Day to Die Hard\" from 2013 features John McClane in Moscow, not at the Great Wall of China [Doc 58791]. Additionally, \"The Great Wall,\" which involves the Great Wall of China, is from 2016 and does not feature John McClane [Doc 72056].", "latency_ms": 12375.617980957031, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that there is no such 2013 movie featuring John McClane blowing up the Great Wall of China. The generated answer is consistent with the (summarized) documents it cites and adds accurate contextual detail about actual films, so it is both correct and faithful.", "main_total_tokens": 3452, "main_prompt_tokens": 3346, "main_completion_tokens": 106, "judge_total_tokens": 587, "judge_prompt_tokens": 494, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with the given description is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 5413.248538970947, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Tomie: Forbidden Fruit' and does not introduce any contradictory or extraneous information.", "main_total_tokens": 3310, "main_prompt_tokens": 3278, "main_completion_tokens": 32, "judge_total_tokens": 506, "judge_prompt_tokens": 443, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years in the 2021 movie \"Lei mi parla ancora\" [Doc 98566].", "latency_ms": 6374.950647354126, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (65 years) exactly and stays within the information given, so it is both correct and faithful.", "main_total_tokens": 3713, "main_prompt_tokens": 3674, "main_completion_tokens": 39, "judge_total_tokens": 482, "judge_prompt_tokens": 424, "judge_completion_tokens": 58}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In \"Scorpion King: Book of Souls\" (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 6853.812456130981, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in 'Scorpion King: Book of Souls' (2018), The Scorpion King and Tala are searching for The Book of Souls, matching the gold answer and not adding contradictory information. The added description as a legendary relic is consistent with the film and does not conflict with the source.", "main_total_tokens": 3712, "main_prompt_tokens": 3668, "main_completion_tokens": 44, "judge_total_tokens": 526, "judge_prompt_tokens": 431, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document does not contain information about a movie titled \"Zombies on the Moon\". [Not Found]", "latency_ms": 4578.29737663269, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer explicitly states that the movie is not in the database, while the generated answer asserts that the document does not contain information about the movie. This changes the source of the limitation (database vs. document), so the generated response is not faithful to the gold and thus is incorrect with respect to the labeled answer.", "main_total_tokens": 3437, "main_prompt_tokens": 3415, "main_completion_tokens": 22, "judge_total_tokens": 507, "judge_prompt_tokens": 412, "judge_completion_tokens": 95}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain any information about a movie or TV show titled \"The Galactic baker\". Therefore, the release year cannot be determined from the given context.", "latency_ms": 5457.199573516846, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the assistant does not have the movie in its database. The generated answer states that the provided documents do not contain information about the title and that the release year cannot be determined. Both effectively communicate that the release year is unknown/unavailable. There is no hallucinated year or extra fabricated details, and it remains grounded in the (hypothetical) context of the provided documents. Thus it is fully correct and faithful.", "main_total_tokens": 3221, "main_prompt_tokens": 3187, "main_completion_tokens": 34, "judge_total_tokens": 540, "judge_prompt_tokens": 422, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain any information about a movie titled \"Return of the Null Pointer\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 5550.272703170776, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information about the movie in the provided documents and does not fabricate a release year. This matches the gold answer in substance (that the movie is not in the database / context), so it is both correct and faithful to the given information.", "main_total_tokens": 2891, "main_prompt_tokens": 2853, "main_completion_tokens": 38, "judge_total_tokens": 515, "judge_prompt_tokens": 428, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"Data Science: The Movie\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 5861.860990524292, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not contain information about \"Data Science: The Movie\" and refrains from hallucinating a release year. This matches the gold answer’s stance of lacking the movie in the database, so it is both correct and faithful to the (implicit) context.", "main_total_tokens": 3397, "main_prompt_tokens": 3360, "main_completion_tokens": 37, "judge_total_tokens": 517, "judge_prompt_tokens": 427, "judge_completion_tokens": 90}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"Python's Revenge 2\". [Doc 28408] mentions a movie titled \"Python 2\" from the year 2002, and [Doc 24222] mentions another movie titled \"Python\" from the year 2000, but there is no information about \"Python's Revenge 2\".", "latency_ms": 10373.709917068481, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no information about a movie titled \"Python's Revenge 2\" in the provided documents, matching the intent of the gold answer (that it is not in the database). It accurately references the documents about related but different movies (\"Python 2\" from 2002 and \"Python\" from 2000) and does not fabricate a release year, staying faithful to the given evidence.", "main_total_tokens": 3276, "main_prompt_tokens": 3191, "main_completion_tokens": 85, "judge_total_tokens": 582, "judge_prompt_tokens": 465, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain any information about a movie titled \"The Last Pixel\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 5357.395172119141, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The gold answer reflects the assistant's overall knowledge base, while the generated answer restricts itself to the provided documents. Although the generated answer is correct in stating the documents lack this information, it introduces a narrower context than the user question specifies, making it unfaithful to the intended scope of the task.", "main_total_tokens": 2890, "main_prompt_tokens": 2854, "main_completion_tokens": 36, "judge_total_tokens": 516, "judge_prompt_tokens": 424, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"The man who ate the sun\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 6292.466640472412, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates that the system lacks information about the movie, and the generated answer similarly states that the provided documents do not contain information about that movie and thus it cannot provide the release year. This is correct, since it avoids hallucinating a year, and faithful to the given context that there is no such information in the documents.", "main_total_tokens": 3713, "main_prompt_tokens": 3675, "main_completion_tokens": 38, "judge_total_tokens": 527, "judge_prompt_tokens": 429, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not specify who directed the movie \"The Last Days of Lehman Brothers\". [Doc 46069]", "latency_ms": 5306.140422821045, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided documents/database. This is factually correct given the constraint and the generated answer is fully faithful to the cited document context, explicitly noting the absence of that information.", "main_total_tokens": 3602, "main_prompt_tokens": 3573, "main_completion_tokens": 29, "judge_total_tokens": 502, "judge_prompt_tokens": 422, "judge_completion_tokens": 80}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about who directed the movie \"A Love to Die For\". [Doc 107701] mentions a movie titled \"Her Affair to Die For\" but does not provide any director's name. There is no document that specifies the director of \"A Love to Die For\".", "latency_ms": 8601.46188735962, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not provide the director information for \"A Love to Die For\" and refrains from guessing. It remains fully grounded in the given documents and stays faithful to the limitation indicated in the gold answer.", "main_total_tokens": 3261, "main_prompt_tokens": 3194, "main_completion_tokens": 67, "judge_total_tokens": 535, "judge_prompt_tokens": 456, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not specify who directed the movie \"Journey into Gaza\". [Doc 111360]", "latency_ms": 4845.904111862183, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not specify who directed the movie, which aligns with the gold answer that the director information is not provided in the database. It does not introduce any unsupported information and is faithful to the given context.", "main_total_tokens": 3203, "main_prompt_tokens": 3176, "main_completion_tokens": 27, "judge_total_tokens": 493, "judge_prompt_tokens": 414, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about the director of the movie \"The Last Night at Tremore Beach\". [Doc 135098]", "latency_ms": 5341.247797012329, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided documents/database. The generated answer is factually consistent with the gold and does not introduce unsupported information, so it is correct and faithful.", "main_total_tokens": 3155, "main_prompt_tokens": 3122, "main_completion_tokens": 33, "judge_total_tokens": 500, "judge_prompt_tokens": 425, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The scenario where a group of archaeology students awaken an ancient mummy occurs in the movie \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 5489.36128616333, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'Rise of the Mummy' and does not introduce contradictions or unsupported claims. It is both correct and faithful to the provided gold answer.", "main_total_tokens": 3046, "main_prompt_tokens": 3010, "main_completion_tokens": 36, "judge_total_tokens": 487, "judge_prompt_tokens": 420, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The scenario where Sarah fakes her own death to escape an abusive spouse occurs in the movie \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 5395.105361938477, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Stranger in My Bed,\" which matches the gold answer exactly. It does not introduce any contradictions or unsupported details beyond referencing a document, so it is both correct and faithful.", "main_total_tokens": 3018, "main_prompt_tokens": 2983, "main_completion_tokens": 35, "judge_total_tokens": 502, "judge_prompt_tokens": 427, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The scenario where a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years is described in the plot of \"Game of Silence\" [Doc 123938]. However, it is important to note that \"Game of Silence\" is listed as a TV show and not a movie.", "latency_ms": 7920.007705688477, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The user asked specifically for a movie, and the gold answer is 'Game of Silence.' The generated answer correctly identifies 'Game of Silence' as matching the described scenario but incorrectly emphasizes that it is a TV show and not a movie, thereby failing to directly answer the question as posed. Under the given evaluation, this is treated as incorrect. Additionally, the reference to a specific document ID without evidence in the prompt makes the statement about the source ungrounded, reducing faithfulness.", "main_total_tokens": 2546, "main_prompt_tokens": 2479, "main_completion_tokens": 67, "judge_total_tokens": 585, "judge_prompt_tokens": 458, "judge_completion_tokens": 127}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie in which a killer from the site Craigslist is featured, based on a true story, is \"The Craigslist Killer\" [Doc 53039].", "latency_ms": 5347.0470905303955, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer,' which matches the gold answer exactly. It remains faithful to the given information and does not introduce contradictions or unsupported claims.", "main_total_tokens": 2900, "main_prompt_tokens": 2865, "main_completion_tokens": 35, "judge_total_tokens": 490, "judge_prompt_tokens": 423, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The described plot happens in the movie \"Wood Job!\" [Doc 66094].", "latency_ms": 4483.417987823486, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", which matches the gold answer exactly. It does not introduce any contradictions or extraneous incorrect details, only restating the correct title, so it is both correct and faithful.", "main_total_tokens": 3358, "main_prompt_tokens": 3337, "main_completion_tokens": 21, "judge_total_tokens": 495, "judge_prompt_tokens": 417, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The event described happens in the movie \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 13265.33818244934, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Sesame Street: The Magical Wand Chase' and does not introduce any contradictions or extraneous claims.", "main_total_tokens": 3027, "main_prompt_tokens": 2999, "main_completion_tokens": 28, "judge_total_tokens": 499, "judge_prompt_tokens": 433, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The plot you described occurs in the movie \"Beyond\" from the year 2016 [Doc 74597].", "latency_ms": 4365.483522415161, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot; the gold answer is \"Beyond.\" The generated answer states that the plot occurs in the movie \"Beyond\" and matches the gold exactly, only adding the release year 2016. This added detail is plausible and does not contradict the provided information. Therefore, the answer is both correct and faithful to the gold.", "main_total_tokens": 2575, "main_prompt_tokens": 2546, "main_completion_tokens": 29, "judge_total_tokens": 534, "judge_prompt_tokens": 431, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The scenario where a woman, fired from a financial corporation during the Asia crisis, returns home with no money occurs in the movie \"6ixtynin9\" [Doc 22478].", "latency_ms": 5921.195507049561, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', which matches the gold answer exactly. It faithfully restates the scenario without adding unsupported details and attributes it to the cited document, so both correctness and faithfulness are high.", "main_total_tokens": 2661, "main_prompt_tokens": 2618, "main_completion_tokens": 43, "judge_total_tokens": 522, "judge_prompt_tokens": 441, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The described scenario happens in the movie \"Club Dread\" [Doc 32410].", "latency_ms": 4358.926057815552, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer exactly. It remains faithful to the query and does not introduce any contradictions or unsupported content; the bracketed document reference is harmless metadata.", "main_total_tokens": 3196, "main_prompt_tokens": 3175, "main_completion_tokens": 21, "judge_total_tokens": 513, "judge_prompt_tokens": 442, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The described scenario occurs in the movie \"Toilet\" [Doc 48873].", "latency_ms": 4406.757354736328, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" which matches the gold answer exactly. It does not introduce any additional, unsupported information and stays faithful to the given question and gold label.", "main_total_tokens": 3377, "main_prompt_tokens": 3356, "main_completion_tokens": 21, "judge_total_tokens": 501, "judge_prompt_tokens": 431, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The described scenario happens in the movie \"Electric Angel\" [Doc 1024].", "latency_ms": 3954.0750980377197, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer identifies the movie as \"Electric Angel,\" which exactly matches the gold answer. There are no contradictions or unsupported additions beyond restating the correct title, so the response is both correct and faithful.", "main_total_tokens": 2981, "main_prompt_tokens": 2961, "main_completion_tokens": 20, "judge_total_tokens": 505, "judge_prompt_tokens": 433, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 based on the provided documents is \"Rare Objects\" with a rating of 4.3 [Doc 105190].", "latency_ms": 4413.612365722656, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer identifies 'Mega Ape' as the lowest rated movie from 2023, while the generated answer claims it is 'Rare Objects' with a rating of 4.3. This directly contradicts the gold, so the correctness is 0. The generated answer does cite a specific document ID and rating, suggesting partial grounding in the provided documents, but since it conflicts with the gold reference answer, its faithfulness to the task-defined truth is only partial, so 0.5.", "main_total_tokens": 3111, "main_prompt_tokens": 3072, "main_completion_tokens": 39, "judge_total_tokens": 546, "judge_prompt_tokens": 413, "judge_completion_tokens": 133}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "Based on the provided documents, the highest rated TV show from 2010 is \"Hawaii Five-0\" with a rating of 7.755 [Doc 119141].", "latency_ms": 4953.600883483887, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the highest rated TV show from 2010 is 'Tea House', whereas the generated answer claims it is 'Hawaii Five-0' with a rating of 7.755. This directly contradicts the gold answer, so the response is incorrect. It is also unfaithful with respect to the gold, since it replaces the correct show with a different one and introduces an unsupported rating detail.", "main_total_tokens": 3573, "main_prompt_tokens": 3527, "main_completion_tokens": 46, "judge_total_tokens": 534, "judge_prompt_tokens": 419, "judge_completion_tokens": 115}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest rated Japanese language movie from 1999 based on the provided documents is \"Kikujiro\" with a rating of 7.5 [Doc 22213].", "latency_ms": 6299.431562423706, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer identifies 'The Uncut Story of Mewtwo's Origin' as the highest rated Japanese language movie from 1999, while the generated answer claims it is 'Kikujiro' with a rating of 7.5. This directly contradicts the gold answer, so the response is factually incorrect relative to the labeled ground truth. Additionally, it references 'the provided documents' (e.g., [Doc 22213]) which are not present in the given context, so the claim is not grounded in the visible information, reducing faithfulness.", "main_total_tokens": 3379, "main_prompt_tokens": 3337, "main_completion_tokens": 42, "judge_total_tokens": 573, "judge_prompt_tokens": 427, "judge_completion_tokens": 146}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "To determine the language of the lowest-rated movie from 1980, I will first identify all movies from 1980 and their ratings:\n\n1. A Story of No Importance - Rating: 5.0 [Doc 849]\n2. Poor Ioanide - Rating: 7.0 [Doc 852]\n3. Refused - Rating: 7.5 [Doc 250]\n4. Sparsh - Rating: 7.3 [Doc 801]\n5. Don't Answer the Phone! - Rating: 4.6 [Doc 768]\n6. Manganinnie - Rating: 6.4 [Doc 435]\n7. Say a Word for the Poor Hussar - Rating: 6.814 [Doc 322]\n8. The Precarious Bank Teller - Rating: 5.5 [Doc 378]\n9. Blank Generation - Rating: 5.4 [Doc 534]\n10. Shaaban Below Zero - Rating: 6.286 [Doc 131]\n11. Melodrama? - Rating: 6.0 [Doc 167]\n12. The Top Scorer - Rating: 6.317 [Doc 471]\n13. Windows - Rating: 4.963 [Doc 693]\n14. The Under-Gifted - Rating: 6.439 [Doc 71]\n15. Little Tragedies - Rating: 5.4 [Doc 14]\n\nThe lowest-rated movie from this list is \"Don't Answer the Phone!\" with a rating of 4.6. According to [Doc 768], the language of this movie is English (en).\n\nTherefore, the language of the lowest-rated movie from 1980 is English.", "latency_ms": 40807.684659957886, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980. The generated answer identifies all 1980 movies with their ratings, correctly finds that 'Don't Answer the Phone!' with rating 4.6 is the lowest, and states that its language is English, matching the gold answer. The reasoning is consistent with the described documents and does not invent unsupported facts, so it is both correct and faithful.", "main_total_tokens": 3604, "main_prompt_tokens": 3196, "main_completion_tokens": 408, "judge_total_tokens": 851, "judge_prompt_tokens": 736, "judge_completion_tokens": 115}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show in 1999 with the highest popularity was \"Freaks and Geeks\" with a popularity score of 5.8043 [Doc 115187].", "latency_ms": 7024.0490436553955, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is factually incorrect relative to the gold label: the correct show is \"Law & Order: Special Victims Unit,\" not \"Freaks and Geeks.\" It is also unfaithful because it fabricates a specific popularity score and cites a document ID without basis in the provided gold answer.", "main_total_tokens": 3722, "main_prompt_tokens": 3677, "main_completion_tokens": 45, "judge_total_tokens": 520, "judge_prompt_tokens": 426, "judge_completion_tokens": 94}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 4457.583427429199, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and explicitly cites the document, so it is both correct and faithful to the provided information.", "main_total_tokens": 3188, "main_prompt_tokens": 3166, "main_completion_tokens": 22, "judge_total_tokens": 457, "judge_prompt_tokens": 400, "judge_completion_tokens": 57}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in multiple years according to the documents:\n\n- In 1999 [Doc 22481]\n- In 2019 [Doc 88811]\n- In 2025 [Doc 112077]", "latency_ms": 8769.490003585815, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question asks for the release year of the movie \"The Story of Us\" and the gold answer is 2019. The generated answer lists three different years (1999, 2019, 2025) and does not select the correct single year, so it is not correct. It also fabricates multiple conflicting release years instead of directly answering with the gold year, so it is not faithful to the required answer.", "main_total_tokens": 3206, "main_prompt_tokens": 3138, "main_completion_tokens": 68, "judge_total_tokens": 552, "judge_prompt_tokens": 435, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The provided documents do not contain specific information about the number of votes \"My Dog Skip\" received. [Doc 24114]", "latency_ms": 4967.721939086914, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates that 'My Dog Skip' received 279 votes, but the generated answer claims that the documents do not contain this information and does not provide the correct number. This makes the answer factually incorrect. However, the answer is faithful to the cited document snippet, which apparently lacks the vote count, so it does not invent unsupported details.", "main_total_tokens": 2935, "main_prompt_tokens": 2905, "main_completion_tokens": 30, "judge_total_tokens": 504, "judge_prompt_tokens": 407, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The provided documents do not contain information about the number of votes for \"Oh My God!\". Therefore, I cannot provide that information based on the given context.", "latency_ms": 5326.725482940674, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that 'Oh My God!' received 12 votes. The generated answer claims that the documents do not contain this information and therefore does not provide the number, which is incorrect relative to the gold. However, given the model is constrained to the provided context and explicitly notes the lack of information rather than fabricating a number, it remains faithful to the context. Thus correctness is 0, faithfulness is 1.", "main_total_tokens": 3247, "main_prompt_tokens": 3215, "main_completion_tokens": 32, "judge_total_tokens": 530, "judge_prompt_tokens": 412, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 5281.519651412964, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie \"The Last Days of Lehman Brothers\" was released in 2009, which matches the gold answer. It is fully consistent with the provided information, so both correctness and faithfulness are satisfied.", "main_total_tokens": 3570, "main_prompt_tokens": 3541, "main_completion_tokens": 29, "judge_total_tokens": 489, "judge_prompt_tokens": 410, "judge_completion_tokens": 79}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 4650.044679641724, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the original language of 'The King’s Speech' is English, which matches the gold answer 'en' (English). The citation format does not contradict anything, so the answer is fully correct and faithful.", "main_total_tokens": 3339, "main_prompt_tokens": 3315, "main_completion_tokens": 24, "judge_total_tokens": 480, "judge_prompt_tokens": 402, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "Wall Street was released in 1987 [Doc 6993].", "latency_ms": 4108.830451965332, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer. It is also faithful to the provided information, with no hallucinated or contradictory content aside from an innocuous document reference.", "main_total_tokens": 2419, "main_prompt_tokens": 2400, "main_completion_tokens": 19, "judge_total_tokens": 467, "judge_prompt_tokens": 392, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The classmates have to play a survival game using their cell phones in the movie \"Brain Fluid Explosion Girl\" [Doc 69929].", "latency_ms": 5177.01268196106, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Brain Fluid Explosion Girl,' which matches the gold answer exactly. It is faithful to the information implied by the question and does not introduce unsupported details beyond the title and brief description.", "main_total_tokens": 2068, "main_prompt_tokens": 2037, "main_completion_tokens": 31, "judge_total_tokens": 490, "judge_prompt_tokens": 415, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show in which six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" [Doc 112849].", "latency_ms": 5451.748609542847, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Dungeons & Dragons,' which matches the gold answer exactly. It remains faithful to the provided information without adding incorrect details.", "main_total_tokens": 2059, "main_prompt_tokens": 2024, "main_completion_tokens": 35, "judge_total_tokens": 485, "judge_prompt_tokens": 420, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie that is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is \"Nacht der Wölfe\" [Doc 2590].", "latency_ms": 7257.222414016724, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'Nacht der Wölfe' as the movie about two rival gangs in Munich. It is fully supported by the cited document reference and does not introduce any unsupported information.", "main_total_tokens": 2107, "main_prompt_tokens": 2063, "main_completion_tokens": 44, "judge_total_tokens": 516, "judge_prompt_tokens": 439, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of Picnic at Hanging Rock take place in the year 1900 [Doc 126191].", "latency_ms": 5192.269325256348, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies 1900 as the year in which the events of Picnic at Hanging Rock take place, matching the gold answer exactly. It is faithful to the provided information and does not introduce contradictions or extraneous claims.", "main_total_tokens": 2372, "main_prompt_tokens": 2343, "main_completion_tokens": 29, "judge_total_tokens": 483, "judge_prompt_tokens": 406, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie that matches the description is \"June\" [Doc 32483]. The plot revolves around June, a violinist who works in a coffee shop in Venice, rehearses for her music career at night, and daydreams about her destined love. The character June also happens to have the name of a calendar month.", "latency_ms": 9458.250999450684, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"June,\" matching the gold answer exactly. The explanation provided is consistent with the question’s description and does not introduce contradictions or unsupported claims beyond elaborating the plot in line with the prompt, so it is faithful to the given information.", "main_total_tokens": 2408, "main_prompt_tokens": 2333, "main_completion_tokens": 75, "judge_total_tokens": 587, "judge_prompt_tokens": 501, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1329.460620880127, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a particular 1980 Russian-language movie, but the generated answer says \"I don't know,\" which is incorrect. However, it does not introduce any fabricated information and is faithful about its lack of knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 399, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 344.052791595459, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific rating (7.6), but the generated answer states ignorance. While this is honest and thus faithful (it does not fabricate information), it does not provide the correct rating, so correctness is 0.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 467, "judge_prompt_tokens": 389, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1154.555320739746, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.366123019050001, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is incorrect. The assistant does not appear to be grounded in the given gold data, so despite being consistent with itself, it is not factually correct with respect to the reference.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 394, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 296.5831756591797, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. However, the assistant directly answered the question without fabricating any reasoning or external information, so it is faithful to its (incorrect) output.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 474, "judge_prompt_tokens": 395, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1689.101219177246, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct specific show ('Fantômas'), so it is incorrect. However, it does not introduce any hallucinated information and accurately reflects uncertainty, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 468, "judge_prompt_tokens": 389, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie that is about two British beauties going to Barbados with a yacht captain is \"Kill Cruise\" from 1990 [Doc 11205].", "latency_ms": 5311.861276626587, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise,' matching the gold answer exactly. It is fully consistent with the provided information and does not introduce unsupported claims.", "main_total_tokens": 2285, "main_prompt_tokens": 2247, "main_completion_tokens": 38, "judge_total_tokens": 479, "judge_prompt_tokens": 415, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838].", "latency_ms": 6661.359071731567, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as \"Won't You Be My Neighbor?\" and does not introduce any contradictions or unsupported claims beyond a neutral document reference.", "main_total_tokens": 2578, "main_prompt_tokens": 2535, "main_completion_tokens": 43, "judge_total_tokens": 510, "judge_prompt_tokens": 440, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The question \"What year was it released?\" does not provide enough context to determine which specific movie release year is being asked for. Multiple movies with different release years are listed in the provided documents. Therefore, based solely on the given question without additional context, I cannot specify a single release year. [Doc 8389] [Doc 675] [Doc 19562] [Doc 25738] [Doc 107589] [Doc 91657] [Doc 94285] [Doc 45703] [Doc 62279] [Doc 82497] [Doc 108737] [Doc 122951] [Doc 135252] [Doc 72859] [Doc 13886]", "latency_ms": 21242.871046066284, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer expects the specific year 2018, so the correct response should have been '2018.' The generated answer instead claims there is insufficient context and does not provide the year, which is incorrect relative to the gold. However, given what the generated answer states about ambiguity, it consistently follows that reasoning and does not introduce fabricated facts, so it is faithful to its own (albeit mistaken) assessment of the context.", "main_total_tokens": 2763, "main_prompt_tokens": 2567, "main_completion_tokens": 196, "judge_total_tokens": 642, "judge_prompt_tokens": 525, "judge_completion_tokens": 117}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that is an animation about a young boy robot in 2030 AD is \"Astro Boy\" [Doc 112591].", "latency_ms": 5818.901777267456, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy,' which matches the gold answer exactly. It is fully supported by the provided information and includes the same content, so it is both correct and faithful.", "main_total_tokens": 2266, "main_prompt_tokens": 2230, "main_completion_tokens": 36, "judge_total_tokens": 490, "judge_prompt_tokens": 416, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 5538.4461879730225, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer. It does not introduce any unsupported information and is fully consistent with the given reference.", "main_total_tokens": 1980, "main_prompt_tokens": 1944, "main_completion_tokens": 36, "judge_total_tokens": 496, "judge_prompt_tokens": 424, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26m before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 6539.9675369262695, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26m before the break-in on 26 November 1983, matching the provided gold answer exactly and staying faithful to it without adding unsupported information.", "main_total_tokens": 2297, "main_prompt_tokens": 2253, "main_completion_tokens": 44, "judge_total_tokens": 508, "judge_prompt_tokens": 428, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6399.587631225586, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model said it did not know the answer, while the gold answer provides a specific numeric value (80.5772). Therefore the response is incorrect. However, it did not fabricate any information and clearly admitted uncertainty, so it is fully faithful to its own stated knowledge limits.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 396, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 437.2231960296631, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer ('0') does not match the gold answer ('1'), so it is incorrect. However, it is a direct, concise answer to the question without hallucinated explanation, so faithfulness to the (incorrect) stated answer is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 466, "judge_prompt_tokens": 385, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 6249.1700649261475, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Angel Eyes'. It is fully correct and aligns with the provided information.", "main_total_tokens": 2652, "main_prompt_tokens": 2612, "main_completion_tokens": 40, "judge_total_tokens": 490, "judge_prompt_tokens": 429, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French [Doc 110681].", "latency_ms": 4610.09669303894, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states that the movie language is French, while the gold answer says Korean. This is factually incorrect. There is no indication that the source document supports the claim of French, so the answer is also unfaithful to the provided ground truth.", "main_total_tokens": 2313, "main_prompt_tokens": 2291, "main_completion_tokens": 22, "judge_total_tokens": 476, "judge_prompt_tokens": 393, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 624.5684623718262, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2 but the generated answer is 0, so the result is incorrect. The evaluation is faithful to the provided gold and generated answers.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 457, "judge_prompt_tokens": 394, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "The provided context does not contain any movies released in 1979. Therefore, it is not possible to determine the best-rated movie from that year based on the given information.", "latency_ms": 5163.387060165405, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that there are no movies from 1979 in the database. The generated answer correctly mirrors this limitation, stating that the provided context has no movies from 1979 and thus the best-rated movie cannot be determined. This is factually correct given the gold and fully faithful to it, with no hallucinated information or contradictions.", "main_total_tokens": 2233, "main_prompt_tokens": 2195, "main_completion_tokens": 38, "judge_total_tokens": 530, "judge_prompt_tokens": 430, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The provided context does not contain any information about a fourth part of the Back to the Future series being released. Therefore, based solely on the given documents, there is no year of release for \"Back to the Future Part IV.\" [Doc X]", "latency_ms": 8092.211484909058, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no release year for 'Back to the Future Part IV' because such a film does not exist, matching the gold answer in substance. It is faithful to the provided context, explicitly tying the lack of information to the documents and not hallucinating a release year.", "main_total_tokens": 3157, "main_prompt_tokens": 3106, "main_completion_tokens": 51, "judge_total_tokens": 541, "judge_prompt_tokens": 450, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that there are no movies from 1980 about the Chernobyl disaster, which is correct since the disaster occurred in 1986. The model does not hallucinate a popularity score and instead suggests rechecking the premise, remaining faithful to the question’s constraints and not adding unsupported facts.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 530, "judge_prompt_tokens": 434, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 188.92383575439453, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 titles, but the generated answer is 0, which is incorrect. The evaluation correctly identifies this discrepancy without adding unsupported information, so it is faithful to the given data but factually wrong relative to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 388, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 265.3341293334961, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that 2 movies in 2025 allude to the Holocaust, whereas the generated answer is 0. This directly contradicts the gold label, so the generated answer is factually incorrect relative to the evaluation standard (correctness = 0). The model did answer the user’s question directly with a numeric value and did not introduce unrelated information, so it is internally consistent and faithful to the prompt structure (faithfulness = 1).", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 509, "judge_prompt_tokens": 386, "judge_completion_tokens": 123}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 374.7127056121826, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there was 1 TV show about Super Mario Bros released in the 1990s, but the generated answer says 0. This numerical fact is incorrect, so correctness is 0. However, the generated answer directly responds to the question without adding fabricated details or unrelated information, so it is faithful to the question context, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 498, "judge_prompt_tokens": 389, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 140.3357982635498, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (0), which is correct: there were no TV shows released between 2000 and 2010 about Super Mario Bros. The response is fully faithful to the provided gold label.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 466, "judge_prompt_tokens": 391, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 243.77942085266113, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Naruto', but the generated answer says 'I don't know', which is incorrect. However, the model does not hallucinate any incorrect facts; it simply expresses a lack of knowledge, so it is fully faithful to its own (stated) uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 473, "judge_prompt_tokens": 386, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The provided documents do not contain any information about a Jurassic Park/Power Ranger crossover movie or its release date. [Doc X]", "latency_ms": 4415.9440994262695, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer states that the movie was not released. The generated answer instead claims that the documents do not contain any information about such a movie or its release date, which does not correctly answer the question and is therefore incorrect. It is partially faithful to the (unstated) evidence in that it does not fabricate a release date, but it fails to state the key gold fact that the movie was not released.", "main_total_tokens": 2495, "main_prompt_tokens": 2468, "main_completion_tokens": 27, "judge_total_tokens": 526, "judge_prompt_tokens": 411, "judge_completion_tokens": 115}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "None of the provided documents mention a movie where Barbie fights the Predator. [Doc X]", "latency_ms": 4522.906303405762, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "Gold answer states explicitly that the movie was not released, while the generated answer instead comments on the content of provided documents and does not answer the question. It neither matches the correct resolution nor follows the gold explanation, so it is incorrect and unfaithful.", "main_total_tokens": 2532, "main_prompt_tokens": 2513, "main_completion_tokens": 19, "judge_total_tokens": 482, "judge_prompt_tokens": 400, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "Based on the provided context, there isn't a direct list of Star Wars movies ordered by their release date. However, we can infer from the information given that \"Return of the Jedi\" (1983) is one of the older films in the franchise [Doc 2858]. To determine the 4th oldest movie, we would need more details about the release dates of other Star Wars movies. The context does not provide enough information to definitively state which is the 4th oldest Star Wars movie.", "latency_ms": 12618.606567382812, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Star Wars: Droids - The Great Heep', but the generated answer claims the question cannot be answered from the context and does not provide the correct movie, so it is factually incorrect. However, it remains faithful to the (limited) provided context by honestly stating that the context is insufficient rather than hallucinating an answer.", "main_total_tokens": 2592, "main_prompt_tokens": 2484, "main_completion_tokens": 108, "judge_total_tokens": 593, "judge_prompt_tokens": 491, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The question \"What year was it released?\" does not provide enough context to determine which specific movie release year is being asked for. Multiple movies with different release years are listed in the provided documents. Therefore, based solely on the given question without additional context, I cannot specify a single release year. [Doc 8389] [Doc 675] [Doc 19562] [Doc 25738] [Doc 107589] [Doc 91657] [Doc 94285] [Doc 45703] [Doc 62279] [Doc 82497] [Doc 108737] [Doc 122951] [Doc 135252] [Doc 72859] [Doc 13886]", "latency_ms": 21542.506456375122, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer, given the gold label, is 1986. The generated answer claims that there is not enough context to determine the year and thus does not provide 1986, so it is incorrect. However, the reasoning is faithful to the (limited) information in the user query as presented and does not introduce fabricated facts; it accurately notes lack of context from the question alone.", "main_total_tokens": 2763, "main_prompt_tokens": 2567, "main_completion_tokens": 196, "judge_total_tokens": 634, "judge_prompt_tokens": 525, "judge_completion_tokens": 109}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 187.2093677520752, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific numeric rating (8.466), while the generated answer states 'I don't know.' This is not the correct rating, so correctness is 0. However, the model does not hallucinate an incorrect rating and honestly admits lack of knowledge, so faithfulness to its own stated uncertainty is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 490, "judge_prompt_tokens": 398, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 4839.745283126831, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med,' which matches the gold answer exactly. It is fully faithful to the information provided in the prompt.", "main_total_tokens": 2019, "main_prompt_tokens": 1982, "main_completion_tokens": 37, "judge_total_tokens": 487, "judge_prompt_tokens": 423, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in Argentina [Doc 53582].", "latency_ms": 5418.597936630249, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the escape took place from a prison in Argentina, which matches the gold answer exactly. It is fully consistent with the provided information and does not introduce any contradictions or unsupported claims.", "main_total_tokens": 2243, "main_prompt_tokens": 2211, "main_completion_tokens": 32, "judge_total_tokens": 489, "judge_prompt_tokens": 417, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In Sweet Trap (2024), the apprentice Li Nai falls into a trap designed by Jiang Jie [Doc 135231].", "latency_ms": 5671.676874160767, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The question asks whose trap the apprentice falls into in Sweet Trap (2024). The gold answer is Jiang Jie. The generated answer states that the apprentice falls into a trap designed by Jiang Jie, which matches the gold answer, so it is correct. However, the generated answer adds unsupported specifics (that the apprentice is Li Nai and cites [Doc 135231]) that are not present in the gold answer. This extra detail is unverified based on the provided information, so the response is not fully faithful.", "main_total_tokens": 2436, "main_prompt_tokens": 2402, "main_completion_tokens": 34, "judge_total_tokens": 542, "judge_prompt_tokens": 409, "judge_completion_tokens": 133}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 532.949686050415, "retrieval_recall": 0.0, "retrieval_mrr": 0.07692307692307693, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (4), so it is incorrect. However, given the prompt and gold label, there is no indication that the model ignored any provided information or fabricated justification; it simply gave the wrong numeric output, so faithfulness to the (incorrect) reasoning is still rated as high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 385, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "37", "latency_ms": 109.26413536071777, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '37' is factually incorrect because there are no movies featuring both Steven Seagal and Bruce Lee together, matching the gold response. It is also unfaithful to the gold answer, which explicitly states that no such movies can be found.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 395, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 4158.103704452515, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 6.3 for Tokyo Mater (2008) and does not introduce any contradictory or extraneous information. It is both correct and faithful.", "main_total_tokens": 2459, "main_prompt_tokens": 2433, "main_completion_tokens": 26, "judge_total_tokens": 472, "judge_prompt_tokens": 403, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in \"Fast Track: No Limits\" is street racing [Doc 41929].", "latency_ms": 5110.790252685547, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies the style of racing as 'Drift', while the generated answer claims it is 'street racing', which is incorrect. However, the generated answer appears to be faithful to the cited document, so faithfulness is high but correctness with respect to the gold is low.", "main_total_tokens": 2635, "main_prompt_tokens": 2610, "main_completion_tokens": 25, "judge_total_tokens": 487, "judge_prompt_tokens": 399, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 9521.887302398682, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, 'Confronting a Serial Killer,' and stays faithful to the provided information without adding unsupported details.", "main_total_tokens": 2450, "main_prompt_tokens": 2399, "main_completion_tokens": 51, "judge_total_tokens": 512, "judge_prompt_tokens": 452, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 6476.732969284058, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and directly uses it without adding unsupported information, so it is both correct and faithful.", "main_total_tokens": 2534, "main_prompt_tokens": 2504, "main_completion_tokens": 30, "judge_total_tokens": 469, "judge_prompt_tokens": 407, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "None of the provided documents contain information about a 1999 psychological thriller movie that alludes to an extra sense. [Doc 22488] is a 1999 thriller, but it does not specifically mention an extra sense in its description. Therefore, based on the given context, the answer cannot be determined.", "latency_ms": 11651.627779006958, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question clearly refers to the well-known 1999 psychological thriller 'The Sixth Sense.' The generated answer claims the title cannot be determined from the context, which is factually incorrect relative to general knowledge, so correctness is 0. However, the answer accurately reflects that the provided documents (as described) do not contain the needed information and does not invent a title, so it is faithful to the given context, yielding faithfulness 1.", "main_total_tokens": 2425, "main_prompt_tokens": 2354, "main_completion_tokens": 71, "judge_total_tokens": 574, "judge_prompt_tokens": 454, "judge_completion_tokens": 120}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 7322.219371795654, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer and not adding any contradictory or unsupported information.", "main_total_tokens": 2570, "main_prompt_tokens": 2547, "main_completion_tokens": 23, "judge_total_tokens": 457, "judge_prompt_tokens": 397, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 6789.738416671753, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly naming the 2015 reboot as 'The Transporter Refueled' and does not introduce unsupported information.", "main_total_tokens": 2572, "main_prompt_tokens": 2537, "main_completion_tokens": 35, "judge_total_tokens": 483, "judge_prompt_tokens": 420, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The movie 'The Transporter Refueled' was released in the year 2015 [Doc 67500].", "latency_ms": 6576.809406280518, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, which matches the gold answer, and it does not introduce any contradictory or extraneous information beyond a benign document reference.", "main_total_tokens": 2684, "main_prompt_tokens": 2654, "main_completion_tokens": 30, "judge_total_tokens": 483, "judge_prompt_tokens": 408, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "None of the provided documents mention a romantic comedy where Abraham Lincoln comes back from the dead. [Doc X]", "latency_ms": 5152.795314788818, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The generated answer is substantively correct: given the provided information, there is no identified romantic comedy where Abraham Lincoln comes back from the dead, which matches the gold answer in meaning. However, it references a specific source artifact ('[Doc X]') that is not present in the user-provided context, so the explanation about the documents is not fully faithful to the given prompt setup. Therefore correctness is 1, but faithfulness is slightly reduced.", "main_total_tokens": 2115, "main_prompt_tokens": 2092, "main_completion_tokens": 23, "judge_total_tokens": 535, "judge_prompt_tokens": 412, "judge_completion_tokens": 123}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by Ernest Hemingway is \"Havana Darkness\" [Doc 90923].", "latency_ms": 6470.039367675781, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'Havana Darkness' as the movie title. It is fully supported by the provided information and does not introduce any unsupported claims.", "main_total_tokens": 2255, "main_prompt_tokens": 2222, "main_completion_tokens": 33, "judge_total_tokens": 483, "judge_prompt_tokens": 415, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "The TV show in which Doll heads to Hollywood to be with her childhood best friend Em, who is now a successful actress, is titled \"Doll & Em\" [Doc 122259].", "latency_ms": 7847.247123718262, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em,' which matches the gold answer. It does not introduce any contradictions or unsupported claims and remains faithful to the provided information.", "main_total_tokens": 2443, "main_prompt_tokens": 2399, "main_completion_tokens": 44, "judge_total_tokens": 497, "judge_prompt_tokens": 428, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The movie in question is \"Hallam Foe,\" which was released in 2007. In this film, Hallam uses his talent for spying on people to try and uncover the true cause of his mother's death, though he ends up searching for love instead. [Doc 39389]", "latency_ms": 10748.136281967163, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2007 movie as 'Hallam Foe' and stays faithful to the prompt and gold answer without adding contradictory or unsupported information. Any slight plot elaboration is accurate and does not affect correctness or faithfulness.", "main_total_tokens": 2457, "main_prompt_tokens": 2391, "main_completion_tokens": 66, "judge_total_tokens": 538, "judge_prompt_tokens": 458, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "None of the provided documents directly answer whether a last name sounds like a friend or foe at first glance. Each document provides plot descriptions and details about various movies and TV shows but does not contain information relevant to interpreting the sound of a last name as indicating friendship or enmity. [Doc 62803] mentions a character changing his name to \"Mohamed\" which is used as part of the plot, but there's no indication given about whether this name sounds like a friend or foe. Therefore, based on the provided context, it is not possible to answer the question.", "latency_ms": 16564.258813858032, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'A foe'. The generated answer claims it is not possible to answer from the documents and does not give 'A foe', so it is incorrect relative to the gold. However, it accurately describes that the documents do not address how a last name sounds and refrains from inventing an answer, so it is faithful to the provided context.", "main_total_tokens": 2652, "main_prompt_tokens": 2532, "main_completion_tokens": 120, "judge_total_tokens": 599, "judge_prompt_tokens": 501, "judge_completion_tokens": 98}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "In the 2017 TV show Atypical, Sam is 18 years old [Doc 124583].", "latency_ms": 6064.046859741211, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, matching the gold answer exactly and not adding unsupported details.", "main_total_tokens": 2228, "main_prompt_tokens": 2198, "main_completion_tokens": 30, "judge_total_tokens": 475, "judge_prompt_tokens": 410, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 5943.697690963745, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the 2017 TV show Atypical, Sam is on the autism spectrum, which matches the gold answer exactly and does not introduce unsupported information.", "main_total_tokens": 2203, "main_prompt_tokens": 2174, "main_completion_tokens": 29, "judge_total_tokens": 478, "judge_prompt_tokens": 410, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "The place that Sandra was born in the movie \"À corde tendue\" is Chamonix [Doc 59659].", "latency_ms": 6419.502258300781, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Chamonix as Sandra's birthplace in the movie and explicitly aligns with the gold answer, without adding unsupported information.", "main_total_tokens": 2637, "main_prompt_tokens": 2607, "main_completion_tokens": 30, "judge_total_tokens": 481, "judge_prompt_tokens": 422, "judge_completion_tokens": 59}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "in this japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 6150.175094604492, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly fills in the blank with 'rains' and matches the gold answer exactly. It is fully consistent with the provided information and does not introduce any unsupported content.", "main_total_tokens": 2725, "main_prompt_tokens": 2699, "main_completion_tokens": 26, "judge_total_tokens": 479, "judge_prompt_tokens": 412, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 5953.110456466675, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which matches the gold answer exactly. It does not introduce any extraneous or contradictory information and remains faithful to the provided context.", "main_total_tokens": 2436, "main_prompt_tokens": 2409, "main_completion_tokens": 27, "judge_total_tokens": 489, "judge_prompt_tokens": 416, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is titled \"One Afternoon\" [Doc 96838].", "latency_ms": 8448.841333389282, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'One Afternoon' and attributing it to the described plot. It aligns with the provided information and does not add unsupported details.", "main_total_tokens": 2041, "main_prompt_tokens": 1990, "main_completion_tokens": 51, "judge_total_tokens": 520, "judge_prompt_tokens": 449, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" [Doc 93260].", "latency_ms": 3771.303653717041, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Life Ahead,\" which matches the gold answer. It does not introduce contradictions or unsupported claims, and the added document reference does not change the factual content. Therefore, it is both correct and faithful.", "main_total_tokens": 2330, "main_prompt_tokens": 2288, "main_completion_tokens": 42, "judge_total_tokens": 515, "judge_prompt_tokens": 435, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary “McLaren” is about Bruce McLaren, a New Zealander who founded the McLaren Motor Racing team. [Doc 73859]", "latency_ms": 7324.05424118042, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the subject of the 2016 documentary 'McLaren' as Bruce McLaren and adds accurate contextual detail about him founding the McLaren motor racing team. It matches the gold answer and appears consistent with the cited document, so it is both correct and faithful.", "main_total_tokens": 2300, "main_prompt_tokens": 2260, "main_completion_tokens": 40, "judge_total_tokens": 507, "judge_prompt_tokens": 419, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The provided context does not specify the name of the racing team for any individual mentioned in the descriptions. Therefore, based strictly on the given information, there is no specific racing team name associated with \"his\" racing team that can be cited. [Doc 89615] to [Doc 51728] do not provide this detail.", "latency_ms": 11350.207567214966, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly indicates the racing team name is 'The McLaren Motor Racing team'. The generated answer incorrectly claims the context does not specify the racing team name, which contradicts the gold. However, the generated answer is internally faithful to its own (mistaken) reading of the context and does not fabricate another team name or extraneous details.", "main_total_tokens": 2487, "main_prompt_tokens": 2413, "main_completion_tokens": 74, "judge_total_tokens": 551, "judge_prompt_tokens": 450, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The provided context does not specify a country for \"he\" without additional information on who \"he\" refers to. The documents contain various individuals and stories but do not collectively or individually identify a specific person's nationality as \"he.\" [Doc 100044] mentions Heinz Stücke, who left Germany, but there is no explicit statement about his country of origin beyond him leaving Germany. Therefore, based strictly on the context given, the country \"he\" is from cannot be determined.", "latency_ms": 14232.922554016113, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'New Zealand', but the generated answer claims the country cannot be determined from context, which is incorrect relative to the gold. However, the model stays faithful to the information it says is in the provided context and does not invent a country, so it is fully faithful to its stated use of the context.", "main_total_tokens": 2209, "main_prompt_tokens": 2106, "main_completion_tokens": 103, "judge_total_tokens": 568, "judge_prompt_tokens": 472, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "The provided context does not mention John McClane blowing up the Great Wall of China in any 2013 movie. The 2013 movie featuring John McClane is \"A Good Day to Die Hard,\" but its plot summary does not include such an event [Doc 58791].", "latency_ms": 11448.369264602661, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a 2013 movie in which John McClane blows up the Great Wall of China. The gold answer says that no such movie exists in the database, implying the premise is false. The generated answer correctly states that there is no mention of such an event, and identifies the actual 2013 John McClane movie ('A Good Day to Die Hard'), clarifying that its plot does not include blowing up the Great Wall. This is factually correct and remains faithful to the given gold answer and context, essentially conveying that no such movie is known.", "main_total_tokens": 2676, "main_prompt_tokens": 2611, "main_completion_tokens": 65, "judge_total_tokens": 604, "judge_prompt_tokens": 457, "judge_completion_tokens": 147}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie that matches this description is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 6443.85552406311, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Tomie: Forbidden Fruit'. There are no contradictions or hallucinations; the added citation-style bracket does not alter the correctness or faithfulness of the response.", "main_total_tokens": 2418, "main_prompt_tokens": 2386, "main_completion_tokens": 32, "judge_total_tokens": 520, "judge_prompt_tokens": 443, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie \"Lei mi parla ancora,\" Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 8509.765148162842, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that Nino and Caterina have been married for 65 years, which matches the gold answer of 65. It is correct and consistent with the provided information.", "main_total_tokens": 2868, "main_prompt_tokens": 2829, "main_completion_tokens": 39, "judge_total_tokens": 492, "judge_prompt_tokens": 424, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for the legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 22495.025873184204, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Scorpion King and Tala are searching for The Book of Souls, matching the gold answer. The added description 'legendary relic known as' is consistent with the film and remains faithful to the provided information.", "main_total_tokens": 2715, "main_prompt_tokens": 2673, "main_completion_tokens": 42, "judge_total_tokens": 508, "judge_prompt_tokens": 429, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about the release year of \"Zombies on the Moon\". [Doc N/A]", "latency_ms": 9029.239654541016, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the release year is not available, and the generated answer accurately reflects the absence of information in the provided documents.", "main_total_tokens": 2482, "main_prompt_tokens": 2457, "main_completion_tokens": 25, "judge_total_tokens": 477, "judge_prompt_tokens": 415, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about the release year of \"The Galactic baker\". [Doc X]", "latency_ms": 9047.379493713379, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the movie is not in the database; the generated answer says the documents do not contain information about the release year. Both convey that the system has no release-year information. This is correct given the context, and the generated answer is faithful to the provided evidence (no documents with that info).", "main_total_tokens": 2362, "main_prompt_tokens": 2340, "main_completion_tokens": 22, "judge_total_tokens": 503, "judge_prompt_tokens": 410, "judge_completion_tokens": 93}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided context does not contain any information about a title called \"Return of the Null Pointer\". Therefore, the release year cannot be determined from the given documents [Doc 70593, Doc 48709, Doc 132506, Doc 11800, Doc 30896, Doc 99959, Doc 133164, Doc 27475, Doc 58699, Doc 65150, Doc 66972, Doc 29451, Doc 296, Doc 697, Doc 8853].", "latency_ms": 24963.213682174683, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold answer states the movie is not in the database; the generated answer states that the context does not contain information about that title and thus the year cannot be determined. Both convey that the release year is unknown/unavailable given the data, which matches and does not hallucinate a year.", "main_total_tokens": 2211, "main_prompt_tokens": 2061, "main_completion_tokens": 150, "judge_total_tokens": 584, "judge_prompt_tokens": 496, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"Data Science: The Movie\". Therefore, the release year cannot be determined from the given context.", "latency_ms": 6584.728479385376, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year cannot be determined and does not hallucinate a year. It is faithful to the information (or lack thereof) in the provided documents, matching the gold answer’s stance that the movie is not in the database.", "main_total_tokens": 2610, "main_prompt_tokens": 2578, "main_completion_tokens": 32, "judge_total_tokens": 504, "judge_prompt_tokens": 422, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain any information about a movie titled \"Python's Revenge 2\". Therefore, based on the given context, the release year of \"Python's Revenge 2\" cannot be determined.", "latency_ms": 10394.231081008911, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year cannot be determined / is not in the database. The generated response is correct (we lack information) and faithful to the context (explicitly referencing absence of information in the provided documents).", "main_total_tokens": 2464, "main_prompt_tokens": 2421, "main_completion_tokens": 43, "judge_total_tokens": 512, "judge_prompt_tokens": 433, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain any information about a release year for \"The Last Pixel\". [Doc X]", "latency_ms": 7241.230010986328, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, effectively giving no release year. The generated answer says the provided documents do not contain release year information, which is logically consistent with the gold response and remains grounded in the (hypothetical) documents, so it is faithful. However, the generated answer does not match the specific phrasing or content of the gold answer about the database and thus is considered incorrect relative to the exact gold response.", "main_total_tokens": 2142, "main_prompt_tokens": 2119, "main_completion_tokens": 23, "judge_total_tokens": 533, "judge_prompt_tokens": 411, "judge_completion_tokens": 122}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a title called \"The man who ate the sun\". Therefore, based solely on the given context, the release year cannot be determined.", "latency_ms": 12799.028158187866, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie not present in the database/context. The gold answer explicitly says it is not in the database. The generated answer correctly states that the documents do not contain information about that title and that the release year cannot be determined from the given context. It does not hallucinate a year, aligns with the lack of information, and is faithful to the provided context.", "main_total_tokens": 2916, "main_prompt_tokens": 2880, "main_completion_tokens": 36, "judge_total_tokens": 538, "judge_prompt_tokens": 427, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not specify who directed the movie \"The Last Days of Lehman Brothers\". [Doc 46069]", "latency_ms": 6598.8171100616455, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director information is not provided in the documents, matching the gold answer in meaning. It is both factually correct relative to the given context and faithful to the database limitation described.", "main_total_tokens": 2818, "main_prompt_tokens": 2789, "main_completion_tokens": 29, "judge_total_tokens": 495, "judge_prompt_tokens": 422, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about the director of the movie \"A Love to Die For\". [Doc X]", "latency_ms": 5799.804925918579, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided sources, which is correct relative to the database context. The generated answer remains faithful to the given documents by explicitly noting their lack of information and does not hallucinate a director name.", "main_total_tokens": 2571, "main_prompt_tokens": 2546, "main_completion_tokens": 25, "judge_total_tokens": 503, "judge_prompt_tokens": 419, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not specify who directed the movie \"Journey into Gaza\". [Doc 111360]", "latency_ms": 5962.307453155518, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not specify the director, which is equivalent in meaning to the gold answer that the director information is not provided in the database. It does not introduce unsupported details and remains faithful to the source constraints.", "main_total_tokens": 2448, "main_prompt_tokens": 2421, "main_completion_tokens": 27, "judge_total_tokens": 493, "judge_prompt_tokens": 414, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about the director of \"The Last Night at Tremore Beach\". [Doc 135098]", "latency_ms": 6242.084741592407, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not contain information about the director, which is aligned with the gold response indicating the database lacks director information. It does not introduce any unsupported facts and stays faithful to the provided evidence.", "main_total_tokens": 2174, "main_prompt_tokens": 2143, "main_completion_tokens": 31, "judge_total_tokens": 499, "judge_prompt_tokens": 423, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 6585.698843002319, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy'. It does not introduce contradictions or unsupported claims beyond a harmless document reference, so it is both correct and faithful.", "main_total_tokens": 2194, "main_prompt_tokens": 2160, "main_completion_tokens": 34, "judge_total_tokens": 492, "judge_prompt_tokens": 418, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The scenario where Sarah fakes her own death to escape her abusive spouse occurs in the movie \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 8902.010440826416, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Stranger in My Bed', which matches the gold answer. It also remains faithful to the provided information, simply restating the scenario and the movie title without introducing contradictions or unsupported claims.", "main_total_tokens": 2324, "main_prompt_tokens": 2289, "main_completion_tokens": 35, "judge_total_tokens": 504, "judge_prompt_tokens": 427, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie where a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years is \"Game of Silence\" [Doc 123938].", "latency_ms": 10819.116592407227, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the title as 'Game of Silence,' which matches the gold answer exactly. It does not introduce any unsupported information beyond rephrasing the question, so it is both correct and faithful.", "main_total_tokens": 1916, "main_prompt_tokens": 1878, "main_completion_tokens": 38, "judge_total_tokens": 502, "judge_prompt_tokens": 429, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie where the plot is based on a true story about a killer from the site Craigslist is \"The Craigslist Killer\" [Doc 53039].", "latency_ms": 8247.168064117432, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'The Craigslist Killer,' which matches the gold answer exactly. It does not introduce contradictions or hallucinations; the brief citation tag does not alter the factual content. Therefore, both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2260, "main_prompt_tokens": 2226, "main_completion_tokens": 34, "judge_total_tokens": 504, "judge_prompt_tokens": 422, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie in which Yuki Hirano sets his eyes on the role of a forestry trainee after failing his university entrance exam is \"Wood Job!\" [Doc 66094].", "latency_ms": 8045.2375411987305, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", which matches the gold answer exactly. It remains faithful to the question and does not introduce any contradictory or extraneous information beyond a citation placeholder.", "main_total_tokens": 2574, "main_prompt_tokens": 2534, "main_completion_tokens": 40, "judge_total_tokens": 507, "judge_prompt_tokens": 435, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The movie in which a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby and friends on a colorful adventure is titled \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 9919.569969177246, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, correctly identifying the movie as \"Sesame Street: The Magical Wand Chase\" and remains faithful to the given information.", "main_total_tokens": 2339, "main_prompt_tokens": 2285, "main_completion_tokens": 54, "judge_total_tokens": 522, "judge_prompt_tokens": 459, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The movie in which the following happens is \"Beyond\" [Doc 74597]. When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "latency_ms": 9622.034549713135, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Beyond' and faithfully restates the query without adding unsupported information.", "main_total_tokens": 1981, "main_prompt_tokens": 1929, "main_completion_tokens": 52, "judge_total_tokens": 509, "judge_prompt_tokens": 456, "judge_completion_tokens": 53}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie where a woman, fired from a financial corporation during the Asia crisis, returns home with no money is titled \"6ixtynin9\" [Doc 22478].", "latency_ms": 10073.498487472534, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as \"6ixtynin9.\" It is correct and grounded in the provided information.", "main_total_tokens": 2053, "main_prompt_tokens": 2012, "main_completion_tokens": 41, "judge_total_tokens": 501, "judge_prompt_tokens": 439, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie in which a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- and it's up to the club's staff to stop the violence ... or at least hide it is titled \"Club Dread\" [Doc 32410].", "latency_ms": 11968.02306175232, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer exactly. It remains faithful to the given information and does not introduce contradictions or unsupported claims.", "main_total_tokens": 2402, "main_prompt_tokens": 2337, "main_completion_tokens": 65, "judge_total_tokens": 552, "judge_prompt_tokens": 485, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie in which a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is titled \"Toilet\" from the year 2010 [Doc 48873].", "latency_ms": 14908.887386322021, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" matching the gold answer exactly and adding only a release year and citation-style bracket that are consistent and not contradictory. It remains faithful to the given information without inventing conflicting details.", "main_total_tokens": 2671, "main_prompt_tokens": 2606, "main_completion_tokens": 65, "judge_total_tokens": 550, "judge_prompt_tokens": 472, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century is \"Electric Angel\" [Doc 1024].", "latency_ms": 8441.924571990967, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Electric Angel,\" which matches the gold answer. It does not introduce contradictory information and remains consistent with the given description, so it is both correct and faithful.", "main_total_tokens": 2324, "main_prompt_tokens": 2274, "main_completion_tokens": 50, "judge_total_tokens": 532, "judge_prompt_tokens": 461, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023, based on the provided context, is \"Down Low\" with a rating of 5.207 [Doc 106995].", "latency_ms": 4410.799264907837, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer indicates that the lowest rated movie from 2023 is 'Mega Ape', while the generated answer claims it is 'Down Low' with a specific rating. This directly contradicts the gold label, so the answer is incorrect. It appears the model is relying on a cited document [Doc 106995], but that document is either misinterpreted or not aligned with the task’s ground truth, so the answer is not faithful to the required output, though it is somewhat faithful to the (incorrect) retrieved context. Therefore correctness is 0, and faithfulness is partial at 0.5.", "main_total_tokens": 2251, "main_prompt_tokens": 2208, "main_completion_tokens": 43, "judge_total_tokens": 566, "judge_prompt_tokens": 415, "judge_completion_tokens": 151}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 1421.8840599060059, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'Tea House', but the generated response says 'I don't know,' which is incorrect. However, the model does not fabricate an answer and honestly admits uncertainty, so it is fully faithful to its own (stated) knowledge state.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 468, "judge_prompt_tokens": 386, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 1776.8304347991943, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer says 'I don't know,' which does not provide the correct movie title ('The Uncut Story of Mewtwo's Origin'), so it is incorrect with respect to the gold answer. However, it does not introduce any fabricated or contradictory information; it simply expresses a lack of knowledge, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 492, "judge_prompt_tokens": 395, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is \"Don't Answer the Phone!\" with a rating of 4.6 [Doc 768]. The language of this movie is English [Doc 768].", "latency_ms": 8016.986608505249, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The answer correctly identifies the language as English, matching the gold answer. The explanation about the specific movie and its rating is supported by the cited document and does not introduce unsupported claims.", "main_total_tokens": 2599, "main_prompt_tokens": 2551, "main_completion_tokens": 48, "judge_total_tokens": 490, "judge_prompt_tokens": 423, "judge_completion_tokens": 67}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "Based on the provided context, the TV show in 1999 with the highest popularity is \"Freaks and Geeks\" with a popularity score of 5.8043 [Doc 115187].", "latency_ms": 10770.975589752197, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states that the most popular TV show in 1999 was 'Law & Order: Special Victims Unit', while the generated answer claims it was 'Freaks and Geeks'. This is factually incorrect with respect to the gold label, so correctness is 0. In addition, the generated answer explicitly cites a document and a popularity score that contradict the gold standard rather than reflecting or being constrained by it, so with respect to the gold-based evaluation it is not faithful, yielding a faithfulness score of 0.", "main_total_tokens": 2603, "main_prompt_tokens": 2552, "main_completion_tokens": 51, "judge_total_tokens": 571, "judge_prompt_tokens": 432, "judge_completion_tokens": 139}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 5474.82967376709, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and attributes it to the document, so it is both correct and faithful to the provided information.", "main_total_tokens": 2439, "main_prompt_tokens": 2417, "main_completion_tokens": 22, "judge_total_tokens": 463, "judge_prompt_tokens": 400, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in the year 1999 [Doc 22481].", "latency_ms": 6114.981651306152, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states that the movie was released in 1999, while the gold answer is 2019, so it is factually incorrect. However, the answer is consistent with its cited source [Doc 22481], so it is faithful to the provided document.", "main_total_tokens": 2431, "main_prompt_tokens": 2403, "main_completion_tokens": 28, "judge_total_tokens": 492, "judge_prompt_tokens": 406, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "2", "latency_ms": 171.59199714660645, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 279 votes, while the generated answer is 2, which is numerically incorrect. However, the model did not contradict the question structure or introduce extraneous information; it simply provided the wrong number, so it is unfaithful to the gold but internally consistent with the question.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 383, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "11", "latency_ms": 124.70746040344238, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (11) does not match the gold answer (12), so it is factually incorrect for this evaluation. However, the model stayed within the scope of the question and did not fabricate unrelated details, so it is fully faithful to the user query.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 467, "judge_prompt_tokens": 383, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 5902.634859085083, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'The Last Days of Lehman Brothers' was released in 2009, which matches the gold answer. It directly uses the information from the provided document citation, so it is faithful to the source.", "main_total_tokens": 2783, "main_prompt_tokens": 2754, "main_completion_tokens": 29, "judge_total_tokens": 490, "judge_prompt_tokens": 410, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 5691.48850440979, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the original language is English (en). The generated answer states the original language is English, which matches the gold and is factually correct. The reference to a document does not contradict anything and stays within the given information, so the answer is both correct and faithful.", "main_total_tokens": 2462, "main_prompt_tokens": 2438, "main_completion_tokens": 24, "judge_total_tokens": 490, "judge_prompt_tokens": 402, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "1987 [Doc 6993]", "latency_ms": 5230.511665344238, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, fully matching the gold answer, and it does not introduce any contradictory or unsupported information.", "main_total_tokens": 3008, "main_prompt_tokens": 2993, "main_completion_tokens": 15, "judge_total_tokens": 450, "judge_prompt_tokens": 387, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Brain Fluid Explosion Girl", "latency_ms": 6013.44633102417, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly names the movie where classmates must play a survival game using their cell phones.", "main_total_tokens": 2687, "main_prompt_tokens": 2666, "main_completion_tokens": 21, "judge_total_tokens": 448, "judge_prompt_tokens": 392, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 4388.563394546509, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly. The TV show described is indeed 'Dungeons & Dragons,' so the response is both correct and fully faithful to the reference.", "main_total_tokens": 2608, "main_prompt_tokens": 2601, "main_completion_tokens": 7, "judge_total_tokens": 459, "judge_prompt_tokens": 394, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 6786.311149597168, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Nacht der Wölfe'), correctly identifying the movie about rival German and Turkish teen gangs in Munich and remaining fully faithful to the provided information.", "main_total_tokens": 2723, "main_prompt_tokens": 2699, "main_completion_tokens": 24, "judge_total_tokens": 472, "judge_prompt_tokens": 403, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900 [Doc 126191]", "latency_ms": 5715.242624282837, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (1900) and is fully supported by the referenced document, so it is both correct and faithful.", "main_total_tokens": 2977, "main_prompt_tokens": 2960, "main_completion_tokens": 17, "judge_total_tokens": 454, "judge_prompt_tokens": 394, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June", "latency_ms": 1634.6955299377441, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the name of the movie/character which is a calendar month; both the gold and generated answers are 'June', which matches exactly and is faithful to the prompt.", "main_total_tokens": 134, "main_prompt_tokens": 132, "main_completion_tokens": 2, "judge_total_tokens": 501, "judge_prompt_tokens": 433, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 312.50905990600586, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer admits not knowing and does not provide an incorrect movie title, so it is faithful but not correct relative to the gold answer, which specifies an actual film.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 464, "judge_prompt_tokens": 399, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 234.7860336303711, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific numeric value (the highest Russian-language movie rating in 1999, which is 7.6). The generated answer says 'I don't know,' which is factually incorrect relative to the gold answer, so correctness is 0. However, the model does not fabricate an answer and honestly admits not knowing, which is faithful to its uncertainty and to the absence of relevant information, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 504, "judge_prompt_tokens": 389, "judge_completion_tokens": 115}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "9", "latency_ms": 4601.481914520264, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.366123019050001, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (9) does not match the gold answer (4), so it is factually incorrect for this example. The evaluation correctly compares the provided generated output against the gold, so faithfulness to the given data is maintained.", "main_total_tokens": 1583, "main_prompt_tokens": 1561, "main_completion_tokens": 22, "judge_total_tokens": 472, "judge_prompt_tokens": 394, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 343.4593677520752, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says there are 2 such TV shows, but the generated answer claims there are none. This directly contradicts the gold, so the generated answer is incorrect. However, the answer is consistent with itself and does not hallucinate specific titles or numbers; it admits inability to find matches, so it is internally faithful to its own (albeit incomplete) reasoning.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 510, "judge_prompt_tokens": 404, "judge_completion_tokens": 106}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 218.87516975402832, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer states uncertainty ('I don't know') instead of providing the correct answer 'Fantômas', so it is factually incorrect with respect to the gold. However, it does not fabricate information and is faithful to the model's stated lack of knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 472, "judge_prompt_tokens": 389, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise", "latency_ms": 3563.666343688965, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is correct for the described movie plot. It is fully faithful to the reference.", "main_total_tokens": 2836, "main_prompt_tokens": 2831, "main_completion_tokens": 5, "judge_total_tokens": 444, "judge_prompt_tokens": 387, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor? [Doc 84838]", "latency_ms": 6106.947660446167, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the movie title as 'Won't You Be My Neighbor?', which matches the gold answer, so correctness is 1. The added bracketed reference '[Doc 84838]' is extraneous to the asked question but does not change the substantive answer, so faithfulness is slightly reduced.", "main_total_tokens": 3210, "main_prompt_tokens": 3191, "main_completion_tokens": 19, "judge_total_tokens": 509, "judge_prompt_tokens": 414, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018", "latency_ms": 6610.065460205078, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (2018), so it is both correct and faithful.", "main_total_tokens": 3171, "main_prompt_tokens": 3150, "main_completion_tokens": 21, "judge_total_tokens": 431, "judge_prompt_tokens": 380, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy", "latency_ms": 4583.510160446167, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, and it correctly identifies the animated TV series about a young boy robot in 2030 AD as Astro Boy.", "main_total_tokens": 2877, "main_prompt_tokens": 2871, "main_completion_tokens": 6, "judge_total_tokens": 451, "judge_prompt_tokens": 389, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 years [Doc 67513]", "latency_ms": 5040.281534194946, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that they were held captive for 7 years, which matches the gold answer exactly and remains faithful to the provided information.", "main_total_tokens": 2569, "main_prompt_tokens": 2555, "main_completion_tokens": 14, "judge_total_tokens": 463, "judge_prompt_tokens": 403, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m [Doc 133469]", "latency_ms": 5046.575546264648, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the provided gold answer exactly (£26m) and does not introduce any unsupported information, so it is both correct and faithful to the given source.", "main_total_tokens": 2281, "main_prompt_tokens": 2266, "main_completion_tokens": 15, "judge_total_tokens": 468, "judge_prompt_tokens": 404, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 670.9408760070801, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a specific numeric answer (80.5772). The generated answer says 'I don't know,' which is incorrect. However, it does not fabricate any information and honestly admits uncertainty, so it is fully faithful to the model's knowledge limits.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 396, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 267.5762176513672, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there was 1 Russian language TV show released in 1997, while the generated answer says it cannot find any, which is factually incorrect. However, the model does not fabricate an incorrect specific number; it honestly states a lack of information, so it is faithful to its own (claimed) uncertainty but still incorrect relative to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 498, "judge_prompt_tokens": 394, "judge_completion_tokens": 104}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes", "latency_ms": 7084.867238998413, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the correct TV show 'Angel Eyes'. It is correct and fully faithful to the reference.", "main_total_tokens": 3264, "main_prompt_tokens": 3259, "main_completion_tokens": 5, "judge_total_tokens": 456, "judge_prompt_tokens": 397, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "en", "latency_ms": 6006.960153579712, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer says the language is 'en' (English), but the gold label is Korean. This is factually incorrect, so correctness is 0. It also does not align with the provided gold answer, so faithfulness is 0.", "main_total_tokens": 3149, "main_prompt_tokens": 3136, "main_completion_tokens": 13, "judge_total_tokens": 453, "judge_prompt_tokens": 377, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 659.2659950256348, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 2 such movies, while the generated answer claims there are none, making it factually incorrect. However, the generated answer is internally consistent and does not contradict itself or add hallucinated details; it simply states inability to find matches, so it is fully faithful to its own reasoning.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 497, "judge_prompt_tokens": 403, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 1112.2441291809082, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The gold answer asserts there are no movies from 1979 in its database, while the generated answer simply says 'I don't know.' Given the context (limited to the database), 'I don't know' is effectively correct because the system cannot answer which is best-rated. However, it is not faithful to the gold explanation about the database limitation, so correctness is high but faithfulness is low.", "main_total_tokens": 620, "main_prompt_tokens": 618, "main_completion_tokens": 2, "judge_total_tokens": 509, "judge_prompt_tokens": 399, "judge_completion_tokens": 110}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "I don't know.", "latency_ms": 7074.002504348755, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer correctly states that 'Back to the Future Part IV' does not exist. The generated answer, 'I don't know,' is also effectively correct because there is no such film and thus no release year to provide. The model does not hallucinate a year and acknowledges uncertainty, which is faithful to the given information and to real-world facts.", "main_total_tokens": 673, "main_prompt_tokens": 626, "main_completion_tokens": 47, "judge_total_tokens": 505, "judge_prompt_tokens": 404, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers convey that there are no movies from 1980 about the Chernobyl disaster, which is correct because the Chernobyl disaster occurred in 1986. The generated response is faithful to the gold answer’s core information (non-existence of such movies) and does not introduce incorrect facts, only an additional suggestion to check the event timing.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 540, "judge_prompt_tokens": 434, "judge_completion_tokens": 106}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 312.50929832458496, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 titles with 'Astro Boy' in the title, while the generated answer incorrectly claims there are none, so it is factually wrong. However, the model does not invent specific incorrect titles or numbers; it instead states it cannot find any, which is consistent with its (albeit mistaken) retrieval, so it is still faithful to its own reasoning process.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 508, "judge_prompt_tokens": 397, "judge_completion_tokens": 111}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 4505.429983139038, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 2 such movies, while the generated answer says there are none, which is factually incorrect relative to the gold label, so correctness is 0. However, the model honestly reports lack of information rather than fabricating details, so its reasoning is faithful to its own knowledge, giving faithfulness a 1.", "main_total_tokens": 3624, "main_prompt_tokens": 3622, "main_completion_tokens": 2, "judge_total_tokens": 495, "judge_prompt_tokens": 395, "judge_completion_tokens": 100}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 315.57369232177734, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 1, indicating there was one TV show in the 1990s about Super Mario Bros. The generated answer claims there are none, which is factually incorrect, so correctness is 0. However, the model does not fabricate specific shows or numbers; it merely states inability to find any, which is consistent with its stated limitations, so faithfulness to its own claim is high, hence faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 518, "judge_prompt_tokens": 398, "judge_completion_tokens": 120}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 156.2652587890625, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0 TV shows. The generated answer says there are none matching the criteria, which aligns with 0, so it is correct. It stays within the information implied by the question and does not introduce unsupported claims, so it is faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 483, "judge_prompt_tokens": 400, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 184.38220024108887, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the most popular TV show or movie about ninjas, with the gold answer being 'Naruto.' The generated answer 'I don't know.' is incorrect because the assistant should have identified 'Naruto' (or at least provided a plausible popular ninja-related show or movie). However, it is faithful in that it honestly reflects the model's stated uncertainty and does not hallucinate an answer.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 498, "judge_prompt_tokens": 386, "judge_completion_tokens": 112}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 5463.064193725586, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold states definitively that the movie was not released. Saying 'I don't know' is factually incorrect relative to the gold, though it is non-hallucinatory and thus faithful to the model's own uncertainty.", "main_total_tokens": 601, "main_prompt_tokens": 561, "main_completion_tokens": 40, "judge_total_tokens": 465, "judge_prompt_tokens": 389, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 8675.449848175049, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is that such a movie was never released, but the model answered 'I don't know,' which is incorrect. It also does not reflect the given gold answer, so it is not faithful.", "main_total_tokens": 653, "main_prompt_tokens": 586, "main_completion_tokens": 67, "judge_total_tokens": 458, "judge_prompt_tokens": 386, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "Return of the Jedi [Doc 2858]", "latency_ms": 16320.902347564697, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect: the 4th oldest Star Wars movie is 'Star Wars: Droids - The Great Heep', not 'Return of the Jedi'. The model, however, stays within the question and does not introduce unrelated content, so it is faithful to the user query but factually wrong.", "main_total_tokens": 3166, "main_prompt_tokens": 3054, "main_completion_tokens": 112, "judge_total_tokens": 493, "judge_prompt_tokens": 399, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "1983 [Doc 2858]", "latency_ms": 6618.400573730469, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 1986, but the generated answer is 1983, so it is factually incorrect. The citation [Doc 2858] does not fix the discrepancy with the gold answer, so the response is also unfaithful to the ground truth.", "main_total_tokens": 2989, "main_prompt_tokens": 2964, "main_completion_tokens": 25, "judge_total_tokens": 472, "judge_prompt_tokens": 386, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 298.2368469238281, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is a specific numerical rating (8.466), but the generated answer says 'I don't know,' which does not match the correct rating. It is therefore incorrect. However, it is fully faithful to the model's uncertainty and does not hallucinate an answer, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 490, "judge_prompt_tokens": 398, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med [Doc 121125]", "latency_ms": 4409.162282943726, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer 'Mighty Med' is correct, as it matches the gold answer. However, it includes an extraneous citation-like tag '[Doc 121125]' that was not present in the gold answer, so it is not fully faithful to the gold phrasing.", "main_total_tokens": 2612, "main_prompt_tokens": 2595, "main_completion_tokens": 17, "judge_total_tokens": 486, "judge_prompt_tokens": 400, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina [Doc 53582]", "latency_ms": 5332.47184753418, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer ('Argentina') exactly and does not introduce any extraneous or contradictory information.", "main_total_tokens": 2885, "main_prompt_tokens": 2872, "main_completion_tokens": 13, "judge_total_tokens": 451, "judge_prompt_tokens": 398, "judge_completion_tokens": 53}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "Jiang Jie [Doc 135231]", "latency_ms": 5803.4772872924805, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, identifying Jiang Jie as the person whose trap the apprentice falls into. There is no hallucinated information or deviation from the provided reference.", "main_total_tokens": 3025, "main_prompt_tokens": 3008, "main_completion_tokens": 17, "judge_total_tokens": 458, "judge_prompt_tokens": 392, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "9", "latency_ms": 7367.5267696380615, "retrieval_recall": 0.0, "retrieval_mrr": 0.07692307692307693, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (9) does not match the gold answer (4), so it is factually incorrect with respect to the provided gold label, giving a correctness score of 0. However, the model directly answered the question without adding unsupported explanations or hallucinated reasoning, so its response is internally consistent and faithful to the (simple) task, resulting in a faithfulness score of 1.", "main_total_tokens": 3812, "main_prompt_tokens": 3787, "main_completion_tokens": 25, "judge_total_tokens": 495, "judge_prompt_tokens": 385, "judge_completion_tokens": 110}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "I cannot find any movies or shows matching all of those criteria.", "latency_ms": 3509.036064147949, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly communicates that no such movies or shows can be found, which aligns with the gold answer stating there are no movies featuring both Bruce Lee and Steven Seagal. It is faithful to the gold response and does not introduce conflicting or extraneous information.", "main_total_tokens": 2413, "main_prompt_tokens": 2411, "main_completion_tokens": 2, "judge_total_tokens": 489, "judge_prompt_tokens": 406, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3 [Doc 42750]", "latency_ms": 4738.152980804443, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (6.3) and does not introduce any unsupported information. Both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2459, "main_prompt_tokens": 2446, "main_completion_tokens": 13, "judge_total_tokens": 454, "judge_prompt_tokens": 393, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "drift racing [Doc 36394]", "latency_ms": 7607.935190200806, "retrieval_recall": 1.0, "retrieval_mrr": 0.1111111111111111, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.3010299956639812, "hit_rate_at_10": 1.0, "correctness_score": 0.5, "faithfulness_score": 0.5, "justification": "Gold answer is 'Drift'; generated answer is 'drift racing'. The generated answer is close in meaning but not an exact match in style label, so partial correctness. Faithfulness is also partial since it adds 'racing' beyond the gold term.", "main_total_tokens": 3214, "main_prompt_tokens": 3183, "main_completion_tokens": 31, "judge_total_tokens": 473, "judge_prompt_tokens": 386, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "Confronting a Serial Killer", "latency_ms": 5323.400974273682, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie title as 'Confronting a Serial Killer.' It is both correct and faithful to the provided reference.", "main_total_tokens": 3031, "main_prompt_tokens": 3022, "main_completion_tokens": 9, "judge_total_tokens": 477, "judge_prompt_tokens": 412, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838 [Doc 20603]", "latency_ms": 4962.63861656189, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and does not introduce any contradictory or extraneous information, so it is both correct and faithful.", "main_total_tokens": 2532, "main_prompt_tokens": 2517, "main_completion_tokens": 15, "judge_total_tokens": 459, "judge_prompt_tokens": 395, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "Victim [Doc 22488]", "latency_ms": 5475.036382675171, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'The Sixth Sense'. The generated answer 'Victim' is incorrect and does not match the gold answer, so it is neither correct nor faithful.", "main_total_tokens": 2962, "main_prompt_tokens": 2948, "main_completion_tokens": 14, "judge_total_tokens": 464, "judge_prompt_tokens": 399, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002 [Doc 27318]", "latency_ms": 5801.034450531006, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly and provides the correct year of release, so it is both correct and faithful to the source.", "main_total_tokens": 3175, "main_prompt_tokens": 3159, "main_completion_tokens": 16, "judge_total_tokens": 447, "judge_prompt_tokens": 389, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled", "latency_ms": 6774.9927043914795, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly giving the title of the 2015 reboot as 'The Transporter Refueled' and is fully supported by the provided information.", "main_total_tokens": 3164, "main_prompt_tokens": 3155, "main_completion_tokens": 9, "judge_total_tokens": 465, "judge_prompt_tokens": 397, "judge_completion_tokens": 68}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015 [Doc 67500]", "latency_ms": 6103.718042373657, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, which matches the gold answer. It does not introduce any contradictory or extraneous information, so it is both correct and faithful.", "main_total_tokens": 3287, "main_prompt_tokens": 3271, "main_completion_tokens": 16, "judge_total_tokens": 470, "judge_prompt_tokens": 393, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "I don't know.", "latency_ms": 5972.10431098938, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that no such movie description can be found, effectively meaning the answer is unknown. The generated answer says 'I don't know,' which is consistent with the gold. It is therefore correct relative to the gold and faithful to it.", "main_total_tokens": 628, "main_prompt_tokens": 587, "main_completion_tokens": 41, "judge_total_tokens": 474, "judge_prompt_tokens": 394, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Havana Darkness [Doc 90923]", "latency_ms": 6009.314060211182, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'Havana Darkness', which matches the gold answer. The extra bracketed document reference does not change the substantive answer and remains faithful to the question and information provided.", "main_total_tokens": 2842, "main_prompt_tokens": 2827, "main_completion_tokens": 15, "judge_total_tokens": 469, "judge_prompt_tokens": 396, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 4835.178375244141, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the TV show as 'Doll & Em.'", "main_total_tokens": 3027, "main_prompt_tokens": 3020, "main_completion_tokens": 7, "judge_total_tokens": 444, "judge_prompt_tokens": 392, "judge_completion_tokens": 52}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam Foe [Doc 39389]", "latency_ms": 5696.939945220947, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly: both state 'Hallam Foe' as the movie title. This is correct for the 2007 film about Hallam investigating his mother’s death, and the answer is fully supported by the provided information.", "main_total_tokens": 3058, "main_prompt_tokens": 3042, "main_completion_tokens": 16, "judge_total_tokens": 491, "judge_prompt_tokens": 409, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "friend or foe\n\nBased on the context provided, the name \"Foe\" in the title \"Hallam Foe\" [Doc 39389] might initially sound like he’s a foe. However, the plot description does not indicate any antagonistic behavior from the character Hallam Foe. Instead, it describes him as someone driven by personal motives and desires, which do not necessarily align with being a foe. Therefore, his last name sounds at first like he’s a foe, but the context does not support this initial impression.", "latency_ms": 19110.047101974487, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The question is a simple yes/no-style classification: does his last name *sound at first* like friend or foe? The gold answer is explicitly 'A foe'. The generated answer 'friend or foe' does not commit to one option and so is incorrect relative to the gold, yielding a correctness score of 0. In terms of faithfulness, the given reasoning mentions that the name \"Foe\" sounds like he’s a foe, but then overcomplicates the issue by discussing plot context that is irrelevant to the specific question, and ultimately the final generated answer text ('friend or foe') contradicts the reasoning and the gold. The explanation partially engages with the provided context but is not aligned with the actual required answer, so it is only partially faithful to the task and context, resulting in a faithfulness score of 0.5.", "main_total_tokens": 3154, "main_prompt_tokens": 3019, "main_completion_tokens": 135, "judge_total_tokens": 697, "judge_prompt_tokens": 494, "judge_completion_tokens": 203}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18 [Doc 124583]", "latency_ms": 5437.685966491699, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18, which matches the gold answer, and it does not introduce any unsupported or extraneous claims.", "main_total_tokens": 2787, "main_prompt_tokens": 2772, "main_completion_tokens": 15, "judge_total_tokens": 455, "judge_prompt_tokens": 395, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "autism spectrum [Doc 124583]", "latency_ms": 5499.025344848633, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly in content, correctly identifying that Sam is on the autism spectrum, and there is no contradiction with the provided information.", "main_total_tokens": 2802, "main_prompt_tokens": 2786, "main_completion_tokens": 16, "judge_total_tokens": 457, "judge_prompt_tokens": 395, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix [Doc 59659]", "latency_ms": 5944.224119186401, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Chamonix') and does not introduce any contradictions or unsupported claims besides a benign document reference, so it is both correct and faithful.", "main_total_tokens": 3255, "main_prompt_tokens": 3240, "main_completion_tokens": 15, "judge_total_tokens": 473, "judge_prompt_tokens": 406, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481]", "latency_ms": 6020.20263671875, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and does not introduce any extraneous information, so it is both correct and faithful.", "main_total_tokens": 3362, "main_prompt_tokens": 3347, "main_completion_tokens": 15, "judge_total_tokens": 453, "judge_prompt_tokens": 398, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland [Doc 114810]", "latency_ms": 5628.044843673706, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks where Tabaluga is crown-prince of; the correct answer is Greenland. The generated answer is exactly 'Greenland', matching the gold answer with no additions or contradictions, so it is both correct and faithful.", "main_total_tokens": 3038, "main_prompt_tokens": 3023, "main_completion_tokens": 15, "judge_total_tokens": 478, "judge_prompt_tokens": 401, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon [Doc 96838]", "latency_ms": 5150.324821472168, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('One Afternoon') and does not introduce any contradictory or extra information beyond a benign document ID tag, so it is both correct and faithful.", "main_total_tokens": 2627, "main_prompt_tokens": 2612, "main_completion_tokens": 15, "judge_total_tokens": 479, "judge_prompt_tokens": 412, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The Life Ahead [Doc 93260]", "latency_ms": 2442.4376487731934, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying 'The Life Ahead' as the film described. It is fully correct and faithful to the reference.", "main_total_tokens": 2918, "main_prompt_tokens": 2903, "main_completion_tokens": 15, "judge_total_tokens": 469, "judge_prompt_tokens": 407, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "Bruce McLaren [Doc 73859]", "latency_ms": 5378.464698791504, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary “McLaren” and aligns exactly with the gold answer, with no added unsupported information.", "main_total_tokens": 2847, "main_prompt_tokens": 2833, "main_completion_tokens": 14, "judge_total_tokens": 459, "judge_prompt_tokens": 393, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren Motor Racing team [Doc 73859]", "latency_ms": 7271.69942855835, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer in substance (name of the team: McLaren Motor Racing team). The added citation tag does not change the meaning. It is both correct and faithful to the given information.", "main_total_tokens": 3212, "main_prompt_tokens": 3182, "main_completion_tokens": 30, "judge_total_tokens": 468, "judge_prompt_tokens": 394, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "New Zealand [Doc 73859]", "latency_ms": 7008.862018585205, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the country as New Zealand, matching the gold answer. The additional bracketed document reference does not alter the factual content, so the response is both correct and faithful.", "main_total_tokens": 3013, "main_prompt_tokens": 2986, "main_completion_tokens": 27, "judge_total_tokens": 453, "judge_prompt_tokens": 384, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "A Good Day to Die Hard [Doc 58791]", "latency_ms": 6147.682905197144, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is factually incorrect: in the 2013 film 'A Good Day to Die Hard', John McClane does not blow up the Great Wall of China, nor is there such a movie matching the question. It also contradicts the gold response, which correctly states that no such movie exists in the database, so the answer is unfaithful to the provided gold standard.", "main_total_tokens": 3265, "main_prompt_tokens": 3247, "main_completion_tokens": 18, "judge_total_tokens": 515, "judge_prompt_tokens": 410, "judge_completion_tokens": 105}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit [Doc 28589]", "latency_ms": 5822.265386581421, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Tomie: Forbidden Fruit'), correctly identifying the 2002 Japanese horror movie from the description. There are no additions or deviations beyond the bracketed document reference, so the response is both correct and faithful to the gold.", "main_total_tokens": 3041, "main_prompt_tokens": 3024, "main_completion_tokens": 17, "judge_total_tokens": 512, "judge_prompt_tokens": 428, "judge_completion_tokens": 84}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 [Doc 98566]", "latency_ms": 5979.979038238525, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years) and does not introduce unsupported information. It is both correct and faithful to the referenced content.", "main_total_tokens": 3484, "main_prompt_tokens": 3470, "main_completion_tokens": 14, "judge_total_tokens": 463, "judge_prompt_tokens": 402, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "The Book of Souls [Doc 82582]", "latency_ms": 6020.82371711731, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that they search for 'The Book of Souls,' which matches the gold answer and is faithful to the question.", "main_total_tokens": 3355, "main_prompt_tokens": 3339, "main_completion_tokens": 16, "judge_total_tokens": 462, "judge_prompt_tokens": 404, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 5944.962978363037, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about a movie that is not in the database. The gold answer explicitly states this. The generated answer says 'I don't know,' which is effectively equivalent: it does not hallucinate a year or contradict the gold. Thus it is correct (no incorrect information) and faithful (aligned with the gold response).", "main_total_tokens": 635, "main_prompt_tokens": 594, "main_completion_tokens": 41, "judge_total_tokens": 490, "judge_prompt_tokens": 395, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2007.3773860931396, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, effectively conveying lack of knowledge. The generated answer 'I don't know.' is consistent with that and does not introduce any incorrect facts. It is therefore correct relative to the gold and fully faithful.", "main_total_tokens": 636, "main_prompt_tokens": 634, "main_completion_tokens": 2, "judge_total_tokens": 475, "judge_prompt_tokens": 393, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1855.374813079834, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer appropriately expresses lack of knowledge, which is effectively equivalent to the gold response stating the movie is not in the database. It does not introduce any fabricated details and stays faithful to the provided information.", "main_total_tokens": 572, "main_prompt_tokens": 570, "main_completion_tokens": 2, "judge_total_tokens": 467, "judge_prompt_tokens": 395, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 2602.9820442199707, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers convey lack of knowledge about the movie rather than fabricating a date. The generated answer is fully consistent with the gold and is correct given the task of not hallucinating.", "main_total_tokens": 600, "main_prompt_tokens": 598, "main_completion_tokens": 2, "judge_total_tokens": 466, "judge_prompt_tokens": 395, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1885.4849338531494, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie that is not in the database. Both the gold answer and the generated answer convey lack of knowledge. The generated response does not invent a year or contradict the gold; it is fully consistent and thus correct and faithful given the information.", "main_total_tokens": 593, "main_prompt_tokens": 591, "main_completion_tokens": 2, "judge_total_tokens": 482, "judge_prompt_tokens": 395, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 8146.158695220947, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database, which is effectively equivalent to not knowing the release year. The generated answer 'I don't know.' expresses the same uncertainty and does not hallucinate a date, so it is both correct and faithful to the gold response.", "main_total_tokens": 623, "main_prompt_tokens": 563, "main_completion_tokens": 60, "judge_total_tokens": 479, "judge_prompt_tokens": 393, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1875.0619888305664, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers appropriately express lack of knowledge about the movie. The generated response is semantically equivalent to the gold: neither fabricates a release year or additional details. Therefore it is fully correct and faithful.", "main_total_tokens": 594, "main_prompt_tokens": 592, "main_completion_tokens": 2, "judge_total_tokens": 471, "judge_prompt_tokens": 396, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "Aldi Purnomo [Doc 46069]", "latency_ms": 6089.977264404297, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer names a specific director (Aldi Purnomo) that conflicts with the gold standard response stating the database does not provide director information. This makes the answer factually incorrect relative to the task and unfaithful to the given source/database constraints.", "main_total_tokens": 3403, "main_prompt_tokens": 3385, "main_completion_tokens": 18, "judge_total_tokens": 490, "judge_prompt_tokens": 407, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 2046.8339920043945, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the information is not available in the database; the generated answer says 'I don't know,' which is equivalent in meaning. It is correct to not fabricate a director and fully faithful to the gold response.", "main_total_tokens": 613, "main_prompt_tokens": 611, "main_completion_tokens": 2, "judge_total_tokens": 474, "judge_prompt_tokens": 399, "judge_completion_tokens": 75}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not specify who directed the movie \"Journey into Gaza\". [Doc 111360]", "latency_ms": 6958.636283874512, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director is not specified, aligning with the gold answer that the database does not provide the director information. It does not introduce any unsupported details and stays faithful to the given evidence.", "main_total_tokens": 3055, "main_prompt_tokens": 3026, "main_completion_tokens": 29, "judge_total_tokens": 487, "judge_prompt_tokens": 414, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about the director of \"The Last Night at Tremore Beach\". The document only provides details such as type, year, rating, popularity, language, genres, and plot but does not mention the director [Doc 135098].", "latency_ms": 10373.409032821655, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not provide information about the director and accurately reflects the content of the referenced document. It matches the gold answer in substance and does not introduce unsupported details.", "main_total_tokens": 2824, "main_prompt_tokens": 2764, "main_completion_tokens": 60, "judge_total_tokens": 520, "judge_prompt_tokens": 450, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 4861.062288284302, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy' and remaining fully faithful to the provided information.", "main_total_tokens": 2820, "main_prompt_tokens": 2811, "main_completion_tokens": 9, "judge_total_tokens": 457, "judge_prompt_tokens": 396, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 6414.875745773315, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Stranger in My Bed' and remaining fully faithful to the given information.", "main_total_tokens": 2982, "main_prompt_tokens": 2959, "main_completion_tokens": 23, "judge_total_tokens": 461, "judge_prompt_tokens": 400, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 4241.64605140686, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title where a successful Atlanta attorney’s long-lost childhood friends reappear after 25 years. The gold answer is 'Game of Silence', and the generated answer matches exactly. The response is correct and faithful to the gold.", "main_total_tokens": 2494, "main_prompt_tokens": 2488, "main_completion_tokens": 6, "judge_total_tokens": 481, "judge_prompt_tokens": 400, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 4628.265142440796, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct movie title for the described plot. It is both correct and fully faithful to the given information.", "main_total_tokens": 2822, "main_prompt_tokens": 2816, "main_completion_tokens": 6, "judge_total_tokens": 458, "judge_prompt_tokens": 395, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 4873.332500457764, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Wood Job!') and correctly identifies the movie based on the plot description, with no added or missing information.", "main_total_tokens": 3190, "main_prompt_tokens": 3184, "main_completion_tokens": 6, "judge_total_tokens": 465, "judge_prompt_tokens": 403, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "Sesame Street: The Magical Wand Chase", "latency_ms": 7332.237243652344, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, providing the correct movie title with no deviations or additions.", "main_total_tokens": 2992, "main_prompt_tokens": 2961, "main_completion_tokens": 31, "judge_total_tokens": 468, "judge_prompt_tokens": 417, "judge_completion_tokens": 51}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond [Doc 74597]", "latency_ms": 4924.789190292358, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Beyond') and does not introduce any extraneous or incorrect information, so it is both correct and faithful.", "main_total_tokens": 2547, "main_prompt_tokens": 2534, "main_completion_tokens": 13, "judge_total_tokens": 476, "judge_prompt_tokens": 415, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9 [Doc 22478]", "latency_ms": 5452.26526260376, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('6ixtynin9') and does not introduce any extraneous or conflicting information, so it is both correct and faithful.", "main_total_tokens": 2630, "main_prompt_tokens": 2612, "main_completion_tokens": 18, "judge_total_tokens": 481, "judge_prompt_tokens": 415, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread [Doc 32410]", "latency_ms": 5463.639259338379, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread', which matches the gold answer exactly. It is fully consistent and faithful to the provided information.", "main_total_tokens": 2988, "main_prompt_tokens": 2974, "main_completion_tokens": 14, "judge_total_tokens": 495, "judge_prompt_tokens": 433, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Toilet", "latency_ms": 4905.336618423462, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Toilet') and correctly identifies the movie described. It is fully faithful to the gold.", "main_total_tokens": 3243, "main_prompt_tokens": 3238, "main_completion_tokens": 5, "judge_total_tokens": 473, "judge_prompt_tokens": 415, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel [Doc 1024]", "latency_ms": 5285.714864730835, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Electric Angel'), so it is correct. It is also fully faithful to the provided information with no added or contradictory content.", "main_total_tokens": 2885, "main_prompt_tokens": 2872, "main_completion_tokens": 13, "judge_total_tokens": 488, "judge_prompt_tokens": 424, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "Down Low [Doc 106995]", "latency_ms": 3219.2721366882324, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer names a different movie (“Down Low”) than the gold answer (“Mega Ape”), so it is factually incorrect relative to the labeled data. There is no indication in the prompt that would justify deviating from the gold answer, so the response is also unfaithful to the task/instruction.", "main_total_tokens": 2805, "main_prompt_tokens": 2790, "main_completion_tokens": 15, "judge_total_tokens": 477, "judge_prompt_tokens": 389, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 296.8759536743164, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the highest rated TV show from 2010 is 'Tea House'. The generated answer says 'I don't know', which does not match the gold and is therefore incorrect. However, it does not introduce any hallucinated facts; it simply expresses a lack of knowledge, so it is fully faithful to the (non-)information it provides.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 489, "judge_prompt_tokens": 386, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 234.39288139343262, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific movie title, but the generated answer says only 'I don't know,' which does not provide the correct movie and is therefore incorrect. However, it is faithful to the model's uncertainty and does not introduce any fabricated information, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 482, "judge_prompt_tokens": 395, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "en [Doc 768]", "latency_ms": 4744.797706604004, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer specifies the language as 'English', while the generated answer is 'en'. Although 'en' is the ISO code for English, the task evaluates exactness against the gold answer, so the generated answer is considered incorrect. Additionally, the generated answer includes an extraneous citation-like fragment '[Doc 768]' that is not supported by the gold reference, reducing faithfulness.", "main_total_tokens": 3163, "main_prompt_tokens": 3152, "main_completion_tokens": 11, "judge_total_tokens": 497, "judge_prompt_tokens": 389, "judge_completion_tokens": 108}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "Freaks and Geeks", "latency_ms": 5185.135364532471, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer \"Freaks and Geeks\" does not match the gold answer \"Law & Order: Special Victims Unit\" and is therefore incorrect. However, the model faithfully answered with a single TV show title as requested, without adding unsupported information, so it is faithful to the prompt format.", "main_total_tokens": 3148, "main_prompt_tokens": 3139, "main_completion_tokens": 9, "judge_total_tokens": 487, "judge_prompt_tokens": 395, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2 [Doc 79355]", "latency_ms": 4726.680040359497, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and stays consistent with the provided information, so it is both correct and faithful.", "main_total_tokens": 2443, "main_prompt_tokens": 2430, "main_completion_tokens": 13, "judge_total_tokens": 452, "judge_prompt_tokens": 392, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "1999 [Doc 22481]", "latency_ms": 5689.324617385864, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1999) does not match the provided gold answer (2019), so it is incorrect under this evaluation. However, the answer is directly taken from the referenced document [Doc 22481], so it is faithful to the given context.", "main_total_tokens": 3027, "main_prompt_tokens": 3011, "main_completion_tokens": 16, "judge_total_tokens": 476, "judge_prompt_tokens": 393, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "1", "latency_ms": 3768.4831619262695, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (1) does not match the gold answer (279), so it is incorrect. However, there is no sign of fabrication about process or unseen sources; it is simply a wrong numeric value, so faithfulness to the (albeit incorrect) stated answer is maintained.", "main_total_tokens": 2379, "main_prompt_tokens": 2376, "main_completion_tokens": 3, "judge_total_tokens": 471, "judge_prompt_tokens": 383, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "2", "latency_ms": 3768.2437896728516, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (2) does not match the gold answer (12), so it is factually incorrect. However, the model directly answered the question about the number of votes without adding unsupported details, so it is faithful to the user query.", "main_total_tokens": 2468, "main_prompt_tokens": 2463, "main_completion_tokens": 5, "judge_total_tokens": 463, "judge_prompt_tokens": 383, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "2009 [Doc 46069]", "latency_ms": 6096.584320068359, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'The Last Days of Lehman Brothers' was released in 2009, which matches the gold answer. It is also faithful to the provided information without adding unsupported details.", "main_total_tokens": 3380, "main_prompt_tokens": 3364, "main_completion_tokens": 16, "judge_total_tokens": 470, "judge_prompt_tokens": 396, "judge_completion_tokens": 74}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "en [Doc 48035]", "latency_ms": 5427.679061889648, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the original language is English (\"en\"), and the generated answer matches this exactly. The response is both correct and faithful to the gold.", "main_total_tokens": 3075, "main_prompt_tokens": 3062, "main_completion_tokens": 13, "judge_total_tokens": 454, "judge_prompt_tokens": 390, "judge_completion_tokens": 64}
