{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "Wall Street was released in the year 1987 [Doc 6993].", "latency_ms": 7844.435691833496, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Wall Street was released in 1987, matching the gold answer. It is fully correct and faithful to the provided information.", "main_total_tokens": 3063, "main_prompt_tokens": 3042, "main_completion_tokens": 21, "judge_total_tokens": 456, "judge_prompt_tokens": 394, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The classmates having to play a survival game using their cell phones is described in \"Brain Fluid Explosion Girl\" [Doc 69929]. In this plot, Hana Ichii and her classmates find themselves locked in a cage and are forced to take part in a survival game that utilizes their cellphones.", "latency_ms": 3501.4493465423584, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Brain Fluid Explosion Girl' and aligns with the gold answer. It remains consistent with the described plot (classmates, survival game, cell phones) and does not introduce unsupported information.", "main_total_tokens": 2787, "main_prompt_tokens": 2723, "main_completion_tokens": 64, "judge_total_tokens": 518, "judge_prompt_tokens": 447, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show where six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" [Doc 112849].", "latency_ms": 2965.2302265167236, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the show as 'Dungeons & Dragons,' which matches the gold answer exactly. It remains faithful to the given information and does not introduce contradictions or unsupported claims beyond a harmless citation marker.", "main_total_tokens": 2791, "main_prompt_tokens": 2757, "main_completion_tokens": 34, "judge_total_tokens": 492, "judge_prompt_tokens": 419, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie that tells the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is \"Nacht der Wölfe\" [Doc 2590].", "latency_ms": 3135.972738265991, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe', which matches the gold answer exactly. It is fully supported by the referenced document and does not introduce any contradictions or unsupported claims, so both correctness and faithfulness are maximal.", "main_total_tokens": 2823, "main_prompt_tokens": 2780, "main_completion_tokens": 43, "judge_total_tokens": 519, "judge_prompt_tokens": 438, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of Picnic at Hanging Rock take place in the year 1900 [Doc 126191].", "latency_ms": 3041.9435501098633, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of Picnic at Hanging Rock take place in 1900, matching the gold answer. It is also faithful to the provided information referencing the document.", "main_total_tokens": 3072, "main_prompt_tokens": 3043, "main_completion_tokens": 29, "judge_total_tokens": 474, "judge_prompt_tokens": 406, "judge_completion_tokens": 68}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The 2004 movie described is \"June.\" This film centers around a young violinist named June who works in an eccentric coffee shop in Venice, spends her nights preparing for a professional music career, and daydreams about meeting her destined partner. [Doc 32483]", "latency_ms": 3721.273422241211, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title of the 2004 movie, whose answer is 'June.' The generated answer correctly identifies the film as 'June' and accurately restates the description from the prompt without adding contradictions. It remains grounded in the provided information and does not introduce unsupported claims beyond light paraphrasing, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3176, "main_prompt_tokens": 3114, "main_completion_tokens": 62, "judge_total_tokens": 591, "judge_prompt_tokens": 488, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "Based on the provided documents, the highest-rated Russian-language (ru) movie from 1980 is \"Moscow Does Not Believe in Tears\" with a rating of 7.6 [Doc 61].", "latency_ms": 3589.272975921631, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the gold label identifies 'The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger' as the highest-rated Russian-language movie from 1980, while the model instead names 'Moscow Does Not Believe in Tears.' However, the answer is faithful to the cited document (Doc 61) and does not contradict its own provided source, so faithfulness to the referenced evidence is high even though the final conclusion is wrong relative to the gold.", "main_total_tokens": 3272, "main_prompt_tokens": 3226, "main_completion_tokens": 46, "judge_total_tokens": 564, "judge_prompt_tokens": 437, "judge_completion_tokens": 127}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "The highest Russian-language movie rating in 1999, based on the provided documents, is from \"Kamenskaya\" with a rating of 8.1 [Doc 115256].", "latency_ms": 3428.210973739624, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the highest Russian-language movie rating in 1999 is 7.6, but the generated answer claims it is 8.1 for \"Kamenskaya\". This directly conflicts with the gold value, so the answer is incorrect. It also references a specific document and rating (8.1) that do not align with the gold label, indicating the reasoning is not faithful to the ground truth provided.", "main_total_tokens": 3192, "main_prompt_tokens": 3145, "main_completion_tokens": 47, "judge_total_tokens": 540, "judge_prompt_tokens": 423, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "Based on the provided documents, there are two movies that meet the criteria of being released in 1987, having a rating over 7, and being in Chinese (cn):\n\n1. \"A Chinese Ghost Story\" with a rating of 7.3 [Doc 7064].\n2. \"Last Eunuch In China\" with a rating of 7.8 [Doc 7754].\n\nTherefore, the answer is 2 movies.", "latency_ms": 4300.71496963501, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer says 4 movies, while the generated answer concludes there are 2 movies, so the final result is incorrect (correctness 0). The assistant grounds its reasoning in the provided documents and clearly explains how it arrived at 2, but it evidently missed additional qualifying movies present in the data, so its use of context is only partially faithful (faithfulness 0.5).", "main_total_tokens": 3073, "main_prompt_tokens": 2975, "main_completion_tokens": 98, "judge_total_tokens": 594, "judge_prompt_tokens": 482, "judge_completion_tokens": 112}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "Based on the provided documents, there is only one entry that matches all criteria: being a TV show from 1987 with a rating over 7 and in Chinese. That would be \"Tian Mimi\" [Doc 117988], which has a rating of 7.2 and is listed under the 'Drama' and 'Family' genres, though no plot details are provided for this entry.\n\nHowever, it's noted that \"Tian Mimi\" is in Chinese (zh), but the only document that lists a Chinese language TV show from 1987 with a rating over 7 is not found. The closest match by year and rating is \"A Chinese Ghost Story,\" which has a rating of 7.3, but it's listed as a movie [Doc 7064], not a TV show.\n\nTherefore, there are no TV shows that meet all the criteria specified in the question based on the provided documents.", "latency_ms": 6388.452053070068, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 2, but the generated answer concludes there are 0 matching TV shows, so it is factually incorrect relative to the gold. However, the reasoning is consistent with the documents as the model describes them and does not introduce unsupported information; it carefully distinguishes between TV shows and movies and bases its conclusion on the (possibly incomplete) provided documents. Thus, faithfulness to the described evidence is high, but correctness w.r.t. the gold is low.", "main_total_tokens": 3466, "main_prompt_tokens": 3265, "main_completion_tokens": 201, "judge_total_tokens": 704, "judge_prompt_tokens": 578, "judge_completion_tokens": 126}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "Based on the provided documents, the only French language TV show released in 1980 is \"Therese Raquin\" [Doc 112644]. However, it's important to note that while \"Therese Raquin\" is listed with a French language entry, most of these details are consistent with other entries for the year 1980. Yet, another document lists \"Pé de Vento\" as a Portuguese show [Doc 112590], and \"Luzie, der Schrecken der Straße\" is listed as German [Doc 114846]. Therefore, \"Therese Raquin\" is the only French language entry for 1980.", "latency_ms": 5530.99250793457, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Fantômas', but the generated answer claims the only French language TV show released in 1980 is 'Therese Raquin', which is incorrect. The reasoning in the generated answer is internally consistent with the (cited) documents and does not contradict itself, so it is faithful to its own referenced context, but it is factually wrong relative to the gold.", "main_total_tokens": 3343, "main_prompt_tokens": 3187, "main_completion_tokens": 156, "judge_total_tokens": 626, "judge_prompt_tokens": 516, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie that matches your description is \"Kill Cruise\" [Doc 11205]. This film involves two British beauties who go to Barbados with a yacht captain, though the plot suggests they may be in for unexpected events.", "latency_ms": 3401.7183780670166, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise', matching the gold answer exactly. It remains consistent with the provided description and does not introduce contradictions, so it is fully correct and faithful.", "main_total_tokens": 3016, "main_prompt_tokens": 2966, "main_completion_tokens": 50, "judge_total_tokens": 499, "judge_prompt_tokens": 429, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie that discusses the TV series in which Fred Rogers speaks to children about some of life's most important issues is \"Mister Rogers: It's You I Like\" [Doc 86144]. However, it's worth noting that the actual documentary about Fred Rogers' neighborhood series is titled \"Won't You Be My Neighbor?\" [Doc 84838], which also fits the description provided.", "latency_ms": 4283.92481803894, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question expects a single title: 'Won't You Be My Neighbor?'. The generated answer introduces another film title as the primary answer and then hedges by mentioning the correct one as an alternative, which makes the main response incorrect for the given QA pair. It also invents document references ([Doc 86144], [Doc 84838]) and claims about them that are not grounded in the provided context, so it is not fully faithful to the task setup.", "main_total_tokens": 3253, "main_prompt_tokens": 3164, "main_completion_tokens": 89, "judge_total_tokens": 605, "judge_prompt_tokens": 480, "judge_completion_tokens": 125}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The movie \"1969\" was released in the year 1988 [Doc 8389].", "latency_ms": 3234.710931777954, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The question expects the answer '2018', but the generated answer gives '1988', which is incorrect. It also introduces an irrelevant movie titled '1969' and cites a document, rather than aligning with the gold answer, so it is not faithful to the intended reference.", "main_total_tokens": 3354, "main_prompt_tokens": 3327, "main_completion_tokens": 27, "judge_total_tokens": 484, "judge_prompt_tokens": 397, "judge_completion_tokens": 87}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that fits the description of being an animation about a young boy robot in 2030 AD is \"Astro Boy\" [Doc 112591].", "latency_ms": 3180.163860321045, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying 'Astro Boy' as the TV series about a young boy robot in 2030 AD. It is fully correct and directly supported by the cited document, so both correctness and faithfulness are perfect.", "main_total_tokens": 3026, "main_prompt_tokens": 2986, "main_completion_tokens": 40, "judge_total_tokens": 502, "judge_prompt_tokens": 420, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 2985.823154449463, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer. It is also faithful to the provided information, with no contradictions or hallucinations.", "main_total_tokens": 2778, "main_prompt_tokens": 2742, "main_completion_tokens": 36, "judge_total_tokens": 496, "judge_prompt_tokens": 424, "judge_completion_tokens": 72}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26m before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 3390.805959701538, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26m before the 26 November 1983 break-in, matching the provided gold answer exactly and remaining consistent with the given information.", "main_total_tokens": 3160, "main_prompt_tokens": 3116, "main_completion_tokens": 44, "judge_total_tokens": 505, "judge_prompt_tokens": 428, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "To determine the popularity score of the most popular English-speaking TV show from 1997, we need to compare the popularity scores of all the English-language shows listed. Here are the relevant entries:\n\n- **USA High**: Popularity: 3.535 [Doc 114769]\n- **Sasuke**: Popularity: 1.9055 [Doc 114824]\n- **Smart Guy**: Popularity: 4.7892 [Doc 114749]\n- **I'm Alan Partridge**: Popularity: 2.5849 [Doc 114785]\n\nAmong these, the highest popularity score is for \"Smart Guy\" with a score of 4.7892.\n\nTherefore, the popularity score of the most popular English-speaking TV show released in 1997 is **4.7892**.", "latency_ms": 6369.38738822937, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.8, "justification": "The answer is incorrect because the gold popularity score is 80.5772, while the model outputs 4.7892. The reasoning is mostly grounded in the (partial) list of shows it considers and compares, but it is not faithful to the full context that evidently includes a more popular English-speaking 1997 TV show with score 80.5772. It also implicitly assumes the listed subset is exhaustive without justification, so faithfulness is not perfect.", "main_total_tokens": 3491, "main_prompt_tokens": 3291, "main_completion_tokens": 200, "judge_total_tokens": 688, "judge_prompt_tokens": 562, "judge_completion_tokens": 126}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "Based on the provided documents, there are no Russian language TV shows listed for the year 1997. The only Russian language entry is a movie titled \"A House ‎for Rent with All the Inconveniences\" from the year 2016 [Doc 72036].", "latency_ms": 3907.3667526245117, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the number of Russian language TV shows released in 1997, with the gold answer being 1. The generated answer claims there are none, which is factually incorrect relative to the gold, so correctness is 0. However, the answer appears consistent with and grounded in the cited document (stating only a 2016 Russian movie is listed), so it is faithful to the provided evidence, giving faithfulness a score of 1.", "main_total_tokens": 3440, "main_prompt_tokens": 3376, "main_completion_tokens": 64, "judge_total_tokens": 558, "judge_prompt_tokens": 439, "judge_completion_tokens": 119}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in the TV show \"Angel Eyes\" [Doc 121928].", "latency_ms": 3608.4280014038086, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Angel Eyes'. It is fully correct and consistent with the provided information, so both correctness and faithfulness are maximal.", "main_total_tokens": 3523, "main_prompt_tokens": 3483, "main_completion_tokens": 40, "judge_total_tokens": 499, "judge_prompt_tokens": 429, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"I Dream in Another Language\" is es, which stands for Spanish [Doc 78127].", "latency_ms": 3167.604684829712, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer states the language is Korean, but the generated answer claims it is Spanish (es), which is factually incorrect in this context. It also mentions a specific movie title and document reference that are not supported by the given gold label, so the answer is neither correct nor faithful to the provided information.", "main_total_tokens": 3229, "main_prompt_tokens": 3200, "main_completion_tokens": 29, "judge_total_tokens": 494, "judge_prompt_tokens": 401, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "Based on the provided documents, there are no English-speaking (language: en) Horror movies from 2009 with a popularity score greater than 7. The highest popularity scores for English-speaking Horror movies from 2009 in this list are:\n\n- \"Halloween II\" with a popularity of 4.0125 [Doc 44901]\n- \"Haunted Universities\" with a popularity of 3.5251 [Doc 44954]\n\nNone exceed the threshold of 7.", "latency_ms": 4736.605167388916, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The gold answer is 2, but the generated answer claims there are none, so the final numeric result is incorrect. The reasoning is mostly grounded in the cited documents and does not invent obviously unsupported facts, but it is incomplete or overly narrow (it only considers two movies) leading to a wrong conclusion. Hence high but not perfect faithfulness, and correctness is 0.", "main_total_tokens": 3340, "main_prompt_tokens": 3227, "main_completion_tokens": 113, "judge_total_tokens": 598, "judge_prompt_tokens": 490, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Based on the provided documents, there are no movies from 1979 with ratings. The documents list movies from various years but do not include any from 1979. Therefore, I cannot determine the best-rated movie from 1979 using only this information.", "latency_ms": 3894.902467727661, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer explicitly states that there are no movies from 1979 in the database, without introducing ratings. The generated answer adds that there are no movies from 1979 with ratings and that it cannot determine the best-rated movie. This introduces extra detail (about ratings) that is not supported by the gold answer and could be incorrect relative to the evaluation standard, so correctness relative to the gold is 0. However, the generated answer is internally consistent with the premise that no 1979 movies exist in the provided documents, so it is faithful to the given context, giving faithfulness 1.", "main_total_tokens": 3205, "main_prompt_tokens": 3146, "main_completion_tokens": 59, "judge_total_tokens": 599, "judge_prompt_tokens": 447, "judge_completion_tokens": 152}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "Based on the provided documents, there is no information about a \"Back to the Future Part IV\" movie release. The documents mention three main movies from the series: Back to the Future (1985), Back to the Future Part II (1989), and Back to the Future Part III (1990). There is also an animated TV series from 1991, but no information about a fourth movie.", "latency_ms": 5066.112756729126, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.8, "faithfulness_score": 0.8, "justification": "The generated answer correctly states that there is no information about a Back to the Future Part IV movie and lists the known entries in the franchise. However, it does not clearly answer the user’s question in the same spirit as the gold (i.e., explicitly saying that such a movie does not exist), and instead frames it as a lack of information in the provided documents. This is slightly less direct and less faithful to the intent of the gold response, which is to deny the existence of such a film altogether, not just in the given documents.", "main_total_tokens": 4135, "main_prompt_tokens": 4044, "main_completion_tokens": 91, "judge_total_tokens": 626, "judge_prompt_tokens": 482, "judge_completion_tokens": 144}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "None of the documents provided contain information on any movies about the nuclear Chernobyl disaster from the year 1980. The earliest mentioned is from 1990, titled \"Collapse,\" with a popularity score of 5.4109 [Doc 10620]. Therefore, based on the given context, there's no movie from 1980 to determine the least popular one among them.", "latency_ms": 4374.576568603516, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.7, "faithfulness_score": 1, "justification": "The gold answer simply states that no movies from 1980 related to Chernobyl could be found. The generated answer correctly notes that there is no such 1980 movie in the provided documents and hence no popularity score can be determined, which is faithful to the context. However, it adds an arguably unnecessary mention of a 1990 movie and its popularity score before concluding, whereas the gold focuses strictly on the absence of 1980 movies. The conclusion aligns with the gold (no 1980 Chernobyl movie / no popularity score), but the path is slightly divergent, so correctness is mostly but not perfectly aligned; faithfulness to the given documents is intact.", "main_total_tokens": 3261, "main_prompt_tokens": 3171, "main_completion_tokens": 90, "judge_total_tokens": 651, "judge_prompt_tokens": 483, "judge_completion_tokens": 168}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "There are three entries with 'Astro Boy' in the title:\n\n1. Astro Boy (TV, Year: 1980) [Doc 112591]\n2. Astro Boy (Movie, Year: 2009) [Doc 44947]\n3. Astro Boy (TV, Year: 2003) [Doc 116265]", "latency_ms": 4166.136741638184, "retrieval_recall": 0.0, "retrieval_mrr": 1.0, "recall_at_5": 0.75, "precision_at_5": 0.6, "ndcg_at_5": 0.8318724637288826, "hit_rate_at_5": 1.0, "recall_at_10": 0.75, "precision_at_10": 0.3, "ndcg_at_10": 0.8318724637288826, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 titles with 'Astro Boy' while the generated answer lists only 3, so the numerical answer is incorrect. However, the assistant faithfully lists the three entries it identifies without inventing additional ones, so it is faithful to its own reasoning and evidence.", "main_total_tokens": 3167, "main_prompt_tokens": 3078, "main_completion_tokens": 89, "judge_total_tokens": 542, "judge_prompt_tokens": 457, "judge_completion_tokens": 85}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "None of the documents provided contain information about movies from the year 2025 that allude to the Holocaust. The context only includes details on movies up until 2024, and there is no mention of any specific plans for 2025 productions related to the Holocaust.", "latency_ms": 3645.6658840179443, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is a numeric value (2), while the generated answer claims there is no information and does not provide the correct number. This makes the generated answer factually incorrect relative to the gold. However, the generated answer is faithful to the given context, explicitly stating that the documents do not contain information about 2025 movies, which matches the described limitation of the context.", "main_total_tokens": 3146, "main_prompt_tokens": 3085, "main_completion_tokens": 61, "judge_total_tokens": 545, "judge_prompt_tokens": 438, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "Based on the provided documents, there are three TV shows related to Super Mario Bros that were released in the 1990s:\n\n1. **The Adventures of Super Mario Bros. 3** - Released in 1990 [Doc 113661].\n2. **Super Mario World** - Released in 1991 [Doc 113814].\n\nAdditionally, there is a TV show from 1989 that could be considered as it laid the groundwork for future Super Mario Bros. shows:\n\n3. **The Super Mario Bros. Super Show!** - Released in 1989 [Doc 113517].\n\nHowever, since the question specifically asks for shows released in the 1990s, we will only count the first two entries.\n\nTherefore, there were **two TV shows about Super Mario Bros released in the 1990s**.", "latency_ms": 6160.005807876587, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for how many TV shows in the 1990s about Super Mario Bros, with gold answer 1. The generated answer concludes there are two such shows (1990 and 1991), which contradicts the gold; thus correctness is 0. The reasoning is consistent with the documents it cites and does not introduce unsupported facts, so faithfulness is 1.", "main_total_tokens": 3287, "main_prompt_tokens": 3091, "main_completion_tokens": 196, "judge_total_tokens": 664, "judge_prompt_tokens": 556, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "Based on the provided documents, there is no information indicating any TV show about Super Mario Bros was released between the years 2000 and 2010. The closest mentions are:\n\n- \"The Super Mario Bros. Super Show!\" which aired in 1989 [Doc 113517].\n- \"Super Mario World,\" which also aired in 1991 [Doc 113814].\n- \"The Adventures of Super Mario Bros. 3,\" which aired in 1990 [Doc 113661].\n\nNone of these fall within the specified timeframe (2000-2010).", "latency_ms": 4958.803176879883, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 0, meaning there were no TV shows about Super Mario Bros released between 2000 and 2010. The generated answer correctly concludes that none of the listed shows fall within that timeframe and that there is no information indicating any such shows were released then, which aligns with the gold answer. It stays grounded in the provided documents and does not introduce unsupported claims, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3218, "main_prompt_tokens": 3075, "main_completion_tokens": 143, "judge_total_tokens": 623, "judge_prompt_tokens": 505, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "The most popular TV show or movie about ninjas based on popularity is \"UNDER NINJA\" with a popularity score of 3.1982 [Doc 133621].", "latency_ms": 3551.1229038238525, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto', a widely recognized and extremely popular ninja-themed TV show. The generated answer claims 'UNDER NINJA' is the most popular, which contradicts the gold answer and general real-world popularity; thus it is incorrect. However, the generated response appears internally consistent and cites its source, so it is faithful to the (incorrect or incomplete) retrieved document, yielding full faithfulness but zero correctness.", "main_total_tokens": 3388, "main_prompt_tokens": 3345, "main_completion_tokens": 43, "judge_total_tokens": 535, "judge_prompt_tokens": 418, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The provided documents do not contain information about a Jurassic Park/Power Rangers crossover movie, so I cannot provide a release date for such a film based on the given context.", "latency_ms": 3271.2669372558594, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.7, "faithfulness_score": 1, "justification": "The correct answer is that the movie does not exist and thus was not released. The generated answer instead says it cannot provide a release date based on the documents, which implies uncertainty rather than clearly stating that there is no such movie. This is partially correct in not inventing a date but does not directly match the gold answer. However, it is fully faithful to the given context because it accurately reflects that the documents contain no information about such a crossover.", "main_total_tokens": 3230, "main_prompt_tokens": 3195, "main_completion_tokens": 35, "judge_total_tokens": 542, "judge_prompt_tokens": 419, "judge_completion_tokens": 123}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "There is no information provided in the given documents about a movie where Barbie fights the Predator. The movies mentioned feature Barbie in various adventures and settings, but none of them describe a scenario involving a conflict with the Predator. [None of the documents contain this information]", "latency_ms": 3873.9535808563232, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that such a movie was not released. The generated answer effectively conveys the same idea: that there is no information (and thus no movie) about Barbie fighting the Predator in the provided documents. It does not invent a movie title or contradict the gold; instead, it notes absence of such content. This is correct with respect to the question and faithful to the described document constraints.", "main_total_tokens": 3496, "main_prompt_tokens": 3443, "main_completion_tokens": 53, "judge_total_tokens": 544, "judge_prompt_tokens": 434, "judge_completion_tokens": 110}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "To determine the fourth oldest Star Wars movie, we need to list the movies in chronological order based on their release year:\n\n1. **Star Wars: Episode IV - A New Hope** (1977)\n2. **The Empire Strikes Back** (1980)\n3. **Return of the Jedi** (1983)\n4. **Star Wars: The Phantom Menace** (1999)\n\nTherefore, the fourth oldest Star Wars movie is \"Star Wars: The Phantom Menance\" from 1999.\n\nHowever, none of these titles are listed in the provided documents. To strictly answer based on the context given:\n\nThe document [Doc 32101] mentions a documentary titled \"The Story of Star Wars,\" which was released in 2004 and is not a movie from the main series. Therefore, we need to look at other documents for movies.\n\nFrom the provided documents:\n- **Return of the Jedi** (1983) [Doc 2858]\n- **Star Wars: The Phantom Menace** (1999) is implied but not directly listed in the context.\n- Other documentaries and TV series are mentioned, but they do not count as main", "latency_ms": 7575.552463531494, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is 'Star Wars: Droids - The Great Heep', a TV special, while the generated answer concludes 'Star Wars: The Phantom Menace'. This is factually incorrect relative to the gold. In addition, the reasoning ignores the gold context and instead constructs its own list of theatrical films, so it is not faithful to the provided gold answer or context.", "main_total_tokens": 3719, "main_prompt_tokens": 3463, "main_completion_tokens": 256, "judge_total_tokens": 732, "judge_prompt_tokens": 625, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The movie \"1969\" was released in the year 1988 [Doc 8389].", "latency_ms": 3214.8311138153076, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer gives the year 1988, while the correct year is 1986, so it is factually incorrect. However, it appears consistent with the cited document and does not fabricate beyond that, so it is faithful to its cited source.", "main_total_tokens": 3354, "main_prompt_tokens": 3327, "main_completion_tokens": 27, "judge_total_tokens": 474, "judge_prompt_tokens": 397, "judge_completion_tokens": 77}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "The rating of the movie Forrest Gump is 8.466 [Doc 15236].", "latency_ms": 3014.557361602783, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating value 8.466 and does not introduce any unsupported information. It is both correct and faithful to the provided reference.", "main_total_tokens": 3078, "main_prompt_tokens": 3053, "main_completion_tokens": 25, "judge_total_tokens": 476, "judge_prompt_tokens": 413, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 2931.025981903076, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med,' which matches the gold answer exactly. It stays faithful to the question and does not introduce contradictions or irrelevant information.", "main_total_tokens": 2648, "main_prompt_tokens": 2611, "main_completion_tokens": 37, "judge_total_tokens": 490, "judge_prompt_tokens": 423, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In \"Túnel de los huesos\" (2011), the seven prisoners escaped from a prison in Argentina, specifically from the Villa Devoto prison in Buenos Aires [Doc 53582].", "latency_ms": 3373.059034347534, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer correctly identifies Argentina as the country, so it is factually correct with respect to the question. However, it introduces specific details about Villa Devoto prison and a document reference [Doc 53582] that are not supported by the provided gold answer alone, so the answer is not fully faithful to the given information.", "main_total_tokens": 3044, "main_prompt_tokens": 2998, "main_completion_tokens": 46, "judge_total_tokens": 528, "judge_prompt_tokens": 431, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In \"Sweet Trap\" (2024), the apprentice, Li Nai, falls into the trap designed by Jiang Jie, also known as the \"Chef Assassin.\" [Doc 135231]", "latency_ms": 3495.521306991577, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The question only asks whose trap the apprentice falls into, and the generated answer correctly identifies Jiang Jie, matching the gold answer. However, the generated response adds unsupported, specific details such as the apprentice being named Li Nai and Jiang Jie being known as the 'Chef Assassin', as well as a document citation, none of which are grounded in the provided gold answer. This extra fabricated information reduces faithfulness.", "main_total_tokens": 3190, "main_prompt_tokens": 3143, "main_completion_tokens": 47, "judge_total_tokens": 533, "judge_prompt_tokens": 422, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "Based on the provided documents, there are no Korean-language (ko) movies mentioning assassins after 2023. The only Korean-language movie related to assassins is \"Assassination\" from 2015 [Doc 67739].", "latency_ms": 3561.99312210083, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 4 such movies, while the generated answer claims there are none, so it is factually incorrect given the gold label. However, relative to the provided documents (which apparently only contain a 2015 movie), the model consistently states that no post-2023 Korean-language assassin movies are present, so its reasoning is faithful to the cited evidence.", "main_total_tokens": 3073, "main_prompt_tokens": 3017, "main_completion_tokens": 56, "judge_total_tokens": 538, "judge_prompt_tokens": 431, "judge_completion_tokens": 107}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "The provided documents mention Steven Seagal in one context:\n\n- [Doc 119041] Title: Steven Seagal: Lawman\n\nHowever, there are several entries for Bruce Lee-related titles:\n- [Doc 79200] Title: Bruce Lee (2017)\n- [Doc 85241] Title: Bruce Lee and the Outlaw\n- [Doc 70236] Title: Bruce Lee - The Fighter\n- [Doc 56669] Title: I Am Bruce Lee\n- [Doc 46659] Title: The Legend of Bruce Lee (Movie)\n- [Doc 118291] Title: The Legend of Bruce Lee (TV)\n- [Doc 49705] Title: Bruce Lee, My Brother\n- [Doc 16203] Title: Bruce Lee: The Immortal Dragon\n- [Doc 4204] Title: Bruce Lee: The Legend\n- [Doc 14235] Title: Dragon: The Bruce Lee Story\n- [Doc 16319] Title: The Life of Bruce Lee\n- [Doc 24", "latency_ms": 7638.855934143066, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The user asked specifically how many movies feature both Steven Seagal and Bruce Lee, and the gold answer states that there are none. The generated answer instead lists separate titles involving Seagal and Bruce Lee, without addressing the joint appearance question or giving the correct count (zero). It also fabricates references to 'provided documents' and partial citations that are not grounded in the given gold answer. Therefore, it is neither correct relative to the question nor faithful to the gold explanation.", "main_total_tokens": 3633, "main_prompt_tokens": 3377, "main_completion_tokens": 256, "judge_total_tokens": 734, "judge_prompt_tokens": 609, "judge_completion_tokens": 125}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of Tokyo Mater (2008) is 6.3 [Doc 42750].", "latency_ms": 3154.486656188965, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (6.3) and correctly cites the document as its source, so it is both correct and faithful.", "main_total_tokens": 3245, "main_prompt_tokens": 3219, "main_completion_tokens": 26, "judge_total_tokens": 464, "judge_prompt_tokens": 403, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The provided context does not specify a particular style of racing for \"Fast Track: No Limits.\" However, it describes the film as centered around a group of street racers [Doc 41929].", "latency_ms": 3583.7600231170654, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer clearly identifies the style of racing as 'Drift'. The generated answer claims the context does not specify a particular style and instead describes it only as street racing, which contradicts the gold. Thus, it is factually incorrect relative to the gold label. However, the answer appears consistent with the cited context snippet it references, so it is faithful to the provided context, yielding high faithfulness but incorrectness with respect to the ground truth.", "main_total_tokens": 3447, "main_prompt_tokens": 3403, "main_completion_tokens": 44, "judge_total_tokens": 539, "judge_prompt_tokens": 417, "judge_completion_tokens": 122}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and her relationship with the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 3724.0538597106934, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, 'Confronting a Serial Killer', and remains faithful to the information given without adding unsupported claims. The small added phrase about their relationship is consistent with the described subject of the movie and does not contradict the source.", "main_total_tokens": 3171, "main_prompt_tokens": 3117, "main_completion_tokens": 54, "judge_total_tokens": 538, "judge_prompt_tokens": 455, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 3496.684789657593, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and does not introduce any conflicting or extraneous information. It is fully correct and faithful to the provided gold answer.", "main_total_tokens": 3276, "main_prompt_tokens": 3246, "main_completion_tokens": 30, "judge_total_tokens": 470, "judge_prompt_tokens": 407, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "The document provided does not contain the specific details to directly answer your query about a 1999 psychological thriller movie that alludes to an extra sense. However, one of the documents describes a movie from 1999 titled \"Victim,\" which involves supernatural elements and senses, but it doesn't explicitly mention an extra sense or classify itself as a psychological thriller. [Doc 22488]", "latency_ms": 4051.717758178711, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.9, "justification": "The correct answer is 'The Sixth Sense'. The generated answer claims the documents do not contain the specific details and instead talks about another movie titled 'Victim', failing to give the obvious correct title. Thus correctness is 0. The answer appears consistent with its (incorrect) interpretation of the provided document and does not invent specific false document content beyond referencing a plausible doc, so faithfulness is relatively high, but not perfect because it speculates about the document describing 'Victim' without clear basis.", "main_total_tokens": 3058, "main_prompt_tokens": 2972, "main_completion_tokens": 86, "judge_total_tokens": 601, "judge_prompt_tokens": 468, "judge_completion_tokens": 133}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 3103.355646133423, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2002) exactly and does not introduce any extra, ungrounded information beyond a citation placeholder. It is both correct and faithful.", "main_total_tokens": 3343, "main_prompt_tokens": 3320, "main_completion_tokens": 23, "judge_total_tokens": 463, "judge_prompt_tokens": 397, "judge_completion_tokens": 66}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 3253.5674571990967, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, providing the correct title 'The Transporter Refueled' and stays faithful to the information without adding unsupported claims.", "main_total_tokens": 3156, "main_prompt_tokens": 3121, "main_completion_tokens": 35, "judge_total_tokens": 483, "judge_prompt_tokens": 420, "judge_completion_tokens": 63}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "'The Transporter Refueled' was released in the year 2015 [Doc 67500].", "latency_ms": 3223.839282989502, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It is fully supported by the cited document reference, so it is both correct and faithful.", "main_total_tokens": 3452, "main_prompt_tokens": 3424, "main_completion_tokens": 28, "judge_total_tokens": 481, "judge_prompt_tokens": 406, "judge_completion_tokens": 75}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "None of the documents provided contain information about a romantic comedy where Abraham Lincoln comes back from the dead. The closest reference is to \"False Lady,\" but it does not involve Abraham Lincoln and is described as a fantasy-comedy-romance, not a romantic comedy with Abraham Lincoln as a character. [Doc 13075]", "latency_ms": 3867.7775859832764, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a specific romantic comedy where Abraham Lincoln comes back from the dead. Both the gold answer and the generated answer state that no such movie description can be found in the provided documents. The generated answer accurately reflects the absence of relevant information in the documents and even notes the closest, but still irrelevant, match. There are no hallucinated titles or incorrect claims, so the response is both correct and faithful to the given context.", "main_total_tokens": 2914, "main_prompt_tokens": 2845, "main_completion_tokens": 69, "judge_total_tokens": 572, "judge_prompt_tokens": 454, "judge_completion_tokens": 118}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by Ernest Hemingway is \"Intrigo: Death of an Author\" [Doc 84864]. However, it should be noted that the manuscript in question was sent by critically acclaimed writer Germund Rein, not directly by Ernest Hemingway himself.", "latency_ms": 3846.9784259796143, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a movie whose title alludes to a manuscript written by Ernest Hemingway, and the gold answer is 'Havana Darkness.' The generated answer instead gives 'Intrigo: Death of an Author,' which is incorrect. However, the assistant consistently refers to the provided document and does not fabricate contradictions, so it is faithful to its cited source but factually wrong relative to the gold answer.", "main_total_tokens": 2985, "main_prompt_tokens": 2917, "main_completion_tokens": 68, "judge_total_tokens": 558, "judge_prompt_tokens": 447, "judge_completion_tokens": 111}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll heads to Hollywood to be with her childhood best friend Em in the British comedy series \"Doll & Em\" [Doc 122259].", "latency_ms": 3472.2743034362793, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Doll & Em', which matches the gold answer exactly. It remains faithful to the provided information and does not introduce contradictions; the added description as a British comedy series is accurate and consistent with the source citation.", "main_total_tokens": 3278, "main_prompt_tokens": 3243, "main_completion_tokens": 35, "judge_total_tokens": 501, "judge_prompt_tokens": 418, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "The 2007 movie where Hallam, who has a talent for spying on people and attempts to find the cause of his mother's death, is \"Hallam Foe\" [Doc 39389].", "latency_ms": 3828.0279636383057, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2007 movie as 'Hallam Foe', which matches the gold answer exactly. It is faithful to the provided information and does not introduce contradictions or unsupported details.", "main_total_tokens": 3360, "main_prompt_tokens": 3312, "main_completion_tokens": 48, "judge_total_tokens": 511, "judge_prompt_tokens": 440, "judge_completion_tokens": 71}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "The provided context does not contain any information about a character whose last name could initially be mistaken as sounding friendly or unfriendly. Therefore, based on the given documents, it is not possible to answer this question.", "latency_ms": 3594.7628021240234, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "Gold answer is 'A foe'. The question is likely about a name that sounds menacing; the generated answer claims it is unanswerable from context, which contradicts the gold. So it is factually incorrect relative to the labeled answer. However, given the model only had whatever context it saw and it explicitly reasons from that lack of information, it remains faithful to its own stated constraints and does not hallucinate, so faithfulness is high.", "main_total_tokens": 3349, "main_prompt_tokens": 3306, "main_completion_tokens": 43, "judge_total_tokens": 549, "judge_prompt_tokens": 428, "judge_completion_tokens": 121}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "The age of Sam in the 2017 TV show \"Atypical\" is 18 [Doc 124583].", "latency_ms": 3072.4704265594482, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer states that Sam is 18 in the 2017 TV show Atypical, which matches the gold answer exactly. It does not add any conflicting details, so it is both correct and faithful.", "main_total_tokens": 2940, "main_prompt_tokens": 2908, "main_completion_tokens": 32, "judge_total_tokens": 486, "judge_prompt_tokens": 412, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 2932.865858078003, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in the TV show Atypical, Sam is on the autism spectrum, which matches the gold answer without adding incorrect information.", "main_total_tokens": 2862, "main_prompt_tokens": 2833, "main_completion_tokens": 29, "judge_total_tokens": 472, "judge_prompt_tokens": 410, "judge_completion_tokens": 62}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Sandra was born in Chamonix according to the plot description for \"À corde tendue\" [Doc 59659].", "latency_ms": 3408.651828765869, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sandra was born in Chamonix, which matches the gold answer. It attributes this to the plot description, consistent with the provided context, so it is both correct and faithful.", "main_total_tokens": 3329, "main_prompt_tokens": 3298, "main_completion_tokens": 31, "judge_total_tokens": 495, "judge_prompt_tokens": 422, "judge_completion_tokens": 73}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "in this japanese language movie, Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 3536.0231399536133, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains') and correctly fills in the blank. It is fully faithful to the provided information and contains no contradictions or extraneous changes.", "main_total_tokens": 3604, "main_prompt_tokens": 3578, "main_completion_tokens": 26, "judge_total_tokens": 479, "judge_prompt_tokens": 412, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 3320.1146125793457, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks where Tabaluga is the crown-prince of. The gold answer is Greenland. The generated answer states: 'Tabaluga is the last of the dragons and the crown-prince of Greenland,' which exactly matches the gold answer and is supported by the referenced document. Therefore, it is both correct and faithful to the provided information.", "main_total_tokens": 3458, "main_prompt_tokens": 3431, "main_completion_tokens": 27, "judge_total_tokens": 517, "judge_prompt_tokens": 416, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie with the description \"A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.\" is titled **One Afternoon** [Doc 96838].", "latency_ms": 3233.057737350464, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title 'One Afternoon' and is directly grounded in the provided description without adding incorrect information.", "main_total_tokens": 2749, "main_prompt_tokens": 2698, "main_completion_tokens": 51, "judge_total_tokens": 505, "judge_prompt_tokens": 449, "judge_completion_tokens": 56}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie where a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is \"The Life Ahead\" [Doc 93260]. This film, released in 2020, has a rating of 6.746 and describes the scenario where a seaside daycare business owner in Italy takes in a young street child after he robs her.", "latency_ms": 8032.612085342407, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the title of an Italian-language movie matching a specific plot. Both the gold answer and the generated answer identify the film as \"The Life Ahead,\" which is correct. The added details in the generated answer (release year, rating, brief plot description) are accurate and consistent with the reference description, and it clearly ties to the described scenario. There are no contradictions or hallucinations relative to the prompt, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 3253, "main_prompt_tokens": 3167, "main_completion_tokens": 86, "judge_total_tokens": 600, "judge_prompt_tokens": 475, "judge_completion_tokens": 125}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 Documentary \"McLaren\" is about Bruce McLaren, a New Zealander who founded the McLaren Motor Racing team. [Doc 73859]", "latency_ms": 3551.098585128784, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary and aligns with the gold answer. The additional detail about him being a New Zealander and founder of the McLaren racing team is accurate and does not contradict the source.", "main_total_tokens": 3042, "main_prompt_tokens": 3002, "main_completion_tokens": 40, "judge_total_tokens": 502, "judge_prompt_tokens": 419, "judge_completion_tokens": 83}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "The documents provided do not specify a particular racing team for any of the characters or scenarios described. Therefore, there is no specific answer to what the name of his racing team is based on the given context.", "latency_ms": 4034.990072250366, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a concrete team name, 'The McLaren Motor Racing team', but the generated answer claims that no specific team name is provided. This directly contradicts the gold, so it is incorrect relative to the reference. However, assuming the model only had access to documents that truly did not mention a team, the answer is faithful to its stated lack of information and does not hallucinate details, so it is fully faithful to the provided context.", "main_total_tokens": 3203, "main_prompt_tokens": 3161, "main_completion_tokens": 42, "judge_total_tokens": 541, "judge_prompt_tokens": 424, "judge_completion_tokens": 117}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The context does not specify a particular \"he\" with a clear reference to a specific character or individual's country of origin without additional details about the context in which \"he\" is being referred. Therefore, based strictly on the provided documents, it is not possible to determine what country \"he\" is from.", "latency_ms": 3693.0363178253174, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that he is from New Zealand, so the correct answer is known. The generated answer claims it is not possible to determine the country, which contradicts the gold answer, making it factually incorrect. However, if we assume the model only had access to the referenced context (and not the gold), then within that limited view it is accurately describing that the context does not specify the country, so it is faithful to its stated evidence but still incorrect relative to the gold.", "main_total_tokens": 2822, "main_prompt_tokens": 2759, "main_completion_tokens": 63, "judge_total_tokens": 566, "judge_prompt_tokens": 437, "judge_completion_tokens": 129}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "None of the provided documents mention a 2013 movie where John McClane blows up the Great Wall of China. The document [Doc 58791] describes \"A Good Day to Die Hard,\" which is from 2013 and involves John McClane, but it does not include any plot about blowing up the Great Wall of China. Instead, it mentions that he travels to Moscow to help his son stop a nuclear weapons heist.", "latency_ms": 5080.49201965332, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly notes that no provided documents mention a 2013 movie where John McClane blows up the Great Wall of China and accurately summarizes the plot of 'A Good Day to Die Hard'. This aligns with the gold answer that such a movie is not in the database, so both correctness and faithfulness are high.", "main_total_tokens": 3442, "main_prompt_tokens": 3346, "main_completion_tokens": 96, "judge_total_tokens": 585, "judge_prompt_tokens": 488, "judge_completion_tokens": 97}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie with the described plot is \"Tomie: Forbidden Fruit\" [Doc 28589].", "latency_ms": 3511.286497116089, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Tomie: Forbidden Fruit', which matches the gold label, and it stays faithful to the provided information without adding incorrect details.", "main_total_tokens": 3310, "main_prompt_tokens": 3278, "main_completion_tokens": 32, "judge_total_tokens": 508, "judge_prompt_tokens": 443, "judge_completion_tokens": 65}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 3509.807586669922, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (65 years) and directly restates it without adding unsupported information, so it is both correct and faithful.", "main_total_tokens": 3697, "main_prompt_tokens": 3674, "main_completion_tokens": 23, "judge_total_tokens": 473, "judge_prompt_tokens": 412, "judge_completion_tokens": 61}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In \"Scorpion King: Book of Souls\" (2018), The Scorpion King and Tala search for a legendary relic known as The Book of Souls. [Doc 82582]", "latency_ms": 3795.215129852295, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in 'Scorpion King: Book of Souls' (2018), The Scorpion King and Tala are searching for the Book of Souls, which matches the gold answer exactly. It is faithful to the provided information and adds only a minor descriptive phrase ('legendary relic') that does not contradict the gold.", "main_total_tokens": 3713, "main_prompt_tokens": 3668, "main_completion_tokens": 45, "judge_total_tokens": 530, "judge_prompt_tokens": 432, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document provided does not contain information about a movie titled \"Zombies on the Moon\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 3434.8411560058594, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers correctly state that the release year cannot be provided. The generated answer is faithful to the given context by explicitly noting the document lacks information about that movie, which aligns with the gold response that the movie is not in the database.", "main_total_tokens": 3452, "main_prompt_tokens": 3415, "main_completion_tokens": 37, "judge_total_tokens": 509, "judge_prompt_tokens": 427, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no document provided that mentions a release year for \"The Galactic baker.\" Therefore, based on the given context, the specific release year for \"The Galactic baker\" cannot be determined.", "latency_ms": 3401.362657546997, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Gold answer says the movie is not in the database (no information). The generated answer states that no document provides a release year and that the year cannot be determined from the context. This is effectively equivalent in meaning and does not hallucinate a year. Both correctness and faithfulness are satisfied.", "main_total_tokens": 3226, "main_prompt_tokens": 3187, "main_completion_tokens": 39, "judge_total_tokens": 516, "judge_prompt_tokens": 427, "judge_completion_tokens": 89}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no document provided that contains information about a movie titled \"Return of the Null Pointer\". Therefore, I cannot provide the release year for this specific title based on the given documents.", "latency_ms": 3058.6278438568115, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the system cannot provide the release year and that the movie is not in the available data. The generated answer correctly notes the absence of relevant documents and does not hallucinate a year, so it is both correct and faithful to the (lack of) provided information.", "main_total_tokens": 2847, "main_prompt_tokens": 2809, "main_completion_tokens": 38, "judge_total_tokens": 519, "judge_prompt_tokens": 428, "judge_completion_tokens": 91}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document provided does not contain information about a movie titled \"Data Science: The Movie\". Therefore, I cannot provide the release year for this specific title based on the given context.", "latency_ms": 4029.83021736145, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer correctly states the movie is not in the database, implying no answer can be given. The generated answer instead grounds its inability to answer on the provided document, not on the database, which changes the basis of the limitation. This contradicts the gold rationale and is therefore not faithful. Additionally, the task is to match the gold response style (database-based), so the generated answer is considered incorrect for this evaluation.", "main_total_tokens": 3397, "main_prompt_tokens": 3360, "main_completion_tokens": 37, "judge_total_tokens": 543, "judge_prompt_tokens": 427, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "None of the documents provided contains information on a movie titled \"Python's Revenge 2\". The closest matches are \"Python 2\", which was released in 2002 [Doc X: Doc 28408], and another \"Python\", which was released in 2000 [Doc X: Doc 24222]. There is no \"Python's Revenge 2\" mentioned.", "latency_ms": 4342.021703720093, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is not in the database; the generated answer similarly explains that no documents mention a movie titled \"Python's Revenge 2\" and clarifies related but different movies. It does not fabricate a release year and stays consistent with the provided documents, so it is both correct and faithful.", "main_total_tokens": 3278, "main_prompt_tokens": 3191, "main_completion_tokens": 87, "judge_total_tokens": 560, "judge_prompt_tokens": 467, "judge_completion_tokens": 93}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "There is no document provided that contains information about the movie \"The Last Pixel\". Therefore, the release year for \"The Last Pixel\" cannot be determined from the given context.", "latency_ms": 3098.1030464172363, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question is about the release year of a movie that is not in the provided data. The gold answer states that the movie is not in the database. The generated answer correctly states that the release year cannot be determined from the given context. This is factually correct and fully aligned with the given context, with no hallucinated information.", "main_total_tokens": 2890, "main_prompt_tokens": 2854, "main_completion_tokens": 36, "judge_total_tokens": 522, "judge_prompt_tokens": 424, "judge_completion_tokens": 98}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "None of the documents provided contains information about a movie titled \"The man who ate the sun\". Therefore, based strictly on the context given, it is not possible to determine in what year this specific title was released.", "latency_ms": 3795.5992221832275, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the release year cannot be determined from the provided documents, which matches the gold answer's inability to provide a year. It does not invent a date or contradict the context, and explicitly bases its limitation on the given documents, so it is both correct and faithful to the context.", "main_total_tokens": 3719, "main_prompt_tokens": 3675, "main_completion_tokens": 44, "judge_total_tokens": 527, "judge_prompt_tokens": 435, "judge_completion_tokens": 92}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The document does not provide information on who directed the movie \"The Last Days of Lehman Brothers\".", "latency_ms": 3504.1329860687256, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the document does not provide the director information, matching the gold answer in content and intent. It does not introduce external information or contradict the source, so it is both correct and faithful.", "main_total_tokens": 3594, "main_prompt_tokens": 3573, "main_completion_tokens": 21, "judge_total_tokens": 491, "judge_prompt_tokens": 417, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about who directed the movie \"A Love to Die For\".", "latency_ms": 2973.7603664398193, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not contain information about the director, matching the gold answer that the director information is not provided in the database. It does not add unsupported details or contradict the given information, so it is both correct and faithful.", "main_total_tokens": 3214, "main_prompt_tokens": 3194, "main_completion_tokens": 20, "judge_total_tokens": 496, "judge_prompt_tokens": 414, "judge_completion_tokens": 82}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The document does not provide information on who directed the movie \"Journey into Gaza\". [Doc 111360]", "latency_ms": 3251.394510269165, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the document does not provide information about the director of 'Journey into Gaza,' which aligns semantically with the gold answer that the director information is not in the database. It does not add unsupported details and remains faithful to the given context.", "main_total_tokens": 3204, "main_prompt_tokens": 3176, "main_completion_tokens": 28, "judge_total_tokens": 499, "judge_prompt_tokens": 415, "judge_completion_tokens": 84}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The document provided does not contain information about who directed the TV show \"The Last Night at Tremore Beach\". [Doc 135098]", "latency_ms": 3230.3290367126465, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not present in the provided/document database. The generated answer is slightly more specific (mentions a TV show and a particular document), but it does not invent a director name or contradict the gold. It is correct relative to the database and faithful to the gold’s intent that the information is unavailable.", "main_total_tokens": 3155, "main_prompt_tokens": 3122, "main_completion_tokens": 33, "judge_total_tokens": 526, "judge_prompt_tokens": 425, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The scenario where a group of archaeology students awaken an ancient mummy occurs in the movie \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 3320.751428604126, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold title exactly, identifying \"Rise of the Mummy\" as the correct movie. It does not introduce any unsupported information beyond a reference tag, so it is both correct and faithful to the provided gold answer.", "main_total_tokens": 3046, "main_prompt_tokens": 3010, "main_completion_tokens": 36, "judge_total_tokens": 498, "judge_prompt_tokens": 420, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The scenario where Sarah fakes her own death to escape her abusive spouse happens in the movie \"Stranger in My Bed\" [Doc 35237].", "latency_ms": 3130.3670406341553, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Stranger in My Bed' and does not introduce any contradictions or unsupported claims. It is both correct and faithful.", "main_total_tokens": 3018, "main_prompt_tokens": 2983, "main_completion_tokens": 35, "judge_total_tokens": 496, "judge_prompt_tokens": 427, "judge_completion_tokens": 69}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The plot described—A successful Atlanta attorney's long-lost childhood friends unexpectedly reappearing after 25 years—matches the description for \"Game of Silence\" [Doc 123938].", "latency_ms": 3145.7953453063965, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie contains the described plot, but the gold answer \"Game of Silence\" is actually a TV series. The generated answer correctly matches the description to \"Game of Silence\" and even cites a document, remaining consistent with the provided information. Within the given context, this is treated as correct and faithful to the gold answer and the source document.", "main_total_tokens": 2523, "main_prompt_tokens": 2479, "main_completion_tokens": 44, "judge_total_tokens": 538, "judge_prompt_tokens": 435, "judge_completion_tokens": 103}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie that is based on a true story about a killer who uses Craigslist is \"The Craigslist Killer\" [Doc 53039]. This film describes Philip Markoff, a charismatic and popular medical student at Boston University, who leads a double life as a brutal sexual deviant who abuses prostitutes he finds via Craigslist.", "latency_ms": 3619.8267936706543, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is \"The Craigslist Killer\" and the generated response correctly identifies the same movie and provides accurate additional context about the plot and main character. It does not contradict the question or add fabricated details beyond common knowledge, so it is both correct and faithful to the prompt and reference.", "main_total_tokens": 2932, "main_prompt_tokens": 2865, "main_completion_tokens": 67, "judge_total_tokens": 543, "judge_prompt_tokens": 455, "judge_completion_tokens": 88}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The plot described happens in the movie \"Wood Job!\" [Doc 66094].", "latency_ms": 3117.271661758423, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", matching the gold answer exactly. It does not introduce contradictions or unsupported claims beyond a trivial document reference, so it is both correct and faithful to the given information.", "main_total_tokens": 3358, "main_prompt_tokens": 3337, "main_completion_tokens": 21, "judge_total_tokens": 493, "judge_prompt_tokens": 417, "judge_completion_tokens": 76}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The described scenario occurs in the movie titled \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 3038.9153957366943, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, correctly identifying the movie as \"Sesame Street: The Magical Wand Chase.\" It is fully consistent with the information in the prompt and does not introduce unsupported claims, so both correctness and faithfulness are perfect.", "main_total_tokens": 3028, "main_prompt_tokens": 2999, "main_completion_tokens": 29, "judge_total_tokens": 515, "judge_prompt_tokens": 434, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The plot described happens in the movie titled \"Beyond\" [Doc 74597].", "latency_ms": 2577.660322189331, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the described plot. Both the gold answer and the generated answer identify the movie as \"Beyond.\" The generated answer directly matches the gold answer and does not introduce contradictions or unsupported details, so it is both correct and faithful.", "main_total_tokens": 2567, "main_prompt_tokens": 2546, "main_completion_tokens": 21, "judge_total_tokens": 506, "judge_prompt_tokens": 425, "judge_completion_tokens": 81}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The scenario described happens in the movie \"6ixtynin9\" [Doc 22478].", "latency_ms": 2739.246129989624, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, identifying the movie as \"6ixtynin9\". It correctly restates the scenario and attributes it to that film without adding unsupported details, so it is both correct and faithful.", "main_total_tokens": 2643, "main_prompt_tokens": 2618, "main_completion_tokens": 25, "judge_total_tokens": 501, "judge_prompt_tokens": 423, "judge_completion_tokens": 78}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The movie where a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort, and it's up to the club's staff to stop the violence or at least hide it is \"Club Dread\" [Doc 32410].", "latency_ms": 3623.847007751465, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread,' which matches the gold answer exactly. It faithfully restates the question context without introducing errors or contradictions.", "main_total_tokens": 3228, "main_prompt_tokens": 3175, "main_completion_tokens": 53, "judge_total_tokens": 537, "judge_prompt_tokens": 473, "judge_completion_tokens": 64}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie where a 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral is titled \"Toilet\" [Doc 48873].", "latency_ms": 3804.490566253662, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Toilet,' matching the gold answer exactly. It is faithful to the provided information, simply restating the title with a reference and no unsupported additions.", "main_total_tokens": 3412, "main_prompt_tokens": 3356, "main_completion_tokens": 56, "judge_total_tokens": 535, "judge_prompt_tokens": 465, "judge_completion_tokens": 70}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The description provided matches the plot details for the movie \"Electric Angel\" [Doc 1024].", "latency_ms": 2903.6929607391357, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks which movie matches the given experimental, polymorphic eroticism-themed description, and the gold answer is \"Electric Angel.\" The generated answer explicitly states that the description matches the movie \"Electric Angel,\" which is correct. There is no overclaiming beyond identifying the film, and the reference to a document ID does not contradict the given information, so the response is both correct and faithful to the prompt and gold answer.", "main_total_tokens": 2984, "main_prompt_tokens": 2961, "main_completion_tokens": 23, "judge_total_tokens": 552, "judge_prompt_tokens": 436, "judge_completion_tokens": 116}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "The lowest rated movie from 2023 based on the provided documents is \"Rare Objects\" with a rating of 4.3 [Doc 109669].", "latency_ms": 2267.8418159484863, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates that the lowest rated 2023 movie is 'Mega Ape', while the generated answer claims it is 'Rare Objects' with a 4.3 rating. This directly contradicts the gold answer, so the response is incorrect. However, the model clearly attributes its answer to a specific provided document ([Doc 109669]) and does not introduce external information beyond that citation, so it is faithful to the given context.", "main_total_tokens": 3111, "main_prompt_tokens": 3072, "main_completion_tokens": 39, "judge_total_tokens": 532, "judge_prompt_tokens": 413, "judge_completion_tokens": 119}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "Based on the provided documents, the highest-rated TV show from 2010 is \"Hawaii Five-0\" with a rating of 7.755 [Doc 119141].", "latency_ms": 2760.7901096343994, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer indicates the correct show is 'Tea House', whereas the generated answer claims it is 'Hawaii Five-0', so the content is factually incorrect relative to the gold. Additionally, the generated answer references an external document ID to justify its claim instead of using the gold label, so it is not faithful to the provided ground truth.", "main_total_tokens": 3573, "main_prompt_tokens": 3527, "main_completion_tokens": 46, "judge_total_tokens": 520, "judge_prompt_tokens": 419, "judge_completion_tokens": 101}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "The highest-rated Japanese language movie from 1999 based on the provided documents is \"Kikujiro\" with a rating of 7.5 [Doc 22213].", "latency_ms": 3524.1165161132812, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect because the gold label identifies 'The Uncut Story of Mewtwo's Origin' as the highest rated Japanese language movie from 1999, while the model instead answers 'Kikujiro'. This directly contradicts the gold answer. It is also unfaithful to the provided supervision signal (gold), so both correctness and faithfulness are scored 0.", "main_total_tokens": 3379, "main_prompt_tokens": 3337, "main_completion_tokens": 42, "judge_total_tokens": 536, "judge_prompt_tokens": 427, "judge_completion_tokens": 109}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "To determine the language of the lowest-rated movie from 1980, I will first identify all movies from 1980 and their respective ratings:\n\n1. \"A Story of No Importance\" - Rating: 5.0 [Doc 849]\n2. \"Poor Ioanide\" - Rating: 7.0 [Doc 852]\n3. \"Refused\" - Rating: 7.5 [Doc 250]\n4. \"Sparsh\" - Rating: 7.3 [Doc 801]\n5. \"Don't Answer the Phone!\" - Rating: 4.6 [Doc 768]\n6. \"Manganinnie\" - Rating: 6.4 [Doc 435]\n7. \"Say a Word for the Poor Hussar\" - Rating: 6.814 [Doc 322]\n8. \"Little Miss Marker\" - Rating: 6.5 [Doc 563]\n9. \"The Precarious Bank Teller\" - Rating: 5.5 [Doc 378]\n10. \"Blank Generation\" - Rating: 5.4 [Doc 534", "latency_ms": 7370.155096054077, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The answer does not actually provide the language of the lowest-rated movie from 1980. It stops mid-list and never identifies which movie is lowest-rated nor its language. It also fabricates a reasoning process (listing movies and ratings) that is not shown to be grounded in the provided gold answer or any cited documents. Therefore it is both incorrect and unfaithful.", "main_total_tokens": 3452, "main_prompt_tokens": 3196, "main_completion_tokens": 256, "judge_total_tokens": 716, "judge_prompt_tokens": 611, "judge_completion_tokens": 105}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "The TV show with the highest popularity in 1999 based on the provided documents is \"Popular\" with a popularity rating of 4.5113 [Doc 115196].", "latency_ms": 3834.810495376587, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is faithful to the (implied) provided document, stating that 'Popular' has the highest popularity based on Doc 115196. However, it is factually incorrect relative to the gold answer, which identifies 'Law & Order: Special Victims Unit' as the correct show. Thus, correctness is 0 while faithfulness to the given documents is 1.", "main_total_tokens": 3722, "main_prompt_tokens": 3677, "main_completion_tokens": 45, "judge_total_tokens": 534, "judge_prompt_tokens": 426, "judge_completion_tokens": 108}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 3022.284746170044, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and correctly attributes it to the movie \"Bluefin\". It is both correct and faithful to the provided information.", "main_total_tokens": 3188, "main_prompt_tokens": 3166, "main_completion_tokens": 22, "judge_total_tokens": 467, "judge_prompt_tokens": 400, "judge_completion_tokens": 67}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in multiple years according to the documents provided:\n\n1. 1999 [Doc 22481]\n2. 2019 [Doc 88811]\n3. 2025 [Doc 112077]", "latency_ms": 3838.6385440826416, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct gold answer is 2019. The generated answer lists three different years (1999, 2019, and 2025) and does not select or emphasize 2019 as the single correct release year, making the overall answer incorrect. It also introduces 1999 and 2025 as additional release years not supported by the gold answer, so it is not faithful to the required ground truth.", "main_total_tokens": 3207, "main_prompt_tokens": 3138, "main_completion_tokens": 69, "judge_total_tokens": 550, "judge_prompt_tokens": 436, "judge_completion_tokens": 114}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "The document does not provide the exact number of votes for \"My Dog Skip,\" but it gives the popularity as 1.7749 [Doc 24114]. This value could represent votes or some other metric, but without additional context, we cannot determine the precise number of votes.", "latency_ms": 3636.685609817505, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer explicitly states that 'My Dog Skip' received 279 votes. The generated answer claims the document does not provide the exact number of votes and instead references a different metric (popularity 1.7749). This is factually incorrect relative to the gold answer and also misrepresents the contents of the supposed document, so it is neither correct nor faithful.", "main_total_tokens": 2968, "main_prompt_tokens": 2905, "main_completion_tokens": 63, "judge_total_tokens": 544, "judge_prompt_tokens": 438, "judge_completion_tokens": 106}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "The document provided does not specify the number of votes for \"Oh My God!\". It only provides a rating and popularity score. [Doc 85434]", "latency_ms": 3321.0537433624268, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that \"Oh My God!\" received 12 votes, but the generated answer claims the document does not specify the number of votes. This is factually incorrect relative to the gold, so correctness is 0. However, assuming the model only had access to the cited document and it indeed lacked vote information, the answer is consistent with the referenced source, so faithfulness is 1.", "main_total_tokens": 3250, "main_prompt_tokens": 3215, "main_completion_tokens": 35, "judge_total_tokens": 525, "judge_prompt_tokens": 412, "judge_completion_tokens": 113}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 3400.613784790039, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie was released in 2009, which matches the gold answer. It is fully supported by the cited document reference, so it is both correct and faithful to the provided evidence.", "main_total_tokens": 3570, "main_prompt_tokens": 3541, "main_completion_tokens": 29, "judge_total_tokens": 484, "judge_prompt_tokens": 410, "judge_completion_tokens": 74}
{"experiment_name": "SysA_Baseline_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 3184.4427585601807, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the original language is English, and the generated answer correctly states that the original language is English. The added citation does not contradict the source, so the response is both correct and faithful.", "main_total_tokens": 3339, "main_prompt_tokens": 3315, "main_completion_tokens": 24, "judge_total_tokens": 474, "judge_prompt_tokens": 402, "judge_completion_tokens": 72}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "The movie \"Wall Street\" was released in 1987 [Doc 6993].", "latency_ms": 2541.6150093078613, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'Wall Street' was released in 1987, matching the gold answer exactly. It does not introduce any contradictions or hallucinations beyond a harmless citation marker.", "main_total_tokens": 2423, "main_prompt_tokens": 2400, "main_completion_tokens": 23, "judge_total_tokens": 467, "judge_prompt_tokens": 396, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "The movie in which classmates have to play a survival game using their cell phones is \"Brain Fluid Explosion Girl\" [Doc 69929]. In this movie, Hana Ichii and her classmates are locked in a cage with each other and forced to take part in a survival game that utilizes their cellphones.", "latency_ms": 3019.728422164917, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Brain Fluid Explosion Girl' and accurately describes the premise involving classmates forced into a survival game using their cell phones, matching the gold answer and staying grounded in the provided information.", "main_total_tokens": 2103, "main_prompt_tokens": 2037, "main_completion_tokens": 66, "judge_total_tokens": 523, "judge_prompt_tokens": 449, "judge_completion_tokens": 74}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "The TV show where six friends need to rely on a Dungeon Master to get home is \"Dungeons & Dragons\" [Doc 112849]. In this animated series from 1983, the group is transported into the Dungeons & Dragons realm and must try to find their way back with the help of their guide, 'Dungeon Master'.", "latency_ms": 3209.6173763275146, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Dungeons & Dragons,\" matching the gold answer. The additional description about the 1983 animated series, the group being transported into the realm, and relying on the Dungeon Master to return home is accurate and consistent with known information, so it is both correct and faithful to the presumed source document reference.", "main_total_tokens": 2099, "main_prompt_tokens": 2024, "main_completion_tokens": 75, "judge_total_tokens": 559, "judge_prompt_tokens": 458, "judge_completion_tokens": 101}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "The movie that is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones, is \"Nacht der Wölfe\" [Doc 2590].", "latency_ms": 2667.9527759552, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Nacht der Wölfe', which matches the gold reference exactly. It is faithful to the provided information and does not introduce any unsupported details.", "main_total_tokens": 2107, "main_prompt_tokens": 2063, "main_completion_tokens": 44, "judge_total_tokens": 508, "judge_prompt_tokens": 439, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "The events of Picnic at Hanging Rock take place in the year 1900 [Doc 126191].", "latency_ms": 2584.6126079559326, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the events of Picnic at Hanging Rock take place in 1900, matching the gold answer exactly and remaining faithful to the provided information.", "main_total_tokens": 2372, "main_prompt_tokens": 2343, "main_completion_tokens": 29, "judge_total_tokens": 470, "judge_prompt_tokens": 406, "judge_completion_tokens": 64}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "The movie described is \"June,\" which is a 2004 film about a young violinist named June who works at an oddball Venice coffee shop by day and rehearses for her professional music career at night while dreaming of the beautiful man destined for her [Doc 32483].", "latency_ms": 3213.1636142730713, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the name of the movie, which the gold answer identifies as 'June.' The generated answer states that the movie is 'June' and correctly restates the plot details, matching the gold answer. There are no contradictions or hallucinations relative to the provided information, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2396, "main_prompt_tokens": 2333, "main_completion_tokens": 63, "judge_total_tokens": 586, "judge_prompt_tokens": 489, "judge_completion_tokens": 97}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 1342.1945571899414, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a specific correct answer, but the generated response says it does not know, so it is not correct. However, the model does not fabricate any information and accurately reflects its uncertainty, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 470, "judge_prompt_tokens": 399, "judge_completion_tokens": 71}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 511.34419441223145, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 7.6, but the generated answer states 'I don't know,' which is incorrect. However, the model does not hallucinate a specific incorrect value and honestly admits not knowing, so it is fully faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 471, "judge_prompt_tokens": 389, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 1214.6985530853271, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.366123019050001, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is incorrect. However, the assistant explicitly provided a single numeric answer to the same question, so it is faithful to the user query and not hallucinating a different task or format.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 394, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 197.70288467407227, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. The justification is not provided, but assuming it was based on the model’s own reasoning about the question, it is internally consistent with that reasoning, so it is faithful to its own chain of thought, even though the final result is wrong.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 497, "judge_prompt_tokens": 395, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 1654.9465656280518, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answer admits not knowing and does not provide an incorrect alternative. This is factually incorrect relative to the gold answer but fully faithful to the model's own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 454, "judge_prompt_tokens": 389, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "The movie that matches your description is \"Kill Cruise\" [Doc 11205]. It's about two British beauties going to Barbados with a yacht captain.", "latency_ms": 2962.7411365509033, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise' and accurately restates the description from the gold/reference. It is both factually correct and fully grounded in the provided information.", "main_total_tokens": 2284, "main_prompt_tokens": 2247, "main_completion_tokens": 37, "judge_total_tokens": 483, "judge_prompt_tokens": 415, "judge_completion_tokens": 68}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "The title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues is \"Won't You Be My Neighbor?\" [Doc 84838]. This documentary delves into the world of Fred Rogers, who used his television program to address significant topics with young audiences.", "latency_ms": 3348.2322692871094, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as \"Won't You Be My Neighbor?\" which matches the gold answer exactly. It stays faithful to the question, simply elaborating slightly about the documentary without adding incorrect or speculative information.", "main_total_tokens": 2602, "main_prompt_tokens": 2535, "main_completion_tokens": 67, "judge_total_tokens": 541, "judge_prompt_tokens": 464, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "The question \"What year was it released?\" can be answered for each document provided, but since no specific title or context is given with the question, I will provide the release years for all movies listed:\n\n1. [Doc 8389] - Year: 1988\n2. [Doc 675] - Year: 1980\n3. [Doc 19562] - Year: 1997\n4. [Doc 25738] - Year: 2001\n5. [Doc 107589] - Year: 2023\n6. [Doc 91657] - Year: 2019\n7. [Doc 94285] - Year: 2020\n8. [Doc 45703] - Year: 2009\n9. [Doc 62279] - Year: 2013\n10. [Doc 82497] - Year: 2018\n11. [Doc 108737] - Year:", "latency_ms": 6928.237676620483, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The gold answer is a single year (2018), but the generated answer lists multiple years for different documents and does not identify 2018 as the specific answer to the question. Therefore it is incorrect. It is also unfaithful to the task, which expects a single release year, not a list for all documents.", "main_total_tokens": 2823, "main_prompt_tokens": 2567, "main_completion_tokens": 256, "judge_total_tokens": 674, "judge_prompt_tokens": 578, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "The TV series that fits the description of being an animation about a young boy robot in 2030 AD is \"Astro Boy\" [Doc 112591]. This show describes a scenario where in 2030 AD, after being abandoned by his creator, a young boy robot is rescued and adopted by Dr Elefun who guides him to use his special powers to aid and protect mankind and robot-kind.", "latency_ms": 3604.6323776245117, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV series as 'Astro Boy,' matching the gold answer exactly. The additional description about the premise of the show aligns with known information about Astro Boy and does not contradict the question or the gold answer, so it is faithful to the prompt and the presumed source document reference.", "main_total_tokens": 2320, "main_prompt_tokens": 2230, "main_completion_tokens": 90, "judge_total_tokens": 560, "judge_prompt_tokens": 468, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "In the movie \"Room\" released in 2015, the woman and her son were held captive for 7 years [Doc 67513].", "latency_ms": 2439.443826675415, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the woman and her son were held captive for 7 years, which matches the gold answer exactly. It is fully consistent with the provided information and does not introduce any unsupported details, so both correctness and faithfulness are maximal.", "main_total_tokens": 1980, "main_prompt_tokens": 1944, "main_completion_tokens": 36, "judge_total_tokens": 506, "judge_prompt_tokens": 424, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "The Brink's-Mat security depot was holding gold bullion worth £26m before it was broken into on 26 November 1983 [Doc 133469].", "latency_ms": 2794.7514057159424, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the Brink's-Mat security depot was holding gold bullion worth £26 million before it was broken into on 26 November 1983, matching the provided gold answer without adding unsupported details.", "main_total_tokens": 2297, "main_prompt_tokens": 2253, "main_completion_tokens": 44, "judge_total_tokens": 503, "judge_prompt_tokens": 428, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 6317.9497718811035, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is a specific numeric popularity score (80.5772). The generated answer says, 'I don't know,' which is incorrect relative to the gold. However, the model does not fabricate an answer and accurately reflects its uncertainty, so it is fully faithful to its own stated knowledge limits.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 396, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 376.8460750579834, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there was 1 Russian language TV show released in 1997, but the generated answer is 0, which is factually incorrect relative to the gold. The explanation here correctly reflects that mismatch, so it is faithful to the provided information.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 469, "judge_prompt_tokens": 385, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "The answer to your question can be found in [Doc 121928]. In the TV show \"Angel Eyes\" from 2014, Yoon Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight.", "latency_ms": 3336.467981338501, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.7, "justification": "The generated answer correctly identifies the TV show as 'Angel Eyes', matching the gold answer, so correctness is 1. However, it adds an invented citation/reference to '[Doc 121928]', which is not grounded in the user's prompt and suggests access to a specific document that wasn't provided. This reduces faithfulness, hence a score below 1.", "main_total_tokens": 2671, "main_prompt_tokens": 2612, "main_completion_tokens": 59, "judge_total_tokens": 549, "judge_prompt_tokens": 446, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of the movie \"Foreign Language\" is French [Doc 110681].", "latency_ms": 2383.181571960449, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer states that the movie language is French, while the gold answer specifies Korean. This is factually incorrect relative to the gold. There is also no support in the given context for claiming it is French, so the answer is unfaithful to the reference information.", "main_total_tokens": 2313, "main_prompt_tokens": 2291, "main_completion_tokens": 22, "judge_total_tokens": 479, "judge_prompt_tokens": 393, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 575.979471206665, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (0) does not match the gold answer (2), so it is incorrect. However, there is no indication that the model contradicted the given information or fabricated additional details; it simply provided the wrong numeric result, so faithfulness to the (incorrect) reasoning it used is not in question based on the prompt alone.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 493, "judge_prompt_tokens": 394, "judge_completion_tokens": 99}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "Based on the provided documents, there is no movie from 1979 that is listed as having a high rating. The document entries do not contain any movies with both a release year of 1979 and a notable rating. Therefore, based strictly on this context, we cannot determine the best-rated movie released in 1979.", "latency_ms": 3299.3922233581543, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0.8, "faithfulness_score": 0.9, "justification": "The gold answer explicitly says there are no movies from 1979 in the database. The generated answer states that, based on the provided documents, there is no movie from 1979 with a rating and thus we cannot determine the best-rated 1979 movie. This is mostly consistent with the gold: both convey that the system cannot answer from its data. Slight deviation: the generated answer adds a condition about having both a release year and notable rating, which is not in the gold, but it's a reasonable inference. Thus faithfulness is high but not perfect. Correctness is slightly reduced because the question asked for the best-rated movie; the fully correct behavior (per gold) is to state that there are no 1979 movies in the database, whereas the generated answer more narrowly claims there is no 1979 movie with both a rating and year and does not clearly assert the absence of any 1979 entries. Overall, close but not identical.", "main_total_tokens": 2268, "main_prompt_tokens": 2195, "main_completion_tokens": 73, "judge_total_tokens": 688, "judge_prompt_tokens": 461, "judge_completion_tokens": 227}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "The provided documents do not contain information about a movie titled \"Back to the Future Part IV.\" Therefore, based strictly on the context given, there is no answer to the question regarding which year Back to the Future part IV was released [Doc 10621][Doc 4834][Doc 9405][Doc 68724][Doc 3944][Doc 28792][Doc 103759][Doc 113788][Doc 68688].", "latency_ms": 4983.671426773071, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer is factually correct that there is no known release year for 'Back to the Future Part IV' because such a movie does not exist. However, it is not faithful to the gold response style: the gold explicitly states that the movie does not exist in the database, whereas the generated answer frames it as absence of information in the provided documents and implies an unanswerable question rather than explicitly denying the movie’s existence in the database. Thus correctness is 1, faithfulness is 0.", "main_total_tokens": 3225, "main_prompt_tokens": 3106, "main_completion_tokens": 119, "judge_total_tokens": 625, "judge_prompt_tokens": 492, "judge_completion_tokens": 133}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot find any movies or shows matching the criteria, which aligns with the gold answer that there were no movies from 1980 related to the Chernobyl disaster. It is accurate and faithful to the information implied in the gold response.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 519, "judge_prompt_tokens": 434, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 344.0084457397461, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 4, but the generated answer is 0, which is factually incorrect. The explanation is consistent with the given gold and generated answers, so it is faithful to the provided information but not correct.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 464, "judge_prompt_tokens": 388, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "0", "latency_ms": 220.04938125610352, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 2 such movies, while the generated answer says 0, which is numerically incorrect. However, the generated answer is internally consistent with itself (no contradictions within its own content), so it is fully faithful to its own reasoning but factually wrong compared to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 386, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "0", "latency_ms": 155.27081489562988, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 1, but the generated answer is 0, which is incorrect. The evaluation correctly identifies the mismatch without adding unsupported information, so it is faithful but not correct relative to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 462, "judge_prompt_tokens": 389, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "0", "latency_ms": 225.48723220825195, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (0). The question is factual and the response correctly reflects the provided gold standard without adding unsupported details.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 450, "judge_prompt_tokens": 391, "judge_completion_tokens": 59}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 241.7144775390625, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect relative to the gold answer, which states 'Naruto' as the most popular ninja-related show. However, the model does not fabricate an alternative; it honestly states 'I don't know,' so it is faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 471, "judge_prompt_tokens": 386, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "The provided documents do not contain information about a Jurassic Park/Power Ranger crossover movie, so the answer to when this specific type of movie was released cannot be found in the given context.", "latency_ms": 3063.903570175171, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "Gold says explicitly that the movie was not released. The generated answer instead claims the release date cannot be found in the context, implying uncertainty rather than stating that there was no release. This contradicts the gold answer, so correctness is 0. However, the generated answer accurately reflects the (hypothetical) provided documents, so it is faithful to the given context, giving faithfulness 1.", "main_total_tokens": 2506, "main_prompt_tokens": 2468, "main_completion_tokens": 38, "judge_total_tokens": 533, "judge_prompt_tokens": 422, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "None of the documents provided contain information about Barbie fighting a Predator. Therefore, based on the given context, there is no movie where Barbie fights the Predator.", "latency_ms": 2692.534923553467, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that there is no movie where Barbie fights the Predator, which matches the gold answer that such a movie was not released. It is faithful to the given context by explicitly tying the conclusion to the absence of such information in the documents.", "main_total_tokens": 2545, "main_prompt_tokens": 2513, "main_completion_tokens": 32, "judge_total_tokens": 495, "judge_prompt_tokens": 413, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "Based on the provided documents, the 4th oldest Star Wars movie is \"Return of the Jedi\" [Doc 2858], which was released in 1983.\n\nThe list of movies based on their release year would be:\n1. \"A New Hope\" (not listed here but considered the first)\n2. \"The Empire Strikes Back\" (also not listed, second movie)\n3. \"Revenge of the Sith\" (Episode III, also not listed explicitly in documents provided but known to be third)\n4. \"Return of the Jedi\" [Doc 2858] (1983)\n\nTherefore, \"Return of the Jedi\" is considered the 4th oldest Star Wars movie among those listed here.", "latency_ms": 5130.74254989624, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The correct answer should be 'Star Wars: Droids - The Great Heep', but the generated answer gives 'Return of the Jedi', so it is factually incorrect. It also invents a release-order list (including films not in the provided context) and mis-orders the films, so while it cites one document, much of the reasoning is not grounded in the given gold answer or evidence, leading to only partial faithfulness.", "main_total_tokens": 2639, "main_prompt_tokens": 2484, "main_completion_tokens": 155, "judge_total_tokens": 657, "judge_prompt_tokens": 537, "judge_completion_tokens": 120}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "The question \"What year was it released?\" can be answered for each document provided, but since no specific title or context is given with the question, I will provide the release years for all movies listed:\n\n1. [Doc 8389] - Year: 1988\n2. [Doc 675] - Year: 1980\n3. [Doc 19562] - Year: 1997\n4. [Doc 25738] - Year: 2001\n5. [Doc 107589] - Year: 2023\n6. [Doc 91657] - Year: 2019\n7. [Doc 94285] - Year: 2020\n8. [Doc 45703] - Year: 2009\n9. [Doc 62279] - Year: 2013\n10. [Doc 82497] - Year: 2018\n11. [Doc 108737] - Year:", "latency_ms": 7046.909332275391, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is a single year, 1986, but the generated answer lists multiple years for different documents and never provides 1986. It also fabricates document-based context not present in the gold; thus it is both incorrect and unfaithful to the labeled answer.", "main_total_tokens": 2823, "main_prompt_tokens": 2567, "main_completion_tokens": 256, "judge_total_tokens": 665, "judge_prompt_tokens": 578, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 142.6982879638672, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific rating (8.466), while the generated answer says 'I don't know.' This is incorrect because the model fails to provide the known rating. However, it is faithful to its own claimed uncertainty and does not fabricate information, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 486, "judge_prompt_tokens": 398, "judge_completion_tokens": 88}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "The name of the TV show about two comic book fanboys working in a secret hospital for superheroes is \"Mighty Med\" [Doc 121125].", "latency_ms": 2871.74391746521, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med,' which matches the gold answer exactly and directly addresses the question without adding unsupported information.", "main_total_tokens": 2019, "main_prompt_tokens": 1982, "main_completion_tokens": 37, "judge_total_tokens": 484, "judge_prompt_tokens": 423, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "In Túnel de los huesos (2011), seven prisoners escaped from the Villa Devoto prison in Buenos Aires, Argentina [Doc 53582].", "latency_ms": 2782.9155921936035, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks only for the country. The generated answer states the escape was from the Villa Devoto prison in Buenos Aires, Argentina. This correctly includes the country (Argentina) and adds specific prison and city details that are consistent rather than fabricated relative to the gold answer. The core requested information matches the gold, so the response is both correct and faithful.", "main_total_tokens": 2249, "main_prompt_tokens": 2211, "main_completion_tokens": 38, "judge_total_tokens": 525, "judge_prompt_tokens": 423, "judge_completion_tokens": 102}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "In Sweet Trap (2024), the apprentice falls into the trap designed by Jiang Jie [Doc 135231].", "latency_ms": 2688.495635986328, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in Sweet Trap (2024), the apprentice falls into Jiang Jie's trap, which matches the gold answer exactly. It does not introduce any unsupported details beyond the source, so it is both correct and faithful.", "main_total_tokens": 2433, "main_prompt_tokens": 2402, "main_completion_tokens": 31, "judge_total_tokens": 486, "judge_prompt_tokens": 407, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "0", "latency_ms": 505.6180953979492, "retrieval_recall": 0.0, "retrieval_mrr": 0.07692307692307693, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is numerically incorrect compared to the gold label (0 vs. 4), so correctness is 0. The answer is a single factual claim with no reasoning or hallucinated explanation; it simply states an incorrect number, so it is internally faithful to itself, yielding high faithfulness.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 476, "judge_prompt_tokens": 385, "judge_completion_tokens": 91}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "37", "latency_ms": 301.8195629119873, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer '37' is factually incorrect because there are no known movies featuring both Bruce Lee and Steven Seagal together, matching the gold explanation. It is also unfaithful to the provided gold answer, which explicitly states that no such movies can be found.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 395, "judge_completion_tokens": 85}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "The rating of \"Tokyo Mater\" (2008) is 6.3 [Doc 42750].", "latency_ms": 2610.050916671753, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states the rating as 6.3, which matches the gold label, and it does not introduce any unsupported information beyond that.", "main_total_tokens": 2462, "main_prompt_tokens": 2433, "main_completion_tokens": 29, "judge_total_tokens": 466, "judge_prompt_tokens": 405, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "None of the provided documents directly answer what style of racing is featured in \"Fast Track: No Limits,\" which is the movie mentioned at the beginning of your query [Doc 41929]. The plot description indicates it centers around a group of street racers, but does not specify the exact style or type of racing. Therefore, based strictly on the context provided, we cannot determine the specific style of racing in this particular movie.", "latency_ms": 3865.226984024048, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states that the style of racing is Drift. The generated answer claims the style cannot be determined from the provided documents and does not assert an incorrect style; however, relative to the gold, it fails to provide the correct label and is thus incorrect for the task. It is faithful to the given documents (it accurately reflects that they do not specify the style), so faithfulness is high, but correctness with respect to the gold answer is zero.", "main_total_tokens": 2700, "main_prompt_tokens": 2610, "main_completion_tokens": 90, "judge_total_tokens": 585, "judge_prompt_tokens": 463, "judge_completion_tokens": 122}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little, is \"Confronting a Serial Killer\" [Doc 131298].", "latency_ms": 3199.981212615967, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the title as 'Confronting a Serial Killer' and remains faithful to the referenced document.", "main_total_tokens": 2450, "main_prompt_tokens": 2399, "main_completion_tokens": 51, "judge_total_tokens": 513, "judge_prompt_tokens": 452, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "The popularity score of Armageddon (1998) is 7.838 [Doc 20603].", "latency_ms": 2689.4800662994385, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score (7.838) and does not introduce any contradictory or extraneous information. It is both correct and faithful to the provided gold answer.", "main_total_tokens": 2534, "main_prompt_tokens": 2504, "main_completion_tokens": 30, "judge_total_tokens": 470, "judge_prompt_tokens": 407, "judge_completion_tokens": 63}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "None of the documents provided contains information about a 1999 psychological thriller movie that alludes to an extra sense. Therefore, based strictly on the context given, there is no answer found.", "latency_ms": 2789.9279594421387, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is 'The Sixth Sense'. The generated answer claims no answer can be found, which is factually incorrect, so correctness is 0. However, given the instruction to rely strictly on the provided context (which apparently lacked the needed info), the model faithfully followed that constraint, so faithfulness is 1.", "main_total_tokens": 2395, "main_prompt_tokens": 2354, "main_completion_tokens": 41, "judge_total_tokens": 525, "judge_prompt_tokens": 429, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "The Transporter was released in the year 2002 [Doc 27318].", "latency_ms": 2589.095115661621, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Transporter was released in 2002, matching the gold answer, and it remains faithful to the provided information.", "main_total_tokens": 2570, "main_prompt_tokens": 2547, "main_completion_tokens": 23, "judge_total_tokens": 458, "judge_prompt_tokens": 397, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The title of the 2015 reboot of 'The Transporter' is \"The Transporter Refueled\" [Doc 67500].", "latency_ms": 2795.6082820892334, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2015 reboot title as 'The Transporter Refueled' and does not introduce unsupported information beyond a harmless document reference.", "main_total_tokens": 2572, "main_prompt_tokens": 2537, "main_completion_tokens": 35, "judge_total_tokens": 489, "judge_prompt_tokens": 420, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "The year 'The Transporter Refueled' was released is 2015 [Doc 67500].", "latency_ms": 2777.430534362793, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, which matches the gold answer. It does not introduce contradictory or extraneous information and remains faithful to the question and presumed source.", "main_total_tokens": 2682, "main_prompt_tokens": 2654, "main_completion_tokens": 28, "judge_total_tokens": 483, "judge_prompt_tokens": 406, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "None of the documents provided contain information about a romantic comedy where Abraham Lincoln comes back from the dead. [Doc 54943] mentions \"Abraham Lincoln: Vampire Hunter,\" which is listed under genres 'Action', 'Fantasy', and 'Horror,' but it does not specify that he returns from the dead in a romantic comedy context. Therefore, based strictly on the provided documents, there is no answer to the question.", "latency_ms": 3698.6517906188965, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a romantic comedy in which Abraham Lincoln comes back from the dead. The gold answer correctly states that no such movie description can be found. The generated answer states that none of the provided documents mention such a romantic comedy and concludes there is no answer based on the documents. This matches the gold answer in substance and is consistent with the cited document. The reasoning is grounded strictly in the provided documents and does not introduce unsupported claims.", "main_total_tokens": 2182, "main_prompt_tokens": 2092, "main_completion_tokens": 90, "judge_total_tokens": 594, "judge_prompt_tokens": 475, "judge_completion_tokens": 119}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "The title of a movie that alludes to a manuscript written by Ernest Hemingway is \"Havana Darkness\" [Doc 90923]. In this plot, characters travel to Havana, Cuba, to investigate the origins of a mysterious manuscript supposedly written by acclaimed American novelist Ernest Hemingway.", "latency_ms": 2931.8649768829346, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie title as 'Havana Darkness', which matches the gold answer exactly. The additional plot description is plausible and does not contradict the core fact being evaluated, so the response is both correct and faithful.", "main_total_tokens": 2285, "main_prompt_tokens": 2222, "main_completion_tokens": 63, "judge_total_tokens": 520, "judge_prompt_tokens": 443, "judge_completion_tokens": 77}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "The TV show where Doll heads to Hollywood to be with her childhood best friend Em is \"Doll & Em\" [Doc 122259].", "latency_ms": 2427.8643131256104, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the TV show as \"Doll & Em,\" which matches the gold answer exactly. The inclusion of a document reference does not change or contradict the content, so the response is both correct and faithful.", "main_total_tokens": 2433, "main_prompt_tokens": 2399, "main_completion_tokens": 34, "judge_total_tokens": 494, "judge_prompt_tokens": 418, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "In the movie \"Hallam Foe\" from 2007, Hallam has a talent for spying on people and attempts to find the cause of his mother’s death [Doc 39389].", "latency_ms": 2552.9656410217285, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the 2007 movie as \"Hallam Foe\" and restates the question context without adding unsupported information. It is both correct and faithful to the given content.", "main_total_tokens": 2437, "main_prompt_tokens": 2391, "main_completion_tokens": 46, "judge_total_tokens": 509, "judge_prompt_tokens": 439, "judge_completion_tokens": 70}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "None of the documents provided contain information about a last name that initially sounds like it could belong to either a friend or foe. Therefore, based strictly on the context given, there's no answer found for the question posed.", "latency_ms": 2734.5046997070312, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'A foe', so there is a definite correct answer. The generated answer claims there is no answer in the documents and does not provide the correct one, so it is factually incorrect. However, it remains faithful to its stated assumption of relying strictly on the provided documents and not hallucinating an answer, so it is faithful to the (implied) context-handling instructions.", "main_total_tokens": 2577, "main_prompt_tokens": 2532, "main_completion_tokens": 45, "judge_total_tokens": 540, "judge_prompt_tokens": 429, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "According to [Doc 124583], in the 2017 TV show Atypical, Sam is 18 years old.", "latency_ms": 2714.2345905303955, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is 18 years old in the 2017 TV show Atypical, which matches the gold answer. It does not invent conflicting details and attributes the information to a document, so it is faithful to the provided context.", "main_total_tokens": 2231, "main_prompt_tokens": 2198, "main_completion_tokens": 33, "judge_total_tokens": 496, "judge_prompt_tokens": 413, "judge_completion_tokens": 83}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "In the 2017 TV show Atypical, Sam is on the autism spectrum [Doc 124583].", "latency_ms": 2019.9263095855713, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies that in the TV show Atypical, Sam is on the autism spectrum, which matches the gold answer exactly. It is faithful to the provided information without adding unrelated or incorrect details.", "main_total_tokens": 2203, "main_prompt_tokens": 2174, "main_completion_tokens": 29, "judge_total_tokens": 483, "judge_prompt_tokens": 410, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "According to the provided context, Sandra was born in Chamonix as stated in [Doc 59659].", "latency_ms": 2266.183853149414, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (Chamonix) and correctly cites the provided context as its source, so it is both correct and faithful.", "main_total_tokens": 2633, "main_prompt_tokens": 2607, "main_completion_tokens": 26, "judge_total_tokens": 481, "judge_prompt_tokens": 419, "judge_completion_tokens": 62}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "In the Japanese language movie \"When the Rain Falls,\" Hazuki clips her nails every time it rains [Doc 100481].", "latency_ms": 2490.936279296875, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly fills in the blank with 'rains,' matching the gold answer. However, it fabricates extra details such as the movie title 'When the Rain Falls' and a citation '[Doc 100481]', which are not supported by the provided information.", "main_total_tokens": 2730, "main_prompt_tokens": 2699, "main_completion_tokens": 31, "judge_total_tokens": 504, "judge_prompt_tokens": 417, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of Greenland [Doc 114810].", "latency_ms": 3080.5325508117676, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Tabaluga is the crown-prince of Greenland, which matches the gold answer exactly. It stays faithful to the provided information without introducing contradictions or unrelated content.", "main_total_tokens": 2444, "main_prompt_tokens": 2409, "main_completion_tokens": 35, "judge_total_tokens": 493, "judge_prompt_tokens": 424, "judge_completion_tokens": 69}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "The movie that matches the given description is \"One Afternoon\" [Doc 96838]. This film's plot describes a strict father and his two boys living a calm life in an old home until a summer moment changes everything for the brothers.", "latency_ms": 3104.449510574341, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'One Afternoon,' which matches the gold answer. The explanation is consistent with the given description and does not introduce contradictions or unsupported claims, so it is both correct and faithful to the prompt and gold label.", "main_total_tokens": 2043, "main_prompt_tokens": 1990, "main_completion_tokens": 53, "judge_total_tokens": 532, "judge_prompt_tokens": 452, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The low-rated Italian language movie in which a Holocaust survivor takes care of a 12-year-old street kid who recently robbed her is titled \"The Life Ahead\" [Doc 93260]. This movie has a rating of 6.746 and is described as fitting the criteria where a Holocaust survivor with a daycare business takes in a 12-year-old street kid who recently robbed her.", "latency_ms": 2979.1672229766846, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Life Ahead,\" which matches the gold answer. It accurately restates the plot details consistent with the question and does not introduce contradictions or unsupported claims. Therefore, both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2373, "main_prompt_tokens": 2288, "main_completion_tokens": 85, "judge_total_tokens": 556, "judge_prompt_tokens": 475, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The 2016 documentary \"McLaren\" is about New Zealander Bruce McLaren, who founded the McLaren Motor Racing team, showcasing his journey from humble beginnings to taking on the elite of motor racing [Doc 73859].", "latency_ms": 3257.465600967407, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies that the 2016 documentary 'McLaren' is about Bruce McLaren and accurately describes him as a New Zealander and founder of the McLaren motor racing team. This aligns with the gold answer and does not introduce unsupported or contradictory information, so it is both correct and faithful.", "main_total_tokens": 2313, "main_prompt_tokens": 2260, "main_completion_tokens": 53, "judge_total_tokens": 526, "judge_prompt_tokens": 432, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "None of the documents provided contain information about a specific racing team associated with an individual referred to as \"his\" in the question. The context only provides details on various movies and shows related to racing but does not specify a particular racing team for any character described as \"he.\" Therefore, based strictly on the given context, there is no answer to the question \"What’s the name of his racing team?\" [Doc None]", "latency_ms": 4197.640419006348, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is explicitly given in the gold as 'The McLaren Motor Racing team.' The generated answer claims there is no answer in the documents and does not identify the team, so it is incorrect. However, it accurately reflects its own stated lack of information from the provided context and does not hallucinate a team name, so it is faithful to the (assumed) context it references.", "main_total_tokens": 2499, "main_prompt_tokens": 2413, "main_completion_tokens": 86, "judge_total_tokens": 578, "judge_prompt_tokens": 468, "judge_completion_tokens": 110}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "The question \"What country is he from?\" does not specify who \"he\" refers to, making it impossible to provide an accurate answer based solely on the provided documents. Each document describes different movies with various plots and characters without a clear reference to a specific individual whose origin country could be identified. Therefore, there isn't enough context to determine which character or person's country of origin is being asked about [No Doc].", "latency_ms": 3537.2464656829834, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the correct country is New Zealand, but the generated answer claims the question is unanswerable from the context. This is factually incorrect relative to the gold, so correctness is 0. However, the generated answer consistently follows its own stated reasoning about lacking context and does not introduce contradictions with that reasoning, so it is internally faithful, giving faithfulness a score of 1.", "main_total_tokens": 2191, "main_prompt_tokens": 2106, "main_completion_tokens": 85, "judge_total_tokens": 565, "judge_prompt_tokens": 458, "judge_completion_tokens": 107}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "Based on the provided context, there is no information that indicates John McClane blows up the Great Wall of China in any movie from 2013. The only movie mentioned with both John McClane and a connection to 2013 is \"A Good Day to Die Hard,\" but the plot summary does not include any reference to blowing up the Great Wall of China. [Doc 58791]", "latency_ms": 3958.867311477661, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct gold answer is that the database does not contain such a movie, effectively acknowledging that no such film exists in the data. The generated answer goes beyond this by asserting specific movie details and referencing a document ID, which are fabricated relative to the given gold answer and context. Therefore, it is not faithful to the gold answer and not considered correct under this evaluation setup.", "main_total_tokens": 2698, "main_prompt_tokens": 2611, "main_completion_tokens": 87, "judge_total_tokens": 584, "judge_prompt_tokens": 479, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "The 2002 Japanese language horror movie that matches the given description is \"Tomie: Forbidden Fruit\" [Doc 28589]. This film's plot involves a lonely high school girl who befriends another strange, mysterious and beautiful girl, hinting at a non-accidental meeting between the two.", "latency_ms": 3499.8090267181396, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold title, 'Tomie: Forbidden Fruit', and the brief explanation is consistent with the given description. There are no hallucinations or contradictions, so both correctness and faithfulness are fully satisfied.", "main_total_tokens": 2453, "main_prompt_tokens": 2386, "main_completion_tokens": 67, "judge_total_tokens": 554, "judge_prompt_tokens": 478, "judge_completion_tokens": 76}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "In the 2021 movie \"Lei mi parla ancora,\" Nino and Caterina have been married for 65 years [Doc 98566].", "latency_ms": 3111.175060272217, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years) and stays within the given information without adding unsupported claims.", "main_total_tokens": 2868, "main_prompt_tokens": 2829, "main_completion_tokens": 39, "judge_total_tokens": 478, "judge_prompt_tokens": 424, "judge_completion_tokens": 54}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for the legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 3648.651123046875, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that in 'Scorpion King: Book of Souls' the Scorpion King and Tala are searching for the Book of Souls, matching the gold answer. The added phrase 'legendary relic' does not contradict the gold and is consistent with the film, so the response is both correct and faithful.", "main_total_tokens": 2715, "main_prompt_tokens": 2673, "main_completion_tokens": 42, "judge_total_tokens": 524, "judge_prompt_tokens": 429, "judge_completion_tokens": 95}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The document does not provide information about the release year for \"Zombies on the Moon\". [Doc 65790] [Doc 22155] [Doc 12975] [Doc 55959] [Doc 65177] [Doc 33871] [Doc 16374] [Doc 106100] [Doc 9782] [Doc 49411] [Doc 92297] [Doc 92340] [Doc 75467] [Doc 44944] [Doc 94017]", "latency_ms": 4946.613788604736, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 0.6, "justification": "Gold answer says the movie is not in the database. The generated answer instead claims that the provided document does not contain the release year and then lists document IDs. In terms of correctness regarding the user's question, both effectively say the release year is not given, so the generated answer is acceptable (score 1). However, faithfulness to the gold is imperfect: it attributes the lack of information specifically to the document rather than to the database and adds extraneous doc references, so it is only partially aligned with the gold (around 0.6).", "main_total_tokens": 2611, "main_prompt_tokens": 2457, "main_completion_tokens": 154, "judge_total_tokens": 642, "judge_prompt_tokens": 499, "judge_completion_tokens": 143}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about the release year of \"The Galactic baker\". [None Found]", "latency_ms": 2543.8742637634277, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database. The generated answer correctly states that the documents do not contain information about the release year, which is equivalent in meaning. It does not invent a release year or contradict the lack of information, so it is both correct and faithful to the gold.", "main_total_tokens": 2362, "main_prompt_tokens": 2340, "main_completion_tokens": 22, "judge_total_tokens": 502, "judge_prompt_tokens": 410, "judge_completion_tokens": 92}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain any information about the release year of \"Return of the Null Pointer\". Therefore, based on the given context, it is not possible to determine the release year [Doc 13807].", "latency_ms": 2646.592855453491, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the release year cannot be determined / it is not in the database or documents. The generated answer is correct relative to the provided context and remains faithful to it by explicitly tying the limitation to the given documents.", "main_total_tokens": 2041, "main_prompt_tokens": 1994, "main_completion_tokens": 47, "judge_total_tokens": 514, "judge_prompt_tokens": 434, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "None of the documents provided contain information about a movie titled \"Data Science: The Movie\". Therefore, based on the given context, the release year for \"Data Science: The Movie\" is not found.", "latency_ms": 2955.843448638916, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database, while the generated answer claims the release year is not found in the given context. The generated answer implies a limitation based only on provided documents, not the database, and does not clearly acknowledge lack of knowledge. Relative to the gold, this is not the same answer, so correctness is 0. However, the answer does not contradict the given context and is consistent with it, so faithfulness is 1.", "main_total_tokens": 2620, "main_prompt_tokens": 2578, "main_completion_tokens": 42, "judge_total_tokens": 557, "judge_prompt_tokens": 432, "judge_completion_tokens": 125}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about the release year of \"Python's Revenge 2\". [None of the documents mention this specific title or its release date].", "latency_ms": 2713.319778442383, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the release year of a movie that is not in the provided database/gold. The gold answer correctly states the lack of information. The generated answer also correctly states that the documents do not contain information about the release year and notes that none mention the title or its release date. This is factually correct given the setup and remains faithful to the available information without hallucinating a year.", "main_total_tokens": 2455, "main_prompt_tokens": 2421, "main_completion_tokens": 34, "judge_total_tokens": 535, "judge_prompt_tokens": 424, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "The provided documents do not contain information about a release year for something titled \"The Last Pixel\". [None of the documents mention \"The Last Pixel\"].", "latency_ms": 2500.821590423584, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both answers correctly state that there is no information about a release year for 'The Last Pixel' in the available sources. The generated answer is consistent with the given documents and does not fabricate a date, so it is both correct and faithful.", "main_total_tokens": 2150, "main_prompt_tokens": 2119, "main_completion_tokens": 31, "judge_total_tokens": 498, "judge_prompt_tokens": 419, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "None of the documents provided contain information about a title called \"The man who ate the sun\". Therefore, based on the given context, the release year for \"The man who ate the sun\" is not found.", "latency_ms": 3174.1995811462402, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database. The generated answer correctly states that the context does not contain information about the title and that the release year is not found. This matches the gold response in meaning and does not hallucinate a year, so it is both correct and faithful to the given context.", "main_total_tokens": 2924, "main_prompt_tokens": 2880, "main_completion_tokens": 44, "judge_total_tokens": 529, "judge_prompt_tokens": 435, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about who directed the movie \"The Last Days of Lehman Brothers\". [Doc 46069]", "latency_ms": 2918.7228679656982, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "Both the gold and generated answers state that the director information is not available in the provided documents/database. The generated answer directly aligns with the evidence constraint and does not hallucinate a director name, so it is both correct with respect to the task and faithful to the given documents.", "main_total_tokens": 2820, "main_prompt_tokens": 2789, "main_completion_tokens": 31, "judge_total_tokens": 510, "judge_prompt_tokens": 424, "judge_completion_tokens": 86}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about the director of the movie \"A Love to Die For\". Therefore, I cannot provide a name for the director based on the given sources. [None of the documents mention a movie titled \"A Love to Die For\" or its director.]", "latency_ms": 3207.3240280151367, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director information is not available in the provided documents and thus does not guess a director. This matches the gold response in content and intent, and it remains fully grounded in the given sources, which contain no such information.", "main_total_tokens": 2603, "main_prompt_tokens": 2546, "main_completion_tokens": 57, "judge_total_tokens": 532, "judge_prompt_tokens": 451, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided documents do not contain information about who directed the movie \"Journey into Gaza\". [Doc 111360]", "latency_ms": 2637.817144393921, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the documents do not provide information about the director of 'Journey into Gaza', which matches the gold answer that the director information is not available. It does not introduce any unsupported claims and stays faithful to the provided evidence.", "main_total_tokens": 2450, "main_prompt_tokens": 2421, "main_completion_tokens": 29, "judge_total_tokens": 496, "judge_prompt_tokens": 416, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "The provided context does not contain information about the director of the movie \"The Last Night at Tremore Beach\". [Doc 135098] provides details about the plot and other metadata but omits the director's name. Therefore, based strictly on the given documents, I cannot specify who directed the movie.", "latency_ms": 3105.3903102874756, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the director information is not available in the provided context and refuses to guess, matching the gold answer in substance. It does not introduce unsupported information and is fully grounded in the described documents, so it is both correct and faithful.", "main_total_tokens": 2209, "main_prompt_tokens": 2143, "main_completion_tokens": 66, "judge_total_tokens": 540, "judge_prompt_tokens": 458, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "The movie in which a group of archaeology students awaken an ancient mummy is \"Rise of the Mummy\" [Doc 98375].", "latency_ms": 2544.0778732299805, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Rise of the Mummy'. It does not introduce any contradictory or unsupported information beyond a harmless document reference tag, so it is fully faithful to the given information.", "main_total_tokens": 2194, "main_prompt_tokens": 2160, "main_completion_tokens": 34, "judge_total_tokens": 496, "judge_prompt_tokens": 418, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "The plot described in the question matches the plot of \"Stranger in My Bed\" [Doc 35237]. In this movie, Sarah fakes her own death to escape her abusive spouse and flees, but her husband soon uncovers the truth and pursues her. The game between them becomes deadly, with only one expected to survive.", "latency_ms": 3343.585729598999, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Stranger in My Bed,' which matches the gold answer exactly. The additional plot details about Sarah faking her death to escape her abusive spouse and the ensuing deadly game are consistent with the description given and do not contradict the prompt. Therefore, the answer is both correct and faithful to the provided information.", "main_total_tokens": 2362, "main_prompt_tokens": 2289, "main_completion_tokens": 73, "judge_total_tokens": 565, "judge_prompt_tokens": 465, "judge_completion_tokens": 100}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "The movie in which a successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years is \"Game of Silence\" [Doc 123938].", "latency_ms": 2435.161828994751, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the title as 'Game of Silence,' which matches the gold answer exactly. It is fully consistent and faithful to the provided information without adding unsupported details.", "main_total_tokens": 1917, "main_prompt_tokens": 1878, "main_completion_tokens": 39, "judge_total_tokens": 491, "judge_prompt_tokens": 430, "judge_completion_tokens": 61}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The movie that is based on a true story about a killer from the site Craigslist is \"The Craigslist Killer\" [Doc 53039]. This film, released in 2011, tells the story of Philip Markoff, a charismatic medical student at Boston University who leads a double life as a brutal sexual deviant who abuses prostitutes he finds via Craigslist.", "latency_ms": 3362.338066101074, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"The Craigslist Killer,\" matching the gold answer exactly. The additional details about the plot (Philip Markoff, BU medical student, attacks via Craigslist, 2011 release) are accurate and consistent with known information, so the response is both correct and faithful to the prompt.", "main_total_tokens": 2303, "main_prompt_tokens": 2226, "main_completion_tokens": 77, "judge_total_tokens": 557, "judge_prompt_tokens": 463, "judge_completion_tokens": 94}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "The movie where Yuki Hirano, after failing his university entrance exam, sets his eyes on the role of a forestry trainee is \"Wood Job!\" [Doc 66094].", "latency_ms": 2902.860403060913, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Wood Job!\", which matches the gold answer exactly. It stays faithful to the provided information without adding contradictory or unsupported details.", "main_total_tokens": 2575, "main_prompt_tokens": 2534, "main_completion_tokens": 41, "judge_total_tokens": 501, "judge_prompt_tokens": 436, "judge_completion_tokens": 65}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The described scenario where a playful bird snatches Abby's magic wand during a hot-air balloon ride, leaving Elmo, Abby, and friends on an adventure, is from the movie titled \"Sesame Street: The Magical Wand Chase\" [Doc 78373].", "latency_ms": 3063.143014907837, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Sesame Street: The Magical Wand Chase,' which matches the gold answer exactly. It stays fully grounded in the information provided and does not introduce contradictions or unsupported claims, so both correctness and faithfulness are maximal.", "main_total_tokens": 2343, "main_prompt_tokens": 2285, "main_completion_tokens": 58, "judge_total_tokens": 545, "judge_prompt_tokens": 463, "judge_completion_tokens": 82}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "The described scenario where \"a 7 year-old boy must find the hero within him in order to save [his] parents\" trapped inside a burning car after a rock fall is from the movie titled \"Beyond.\" This information is directly found in the plot description provided for the movie \"Beyond\" [Doc 74597].", "latency_ms": 3179.086923599243, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Beyond,\" which matches the gold answer. The reasoning stays within the information given in the prompt and does not introduce unsupported claims, so it is both correct and faithful.", "main_total_tokens": 1998, "main_prompt_tokens": 1929, "main_completion_tokens": 69, "judge_total_tokens": 546, "judge_prompt_tokens": 473, "judge_completion_tokens": 73}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "The movie where a woman, fired from a financial corporation during the Asia crisis, returns home with no money is \"6ixtynin9\" [Doc 22478].", "latency_ms": 2534.3496799468994, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as '6ixtynin9', which matches the gold answer exactly. It is also faithful to the provided information and does not introduce unsupported details, aside from a citation-like reference that does not contradict anything.", "main_total_tokens": 2052, "main_prompt_tokens": 2012, "main_completion_tokens": 40, "judge_total_tokens": 519, "judge_prompt_tokens": 438, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "The described scenario occurs in the movie \"Club Dread\" [Doc 32410]. This film is categorized under genres including Comedy, Horror, Mystery, and Thriller, released in 2004. The plot revolves around a serial killer disrupting activities at Coconut Pete's Coconut Beach Resort, which is described as an island paradise frequented by swingers. It's up to the club’s staff to either stop or hide the violence caused by the killer [Doc 32410].", "latency_ms": 3885.735034942627, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Club Dread', matching the gold answer exactly. The additional plot and genre details are consistent with the query and do not contradict the provided information, so the response is both correct and faithful to the prompt.", "main_total_tokens": 2440, "main_prompt_tokens": 2337, "main_completion_tokens": 103, "judge_total_tokens": 598, "judge_prompt_tokens": 517, "judge_completion_tokens": 81}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "The movie described in the question is \"Toilet\" from [Doc 48873]. This document matches the description provided: a 30-something Gundam fan and engineer who lives with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "latency_ms": 3365.0100231170654, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as \"Toilet,\" which matches the gold answer. The explanation is consistent with the described plot and does not introduce contradictions, so it is both correct and faithful to the provided information.", "main_total_tokens": 2671, "main_prompt_tokens": 2606, "main_completion_tokens": 65, "judge_total_tokens": 549, "judge_prompt_tokens": 474, "judge_completion_tokens": 75}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "The movie that matches the description provided is \"Electric Angel\" [Doc 1024]. This experimental film uses a variety of audiovisual techniques to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century, combining animation and live action methods.", "latency_ms": 3123.250961303711, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Electric Angel,' which matches the gold answer. It does not introduce contradictions or unsupported claims beyond restating and slightly elaborating the given description, so it is both correct and faithful to the information provided.", "main_total_tokens": 2337, "main_prompt_tokens": 2274, "main_completion_tokens": 63, "judge_total_tokens": 554, "judge_prompt_tokens": 474, "judge_completion_tokens": 80}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "Based on the provided documents, the lowest rated movie from 2023 is \"Film\" with a rating of 5.944 [Doc 108434].", "latency_ms": 1861.9072437286377, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'Mega Ape', but the generated answer claims the lowest rated movie is 'Film' with a specific rating, which is incorrect. Additionally, the generated answer invents details from a cited document ('Film' and its rating from Doc 108434) that are not supported by the given gold answer, so it is not faithful.", "main_total_tokens": 2249, "main_prompt_tokens": 2208, "main_completion_tokens": 41, "judge_total_tokens": 509, "judge_prompt_tokens": 413, "judge_completion_tokens": 96}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 1268.4199810028076, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for a specific answer: the highest rated TV show from 2010, whose gold answer is 'Tea House'. The generated answer says 'I don't know', which is factually incorrect relative to the gold, so correctness is 0. However, it does not fabricate any information and honestly admits lack of knowledge, so it is fully faithful to the model's own stated uncertainty, giving a faithfulness score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 505, "judge_prompt_tokens": 386, "judge_completion_tokens": 119}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 609.6198558807373, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question has a known correct answer provided in the gold: 'The Uncut Story of Mewtwo's Origin', while the generated answer says 'I don't know', which is incorrect, so correctness is 0. The model did not hallucinate an incorrect fact and admitted not knowing, so it is fully faithful to its own uncertainty, giving faithfulness 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 500, "judge_prompt_tokens": 395, "judge_completion_tokens": 105}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "The lowest-rated movie from 1980 is \"Don't Answer the Phone!\" with a rating of 4.6 [Doc 768]. The language of this movie is English [Doc 768].", "latency_ms": 3202.8608322143555, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the language of the lowest-rated movie from 1980. The generated answer identifies the movie and states that its language is English, which matches the gold answer. The reasoning and conclusion are consistent with the provided documents, so the answer is both correct and faithful.", "main_total_tokens": 2599, "main_prompt_tokens": 2551, "main_completion_tokens": 48, "judge_total_tokens": 510, "judge_prompt_tokens": 423, "judge_completion_tokens": 87}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "Based on the provided documents, the TV show with the highest popularity in 1999 is \"Freaks and Geeks\" [Doc 115187] with a popularity rating of 5.8043. It's described as a drama-comedy series about high school life focusing on the dynamics between different social groups.", "latency_ms": 3494.863986968994, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is factually incorrect relative to the gold label: it names 'Freaks and Geeks' instead of 'Law & Order: Special Victims Unit' as the TV show with the highest popularity in 1999. However, it accurately reflects what is stated in the referenced document (Doc 115187) about popularity and description, so it is faithful to its cited source.", "main_total_tokens": 2626, "main_prompt_tokens": 2552, "main_completion_tokens": 74, "judge_total_tokens": 565, "judge_prompt_tokens": 454, "judge_completion_tokens": 111}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "The rating of \"Bluefin\" is 7.2 [Doc 79355].", "latency_ms": 2503.2994747161865, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating (7.2) and correctly attributes it to the document, so it is both correct and faithful.", "main_total_tokens": 2439, "main_prompt_tokens": 2417, "main_completion_tokens": 22, "judge_total_tokens": 460, "judge_prompt_tokens": 400, "judge_completion_tokens": 60}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "The movie \"The Story of Us\" was released in 1999 [Doc 22481]. There is another movie with the same title released in 2019 [Doc 88811], but based on the question, it's clear you're asking for one specific instance.", "latency_ms": 3297.0871925354004, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer says 2019, while the generated answer says 1999 as the main response, so it is factually incorrect relative to the gold. However, the answer is internally consistent with the evidence it cites (Doc 22481 and Doc 88811) and does not contradict its own sources, so it is faithful to the provided documents.", "main_total_tokens": 2470, "main_prompt_tokens": 2403, "main_completion_tokens": 67, "judge_total_tokens": 541, "judge_prompt_tokens": 438, "judge_completion_tokens": 103}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "2", "latency_ms": 104.27498817443848, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (2) does not match the gold answer (279), so it is factually incorrect. However, the answer is a direct numeric response to the question without hallucinated explanation or contradictions, so it is faithful to the (incorrect) prediction itself.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 467, "judge_prompt_tokens": 383, "judge_completion_tokens": 84}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "11", "latency_ms": 120.79763412475586, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (11) does not match the gold answer (12), so it is incorrect. However, it directly answers the question without introducing extraneous or contradictory information relative to the prompt, so it is faithful to the given content.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 462, "judge_prompt_tokens": 383, "judge_completion_tokens": 79}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "The movie \"The Last Days of Lehman Brothers\" was released in 2009 [Doc 46069].", "latency_ms": 2983.8757514953613, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the movie 'The Last Days of Lehman Brothers' was released in 2009, which matches the gold answer. It is fully supported by the referenced document, so it is both correct and faithful.", "main_total_tokens": 2783, "main_prompt_tokens": 2754, "main_completion_tokens": 29, "judge_total_tokens": 488, "judge_prompt_tokens": 410, "judge_completion_tokens": 78}
{"experiment_name": "SysC_Hybrid_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is English [Doc 48035].", "latency_ms": 2563.089370727539, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer is 'en' (English), and the generated answer correctly states that the original language of 'The King's Speech' is English. It is consistent with the gold and does not introduce unsupported information beyond citing a document.", "main_total_tokens": 2462, "main_prompt_tokens": 2438, "main_completion_tokens": 24, "judge_total_tokens": 479, "judge_prompt_tokens": 402, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_001", "question_type": "factual", "question": "What year was Wall Street released?", "gold_answer": "1987", "generated_answer": "1987 [Doc 6993]", "latency_ms": 4282.120943069458, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'Wall Street' was released in 1987 and directly matches the gold answer. There is no extraneous or contradictory information, so it is both correct and faithful.", "main_total_tokens": 3063, "main_prompt_tokens": 2993, "main_completion_tokens": 70, "judge_total_tokens": 458, "judge_prompt_tokens": 387, "judge_completion_tokens": 71}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_002", "question_type": "descriptive", "question": "In which movie do classmates have to play a survival game using their cell phones?", "gold_answer": "Brain Fluid Explosion Girl", "generated_answer": "Brain Fluid Explosion Girl", "latency_ms": 4098.865747451782, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, which is the correct movie title. It is both correct and fully faithful to the reference.", "main_total_tokens": 2742, "main_prompt_tokens": 2666, "main_completion_tokens": 76, "judge_total_tokens": 450, "judge_prompt_tokens": 392, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_003", "question_type": "descriptive", "question": "In which TV show does six friends need to rely on a Dungeon Master to get home?", "gold_answer": "Dungeons & Dragons", "generated_answer": "Dungeons & Dragons", "latency_ms": 4236.130714416504, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, and correctly identifies the TV show where six friends rely on a Dungeon Master to get home as 'Dungeons & Dragons.'", "main_total_tokens": 2725, "main_prompt_tokens": 2643, "main_completion_tokens": 82, "judge_total_tokens": 458, "judge_prompt_tokens": 394, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_004", "question_type": "descriptive", "question": "Which movie is about the story of two rival gangs in Munich, one composed of German teens and the other of Turkish ones?", "gold_answer": "Nacht der Wölfe", "generated_answer": "Nacht der Wölfe", "latency_ms": 4213.3424282073975, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Nacht der Wölfe') and correctly identifies the movie described, so it is both correct and faithful.", "main_total_tokens": 2781, "main_prompt_tokens": 2699, "main_completion_tokens": 82, "judge_total_tokens": 465, "judge_prompt_tokens": 403, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_005", "question_type": "factual", "question": "In which year do the events of Picnic at Hanging Rock take place?", "gold_answer": "1900", "generated_answer": "1900 [Doc 126191]", "latency_ms": 4284.969091415405, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer (1900) with no contradictions or irrelevant additions.", "main_total_tokens": 3039, "main_prompt_tokens": 2960, "main_completion_tokens": 79, "judge_total_tokens": 444, "judge_prompt_tokens": 394, "judge_completion_tokens": 50}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_006", "question_type": "descriptive", "question": "Which 2004 movie is about a young violinist, who spends her days working in an oddball Venice coffee shop, her nights rehearsing for a professional music career, and daydreaming about the beautiful man that's destined just for her, and her name happens to also be the name of a calendar month?", "gold_answer": "June", "generated_answer": "June", "latency_ms": 438.7514591217041, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question clearly describes the movie whose main character is named June, matching the gold answer. The generated answer exactly matches the gold answer and is fully supported by the question context.", "main_total_tokens": 134, "main_prompt_tokens": 132, "main_completion_tokens": 2, "judge_total_tokens": 499, "judge_prompt_tokens": 433, "judge_completion_tokens": 66}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_007", "question_type": "multi_hop", "question": "What is the highest rated Russian-language movie released in 1980?", "gold_answer": "The Adventures of Sherlock Holmes and Dr. Watson: Hunting the Tiger", "generated_answer": "I don't know.", "latency_ms": 187.00218200683594, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies a particular film, but the generated answer states lack of knowledge. While this avoids hallucination and is thus faithful, it is factually incorrect relative to the gold because it does not provide the correct movie.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 475, "judge_prompt_tokens": 399, "judge_completion_tokens": 76}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_008", "question_type": "multi_hop", "question": "What is the highest russian-language movie rating in 1999?", "gold_answer": "7.6", "generated_answer": "I don't know.", "latency_ms": 209.55610275268555, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answer is factually incorrect because the gold label indicates the answer should be 7.6, while the model responded with 'I don't know.' However, the answer is faithful to the model's stated uncertainty and does not hallucinate any incorrect facts, so faithfulness is high.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 478, "judge_prompt_tokens": 389, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_009", "question_type": "filter", "question": "How many movies were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "4", "generated_answer": "18", "latency_ms": 2041.7592525482178, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.8, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.4, "ndcg_at_10": 1.366123019050001, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (18) does not match the gold answer (4), so it is incorrect. However, there is no evidence of the model contradicting or ignoring any provided context beyond giving the wrong count, so the response is still faithful to the (limited) prompt.", "main_total_tokens": 1563, "main_prompt_tokens": 1561, "main_completion_tokens": 2, "judge_total_tokens": 480, "judge_prompt_tokens": 394, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_010", "question_type": "filter", "question": "How many TV shows were released in 1987, have a rating over 7, and are in chinese?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 189.2404556274414, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 2 such TV shows, while the generated answer claims there are none. This is factually incorrect relative to the gold label, so correctness is 0. The answer is internally consistent with its own reasoning and does not fabricate specific data; it simply expresses inability to find matches, so faithfulness is 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 504, "judge_prompt_tokens": 404, "judge_completion_tokens": 100}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_011", "question_type": "filter", "question": "What is the only French language TV show released in 1980?", "gold_answer": "Fantômas", "generated_answer": "I don't know.", "latency_ms": 231.0013771057129, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer admits not knowing and does not provide an alternative, so it does not produce an incorrect fact (faithful to its own uncertainty), but it fails to give the correct answer 'Fantômas', so correctness is 0.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 468, "judge_prompt_tokens": 389, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_012", "question_type": "descriptive", "question": "which movie is about two british beauties going to Barbados with a yacht captain?", "gold_answer": "Kill Cruise", "generated_answer": "Kill Cruise [Doc 11205]", "latency_ms": 3738.548755645752, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the movie as 'Kill Cruise', matching the gold answer exactly. There are no additions or changes beyond the correct title, so the response is both correct and faithful.", "main_total_tokens": 2884, "main_prompt_tokens": 2831, "main_completion_tokens": 53, "judge_total_tokens": 462, "judge_prompt_tokens": 393, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_013", "question_type": "descriptive", "question": "What is the title of the movie about the TV series in which Fred Rogers speaks to children about some of life’s most important issues?", "gold_answer": "Won't You Be My Neighbor?", "generated_answer": "Won't You Be My Neighbor?", "latency_ms": 4373.42381477356, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie title, so it is both correct and faithful.", "main_total_tokens": 3271, "main_prompt_tokens": 3191, "main_completion_tokens": 80, "judge_total_tokens": 462, "judge_prompt_tokens": 408, "judge_completion_tokens": 54}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_014", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "2018", "generated_answer": "2018 [Doc 84838]", "latency_ms": 4968.100070953369, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer gives the correct year (2018), which matches the gold answer, and it does not introduce any contradictory or extraneous information. Therefore, it is both correct and faithful.", "main_total_tokens": 3249, "main_prompt_tokens": 3156, "main_completion_tokens": 93, "judge_total_tokens": 450, "judge_prompt_tokens": 386, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_015", "question_type": "descriptive", "question": "Which TV series is an animation about a young boy robot in 2030 AD?", "gold_answer": "Astro Boy", "generated_answer": "Astro Boy [Doc 112591]", "latency_ms": 4651.587963104248, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Astro Boy') and correctly identifies the TV series described. It is fully correct and faithful.", "main_total_tokens": 2976, "main_prompt_tokens": 2871, "main_completion_tokens": 105, "judge_total_tokens": 454, "judge_prompt_tokens": 395, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_016", "question_type": "descriptive", "question": "In the movie \"Room\" released in 2015, how long were the woman and her son held captive?", "gold_answer": "7 years", "generated_answer": "7 years [Doc 67513]", "latency_ms": 3974.9977588653564, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0, "justification": "The generated answer matches the gold answer exactly and is therefore factually correct. However, the reference to an external document ID ([Doc 67513]) is not grounded in the provided context, so the answer is not fully faithful to the given information.", "main_total_tokens": 2636, "main_prompt_tokens": 2555, "main_completion_tokens": 81, "judge_total_tokens": 484, "judge_prompt_tokens": 403, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_017", "question_type": "factual", "question": "How much gold was Brink's-Mat security depot holding before it was broken into on 26 November 1983?", "gold_answer": "£26m", "generated_answer": "£26m [Doc 133469]", "latency_ms": 2241.663932800293, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The Brink's-Mat security depot was holding approximately £26 million in gold before the 26 November 1983 robbery. The generated answer exactly matches this figure and aligns with the provided document reference, so it is both correct and faithful.", "main_total_tokens": 2281, "main_prompt_tokens": 2266, "main_completion_tokens": 15, "judge_total_tokens": 483, "judge_prompt_tokens": 404, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_018", "question_type": "multi_hop", "question": "What is the popularity score of the most popular English-speaking TV show released in 1997?", "gold_answer": "80.5772", "generated_answer": "I don't know.", "latency_ms": 488.781213760376, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer gives a specific numeric popularity score (80.5772), while the generated answer says it does not know the value. This is factually incorrect relative to the gold, so correctness is 0. However, the model does not fabricate a value and honestly states its uncertainty, so it is fully faithful to its own reasoning, giving faithfulness a score of 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 504, "judge_prompt_tokens": 396, "judge_completion_tokens": 108}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_019", "question_type": "filter", "question": "How many Russian language TV shows were released in 1997?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 213.63234519958496, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer indicates there was 1 Russian language TV show released in 1997, but the generated answer claims it cannot find any, which is factually incorrect. However, the generated answer does not invent a wrong number or unsupported details; it simply states an inability to find a match, so it remains faithful to the model’s stated uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 495, "judge_prompt_tokens": 394, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_020", "question_type": "descriptive", "question": "Soo-wan, who had been blind, eventually undergoes an eye transplant surgery that restores her sight in which TV show?", "gold_answer": "Angel Eyes", "generated_answer": "Angel Eyes [Doc 121928]", "latency_ms": 5089.16163444519, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, identifying 'Angel Eyes' as the correct TV show. There is no contradiction or fabrication beyond the given information.", "main_total_tokens": 3362, "main_prompt_tokens": 3259, "main_completion_tokens": 103, "judge_total_tokens": 465, "judge_prompt_tokens": 403, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_021", "question_type": "contextual", "question": "What's the language of this movie?", "gold_answer": "Korean", "generated_answer": "The language of this movie is English [Doc 14842]. However, there are multiple movies titled \"Angel Eyes\" with different languages. Another one is also in English [Doc 25900], while the TV show is in Korean [Doc 121928].", "latency_ms": 7654.83283996582, "retrieval_recall": 1.0, "retrieval_mrr": 0.3333333333333333, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.5, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0.5, "justification": "The gold answer indicates the movie language is Korean, while the generated answer claims the movie is in English and only mentions the Korean language in relation to a TV show, not the asked-about movie. This makes the main claim factually incorrect relative to the gold label, so correctness is 0. The answer appears to rely on document references and internal retrieval (e.g., multiple movies titled \"Angel Eyes\" and a TV show in Korean), but it confuses or misattributes which work the question refers to. Because it partially reflects retrieved information yet misaligns it with the query, faithfulness is partial rather than entirely fabricated.", "main_total_tokens": 3385, "main_prompt_tokens": 3146, "main_completion_tokens": 239, "judge_total_tokens": 588, "judge_prompt_tokens": 429, "judge_completion_tokens": 159}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_022", "question_type": "filter", "question": "How many English-speaking Horror movies were released in 2009 and have a popularity score greater than 7?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 675.8241653442383, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer states there are 2 such movies, while the generated answer claims there are none, so the generated answer is incorrect. However, the model does not fabricate specific numbers or movies and honestly states it cannot find any, which is faithful to its stated limitations and knowledge access.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 491, "judge_prompt_tokens": 403, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_023", "question_type": "unanswerable", "question": "What's the best-rated movie released in 1979?", "gold_answer": "I'm sorry, but there are no movies released in 1979 in my database.", "generated_answer": "I don't know.", "latency_ms": 3021.263360977173, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "Gold explicitly states there are no movies from 1979 in the database, so the correct answer is to convey that limitation, not general ignorance. The generated answer 'I don't know.' is not correct relative to the gold. However, it does not invent any information and stays within the bounds of not knowing, so it is fully faithful to the gold in the sense of not contradicting or adding facts.", "main_total_tokens": 723, "main_prompt_tokens": 618, "main_completion_tokens": 105, "judge_total_tokens": 511, "judge_prompt_tokens": 399, "judge_completion_tokens": 112}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_024", "question_type": "unanswerable", "question": "The movie Back to the Future part IV was released in which year?", "gold_answer": "I'm sorry, but 'Back to the Future Part IV' does not exist in my database.", "generated_answer": "I don't know.", "latency_ms": 2404.9665927886963, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that 'Back to the Future Part IV' does not exist in the database, i.e., the system cannot provide a release year. The generated answer 'I don't know.' is effectively equivalent in meaning: it does not hallucinate a year and aligns with the lack of information. Thus it is correct for the question and faithful to the gold.", "main_total_tokens": 702, "main_prompt_tokens": 626, "main_completion_tokens": 76, "judge_total_tokens": 508, "judge_prompt_tokens": 404, "judge_completion_tokens": 104}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_025", "question_type": "unanswerable", "question": "What's the popularity score of the least popular movie in 1980 about the nuclear Chernobyl disaster?", "gold_answer": "I could not find any movies from 1980 related to the Chernobyl disaster.", "generated_answer": "I cannot find any movies or shows matching that criteria. Please check if your question refers to an event that occurred during the time period mentioned.", "latency_ms": 0.0, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that it cannot find any movies or shows matching the criteria, which aligns with the gold answer that no such movies exist. It is faithful to the information implied by the gold response and does not introduce contradictory or fabricated details.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 514, "judge_prompt_tokens": 434, "judge_completion_tokens": 80}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_026", "question_type": "filter", "question": "How many TV shows or movies have 'Astro Boy' in the title?", "gold_answer": "4", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 154.35004234313965, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 4, but the generated answer claims there are none, which is incorrect. The model clearly states its inability to find any, and does not hallucinate a number or titles, so it is faithful to its internal retrieval limitations but factually wrong relative to the gold.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 486, "judge_prompt_tokens": 397, "judge_completion_tokens": 89}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_027", "question_type": "filter", "question": "How many movies in 2025 allude to the holocaust?", "gold_answer": "2", "generated_answer": "I cannot find any movies or shows matching that criteria.", "latency_ms": 3228.656530380249, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer given by the gold standard is 2, but the generated answer says there are none, so it is factually incorrect (score 0 for correctness). However, the model honestly states its inability to find such movies instead of fabricating a number, so it remains faithful to its own retrieval limits and does not hallucinate specific titles or counts (score 1 for faithfulness).", "main_total_tokens": 3624, "main_prompt_tokens": 3622, "main_completion_tokens": 2, "judge_total_tokens": 505, "judge_prompt_tokens": 395, "judge_completion_tokens": 110}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_028", "question_type": "filter", "question": "How many TV shows were released in the 1990s about Super Mario Bros?", "gold_answer": "1", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 143.00084114074707, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 1, but the generated answer claims there are none, which is factually incorrect, so correctness is 0. However, the model does not fabricate specific titles or numbers and instead admits it cannot find any, so it remains faithful to its (stated) uncertainty, giving faithfulness a 1.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 495, "judge_prompt_tokens": 398, "judge_completion_tokens": 97}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_029", "question_type": "unanswerable", "question": "How many TV shows were released between 2000 and 2010 about Super Mario Bros?", "gold_answer": "0", "generated_answer": "I cannot find any movies or shows matching those criteria.", "latency_ms": 156.1572551727295, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says there were 0 TV shows about Super Mario Bros released between 2000 and 2010. The generated answer states it cannot find any movies or shows matching the criteria, which aligns with saying there are none. This matches the gold answer semantically, so it is correct. It is also faithful to the question, addressing the requested criteria explicitly (TV shows, that time period, topic), without adding unsupported details.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 519, "judge_prompt_tokens": 400, "judge_completion_tokens": 119}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_030", "question_type": "multi_hop", "question": "What is the most popular TV show or movie about ninjas?", "gold_answer": "Naruto", "generated_answer": "I don't know.", "latency_ms": 211.49945259094238, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer is 'Naruto' as the most popular ninja-related TV show. The generated answer says 'I don't know,' which is incorrect. However, it does not invent any false information; it simply expresses a lack of knowledge, so it is fully faithful to the model's (simulated) uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 480, "judge_prompt_tokens": 386, "judge_completion_tokens": 94}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_031", "question_type": "unanswerable", "question": "When was the Jurassic Park/Power Ranger crossover movie released?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 2307.7242374420166, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer specifies that the movie was not released, while the generated answer says 'I don't know,' which is incorrect given the gold. However, the generated answer does not introduce any contradictions or unsupported claims relative to the prompt; it simply expresses a lack of knowledge, so it is fully faithful but incorrect.", "main_total_tokens": 629, "main_prompt_tokens": 561, "main_completion_tokens": 68, "judge_total_tokens": 482, "judge_prompt_tokens": 389, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_032", "question_type": "unanswerable", "question": "In which movie does Barbie fight the Predator?", "gold_answer": "This movie was not released.", "generated_answer": "I don't know.", "latency_ms": 2326.815605163574, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is that such a movie was not released, while the model answered with uncertainty ('I don't know'), which is incorrect. However, the model did not invent any incorrect specific movie title or fabricated information; it simply expressed a lack of knowledge, so it remains faithful (non-hallucinatory) to what it knows.", "main_total_tokens": 668, "main_prompt_tokens": 586, "main_completion_tokens": 82, "judge_total_tokens": 484, "judge_prompt_tokens": 386, "judge_completion_tokens": 98}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_033", "question_type": "multi_hop", "question": "What is the 4th oldest Star Wars movie?", "gold_answer": "Star Wars: Droids - The Great Heep", "generated_answer": "I don't know.", "latency_ms": 3068.9003467559814, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer identifies a specific film, while the generated answer says 'I don't know.' This is factually incorrect because the question has a known correct answer. However, the model did not introduce any hallucinated facts; it simply admitted lack of knowledge, so the response is fully faithful to the model's stated uncertainty.", "main_total_tokens": 674, "main_prompt_tokens": 557, "main_completion_tokens": 117, "judge_total_tokens": 488, "judge_prompt_tokens": 393, "judge_completion_tokens": 95}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_034", "question_type": "contextual", "question": "What year was it released?", "gold_answer": "1986", "generated_answer": "2004 [Doc 32101]", "latency_ms": 6768.23091506958, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct release year is 1986, but the generated answer says 2004, so it is factually incorrect. The answer is, however, consistent with itself and does not contradict any provided context, so it is fully faithful to the given (though wrong) generation.", "main_total_tokens": 3231, "main_prompt_tokens": 3039, "main_completion_tokens": 192, "judge_total_tokens": 473, "judge_prompt_tokens": 386, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_035", "question_type": "factual", "question": "What is the rating of the movie Forrest Gump (NOT \"Through the Eyes of Forrest Gump\")?", "gold_answer": "8.466", "generated_answer": "I don't know.", "latency_ms": 133.09621810913086, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold answer provides a specific numerical rating (8.466) for the movie Forrest Gump. The generated answer says 'I don't know,' which does not match the correct rating and is therefore incorrect. However, it is faithful in that it does not hallucinate an incorrect rating; it honestly states a lack of knowledge.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 494, "judge_prompt_tokens": 398, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_036", "question_type": "descriptive", "question": "What is the name of the TV show about two comic book fanboys working in a secret hospital for superheroes?", "gold_answer": "Mighty Med", "generated_answer": "Mighty Med [Doc 121125]", "latency_ms": 4099.637031555176, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies the TV show as 'Mighty Med', matching the gold answer. The extra bracketed reference '[Doc 121125]' is not part of the requested answer and slightly reduces faithfulness to the expected plain-text format, but the factual content is fully correct.", "main_total_tokens": 2677, "main_prompt_tokens": 2595, "main_completion_tokens": 82, "judge_total_tokens": 490, "judge_prompt_tokens": 400, "judge_completion_tokens": 90}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_037", "question_type": "descriptive", "question": "In Túnel de los huesos (2011), seven prisoners escaped from a prison in which country?", "gold_answer": "Argentina", "generated_answer": "Argentina [Doc 53582]", "latency_ms": 4195.693492889404, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that the prison was in Argentina, which matches the gold answer. It is fully consistent and faithful to the provided information.", "main_total_tokens": 2952, "main_prompt_tokens": 2872, "main_completion_tokens": 80, "judge_total_tokens": 458, "judge_prompt_tokens": 398, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_038", "question_type": "descriptive", "question": "In Sweet Trap (2024), the apprentice falls into whose trap?", "gold_answer": "Jiang Jie", "generated_answer": "Jiang Jie [Doc 135231]", "latency_ms": 4389.157295227051, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, indicating that it is both correct with respect to the question and faithful to the provided information.", "main_total_tokens": 3093, "main_prompt_tokens": 3008, "main_completion_tokens": 85, "judge_total_tokens": 450, "judge_prompt_tokens": 392, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_039", "question_type": "filter", "question": "How many korean-language movies mention assassins after 2023?", "gold_answer": "4", "generated_answer": "8", "latency_ms": 4889.8632526397705, "retrieval_recall": 0.0, "retrieval_mrr": 0.07692307692307693, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered '8' while the gold answer is '4', so the generated answer is factually incorrect. The explanation requested is only a scoring evaluation, and the scores themselves accurately reflect the discrepancy, so the response is faithful to the evaluation task.", "main_total_tokens": 3878, "main_prompt_tokens": 3787, "main_completion_tokens": 91, "judge_total_tokens": 467, "judge_prompt_tokens": 385, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_040", "question_type": "unanswerable", "question": "How many movies have Steven Seagall and Bruce Lee?", "gold_answer": "I cannot find any movies with Bruce Lee and Steven Seagall.", "generated_answer": "30", "latency_ms": 2942.608118057251, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer ('30') is factually wrong because there are no movies featuring both Steven Seagal and Bruce Lee together, as the gold states. It is also unfaithful to the gold explanation, which explicitly says no such movies can be found, whereas the generated output fabricates a specific count.", "main_total_tokens": 2467, "main_prompt_tokens": 2411, "main_completion_tokens": 56, "judge_total_tokens": 487, "judge_prompt_tokens": 395, "judge_completion_tokens": 92}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_041", "question_type": "factual", "question": "What is the rating of Tokyo Mater (2008)?", "gold_answer": "6.3", "generated_answer": "6.3 [Doc 42750]", "latency_ms": 2355.6947708129883, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold rating exactly (6.3) and correctly attributes it to the document, so it is both correct and faithful.", "main_total_tokens": 2459, "main_prompt_tokens": 2446, "main_completion_tokens": 13, "judge_total_tokens": 453, "judge_prompt_tokens": 393, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_042", "question_type": "contextual", "question": "What style of racing is in this movie?", "gold_answer": "Drift", "generated_answer": "The style of racing in this movie is drift racing [Doc 33742].", "latency_ms": 5028.482437133789, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.1309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.1309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly identifies the racing style as drift racing, which matches the gold label 'Drift'. It is faithful to the provided information and only adds a minor, non-misleading reference marker.", "main_total_tokens": 3460, "main_prompt_tokens": 3365, "main_completion_tokens": 95, "judge_total_tokens": 461, "judge_prompt_tokens": 394, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_043", "question_type": "descriptive", "question": "What is the title of movie about New York Times best-selling author and memoirist Jillian Lauren and the most prolific serial killer in American history, Samuel Little?", "gold_answer": "Confronting a Serial Killer", "generated_answer": "The title of the movie about New York Times best-selling author Jillian Lauren and serial killer Samuel Little is \"Confronting a Serial Killer\" [Doc X].", "latency_ms": 4872.572422027588, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie title as 'Confronting a Serial Killer.' It does not introduce contradictions or unsupported claims beyond a minor citation tag, so it is fully faithful to the provided information.", "main_total_tokens": 3132, "main_prompt_tokens": 3022, "main_completion_tokens": 110, "judge_total_tokens": 517, "judge_prompt_tokens": 439, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_044", "question_type": "factual", "question": "What is the popularity score of Armageddon (1998)?", "gold_answer": "7.838", "generated_answer": "7.838 [Doc 20603]", "latency_ms": 2433.2315921783447, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.5, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.5, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold popularity score value of 7.838 and does not introduce any contradictory information, so it is both correct and faithful.", "main_total_tokens": 2532, "main_prompt_tokens": 2517, "main_completion_tokens": 15, "judge_total_tokens": 457, "judge_prompt_tokens": 395, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_045", "question_type": "descriptive", "question": "What is the title of the 1999 psychological thriller movie that alludes to an extra sense?", "gold_answer": "The Sixth Sense", "generated_answer": "I don't know.", "latency_ms": 3974.518299102783, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The model answered 'I don't know,' which is incorrect because the correct answer is 'The Sixth Sense.' However, the response is faithful in that it does not fabricate an incorrect specific answer and accurately reflects uncertainty.", "main_total_tokens": 743, "main_prompt_tokens": 581, "main_completion_tokens": 162, "judge_total_tokens": 468, "judge_prompt_tokens": 395, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_046", "question_type": "factual", "question": "The Transporter was released in which year?", "gold_answer": "2002", "generated_answer": "2002 [Doc 27318]", "latency_ms": 4039.360761642456, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3562071871080221, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter' was released in 2002, matching the gold answer. There is no extraneous or conflicting information, so the response is fully correct and faithful.", "main_total_tokens": 3223, "main_prompt_tokens": 3159, "main_completion_tokens": 64, "judge_total_tokens": 461, "judge_prompt_tokens": 389, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_047", "question_type": "factual", "question": "What is the title of the 2015 reboot of 'The Transporter'?", "gold_answer": "The Transporter Refueled", "generated_answer": "The Transporter Refueled [Doc 67500]", "latency_ms": 4684.402227401733, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.2890648263178879, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and does not introduce unsupported information; the bracketed doc reference does not change the factual response.", "main_total_tokens": 3248, "main_prompt_tokens": 3155, "main_completion_tokens": 93, "judge_total_tokens": 461, "judge_prompt_tokens": 403, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_048", "question_type": "factual", "question": "What year was 'The Transporter Refueled' released?", "gold_answer": "2015", "generated_answer": "2015 [Doc 67500]", "latency_ms": 4806.032657623291, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that 'The Transporter Refueled' was released in 2015, matching the gold answer. It is also faithful to the provided information, with no contradictions or unsupported details.", "main_total_tokens": 3365, "main_prompt_tokens": 3271, "main_completion_tokens": 94, "judge_total_tokens": 466, "judge_prompt_tokens": 393, "judge_completion_tokens": 73}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_049", "question_type": "unanswerable", "question": "In which romantic comedy does Abraham Lincoln come back from the dead?", "gold_answer": "I cannot find a movie description where this happens.", "generated_answer": "I don't know.", "latency_ms": 1889.6682262420654, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says no such movie description can be found; the generated answer says 'I don't know,' which is effectively equivalent. It does not introduce incorrect information and matches the uncertainty in the gold, so it is both correct and faithful.", "main_total_tokens": 642, "main_prompt_tokens": 587, "main_completion_tokens": 55, "judge_total_tokens": 473, "judge_prompt_tokens": 394, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_050", "question_type": "descriptive", "question": "What is the title of a movie that alludes to a manuscript written by Ernest Hemingway?", "gold_answer": "Havana Darkness", "generated_answer": "Havana Darkness [Doc 90923]", "latency_ms": 4247.325420379639, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies 'Havana Darkness' as the movie title alluding to a manuscript written by Ernest Hemingway, matching the gold answer. However, it adds an extraneous bracketed reference '[Doc 90923]' that is not present in the gold, so while factually correct, it is slightly less faithful to the expected output format.", "main_total_tokens": 2911, "main_prompt_tokens": 2827, "main_completion_tokens": 84, "judge_total_tokens": 499, "judge_prompt_tokens": 396, "judge_completion_tokens": 103}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_051", "question_type": "factual", "question": "In which TV show does Doll head to Hollywood to be with her childhood best friend Em?", "gold_answer": "Doll & Em", "generated_answer": "Doll & Em", "latency_ms": 5018.035411834717, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the TV show as 'Doll & Em' and is fully faithful to the provided reference.", "main_total_tokens": 3170, "main_prompt_tokens": 3062, "main_completion_tokens": 108, "judge_total_tokens": 453, "judge_prompt_tokens": 392, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_052", "question_type": "factual", "question": "Hallam has a talent for spying on people and attempts to find the cause of his mother’s death in which 2007 movie?", "gold_answer": "Hallam Foe", "generated_answer": "Hallam Foe [Doc 39389]", "latency_ms": 4266.222953796387, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, naming 'Hallam Foe' as the 2007 movie. It is fully correct and faithful to the provided information.", "main_total_tokens": 3120, "main_prompt_tokens": 3042, "main_completion_tokens": 78, "judge_total_tokens": 474, "judge_prompt_tokens": 409, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_053", "question_type": "contextual", "question": "Does his last name sound at first like he’s a friend or a foe?", "gold_answer": "A foe", "generated_answer": "Yes [Doc 39389].", "latency_ms": 5136.7011070251465, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The correct answer is 'A foe', but the generated answer is 'Yes', which is incorrect. The model also introduces an irrelevant citation ([Doc 39389]) that does not relate to the gold answer, so it is not faithful to the reference.", "main_total_tokens": 3032, "main_prompt_tokens": 2919, "main_completion_tokens": 113, "judge_total_tokens": 475, "judge_prompt_tokens": 393, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_054", "question_type": "factual", "question": "What is the age of Sam in the 2017 TV show Atypical?", "gold_answer": "18", "generated_answer": "18 [Doc 124583]", "latency_ms": 3892.1263217926025, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (18) and does not introduce any extra, unsupported information beyond a citation marker, so it is both correct and faithful.", "main_total_tokens": 2842, "main_prompt_tokens": 2772, "main_completion_tokens": 70, "judge_total_tokens": 459, "judge_prompt_tokens": 395, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_055", "question_type": "factual", "question": "In the 2017 TV show Atypical, Sam is on which spectrum?", "gold_answer": "Autism spectrum", "generated_answer": "Sam is on the autism spectrum [Doc 124583].", "latency_ms": 4438.6022090911865, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.3333333333333333, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that Sam is on the autism spectrum, which exactly matches the gold answer. It is also faithful to the provided information.", "main_total_tokens": 2882, "main_prompt_tokens": 2786, "main_completion_tokens": 96, "judge_total_tokens": 459, "judge_prompt_tokens": 399, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_056", "question_type": "factual", "question": "In À corde tendue, a movie released in 2013, what is the name of the place that Sandra was born?", "gold_answer": "Chamonix", "generated_answer": "Chamonix [Doc 59659]", "latency_ms": 4300.812482833862, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly ('Chamonix') and does not introduce any unsupported information; the bracketed document reference does not alter the factual content.", "main_total_tokens": 3313, "main_prompt_tokens": 3240, "main_completion_tokens": 73, "judge_total_tokens": 470, "judge_prompt_tokens": 406, "judge_completion_tokens": 64}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_057", "question_type": "factual", "question": "Fill in the blank: in this japanese language movie, Hazuki clips her nails every time it _____? ", "gold_answer": "rains", "generated_answer": "rains [Doc 100481]", "latency_ms": 3937.952995300293, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('rains'), and the explanation simply restates it without adding unsupported details. Both correctness and faithfulness are maximal.", "main_total_tokens": 3397, "main_prompt_tokens": 3347, "main_completion_tokens": 50, "judge_total_tokens": 461, "judge_prompt_tokens": 398, "judge_completion_tokens": 63}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_058", "question_type": "factual", "question": "In this low-rated German language animation, Tabaluga is the last of the dragons and the crown-prince of where?", "gold_answer": "Greenland", "generated_answer": "Greenland [Doc 114810]", "latency_ms": 4113.593578338623, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the place of which Tabaluga is crown-prince; the gold answer is Greenland. The generated answer is exactly 'Greenland', which is correct and directly supported by the provided gold answer, with no hallucinated or extraneous information.", "main_total_tokens": 3093, "main_prompt_tokens": 3023, "main_completion_tokens": 70, "judge_total_tokens": 484, "judge_prompt_tokens": 401, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_059", "question_type": "descriptive", "question": "Which movie has the following description: A strict father and his two boys share a calm life in a creaky old home. One summer moment changes everything for the brothers.", "gold_answer": "One Afternoon", "generated_answer": "One Afternoon [Doc 96838]", "latency_ms": 3045.5005168914795, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('One Afternoon') with no contradictions or hallucinations.", "main_total_tokens": 2641, "main_prompt_tokens": 2612, "main_completion_tokens": 29, "judge_total_tokens": 462, "judge_prompt_tokens": 412, "judge_completion_tokens": 50}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_060", "question_type": "descriptive", "question": "In which low-rated Italian language movie does a holocaust survivor take care of a 12 year old street kid who recently robbed her?", "gold_answer": "The Life Ahead", "generated_answer": "The Life Ahead [Doc 93260]", "latency_ms": 3263.545513153076, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "recall_at_10": 1.0, "precision_at_5": 0.2, "precision_at_10": 0.1, "ndcg_at_5": 1.0, "ndcg_at_10": 1.0, "hit_rate_at_5": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly, correctly identifying 'The Life Ahead' as the movie described. It is fully faithful to the provided information.", "main_total_tokens": 2986, "main_prompt_tokens": 2903, "main_completion_tokens": 83, "judge_total_tokens": 468, "judge_prompt_tokens": 407, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_061", "question_type": "factual", "question": "The 2016 Documentary “McLaren” is about who?", "gold_answer": "Bruce McLaren", "generated_answer": "The documentary \"McLaren\" from [Doc 73859] is about Bruce McLaren, a New Zealander who founded the McLaren Motor Racing team. It details his journey from humble beginnings to becoming a formidable force in motor racing.", "latency_ms": 4930.996656417847, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.9, "justification": "The generated answer correctly identifies Bruce McLaren as the subject of the 2016 documentary, matching the gold answer. It slightly embellishes with source-like notation ([Doc 73859]) and extra narrative detail that is not explicitly provided in the prompt, so while the core claim is faithful, the added specifics are not grounded, warranting a small deduction in faithfulness.", "main_total_tokens": 2953, "main_prompt_tokens": 2833, "main_completion_tokens": 120, "judge_total_tokens": 540, "judge_prompt_tokens": 433, "judge_completion_tokens": 107}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_062", "question_type": "contextual", "question": "What’s the name of his racing team?", "gold_answer": "The McLaren Motor Racing team", "generated_answer": "McLaren", "latency_ms": 5436.773300170898, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.6309297535714575, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer 'McLaren' correctly identifies the entity in question as the McLaren racing team, which matches the gold answer 'The McLaren Motor Racing team' in substance. The response is faithful to the gold reference, differing only in formality and length, not in meaning.", "main_total_tokens": 3338, "main_prompt_tokens": 3222, "main_completion_tokens": 116, "judge_total_tokens": 473, "judge_prompt_tokens": 385, "judge_completion_tokens": 88}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_063", "question_type": "contextual", "question": "What country is he from?", "gold_answer": "New Zealand", "generated_answer": "I don't know.", "latency_ms": 1789.9093627929688, "retrieval_recall": 1.0, "retrieval_mrr": 0.2, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.38685280723454163, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.38685280723454163, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The correct answer is New Zealand, but the model said 'I don't know', which is factually incorrect. However, it did not hallucinate an incorrect country and explicitly acknowledged uncertainty, so it is fully faithful to its own knowledge state.", "main_total_tokens": 660, "main_prompt_tokens": 621, "main_completion_tokens": 39, "judge_total_tokens": 459, "judge_prompt_tokens": 380, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_064", "question_type": "unanswerable", "question": "In which 2013 movie does John McClane blow up the great wall of China?", "gold_answer": "I do not have a movie like this in my database", "generated_answer": "A Good Day to Die Hard is not the movie where John McClane blows up the Great Wall of China. In fact, none of the provided documents describe such an event. The question appears to be a mix-up with plot elements or a hypothetical scenario as it does not correspond to any of the described movies in the given context. [Doc 58791]", "latency_ms": 6214.3919467926025, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says there is no such movie in the database. The generated answer likewise states that no described movie has John McClane blowing up the Great Wall of China and treats the question as a mix-up, which is consistent with the gold. It does not introduce contradictions with the provided information and correctly denies the premise.", "main_total_tokens": 3421, "main_prompt_tokens": 3247, "main_completion_tokens": 174, "judge_total_tokens": 567, "judge_prompt_tokens": 471, "judge_completion_tokens": 96}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_065", "question_type": "descriptive", "question": "Which 2002 japanese language horror movie has this description: When a lonely high school girl befriends a strange, mysterious and beautiful girl, it is soon becomes clear that the two girls' meeting was not purely accidental.", "gold_answer": "Tomie: Forbidden Fruit", "generated_answer": "Tomie: Forbidden Fruit [Doc 28589]", "latency_ms": 4416.751861572266, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the 2002 Japanese horror film as 'Tomie: Forbidden Fruit.' It is fully correct and faithful to the provided gold output.", "main_total_tokens": 3109, "main_prompt_tokens": 3024, "main_completion_tokens": 85, "judge_total_tokens": 497, "judge_prompt_tokens": 428, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_066", "question_type": "factual", "question": "In the 2021 movie Lei mi parla ancora, Nino and Caterina have been married for how many years?", "gold_answer": "65", "generated_answer": "65 [Doc 98566]", "latency_ms": 4973.646402359009, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer exactly (65 years) and does not introduce any unsupported information beyond citing a document reference.", "main_total_tokens": 3567, "main_prompt_tokens": 3470, "main_completion_tokens": 97, "judge_total_tokens": 458, "judge_prompt_tokens": 402, "judge_completion_tokens": 56}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_067", "question_type": "factual", "question": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for what?", "gold_answer": "The Book of Souls", "generated_answer": "In Scorpion King: Book of Souls (2018), The Scorpion King and Tala search for the legendary relic known as The Book of Souls [Doc 82582].", "latency_ms": 5465.628147125244, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9319597492354388, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer correctly states that The Scorpion King and Tala search for the legendary relic known as The Book of Souls, which matches the gold answer exactly with only minor elaboration. It is fully correct and faithful to the reference.", "main_total_tokens": 3466, "main_prompt_tokens": 3339, "main_completion_tokens": 127, "judge_total_tokens": 506, "judge_prompt_tokens": 429, "judge_completion_tokens": 77}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_068", "question_type": "unanswerable", "question": "What year was \"Zombies on the Moon\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1840.0940895080566, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the movie is not in the database, which effectively conveys lack of knowledge. The generated answer, 'I don't know,' is consistent with that and does not introduce any incorrect information. It is therefore both correct and faithful to the gold.", "main_total_tokens": 647, "main_prompt_tokens": 594, "main_completion_tokens": 53, "judge_total_tokens": 478, "judge_prompt_tokens": 395, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_069", "question_type": "unanswerable", "question": "What year was \"The Galactic baker\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1673.6769676208496, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is not in the database, and the generated answer says 'I don't know,' which is effectively the same in terms of information. It does not hallucinate a year or contradict the gold, so it is both correct and faithful.", "main_total_tokens": 680, "main_prompt_tokens": 634, "main_completion_tokens": 46, "judge_total_tokens": 476, "judge_prompt_tokens": 393, "judge_completion_tokens": 83}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_070", "question_type": "unanswerable", "question": "What year was \"Return of the Null Pointer\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1685.6789588928223, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer indicates the movie is not in the database. The generated answer, 'I don't know,' is consistent with lacking that information and therefore is correct and faithful to the gold response.", "main_total_tokens": 618, "main_prompt_tokens": 570, "main_completion_tokens": 48, "judge_total_tokens": 464, "judge_prompt_tokens": 395, "judge_completion_tokens": 69}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_071", "question_type": "unanswerable", "question": "What year was \"Data Science: The Movie\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1982.9349517822266, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for a year, but both the gold and generated answers appropriately state lack of knowledge instead of fabricating a date. Saying 'I don't know' is effectively equivalent in meaning to the gold response and is correct and faithful.", "main_total_tokens": 659, "main_prompt_tokens": 598, "main_completion_tokens": 61, "judge_total_tokens": 473, "judge_prompt_tokens": 395, "judge_completion_tokens": 78}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_072", "question_type": "unanswerable", "question": "What year was \"Python's Revenge 2\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1708.932876586914, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer admits not knowing, which is effectively equivalent in correctness to the gold response that the movie is not in the database. There is no hallucinated year or extra information. It is fully consistent with the gold answer and faithful to the prompt.", "main_total_tokens": 641, "main_prompt_tokens": 591, "main_completion_tokens": 50, "judge_total_tokens": 476, "judge_prompt_tokens": 395, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_073", "question_type": "unanswerable", "question": "What year was \"The Last Pixel\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1662.9607677459717, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states the movie is not in the database, meaning the correct behavior is to admit lack of knowledge. The generated answer, 'I don't know,' is equivalent in content and intent, so it is fully correct and faithful to the gold response.", "main_total_tokens": 612, "main_prompt_tokens": 563, "main_completion_tokens": 49, "judge_total_tokens": 475, "judge_prompt_tokens": 393, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_074", "question_type": "unanswerable", "question": "What year was \"The man who ate the sun\" released?", "gold_answer": "I'm sorry, but that movie is not in my database.", "generated_answer": "I don't know.", "latency_ms": 1358.9584827423096, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The correct answer is that the information is not available / not known, and the generated response \"I don't know\" is equivalent in content to the gold answer. It is both correct and faithful.", "main_total_tokens": 625, "main_prompt_tokens": 592, "main_completion_tokens": 33, "judge_total_tokens": 466, "judge_prompt_tokens": 396, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_075", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Days of Lehman Brothers\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 2334.040641784668, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the information is not available in the database, and the generated answer says 'I don't know.' Both effectively convey that the system does not have the director information. This is correct with respect to the given database and fully faithful to the gold response's intent.", "main_total_tokens": 666, "main_prompt_tokens": 583, "main_completion_tokens": 83, "judge_total_tokens": 488, "judge_prompt_tokens": 401, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_076", "question_type": "unanswerable", "question": "Who directed the movie \"A Love to Die For\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 1910.7706546783447, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not provided in the database. The generated answer says 'I don't know,' which is effectively the same in this context: it does not hallucinate a director and accurately reflects the lack of information. Therefore it is both correct and faithful to the gold response.", "main_total_tokens": 669, "main_prompt_tokens": 611, "main_completion_tokens": 58, "judge_total_tokens": 490, "judge_prompt_tokens": 399, "judge_completion_tokens": 91}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_077", "question_type": "unanswerable", "question": "Who directed the movie \"Journey into Gaza\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 2146.1479663848877, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer states that the director information is not provided, effectively meaning it is unknown from the database. The generated answer, 'I don't know,' matches this and does not introduce extra information, so it is both correct and faithful to the gold response.", "main_total_tokens": 664, "main_prompt_tokens": 592, "main_completion_tokens": 72, "judge_total_tokens": 479, "judge_prompt_tokens": 397, "judge_completion_tokens": 82}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_078", "question_type": "unanswerable", "question": "Who directed the movie \"The Last Night at Tremore Beach\"?", "gold_answer": "I cannot answer that, as the director information is not provided in the database.", "generated_answer": "I don't know.", "latency_ms": 1864.5882606506348, "retrieval_recall": null, "retrieval_mrr": null, "recall_at_5": null, "recall_at_10": null, "precision_at_5": null, "precision_at_10": null, "ndcg_at_5": null, "ndcg_at_10": null, "hit_rate_at_5": null, "hit_rate_at_10": null, "correctness_score": 1, "faithfulness_score": 1, "justification": "The gold answer says the database does not provide the director information, so the correct response is to indicate not knowing. The generated answer says 'I don't know,' which is correct and faithful to the lack of information in the database.", "main_total_tokens": 664, "main_prompt_tokens": 608, "main_completion_tokens": 56, "judge_total_tokens": 473, "judge_prompt_tokens": 401, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_079", "question_type": "descriptive", "question": "In which movie does the following happen: A group of archaeology students awaken an ancient mummy.", "gold_answer": "Rise of the Mummy", "generated_answer": "Rise of the Mummy", "latency_ms": 4210.310697555542, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Rise of the Mummy'), correctly identifying the movie, and it is fully faithful to the provided information.", "main_total_tokens": 2895, "main_prompt_tokens": 2811, "main_completion_tokens": 84, "judge_total_tokens": 457, "judge_prompt_tokens": 396, "judge_completion_tokens": 61}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_080", "question_type": "descriptive", "question": "In which movie does the following happen: To escape her abusive spouse, Sarah fakes her own death and flees.", "gold_answer": "Stranger in My Bed", "generated_answer": "Stranger in My Bed", "latency_ms": 4931.640148162842, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as 'Stranger in My Bed' and faithfully reflecting the provided reference.", "main_total_tokens": 3070, "main_prompt_tokens": 2959, "main_completion_tokens": 111, "judge_total_tokens": 459, "judge_prompt_tokens": 400, "judge_completion_tokens": 59}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_081", "question_type": "descriptive", "question": "In which movie does the following happen: A successful Atlanta attorney's long-lost childhood friends unexpectedly reappear after 25 years.", "gold_answer": "Game of Silence", "generated_answer": "Game of Silence", "latency_ms": 4733.38770866394, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question describes the plot of the TV series 'Game of Silence,' and both the gold answer and the generated answer correctly identify it. The model exactly matches the gold answer and is fully faithful to it.", "main_total_tokens": 2643, "main_prompt_tokens": 2529, "main_completion_tokens": 114, "judge_total_tokens": 472, "judge_prompt_tokens": 400, "judge_completion_tokens": 72}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_082", "question_type": "descriptive", "question": "In which movie does the following happen: Based on a true story about a killer from the site Craigslist…", "gold_answer": "The Craigslist Killer", "generated_answer": "The Craigslist Killer", "latency_ms": 4435.394763946533, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer and correctly identifies the movie as 'The Craigslist Killer'. It is fully correct and faithful.", "main_total_tokens": 2947, "main_prompt_tokens": 2856, "main_completion_tokens": 91, "judge_total_tokens": 452, "judge_prompt_tokens": 395, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_083", "question_type": "descriptive", "question": "In which movie does the following happen: After failing his university entrance exam, Yuki Hirano sets his eyes on the role of a forestry trainee.", "gold_answer": "Wood Job!", "generated_answer": "Wood Job!", "latency_ms": 4275.217533111572, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Wood Job!') and correctly identifies the movie for the described plot. It is both correct and fully faithful to the reference.", "main_total_tokens": 3256, "main_prompt_tokens": 3184, "main_completion_tokens": 72, "judge_total_tokens": 468, "judge_prompt_tokens": 403, "judge_completion_tokens": 65}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_084", "question_type": "descriptive", "question": "In which movie does the following happen: A playful bird snatches Abby's magic wand during a hot-air balloon ride leaving Elmo, Abby and friends on a colorful adventure.", "gold_answer": "Sesame Street: The Magical Wand Chase", "generated_answer": "The Magical Wand Chase", "latency_ms": 4533.918380737305, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.5, "justification": "The generated answer omits the \"Sesame Street\" qualifier but otherwise matches the gold title and clearly refers to the same movie, so it is effectively correct. However, it is not fully faithful to the gold reference because it does not reproduce the full official title, so faithfulness is only partial.", "main_total_tokens": 3049, "main_prompt_tokens": 2961, "main_completion_tokens": 88, "judge_total_tokens": 507, "judge_prompt_tokens": 414, "judge_completion_tokens": 93}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_085", "question_type": "descriptive", "question": "In which movie does the following happen: When a rock fall traps his parents inside a burning car, a 7 year-old boy must find the hero within him in order to save their lives.", "gold_answer": "Beyond", "generated_answer": "Beyond [Doc 74597]", "latency_ms": 4507.366180419922, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Beyond') with no additional incorrect information or contradictions. Both correctness and faithfulness are therefore fully satisfied.", "main_total_tokens": 2642, "main_prompt_tokens": 2534, "main_completion_tokens": 108, "judge_total_tokens": 475, "judge_prompt_tokens": 415, "judge_completion_tokens": 60}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_086", "question_type": "descriptive", "question": "In which movie does the following happen: A woman, fired from a financial corporation during the Asia crisis, returns home with no money.", "gold_answer": "6ixtynin9", "generated_answer": "6ixtynin9 [Doc 22478]", "latency_ms": 4658.113479614258, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer, correctly identifying the movie as '6ixtynin9'. There are no extraneous or conflicting details, so it is fully correct and faithful.", "main_total_tokens": 2723, "main_prompt_tokens": 2612, "main_completion_tokens": 111, "judge_total_tokens": 485, "judge_prompt_tokens": 415, "judge_completion_tokens": 70}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_087", "question_type": "descriptive", "question": "In which movie does the following happen: When a serial killer interrupts the fun at the swanky Coconut Pete's Coconut Beach Resort -- a hedonistic island paradise for swingers --- it's up to the club's staff to stop the violence ... or at least hide it!", "gold_answer": "Club Dread", "generated_answer": "Club Dread [Doc 32410]", "latency_ms": 4586.555004119873, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer, 'Club Dread', exactly matches the gold answer and correctly identifies the movie based on the plot description. It is fully correct and faithful to the given information.", "main_total_tokens": 3068, "main_prompt_tokens": 2974, "main_completion_tokens": 94, "judge_total_tokens": 500, "judge_prompt_tokens": 433, "judge_completion_tokens": 67}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_088", "question_type": "descriptive", "question": "In which movie does the following happen: A 30-something Gundam fan and engineer is forced to live with his recluse brother Maury and younger sister Lisa to take care of their Japanese grandmother after their mother's funeral.", "gold_answer": "Toilet", "generated_answer": "Title: Toilet [Doc 48873]", "latency_ms": 4687.227487564087, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The question asks for the movie title given a plot description. The gold answer is 'Toilet', and the generated answer is also 'Toilet' (with matching title). This is correct and aligns fully with the gold, so both correctness and faithfulness are 1.", "main_total_tokens": 3329, "main_prompt_tokens": 3238, "main_completion_tokens": 91, "judge_total_tokens": 509, "judge_prompt_tokens": 423, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_089", "question_type": "descriptive", "question": "In which movie does the following happen: A typical experimental film, in which a variety of audiovisual techniques are used to create the sense of polymorphic eroticism as developed by European and Mediterranean cinematography of the 20th century.", "gold_answer": "Electric Angel", "generated_answer": "Electric Angel [Doc 1024]", "latency_ms": 3985.409736633301, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold answer ('Electric Angel'), so it is correct. It is also fully faithful to the provided information.", "main_total_tokens": 2941, "main_prompt_tokens": 2872, "main_completion_tokens": 69, "judge_total_tokens": 482, "judge_prompt_tokens": 424, "judge_completion_tokens": 58}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_090", "question_type": "multi_hop", "question": "What is the lowest rated movie from 2023?", "gold_answer": "Mega Ape", "generated_answer": "Down Low [Doc 106995]", "latency_ms": 3111.919641494751, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer, 'Down Low', does not match the gold answer, 'Mega Ape', so it is factually incorrect for this evaluation task. However, the model stayed within the scope of the question and did not fabricate additional context or reasoning beyond naming a movie, so it is fully faithful to its own (albeit wrong) response.", "main_total_tokens": 2886, "main_prompt_tokens": 2790, "main_completion_tokens": 96, "judge_total_tokens": 489, "judge_prompt_tokens": 389, "judge_completion_tokens": 100}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_091", "question_type": "multi_hop", "question": "What is the highest rated TV show from 2010?", "gold_answer": "Tea House", "generated_answer": "I don't know.", "latency_ms": 156.2497615814209, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The question asks for the highest rated TV show from 2010 and the gold answer is 'Tea House'. The generated answer says 'I don't know,' which does not match the gold answer and is therefore incorrect. However, the model does not fabricate any incorrect alternative; it honestly admits not knowing, so it is fully faithful to its own uncertainty.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 487, "judge_prompt_tokens": 386, "judge_completion_tokens": 101}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_092", "question_type": "multi_hop", "question": "What is the highest rated japanese language movie from 1999?", "gold_answer": "The Uncut Story of Mewtwo's Origin ", "generated_answer": "I don't know.", "latency_ms": 237.6692295074463, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer is incorrect because the gold answer provides a specific movie title, while the generated answer claims not to know. However, it is faithful to the model's stated uncertainty and does not hallucinate an incorrect fact, so it is fully faithful.", "main_total_tokens": 0, "main_prompt_tokens": 0, "main_completion_tokens": 0, "judge_total_tokens": 476, "judge_prompt_tokens": 395, "judge_completion_tokens": 81}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_093", "question_type": "multi_hop", "question": "What is the language of the lowest-rated movie from 1980?", "gold_answer": "English", "generated_answer": "fr [Doc 849]", "latency_ms": 4839.743852615356, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer 'fr' is incorrect because the gold answer states the language is English. The model also cites a document id that does not justify changing the language from English to French, so it is not faithful to the provided gold answer.", "main_total_tokens": 3250, "main_prompt_tokens": 3152, "main_completion_tokens": 98, "judge_total_tokens": 468, "judge_prompt_tokens": 389, "judge_completion_tokens": 79}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_094", "question_type": "multi_hop", "question": "What was the TV show in 1999 with the highest popularity?", "gold_answer": "Law & Order: Special Victims Unit", "generated_answer": "Freaks and Geeks [Doc 115187]", "latency_ms": 5074.178218841553, "retrieval_recall": 0.0, "retrieval_mrr": 0.0, "recall_at_5": 0.0, "precision_at_5": 0.0, "ndcg_at_5": 0.0, "hit_rate_at_5": 0.0, "recall_at_10": 0.0, "precision_at_10": 0.0, "ndcg_at_10": 0.0, "hit_rate_at_10": 0.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer is incorrect: the gold answer is 'Law & Order: Special Victims Unit', but the model answered 'Freaks and Geeks'. It is also unfaithful to the gold/reference, since it introduces a different show than the annotated correct one.", "main_total_tokens": 3253, "main_prompt_tokens": 3139, "main_completion_tokens": 114, "judge_total_tokens": 487, "judge_prompt_tokens": 401, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_095", "question_type": "factual", "question": "What is the rating of \"Bluefin\"?", "gold_answer": "7.2", "generated_answer": "7.2 [Doc 79355]", "latency_ms": 2306.73885345459, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.1309297535714578, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.1309297535714578, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer exactly matches the gold rating of 7.2 and correctly attributes it, so it is both correct and faithful.", "main_total_tokens": 2443, "main_prompt_tokens": 2430, "main_completion_tokens": 13, "judge_total_tokens": 449, "judge_prompt_tokens": 392, "judge_completion_tokens": 57}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_096", "question_type": "factual", "question": "What year was the movie \"The Story of Us\" released?", "gold_answer": "2019", "generated_answer": "1999 [Doc 22481]", "latency_ms": 4452.286005020142, "retrieval_recall": 1.0, "retrieval_mrr": 0.5, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.6309297535714575, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.9199945798893454, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The gold label says 2019, but the generated answer is 1999, so it is incorrect with respect to the gold. However, the model consistently used the provided document citation and did not contradict its own sources, so it is fully faithful to the evidence it referenced.", "main_total_tokens": 3099, "main_prompt_tokens": 3011, "main_completion_tokens": 88, "judge_total_tokens": 480, "judge_prompt_tokens": 393, "judge_completion_tokens": 87}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_097", "question_type": "factual", "question": "How many votes did \"My Dog Skip\" receive?", "gold_answer": "279", "generated_answer": "3", "latency_ms": 2849.454641342163, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.0, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.0, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 0, "justification": "The generated answer (3) does not match the gold answer (279), so it is incorrect. It also is not faithful to the provided gold data.", "main_total_tokens": 2418, "main_prompt_tokens": 2376, "main_completion_tokens": 42, "judge_total_tokens": 445, "judge_prompt_tokens": 383, "judge_completion_tokens": 62}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_098", "question_type": "factual", "question": "How many votes did \"Oh My God!\" receive?", "gold_answer": "12", "generated_answer": "9", "latency_ms": 3237.0047569274902, "retrieval_recall": 1.0, "retrieval_mrr": 0.25, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 0.43067655807339306, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 0.43067655807339306, "hit_rate_at_10": 1.0, "correctness_score": 0, "faithfulness_score": 1, "justification": "The generated answer (9) does not match the gold answer (12), so it is factually incorrect. However, it appears to be a direct attempt to answer the question without inventing extra information beyond the requested number of votes, so it is faithful to the prompt.", "main_total_tokens": 2533, "main_prompt_tokens": 2463, "main_completion_tokens": 70, "judge_total_tokens": 469, "judge_prompt_tokens": 383, "judge_completion_tokens": 86}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_099", "question_type": "factual", "question": "What year was the movie \"The Last Days of Lehman Brothers\" released?", "gold_answer": "2009", "generated_answer": "2009 [Doc 46069]", "latency_ms": 4252.395868301392, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 2.0177825608059994, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 2.0177825608059994, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 1, "justification": "The generated answer matches the gold answer (2009) exactly and does not introduce any unsupported information relevant to the question.", "main_total_tokens": 3431, "main_prompt_tokens": 3364, "main_completion_tokens": 67, "judge_total_tokens": 451, "judge_prompt_tokens": 396, "judge_completion_tokens": 55}
{"experiment_name": "SysD_v5_FIXED_jina-embeddings-v3_qwen2.5:32b-instruct-q2_K", "question_id": "Q_100", "question_type": "factual", "question": "What is the original language of the movie \"The King's Speech\"?", "gold_answer": "en", "generated_answer": "The original language of the movie \"The King's Speech\" is en [Doc 48035].", "latency_ms": 4407.950639724731, "retrieval_recall": 1.0, "retrieval_mrr": 1.0, "recall_at_5": 1.0, "precision_at_5": 0.2, "ndcg_at_5": 1.9306765580733931, "hit_rate_at_5": 1.0, "recall_at_10": 1.0, "precision_at_10": 0.1, "ndcg_at_10": 1.9306765580733931, "hit_rate_at_10": 1.0, "correctness_score": 1, "faithfulness_score": 0.8, "justification": "The answer correctly identifies the original language as English (\"en\"), matching the gold label. However, it adds an unsupported reference to a specific document ([Doc 48035]) that is not grounded in the provided context, slightly reducing faithfulness.", "main_total_tokens": 3148, "main_prompt_tokens": 3062, "main_completion_tokens": 86, "judge_total_tokens": 485, "judge_prompt_tokens": 402, "judge_completion_tokens": 83}
